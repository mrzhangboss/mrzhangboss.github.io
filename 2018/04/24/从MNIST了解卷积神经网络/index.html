
 <!DOCTYPE HTML>
<html lang="zh-cn">
<head>
  <meta charset="UTF-8">
  
    <title>从MNIST了解卷积神经网络 | lunge博客</title>
    <meta name="viewport" content="width=device-width, initial-scale=1,user-scalable=no">
    
    <meta name="author" content="张伦">
    

    
    <meta name="description" content="引言
本文是学习Tensorflow官方文档的过程中的一点感悟，本文假设你对矩阵运算有一定的了解，具体可以看看下面资料

加载数据
首先我们得先把数据下载下来，Tensorflow给我们提供了一个函数来进行下载，这个函数为read_data_sets

这个函数read_data_sets函数很简单，查看在目录下面有没有文件没有就去下载，有就解析加载，一方面方便我们获取数据，一方面方便我们直接开箱">
<meta property="og:type" content="article">
<meta property="og:title" content="从MNIST了解卷积神经网络">
<meta property="og:url" content="https://mrzhangboss.github.io/2018/04/24/从MNIST了解卷积神经网络/index.html">
<meta property="og:site_name" content="lunge博客">
<meta property="og:description" content="引言
本文是学习Tensorflow官方文档的过程中的一点感悟，本文假设你对矩阵运算有一定的了解，具体可以看看下面资料

加载数据
首先我们得先把数据下载下来，Tensorflow给我们提供了一个函数来进行下载，这个函数为read_data_sets

这个函数read_data_sets函数很简单，查看在目录下面有没有文件没有就去下载，有就解析加载，一方面方便我们获取数据，一方面方便我们直接开箱">
<meta property="og:image" content="https://mrzhangboss.github.io/images/mnist-nums.png">
<meta property="og:image" content="https://mrzhangboss.github.io/images/mnist-nums.png">
<meta property="og:image" content="https://mrzhangboss.github.io/images/Tensorflow-mult.gif">
<meta property="og:image" content="https://mrzhangboss.github.io/images/tensorflow_pool.png">
<meta property="og:updated_time" content="2018-04-25T00:26:53.323Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="从MNIST了解卷积神经网络">
<meta name="twitter:description" content="引言
本文是学习Tensorflow官方文档的过程中的一点感悟，本文假设你对矩阵运算有一定的了解，具体可以看看下面资料

加载数据
首先我们得先把数据下载下来，Tensorflow给我们提供了一个函数来进行下载，这个函数为read_data_sets

这个函数read_data_sets函数很简单，查看在目录下面有没有文件没有就去下载，有就解析加载，一方面方便我们获取数据，一方面方便我们直接开箱">
<meta name="twitter:image" content="https://mrzhangboss.github.io/images/mnist-nums.png">
<meta name="twitter:creator" content="@mrzhangboss">

    
    <link rel="alternative" href="/atom.xml" title="lunge博客" type="application/atom+xml">
    
    
    <link rel="icon" href="/img/wx2.png">
    
    
    <link rel="apple-touch-icon" href="/img/wx2.png">
    <link rel="apple-touch-icon-precomposed" href="/img/wx2.png">
    
    <link rel="stylesheet" href="/css/style.css">
</head>

  <body>
    <header>
      
<div>
		
			<div id="imglogo">
				<a href="/"><img src="/img/wx2.png" alt="lunge博客" title="lunge博客"/></a>
			</div>
			
			<div id="textlogo">
				<h1 class="site-name"><a href="/" title="lunge博客">lunge博客</a></h1>
				<h2 class="blog-motto"></h2>
			</div>
			<div class="navbar"><a class="navbutton navmobile" href="#" title="Menu">
			</a></div>
			<nav class="animated">
				<ul>
					<ul>
					 
						<li><a href="/">Home</a></li>
					
						<li><a href="/archives">Archives</a></li>
					
						<li><a href="/about">About</a></li>
					
					<li>
 					
					<form class="search" action="/search/index.html" method="get" accept-charset="utf-8">
						<label>Search</label>
						<input type="search" id="search" autocomplete="off" name="q" maxlength="20" placeholder="Search" />
					</form>
					
					</li>
				</ul>
			</nav>			
</div>
    </header>
    <div id="container">
      <div id="main" class="post" itemscope itemprop="blogPost">
  
	<article itemprop="articleBody"> 
		<header class="article-info clearfix">
  <h1 itemprop="name">
    
      <a href="/2018/04/24/从MNIST了解卷积神经网络/" title="从MNIST了解卷积神经网络" itemprop="url">从MNIST了解卷积神经网络</a>
  </h1>
  <p class="article-author">By
       
		<a href="/about" title="张伦" target="_blank" itemprop="author">张伦</a>
		
  <p class="article-time">
    <time datetime="2018-04-23T16:00:00.000Z" itemprop="datePublished"> Published 2018-04-24</time>
    
  </p>
</header>
	<div class="article-content">
		
		<div id="toc" class="toc-article">
			<strong class="toc-title">Contents</strong>
		
			<ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#引言"><span class="toc-number">1.</span> <span class="toc-text">引言</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#加载数据"><span class="toc-number">2.</span> <span class="toc-text">加载数据</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#观察数据"><span class="toc-number">3.</span> <span class="toc-text">观察数据</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#传统做法"><span class="toc-number">4.</span> <span class="toc-text">传统做法</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#从项目了解卷积"><span class="toc-number">5.</span> <span class="toc-text">从项目了解卷积</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#准备数据"><span class="toc-number">5.1.</span> <span class="toc-text">准备数据</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#第一层"><span class="toc-number">5.2.</span> <span class="toc-text">第一层</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#全连接层"><span class="toc-number">5.3.</span> <span class="toc-text">全连接层</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#输出层"><span class="toc-number">5.4.</span> <span class="toc-text">输出层</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#反向传播"><span class="toc-number">5.5.</span> <span class="toc-text">反向传播</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#总结"><span class="toc-number">6.</span> <span class="toc-text">总结</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#引用"><span class="toc-number">7.</span> <span class="toc-text">引用</span></a></li></ol>
		
		</div>
		
		<h2 id="引言"><a href="#引言" class="headerlink" title="引言"></a>引言</h2><blockquote>
<p>本文是学习Tensorflow<a href="http://www.tensorfly.cn/tfdoc/tutorials/mnist_pros.html" target="_blank" rel="external">官方文档</a>的过程中的一点感悟，本文假设你对矩阵运算有一定的了解，具体可以看看下面<a href="http://www.ruanyifeng.com/blog/2015/09/matrix-multiplication.html" target="_blank" rel="external">资料</a></p>
</blockquote>
<h2 id="加载数据"><a href="#加载数据" class="headerlink" title="加载数据"></a>加载数据</h2><blockquote>
<p>首先我们得先把数据下载下来，Tensorflow给我们提供了一个函数来进行下载，这个<a href="https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/learn/python/learn/datasets/mnist.py" target="_blank" rel="external">函数</a>为<a href="https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/learn/python/learn/datasets/mnist.py" target="_blank" rel="external">read_data_sets</a></p>
</blockquote>
<p>这个函数<code>read_data_sets</code>函数很简单，查看在目录下面有没有文件没有就去下载，有就解析加载，一方面方便我们获取数据，一方面方便我们直接开箱即食，但是由于这个默认下载地址是需要翻墙，所以我这里提供一个不需要翻墙的<a href="http://yann.lecun.com/exdb/mnist/" target="_blank" rel="external">地址</a>，你只需要加载下面的函数</p>
<pre><code>from tensorflow.examples.tutorials.mnist import input_data
mnist = input_data.read_data_sets(&quot;input/&quot;, one_hot=True, source_url=&quot;http://yann.lecun.com/exdb/mnist/&quot;)
</code></pre><p>等几分钟，数据就会下载到当前目录的<code>input</code>文件夹中，这样你下次运行就能直接本地文件夹中加载图片数据了</p>
<h2 id="观察数据"><a href="#观察数据" class="headerlink" title="观察数据"></a>观察数据</h2><p>首先我们看看下载了什么数据，打开<code>input</code>文件夹，我们可以看到，Tensorflow给我下载好了四个文件，分为两组，一组训练集一组测试集，每组里面2个文件，一个是手写图片文件，一个标签文件（每张手写的图片代表的数字）</p>
<p>加载图片数据对于新手来说挺麻烦的，为了让我们专注于模型而不是编程，Tensorflow直接帮我们做好了加载数据的事情，我们上面得到的<code>mnist</code>变量里面就存贮了我们这个项目所需要的数据，我们来看看这个<code>mnist</code>有什么</p>
<p>我们最关心的就是<code>mnist</code>里面训练数据，这里推荐使用<code>notebook</code>来操作这个数据集，我们首先<code>mnist</code>的训练数据是什么</p>
<p><img src="/images/mnist-nums.png" alt="mnist数据来源网络"></p>
<p><code>mnist</code>数据就是上面这些图片，我们把图片把每个像素的二值化，然后把他们放到一个数组中，每张图片对应一个数组</p>
<p>mnist训练数据存贮在这两个变量中</p>
<pre><code>mnist.train.labels
mnist.train.images
</code></pre><p>其中<code>mnist.train.images</code>是一个<code>(55000, 784)</code>的二维数组，其中<code>mnist.train.labels.shape</code>是一个<code>(55000, 10)</code>的二维数组，现在摆在我们面前的其实很简单，通过<code>55000</code>个图片像素值来训练我们模型，以便能让模型能给一张图片像素值来预测图片代表的数字</p>
<p>这些数字在人看来非常容易辨认，但是怎么能让电脑也能辨别他呢，这就要用到卷积神经网络的力量，通过卷积神经网络，电脑的准确率能到99%，这就非常恐怖了，我们人有时候也会看走眼呢。</p>
<p>在谈卷积之前我们先谈谈我们以前的做法，这样通过对比就能知道卷积到底做了什么优化</p>
<h2 id="传统做法"><a href="#传统做法" class="headerlink" title="传统做法"></a>传统做法</h2><p>其实从传统的角度来看，其实图像识别也就是通过图片的特征值来判断图片代表的含义，但是图片这个东西又很特殊，相比于其他机器学习问题，他的特征值很多，这里我们使用28X28的图片就有784个特征，如果我们图片尺寸再大，这个特征值会变得非常巨大，而且我们知道机器学习需要大量数据才能大展身手，然而每个图片如此巨大，训练巨大的数据集电脑也吃不消</p>
<p>所以我们必须要将数据进行降维，机器学习里面有很多降维的方法，比如PCA，LDA这些，但是这些方法都有一个问题他们必须把一个图片看做一个整体输入，也就是前面的将28X28转换成一个784的数组，这个数组我们知道，他丧失了一个非常重要的东西维度，我们仔细观察上面的图片</p>
<p><img src="/images/mnist-nums.png" alt="mnist数据来源网络"></p>
<p>每个图片其实我们关注的都是数字的二维分布，我们通过闭合的圆的个数来区分8和0，我们通过中间的空白部分来区分0和1，所以我们希望能使用一种新的方法来确定图片特征，一方面能够保存图片的空间信息，一方面能最终数据一维的结果（图片代表的数字），这个就是卷积的引入了，卷积从二维的角度来提取图片的特征，相比于传统的一维提取，它能最大程度保留图片的信息，并且进行深度降维</p>
<h2 id="从项目了解卷积"><a href="#从项目了解卷积" class="headerlink" title="从项目了解卷积"></a>从项目了解卷积</h2><blockquote>
<p>一开始学习深度学习卷积神经网络，看了很多资料，但是总是感觉并没有很深的理解，至到接触这个项目，从代码的层次上再去理解卷积才给我恍然大悟的感觉</p>
</blockquote>
<p>首先先谈一谈Tensorflow这个库的基础知识，由于Python速度有点慢，所以Tensorflow的后端全部由C++写的，你可以这样理解Tensorflow，Python相当于一个客户端，你可以使用一个session（回话）与服务器（C++）进行交互，这样的话，我们在客户端可以享受Python的方便快捷，也可以享受C++运行的高效性，但是这个也带来一个麻烦，原来Python是一个所见即所得的，现在运行一些东西必须使用session来通知服务器来运行，我们很多中间过程就没法知道，只能通过返回的结果来进行推断了。在官方教程并没有讲太多中间过程，只是一笔带过，所以为了更好的理解卷积神经网络，我们将会以一种很难看的方法运行Tensorflow，但是我们能从这个过程中对卷积的理解更加深刻</p>
<p><strong>所以接下来我们基本上每个操作都会让后端运行并且分析返回结果，为了方便叙述，我们假设你在运行<code>session.run</code>之前都会运行这个<code>session.run(tf.global_variables_initializer())</code>来初始化所以的变量</strong></p>
<p>PS：之所以要运行这个，因为我们使用session与<code>C++</code>进行交互，如果我们“不声明”变量，c++会报错的</p>
<p>下面我们就从这个项目一行一行讲起</p>
<h3 id="准备数据"><a href="#准备数据" class="headerlink" title="准备数据"></a>准备数据</h3><p>前面我们知道，卷积就是要从二维空间中来提取我们想要的特征，首先我们把数据还原成二维的</p>
<pre><code>x_image = tf.reshape(x, [-1,28,28,1])
</code></pre><p>x是上面我们输入的数据，来我们来检测一下，首先我们声明一个<code>session</code></p>
<pre><code>session = tf.Session()
</code></pre><p>再从数据集中掏出50张图</p>
<pre><code>data = mnist.train.next_batch(50)[0]
</code></pre><p>接下来我们看看这个<code>x_image</code>变成了什么</p>
<pre><code>session.run(tf.global_variables_initializer())

x_image_data = session.run(x_image, feed_dict={x: data})
</code></pre><p>我们输入两者的<code>shape</code></p>
<pre><code>data.shape, x_image_data.shape
(50, 784) (50, 28, 28, 1)
</code></pre><p>我们很清楚的看到，我们成功将一维的数组图像（784）变成了二维的数组图像（28X28），其实我们生成了三维（28 X 28 X 1），但是由于我们只有有些图片还会有多个色道（RGB），所以我们为了兼容，声明成28 X 28 X 1</p>
<p>好的，现在我们成功将一维图片还原成二维的，接下来就是将他们卷起来的时候了</p>
<h3 id="第一层"><a href="#第一层" class="headerlink" title="第一层"></a>第一层</h3><p>如果你学过一些信号处理你会发现，深度学习使用的卷积其实并不是原始意义上的卷积，他没有“旋转180”的操作，但是他的形式其实是类似的。这个“积”的操作主要是通过矩阵运算来实现的，为了更好的理解卷这个操作，我从网上找了前辈们辛苦做的动图</p>
<p><img src="/images/Tensorflow-mult.gif" alt="卷积操作-来源网络"></p>
<p>PS： 这个图与我们数据有点不同，我们每张只有一个色道，这个有三个色道，这张图有两个卷积核，但是我们这个第一层会使用32个，但是其实原理都一样，如果你实在理解不过来，你可以先值看最上面那一排</p>
<p>我们回到这种图，最左边就是图像输入，中间是卷积核，最后右边是输出，我们可以从图中可以很清楚的看到卷积的与我们平常操作不同，首先输入上我们是二维数据，通过二维的卷积核进行矩阵运算，最后我们输出二维结果，这就是卷积的强大之处，不但保留了原来的二维信息而且能够使用高效的矩阵运算来加速提取特征</p>
<p>现在我们回到代码</p>
<p>首先是要声明卷积核，我们可以使用简单的方法，将卷积核全部声明为全0矩阵，但是这个有可能造成0梯度，所以我们加入一点噪音，我们看看加入噪音的卷积核是什么值</p>
<pre><code>initial = tf.truncated_normal([5, 5, 1, 32], stddev=0.1)
W_conv1 = tf.Variable(initial)
session.run(tf.global_variables_initializer())

W_conv1_value = session.run(W_conv1)

W_conv1_value.mean(), W_conv1_value.std()
(0.001365028, 0.08744488)
</code></pre><p>我们使用<code>tf.truncated_normal</code>函数声明了32个5X5X1的随机卷积核，看起来随机性还挺不错哦</p>
<p>PS：前面（5，5，1）代表输入长、宽、色道，后面代表输出输出数量当然我说它是32个它不一点为32个矩阵，应该是（色道X输出数量）个卷积核，但是我们这里只有一个色道，所以只有32个，我们可以通过<code>W_conv1_value.shape</code>查看真实的维度（当前的维度为（5， 5， 1， 32））</p>
<p>这个卷积核就对应上面图中间的小矩阵，他的长宽都为5，图中长宽都为3，当然我们可以把这个长宽修改，使用5是我们的经验值，通过这个大小的卷积核能够在模型表现能力更好。</p>
<p>接下来我们就进行最重要的卷积操作了，由上面图可知，要进行卷积必须要有三维的数据与对应的卷积核进行相卷，其实我们在图中还可以看到一个重要的东西，卷积的步长也就是每个框移动的位置（图中的步长为2）</p>
<p>还有一个较隐秘的知识，你有没有注意到图中的数据原来是7X7的数据，通过卷积核转换之后就变成了3X3了，影响卷积后图像尺寸不但有步长还有框子的大小，假如你的框是7，那图中只剩下一个值了，所以我们避免尺寸减少，我们使用周围填充0来使最边缘的位置卷积也成为到框子的中心，一方面避免边缘数据流失，一方面也能突出边缘数据（周边全为0）</p>
<p>Tensorflow为我们封装好了上面所以的方法，我们只要通过传参过去就能改变部长，改变填充方式，好了现在就开始来正式“卷”了</p>
<pre><code>session.run(tf.global_variables_initializer())

v = session.run(tf.nn.conv2d(x_image, W_conv1, strides=[1, 1, 1, 1], padding=&apos;SAME&apos;), feed_dict={x: data})
</code></pre><p>现在我们来看看卷完后<code>v</code>的<code>shape</code></p>
<pre><code>   v.shape
(50, 28, 28, 32)
</code></pre><p>50代表50个数据，（28、28）代表图片维度，这个32就是卷积核数，50和32这两个应该是固定的，不难理解，我们现在来看看为什么通过卷积核的“卷”，图片还是保持28X28的，这个也是在知乎上涉及到的一个<a href="https://www.zhihu.com/question/46889310" target="_blank" rel="external">问题</a>，现在我们从实验上来解决一下</p>
<p>首先我们看<code>tf.nn.conv2d</code>函数，他接受四个参数，第一个图片、第二个卷积核、第三个步长，第四个卷积方式</p>
<p>首先问题是觉得，卷完之后应该是变成24 X 24，这个理解是没错的，我们将pading的值改成VALID再次运行</p>
<pre><code>session.run(tf.global_variables_initializer())

v = session.run(tf.nn.conv2d(x_image, W_conv1, strides=[1, 1, 1, 1], padding=&apos;VALID&apos;), feed_dict={x: data})

v.shape
(50, 24, 24, 32)
</code></pre><p>我们得到了24 X 24的图片，这个SAME和VALID有什么区别呢，这个区别就是填充0没有填充0的原因，SAME在图像周边填0这样就能得到28 X 28</p>
<p>我们也发现，这个还有一个参数strides，这个就是前面填的步长，步长的长宽就是中间两位设置的（最边上两位跟输入有关，第一个是输入图片数量，最后一个是图片的色道），我们在这里使用使用1步长，我们来试试2步长试试</p>
<pre><code>v = session.run(tf.nn.conv2d(x_image, W_conv1, strides=[1, 2, 2, 1], padding=&apos;SAME&apos;), feed_dict={x: data})

v.shape
(50, 14, 14, 32)
</code></pre><p>果然输出的图像变成28的1/2了</p>
<p>接下来我们就要把卷积的值丢到神经元函数里面去了，为了符合实际，我们加入一个偏置量<code>b_conv1</code></p>
<pre><code>def bias_variable(shape):
  initial = tf.constant(0.1, shape=shape)
  return tf.Variable(initial)
b_conv1 = bias_variable([32])
</code></pre><p>这里我们使用0.1来初始化偏置量，接下来就是丢到神经元函数，这里我们使用numpy 的array的传播性，将b_conv1传递给所有的28X28的维度</p>
<pre><code>h_conv1 = tf.nn.relu(tf.nn.conv2d(x_image, W_conv1, strides=[1, 1, 1, 1], padding=&apos;SAME&apos;) + b_conv1)

v = session.run(h_conv1,  feed_dict={x: data})

v.shape
(50, 28, 28, 32)
</code></pre><p>我们可以看到卷积完后从神经元函数生成的数据是（50X28X28X32）的，最后维度由1变成32，所有我们得使用点方法来缩减数据维度，这里我们使用卷积池的方法</p>
<p><img src="/images/tensorflow_pool.png" alt="卷积池"></p>
<p>由上面可以看到，其实很简单就是把最大的挑出来</p>
<pre><code>h_pool1 = tf.nn.max_pool(h_conv1, ksize=[1, 2, 2, 1],
                    strides=[1, 2, 2, 1], padding=&apos;SAME&apos;)
</code></pre><p>这里的参数很简单我就不介绍，这样“瘦身”之后，数据的维度由(50, 28, 28, 32)变成(50, 14, 14, 32)，减少4倍</p>
<p>到这里我们的第一层卷积就结束了，接下来就是第二层卷积，为什么要多卷一次呢，因为前一层学到的还是太少了，要加强学习，这层和第一层没什么差别，所以我们就跳过这层</p>
<p>直接贴代码（函数就不复制了，文档里面有）</p>
<pre><code>W_conv2 = weight_variable([5, 5, 32, 64])
b_conv2 = bias_variable([64])

h_conv2 = tf.nn.relu(conv2d(h_pool1, W_conv2) + b_conv2)
h_pool2 = max_pool_2x2(h_conv2)
</code></pre><h3 id="全连接层"><a href="#全连接层" class="headerlink" title="全连接层"></a>全连接层</h3><p>当我们完成两层卷积之后，我们的数据变成了（50，7,7，64）的四维数组了，我们知道我们传统的机器学习其实最后都是采用二维数组来当做训练数据（X代表特征，Y代表样本），所以全连接层就是把卷积给“反卷”过来，这样后面你方便对接传统机器学习，而且最后我们需要的数据也是输出的也是二维的（对一堆数据统一进行预测，所以这里称二维），但是这里要注意全连接层不是输出层，所以我们可以随意设置输出的维度，最后输出层对接再进行一次全连接层类似操作就能输出我们想输出的维度，这里我们看看全连接层权值变量</p>
<pre><code>W_fc1 = weight_variable([7 * 7 * 64, 1024])
b_fc1 = bias_variable([1024])
</code></pre><p>这里我们声明全连接层的权值变量<code>W_fc1</code>和偏置量<code>b_fc1</code>，我们可以看看<code>W_fc1</code>的<code>shape</code>是多少</p>
<pre><code>session.run(tf.global_variables_initializer())

session.run(W_fc1).shape
(3136, 1024)
</code></pre><p>我们可以看到其实就是一个二维数组维度为（3136,1024），第一个维度跟输入有关，第二个维度影响输出维度，前面我们使用<code>tf.nn.conv2d</code>卷积操作来转换图片，在全连接层我们要使用矩阵运算来转换我们的维度</p>
<p>矩阵运算非常有趣，我们在前面其实也提到过一点，就是降维的实现PCA就是使用矩阵运算来进行降维，我们把数据分为X（特征），Y（数量），经过一次矩阵运算我们可以实现数量不变，而特征改变，这个就非常强大了，我们可以随便修改矩阵参数来动态修改我们特征数量</p>
<p>但是矩阵运算也有一定局限性，就是两个运算的矩阵必须是前者长与后者的宽想同，这个跟矩阵运算特性有关，具体可以看看矩阵运算相关<a href="http://www.ruanyifeng.com/blog/2015/09/matrix-multiplication.html" target="_blank" rel="external">资料</a></p>
<p>所以为了进行矩阵运算我们第一件事就是改变输入的<code>shape</code>，让它由四维变成二维，以便能够与我们权值矩阵<code>W_fc1</code>进行运算</p>
<pre><code>h_pool2_flat = tf.reshape(h_pool2, [-1, 7*7*64])
</code></pre><p>我们简单的使用<code>tf.reshape</code>就能把第二层卷积后的输出变量转换成（50，7<em>7</em>64）的维度，这样我们就能直接与权值矩阵<code>W_fc1</code>进行运算</p>
<pre><code>h_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat, W_fc1) + b_fc1)
</code></pre><p>我们这里直接将运算后的值放到激活函数里面去完成全连接层的功能</p>
<h3 id="输出层"><a href="#输出层" class="headerlink" title="输出层"></a>输出层</h3><p>其实输出层同全连接层很类似，我们就是把前面的变量转换成我们想输出的维度，在进行这个输出层之前，我们得先搞一层<code>Dropout</code>层，这个能有效的避免神经网络的过拟合问题，具体可以看看这篇<a href="http://jmlr.org/papers/volume15/srivastava14a.old/srivastava14a.pdf" target="_blank" rel="external">论文</a></p>
<pre><code>keep_prob = tf.placeholder(&quot;float&quot;)
h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob)
</code></pre><p>因为同全连接层原理类似，输出层我就不就不详细介绍了</p>
<pre><code>W_fc2 = weight_variable([1024, 10])
b_fc2 = bias_variable([10])

y_conv=tf.nn.softmax(tf.matmul(h_fc1_drop, W_fc2) + b_fc2)
</code></pre><p>我们可以看看最后我们输出是什么</p>
<pre><code>session.run(tf.global_variables_initializer())

session.run(y_conv, feed_dict={x:data, keep_prob:0.5}).shape
(50, 10)
</code></pre><p>ok，我们最后得到一个二维数组，50个预测结果（输出采用OneHot方法）</p>
<h3 id="反向传播"><a href="#反向传播" class="headerlink" title="反向传播"></a>反向传播</h3><p>在前面我们得到了在初始话随机权值下得到输出结果，但是这个结果肯定是错误的，我们必须通过修改每层的权值来修正模型，使模型越来越聪明，所以第一步，我们必须“自我反省”，了解自己与真实结果差距多少</p>
<pre><code>y_ = tf.placeholder(&quot;float&quot;, [None, 10])
cross_entropy = -tf.reduce_sum(y_*tf.log(y_conv))
</code></pre><p>我们引入<code>y_</code>作为实际值（我们模型预测值为<code>y</code>），我们这里使用交叉熵来评判预测准确性，但是单单知道“自己错了”没有什么卵用，我们必须要“改正”，这里我们使用<code>AdamOptimizer</code>优化算法来反向传播我们误差，让模型好好“反省改正”</p>
<pre><code>train_step = tf.train.AdamOptimizer(1e-4).minimize(cross_entropy)
</code></pre><p>到这里基本上差不多了，我们已经形成了一个闭环，预测-&gt;评估-&gt;改正-&gt;预测-&gt;……，只有让它不断的训练下去直到我们能接受他的误差我们的模型就训练好了</p>
<pre><code>correct_prediction = tf.equal(tf.argmax(y_conv,1), tf.argmax(y_,1))
accuracy = tf.reduce_mean(tf.cast(correct_prediction, &quot;float&quot;))
session.run(tf.initialize_all_variables())
for i in range(18000):
  batch = mnist.train.next_batch(50)
  if i%100 == 0:
    train_accuracy = accuracy.eval(feed_dict={
        x:batch[0], y_: batch[1], keep_prob: 1.0}, session=session)
    print(&quot;step %d, training accuracy %g&quot;%(i, train_accuracy))
    if abs(train_accuracy - 1) &lt; 0.01:
        break
  train_step.run(feed_dict={x: batch[0], y_: batch[1], keep_prob: 0.5}, session=session)
</code></pre><p>由于我们使用<code>OneHot</code>方法来输出预测变量，所以我们要使用<code>tf.argmax</code>来得到我们想要的真实数字，经过20000轮训练我们正确率可以达到99%，至此卷积神经网络发挥他的威力。</p>
<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>卷积神经网络是深度学习的一个很重要的组成部分，了解卷积必须要知道为什么要用卷积，用了有什么好处。总而言之，卷积并不是一个很新奇的东西，很早在信号处理中就有应用，但是在图像处理上由于他能保留图像维度信息从而在深度学习领域大放异彩，这也可以看做“是金子总会发光吧”</p>
<h2 id="引用"><a href="#引用" class="headerlink" title="引用"></a>引用</h2><p><a href="http://www.tensorfly.cn/tfdoc/tutorials/mnist_pros.html" target="_blank" rel="external">http://www.tensorfly.cn/tfdoc/tutorials/mnist_pros.html</a><br><a href="http://www.ruanyifeng.com/blog/2015/09/matrix-multiplication.html" target="_blank" rel="external">矩阵运算</a><br><a href="https://blog.csdn.net/v_july_v/article/details/51812459" target="_blank" rel="external">通俗理解卷积神经网络</a><br><a href="http://jmlr.org/papers/volume15/srivastava14a.old/srivastava14a.pdf" target="_blank" rel="external">Dropout</a></p>
  
	</div>
		<footer class="article-footer clearfix">
<div class="article-catetags">


  <div class="article-tags">
  
  <span></span> <a href="/tags/DL/">DL</a>
  </div>

</div>



	<div class="article-share" id="share">
	
	  <div data-url="https://mrzhangboss.github.io/2018/04/24/从MNIST了解卷积神经网络/" data-title="从MNIST了解卷积神经网络 | lunge博客" data-tsina="5364356330" class="share clearfix">
	  </div>
	
	</div>


</footer>

   	       
	</article>
	
<nav class="article-nav clearfix">
 
 <div class="prev" >
 <a href="/2018/05/28/从例子里解Spring IOC/" title="从例子里解Spring IOC">
  <strong>上一篇：</strong><br/>
  <span>
  从例子里解Spring IOC</span>
</a>
</div>


<div class="next">
<a href="/2018/04/19/Ubuntu14安装Tensorflow的CPU优化/"  title="Ubuntu14安装Tensorflow的CPU优化">
 <strong>下一篇：</strong><br/> 
 <span>Ubuntu14安装Tensorflow的CPU优化
</span>
</a>
</div>

</nav>

	
<section id="comments" class="comment">
	<div class="ds-thread" data-thread-key="2018/04/24/从MNIST了解卷积神经网络/" data-title="从MNIST了解卷积神经网络" data-url="https://mrzhangboss.github.io/2018/04/24/从MNIST了解卷积神经网络/"></div>
</section>




</div>  
      <div class="openaside"><a class="navbutton" href="#" title="Show Sidebar"></a></div>

  <div id="toc" class="toc-aside">
  <strong class="toc-title">Contents</strong>
 
 <ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#引言"><span class="toc-number">1.</span> <span class="toc-text">引言</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#加载数据"><span class="toc-number">2.</span> <span class="toc-text">加载数据</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#观察数据"><span class="toc-number">3.</span> <span class="toc-text">观察数据</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#传统做法"><span class="toc-number">4.</span> <span class="toc-text">传统做法</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#从项目了解卷积"><span class="toc-number">5.</span> <span class="toc-text">从项目了解卷积</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#准备数据"><span class="toc-number">5.1.</span> <span class="toc-text">准备数据</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#第一层"><span class="toc-number">5.2.</span> <span class="toc-text">第一层</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#全连接层"><span class="toc-number">5.3.</span> <span class="toc-text">全连接层</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#输出层"><span class="toc-number">5.4.</span> <span class="toc-text">输出层</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#反向传播"><span class="toc-number">5.5.</span> <span class="toc-text">反向传播</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#总结"><span class="toc-number">6.</span> <span class="toc-text">总结</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#引用"><span class="toc-number">7.</span> <span class="toc-text">引用</span></a></li></ol>
 
  </div>

<div id="asidepart">
<div class="closeaside"><a class="closebutton" href="#" title="Hide Sidebar"></a></div>
<aside class="clearfix">

  

  
<div class="tagslist">
	<p class="asidetitle">Tags</p>
		<ul class="clearfix">
		
			
				<li><a href="/tags/summary/" title="summary">summary<sup>11</sup></a></li>
			
		
			
				<li><a href="/tags/programming/" title="programming">programming<sup>10</sup></a></li>
			
		
			
				<li><a href="/tags/software/" title="software">software<sup>8</sup></a></li>
			
		
			
				<li><a href="/tags/python/" title="python">python<sup>8</sup></a></li>
			
		
			
				<li><a href="/tags/mit6-828/" title="mit6.828">mit6.828<sup>7</sup></a></li>
			
		
			
				<li><a href="/tags/HTTP/" title="HTTP">HTTP<sup>4</sup></a></li>
			
		
			
				<li><a href="/tags/github/" title="github">github<sup>4</sup></a></li>
			
		
			
				<li><a href="/tags/competition/" title="competition">competition<sup>3</sup></a></li>
			
		
			
				<li><a href="/tags/TDD/" title="TDD">TDD<sup>2</sup></a></li>
			
		
			
				<li><a href="/tags/Java/" title="Java">Java<sup>2</sup></a></li>
			
		
			
				<li><a href="/tags/js/" title="js">js<sup>1</sup></a></li>
			
		
			
				<li><a href="/tags/software-DL/" title="software, DL">software, DL<sup>1</sup></a></li>
			
		
			
				<li><a href="/tags/DL/" title="DL">DL<sup>1</sup></a></li>
			
		
			
				<li><a href="/tags/python3/" title="python3">python3<sup>1</sup></a></li>
			
		
			
				<li><a href="/tags/Python3/" title="Python3">Python3<sup>1</sup></a></li>
			
		
			
				<li><a href="/tags/MySQL-ElasticSearch-Python3/" title="MySQL, ElasticSearch, Python3">MySQL, ElasticSearch, Python3<sup>1</sup></a></li>
			
		
			
				<li><a href="/tags/Python3-C/" title="Python3, C++">Python3, C++<sup>1</sup></a></li>
			
		
			
				<li><a href="/tags/Python3-GIL/" title="Python3, GIL">Python3, GIL<sup>1</sup></a></li>
			
		
			
				<li><a href="/tags/transform/" title="transform">transform<sup>1</sup></a></li>
			
		
		</ul>
</div>


  <div class="linkslist">
  <p class="asidetitle">Links</p>
    <ul>
        
          <li>
            
            	<a href="https://segmentfault.com/ " target="_blank" title="一个面向程序员交流分享的新一代社区">segmentfault</a>
            
          </li>
        
    </ul>
</div>

  


  <div class="rsspart">
	<a href="/atom.xml" target="_blank" title="rss">RSS</a>
</div>

  <div class="weiboshow">
  <p class="asidetitle">Weibo</p>
    <iframe width="100%" height="119" class="share_self"  frameborder="0" scrolling="no" src="http://widget.weibo.com/weiboshow/index.php?language=&width=0&height=119&fansRow=2&ptype=1&speed=0&skin=9&isTitle=1&noborder=1&isWeibo=0&isFans=0&uid=5364356330&verifier=&dpc=1"></iframe>
</div>


</aside>
</div>
    </div>
    <footer><div id="footer" >
	
	<div class="line">
		<span></span>
		<div class="author"></div>
	</div>
	
	
	<section class="info">
		<p> Hello ,I&#39;m zhanglun. <br/>
			This is my blog.</p>
	</section>
	 
	<div class="social-font" class="clearfix">
		
		<a href="http://weibo.com/5364356330" target="_blank" class="icon-weibo" title="微博"></a>
		
		
		<a href="https://github.com/mrzhangboss" target="_blank" class="icon-github" title="github"></a>
		
		
		<a href="http://stackoverflow.com/users/6599843" target="_blank" class="icon-stack-overflow" title="stackoverflow"></a>
		
		
		<a href="https://twitter.com/mrzhangboss" target="_blank" class="icon-twitter" title="twitter"></a>
		
		
		<a href="https://www.facebook.com/100011375031071" target="_blank" class="icon-facebook" title="facebook"></a>
		
		
		
		
		<a href="http://www.zhihu.com/people/zhang-lun-59-53" target="_blank" class="icon-zhihu" title="知乎"></a>
		
		
		
		<a href="mailto:2529450174@qq.com" target="_blank" class="icon-email" title="Email Me"></a>
		
	</div>
			
		

		<p class="copyright">
		Powered by <a href="http://hexo.io" target="_blank" title="hexo">hexo</a> and Theme by <a href="https://github.com/wuchong/jacman" target="_blank" title="Jacman">Jacman</a> © 2018 
		
		<a href="/about" target="_blank" title="张伦">张伦</a>
		
		
		</p>
</div>
</footer>
    <script src="/js/jquery-2.0.3.min.js"></script>
<script src="/js/jquery.imagesloaded.min.js"></script>
<script src="/js/gallery.js"></script>
<script src="/js/jquery.qrcode-0.12.0.min.js"></script>

<script type="text/javascript">
$(document).ready(function(){ 
  $('.navbar').click(function(){
    $('header nav').toggleClass('shownav');
  });
  var myWidth = 0;
  function getSize(){
    if( typeof( window.innerWidth ) == 'number' ) {
      myWidth = window.innerWidth;
    } else if( document.documentElement && document.documentElement.clientWidth) {
      myWidth = document.documentElement.clientWidth;
    };
  };
  var m = $('#main'),
      a = $('#asidepart'),
      c = $('.closeaside'),
      o = $('.openaside');
  c.click(function(){
    a.addClass('fadeOut').css('display', 'none');
    o.css('display', 'block').addClass('fadeIn');
    m.addClass('moveMain');
  });
  o.click(function(){
    o.css('display', 'none').removeClass('beforeFadeIn');
    a.css('display', 'block').removeClass('fadeOut').addClass('fadeIn');      
    m.removeClass('moveMain');
  });
  $(window).scroll(function(){
    o.css("top",Math.max(80,260-$(this).scrollTop()));
  });
  
  $(window).resize(function(){
    getSize(); 
    if (myWidth >= 1024) {
      $('header nav').removeClass('shownav');
    }else{
      m.removeClass('moveMain');
      a.css('display', 'block').removeClass('fadeOut');
      o.css('display', 'none');
      
      $('#toc.toc-aside').css('display', 'none');
        
    }
  });
});
</script>

<script type="text/javascript">
$(document).ready(function(){ 
  var ai = $('.article-content>iframe'),
      ae = $('.article-content>embed'),
      t  = $('#toc'),
      ta = $('#toc.toc-aside'),
      o  = $('.openaside'),
      c  = $('.closeaside');
  if(ai.length>0){
    ai.wrap('<div class="video-container" />');
  };
  if(ae.length>0){
   ae.wrap('<div class="video-container" />');
  };
  c.click(function(){
    ta.css('display', 'block').addClass('fadeIn');
  });
  o.click(function(){
    ta.css('display', 'none');
  });
  $(window).scroll(function(){
    ta.css("top",Math.max(140,320-$(this).scrollTop()));
  });
});
</script>


<script type="text/javascript">
$(document).ready(function(){ 
  var $this = $('.share'),
      url = $this.attr('data-url'),
      encodedUrl = encodeURIComponent(url),
      title = $this.attr('data-title'),
      tsina = $this.attr('data-tsina'),
      description = $this.attr('description');
  var html = [
  '<div class="hoverqrcode clearfix"></div>',
  '<a class="overlay" id="qrcode"></a>',
  '<a href="https://www.facebook.com/sharer.php?u=' + encodedUrl + '" class="article-share-facebook" target="_blank" title="Facebook"></a>',
  '<a href="https://twitter.com/intent/tweet?url=' + encodedUrl + '" class="article-share-twitter" target="_blank" title="Twitter"></a>',
  '<a href="#qrcode" class="article-share-qrcode" title="微信"></a>',
  '<a href="http://widget.renren.com/dialog/share?resourceUrl=' + encodedUrl + '&srcUrl=' + encodedUrl + '&title=' + title +'" class="article-share-renren" target="_blank" title="人人"></a>',
  '<a href="http://service.weibo.com/share/share.php?title='+title+'&url='+encodedUrl +'&ralateUid='+ tsina +'&searchPic=true&style=number' +'" class="article-share-weibo" target="_blank" title="微博"></a>',
  '<span title="Share to"></span>'
  ].join('');
  $this.append(html);

  $('.hoverqrcode').hide();

  var myWidth = 0;
  function updatehoverqrcode(){
    if( typeof( window.innerWidth ) == 'number' ) {
      myWidth = window.innerWidth;
    } else if( document.documentElement && document.documentElement.clientWidth) {
      myWidth = document.documentElement.clientWidth;
    };
    var qrsize = myWidth > 1024 ? 200:100;
    var options = {render: 'image', size: qrsize, fill: '#2ca6cb', text: url, radius: 0.5, quiet: 1};
    var p = $('.article-share-qrcode').position();
    $('.hoverqrcode').empty().css('width', qrsize).css('height', qrsize)
                          .css('left', p.left-qrsize/2+20).css('top', p.top-qrsize-10)
                          .qrcode(options);
  };
  $(window).resize(function(){
    $('.hoverqrcode').hide();
  });
  $('.article-share-qrcode').click(function(){
    updatehoverqrcode();
    $('.hoverqrcode').toggle();
  });
  $('.article-share-qrcode').hover(function(){}, function(){
      $('.hoverqrcode').hide();
  });
});   
</script>



<script type="text/javascript">
  var duoshuoQuery = {short_name:"lunge"};
  (function() {
    var ds = document.createElement('script');
    ds.type = 'text/javascript';ds.async = true;
    ds.src = '//static.duoshuo.com/embed.js';
    ds.charset = 'UTF-8';
    (document.getElementsByTagName('head')[0] 
    || document.getElementsByTagName('body')[0]).appendChild(ds);
  })();
</script> 









<link rel="stylesheet" href="/fancybox/jquery.fancybox.css" media="screen" type="text/css">
<script src="/fancybox/jquery.fancybox.pack.js"></script>
<script type="text/javascript">
$(document).ready(function(){ 
  $('.article-content').each(function(i){
    $(this).find('img').each(function(){
      if ($(this).parent().hasClass('fancybox')) return;
      var alt = this.alt;
      if (alt) $(this).after('<span class="caption">' + alt + '</span>');
      $(this).wrap('<a href="' + this.src + '" title="' + alt + '" class="fancybox"></a>');
    });
    $(this).find('.fancybox').each(function(){
      $(this).attr('rel', 'article' + i);
    });
  });
  if($.fancybox){
    $('.fancybox').fancybox();
  }
}); 
</script>



<!-- Analytics Begin -->

<script type="text/javascript">
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');
ga('create', 'UA-80810286-1,UA-80810286-2', 'auto,auto');  
ga('send', 'pageview');
</script>



<script>
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "//hm.baidu.com/hm.js?e6d1f421bbc9962127a50488f9ed37d1";
  var s = document.getElementsByTagName("script")[0]; 
  s.parentNode.insertBefore(hm, s);
})();
</script>



<!-- Analytics End -->

<!-- Totop Begin -->

	<div id="totop">
	<a title="Back to Top"><img src="/img/scrollup.png"/></a>
	</div>
	<script src="/js/totop.js"></script>

<!-- Totop End -->

<!-- MathJax Begin -->
<!-- mathjax config similar to math.stackexchange -->


<!-- MathJax End -->

<!-- Tiny_search Begin -->

<!-- Tiny_search End -->

  </body>
</html>
