<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>Spring Cloud Alibaba浅探</title>
      <link href="2021/03/16/java/springcloud/springcloudalibaba%E6%B5%85%E6%8E%A2/"/>
      <url>2021/03/16/java/springcloud/springcloudalibaba%E6%B5%85%E6%8E%A2/</url>
      
        <content type="html"><![CDATA[<blockquote><p>花了半天时间把Spring Cloud Alibaba 的<code>Nacos</code> 、 <code>Sentinel</code> 和 <code>Seata</code>简单的使用了一下，下面是我的一些看法</p></blockquote><h2 id="Nacos"><a href="#Nacos" class="headerlink" title="Nacos"></a>Nacos</h2><p>Nacos 干了三件事，第一个就是把服务注册的活给揽过来了，第二个就是把配置中心的活给揽过来了，而且由于他是要监控各个服务的心跳的，<br>所以他顺便把配置同步这个活也给抢过来了</p><p>Nacos做了很多事，美中不足的地方是，Nacos更希望你在网页上进行配置（当然可以通过url），比如说Spring Cloud Config 是希望你更新源代码然后再来将配置给更新 掉</p><h2 id="Sentinel"><a href="#Sentinel" class="headerlink" title="Sentinel"></a>Sentinel</h2><p>Sentinel是一个流量控制的中间件，作用和Hystrix差不多，不过，相比于Hystrix，Sentinel提供了一个web界面来<br>帮助你定制限流规则，主要是保护后端</p><p>这些规则其实也很简单，多尝试一下就好了，我其实很好奇，Sentinel在其中做的工作是什么，他是如何保证异常流量不<br>把自己给打爆的，当然负载均衡可以一定量的缓解这些</p><h2 id="Seata"><a href="#Seata" class="headerlink" title="Seata"></a>Seata</h2><p>这个号称实现了分布式事务的软件，我没搭起来，尝试几个小时无果后，而且网上的资料也异常的少，甚至他们官方写的demo我也跑不起来<br>我看他们的start虽然多，但是issue也多，每个版本都有坑</p><p>我也尝试去搜索有没有大公司尝试使用这个分布式事务，很遗憾没找到，首先限死了用MySQL，而且非常恶心的事每个数据库上面都得创建一个<code>undo</code>表</p><p>我可以理解你在一个数据库上记录这些东西，但是我没法理解你要所有的数据库都建这么个表</p><p>而且非常坑的事，官网SSL证书当天正好到期，在我看了引用了这么个分布式事务，第一肯定涉及到了很多锁，我看原理介绍上<br>也说了开始事务前会上锁，对于现在通用解决方案都是消息队列来处理这个东西的,引入分布式锁并发不高，而且万一锁表凉凉了</p><p>所以我就去找有没有什么大公司实践过了，可是很遗憾好像没有，也没没看到有什么性能测试，而且部署半天报各种问题，每个版本都不一样，怕是个PPT项目<br>还是改天有空的时候在研究研究</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>其实Spring Cloud Alibaba比较重要的一个部分就是将Dubbo集合到了Spring Cloud，所以我接下来的时间得好好看看Dubbo了，目前Spring Cloud这些组件只能说都上手摸了一会<br>要想真正的成长还得去实际项目考验</p>]]></content>
      
      
      <categories>
          
          <category> Java </category>
          
      </categories>
      
      
        <tags>
            
            <tag> SpringCloud SpringCloudAlibaba </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>SpringCloud浅析</title>
      <link href="2021/03/14/java/springcloud/springcloud%E6%B5%85%E6%9E%90/"/>
      <url>2021/03/14/java/springcloud/springcloud%E6%B5%85%E6%9E%90/</url>
      
        <content type="html"><![CDATA[<blockquote><p>最近在学SpringCloud，之前一直对用视频学嗤之以鼻，觉得只有弱者才会这样，但是其实对于一些已经非常常见的技术<br>比如SpringCloud这种，已经出来很长一段时间了，而且其实非常杂，用视频学起来其实非常快，当然前提是你要三倍速播放，而且<br>你得把视频配套的代码找到，这样你就能很快的掌握这个。</p></blockquote><p>我把SpringCloud视频过了一遍，代码也运行了一遍花的时间可能不超过<br>5个小时吧，假如自己按照原来的做法去看别人博客然后自己慢慢搭建可能入门花的时间远远不止</p><p>下面是我学习SpringCloud用到的视频已经代码，推荐你视频三倍速播放，把视频看懂之后去实际运行一下相应代码，这样你能很轻松知道这个东西是什么</p><ul><li><a href="https://github.com/zzyybs/atguigu_spirngcloud2020" target="_blank" rel="noopener">代码</a></li><li><a href="https://www.bilibili.com/video/BV18E411x7eT?p=1" target="_blank" rel="noopener">B站视频</a></li></ul><p>目前我就总结一下我入门Spring Cloud的一些心得</p><h2 id="什么是微服务"><a href="#什么是微服务" class="headerlink" title="什么是微服务"></a>什么是微服务</h2><p>其实微服务这个东西是模块化的一种体现，在企业开发中你会发现，好多时候我们要做一个网站如果要暴露在外网地址，<br>其实最基础就得要有一个登录，每个网站写一个登录其实非常没有必要</p><p>第一个是安全，第二个是耦合，假如你想完全独立于其他系统，你必须重新使用数据库存储，这样用户在同一家公司可能得注册两个不同的账号</p><p>所以微服务的理念就是模块化，把登录和其他业务给组装成不同模块，这样你就专注于你自己的业务而不用去担心其他的问题了</p><h2 id="怎么做"><a href="#怎么做" class="headerlink" title="怎么做"></a>怎么做</h2><p>微服务一开始也很简单，就是单独起不同的网站，比如我们把登录单独部署在一个网站上面去，<br>登录完了就给一个<code>token</code>， 然后其他系统使用<code>token</code>来验证用户权限</p><p>刚开始这样很简单，但是对于系统来说，出现了一个问题，首先是用户登录次数很少（如果他不退出登录），然而其他业务<br>使用的很多，这时候我们就想多起个实例来跑其他微服务，但是我们之前写死了，所有的业务调用都只是跳转到一个<br>网址上去了</p><h2 id="服务注册"><a href="#服务注册" class="headerlink" title="服务注册"></a>服务注册</h2><p>所以我们就想到做一个调度中心来把所有的服务都注册在这里，这样你想要的时候过来查就好了，这个服务中心就是我们所说的<br>服务注册，就是他来监控所有服务实例的，每个实例都得定时发送心跳，如果一段时间没有发送就会默认认为服务下线了</p><p>对于服务注册中心来说，其实就是一个登记的作用，就是记录一个服务名，以及他们的实际IP地址</p><h2 id="服务调用"><a href="#服务调用" class="headerlink" title="服务调用"></a>服务调用</h2><p>服务注册核心在于注册，对于真正的调用方来说，他只能从注册中心拿到最新存活的实例的IP，实际上他还得去通过这个IP来远程调用实际服务</p><p>所以Spring Cloud就创建了两个调用框架，第一个用来选择选择哪个实例，比如随机选择一个实例，还是说按照实例响应速度等等来选择</p><p>这个其实是一个拦截器，他只是帮你在远程调用之前选择哪个服务</p><p>第二个就是<code>OpenFeign</code>，其实这个相当于把远程调用伪装成为一个<code>Service</code>，你只要把这个<code>Service</code>名字以及相关配置给配置好就可以像普通<br><code>Service</code>一样调用了</p><p>服务调用，这个本质上是给调用方写的HTTP请求库，对于被调用者来说其实什么没有影响</p><h2 id="服务降级"><a href="#服务降级" class="headerlink" title="服务降级"></a>服务降级</h2><p>其实服务降级，我认为说成异常处理更好，对于微服务来说，最大的区别就是原来一个网站，现在好多网站，我以前实现一个功能</p><p>一个接口就好了，目前可能需调用很多很多接口，这个调用一多就会出问题，第一个时间变长了，<code>HTTP</code>请求其实很有冷启动的缺点的，随着你的调用越来越多<br>其实花在建立链接的时间也越来越多了，而且<code>HTTP</code>其实非常啰嗦，有很多不需要的头部信息，会重复的发来发去</p><p>回到刚才的问题，就是我们调用方最终要实现一个功能的时候，可能得需要很多步骤，假如正常来写，我们得写很多<code>try catch</code><br>我们得考虑这个接口调用失败怎么办，我们得返回什么东西</p><p><code>Hystrix</code>就是一套异常处理框架，他能帮你把接口给管理起来，在调用前他会检查这个接口是否已经熔断，在调用的时候他会检查是否<br>会抛异常，如果抛异常就调用其他兜底的方法</p><h2 id="辅助"><a href="#辅助" class="headerlink" title="辅助"></a>辅助</h2><blockquote><p>网关</p></blockquote><p>Spring Cloud还给了我们一些其他可以选择使用的辅助功能，比如假如你想自己做一层网关，那可以使用<code>gateway</code>做一个网关，<br>网关能帮我们做一下杂碎但是非常重要的，比如说权限认证，异常流量过滤啊，日志记录啊</p><p>你可以用<code>Nginx</code>来做这些，但是<code>gateway</code>这些网关是用<code>Java</code>写的，你很容易就可以进行扩展实现你自己想要的功能</p><blockquote><p>配置中心</p></blockquote><p>对于一个大的公司来说，代码运行的环境非常多，所以<code>Spring Cloud</code>提出配置中心的概念，你可以让所有的应用接入配置中心<br>这样当你想临时关闭某些东西的时候，只要修改配置中心，所有的实例都能立刻得到响应，而不需要重启</p><blockquote><p>调用链监控</p></blockquote><p>这个有点调试工具了，就是在调用方和被调用方都安上这个就能很轻松的把整个调用的线给记录下来，第一个方便你监控异常，<br>第二就是运维也能根据这个及时添加更多实例来面对突发流量</p><blockquote><p>Spring Cloud Stream</p></blockquote><p>其实我感觉这个东西设计的挺失败的，本来想法是把所有的消息队列框架都能用一个框架去调用，但是目前其实只支持两个，那其实我用原生的也没有什么太大的影响，原生的Spring Boot就支持<code>RabbitMQ</code>等这些所有的消息队列分发和消费</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>微服务架构其实就是将大web系统给模块话，其实本质上每个小的服务还是一个<code>Spring Boot</code>网站</p><p>这个给系统解耦合的同时也带来一个问题，如果去保证一个复杂多模块的安全稳定的运行以及更新，这个也是后面需要去解决的问题</p><p>其实<code>Spring Cloud</code>引进了微服务架构，也引进了一些其他问题，比如说发起服务调用的消耗，以及多系统日志收集等等一系列问题<br>接下来就是对<code>Spring Cloud Alibaba</code>的学习，看看<code>Spring Cloud Alibaba</code>在Spring Cloud的基础上做了什么优化</p><blockquote><p>当然在快速过了一遍实验代码和视频之后我也有一些疑惑，等我把<code>Spring Cloud Alibaba</code>过了之后就得解决下面问题</p></blockquote><h2 id="TODO"><a href="#TODO" class="headerlink" title="TODO"></a>TODO</h2><ol><li>服务注册中心能否添加报警功能，假如服务实例低于某个值</li><li>批量启动所有服务的最佳实践是什么，一般企业是怎么做的</li><li>Spring Boot如何实现只添加一个依赖来插入自己想要的插件</li></ol>]]></content>
      
      
      <categories>
          
          <category> Java </category>
          
      </categories>
      
      
        <tags>
            
            <tag> SpringCloud </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>未来一年规划</title>
      <link href="2021/03/09/summary/%E6%9C%AA%E6%9D%A5%E4%B8%80%E5%B9%B4%E8%A7%84%E5%88%92/"/>
      <url>2021/03/09/summary/%E6%9C%AA%E6%9D%A5%E4%B8%80%E5%B9%B4%E8%A7%84%E5%88%92/</url>
      
        <content type="html"><![CDATA[<blockquote><p>在上一篇博客上我写了我关于未来规划的计划，接下来这篇博客就好好介绍一下怎么做规划</p></blockquote><h2 id="长期目标"><a href="#长期目标" class="headerlink" title="长期目标"></a>长期目标</h2><p>首先一个长期目标很好找到，比如成为某个方向技术专家，首先我想到的是要把我的英语能力提上去，那么很简单就是成为一个英语高手。</p><p>这个长期目标有了，接下来我们要思考我们如何来做到这一点。我们中期目标很简单就是能流利的说，流利的写。因为我前面有经验，要想达到这个就是大量的背诵，大量的默写（当然我后面主要是敲代码为主，可以通过打字的方式默写）。所以我的短期目标就很简单，就是将新概念2、 3 、4给全部背诵并默写出来。这个方法虽然有的傻，但是很扎实，唯一的缺点就是我的读音可能不那么native。但是我的目标主要是阅读与写作能力，开口能力够用就行，其实大部分外国人能接受咖喱味的英语也能接受中国式英语</p><p>我第二个长期目标就是成为某个技术领域专家，上面的英语阅读与写作能力也是为了这个服务的，因为英语比较重要，现在主流软件全部是英语写的。</p><p>其实我这个东西很早就在我的目标当中，但是我太贪婪了，学东西的时候没有把力往一处使，导致学着学着就偏航了。人的精力是有限的，如果不专注，那么很难成为一个方向的专家。</p><p>要想成为一个专家，首先你就得看看专家怎么做的。我的目标很简单就是成为后端领域专家，首先我去一下技术行业里面的专家的博客上去看，看人家博客怎么写的，最下面是我找到的三个技术博客</p><h2 id="如何成为技术专家"><a href="#如何成为技术专家" class="headerlink" title="如何成为技术专家"></a>如何成为技术专家</h2><p>专家博客给我的第一印象是他们的体系非常的健全，而且非常重要的一个地方他们博客文章的最多一定是就是他们擅长的方向</p><p>比如说Java方向或者Spring这些文章一定是写的最多的，而且你去看人家的主方向内的内容，体系非常健全，基本上把这个体系上所有的知识点都囊括了。我在看人家的知识罗列的时候发现，自己和专家的差距就在这一点上，很多知识点我都没有涉及过。当你点击其中详细的内容的时候，你会发现其实人家写的非常简单明了，其实你自己把知识点给掌握之后也能写的出来。</p><p>所以成为一个技术专家也很简单，也是“背”，但是没有人给你背书的目录，你得自己写一本书。而且技术这个东西与时俱进，我发现这些十几年的技术专家还在更新他们主力技能的文章，所以成为一个技术专家必须有两点</p><ul><li>健全的知识架构</li><li>不断学习的能力</li></ul><p>现在说回我吧，其实我以前对博客的态度，很多时候只是为了写而写。我只是为了“炫耀”我懂的一些知识点。我从来没想过要把这些点连成一条线。</p><p>其实自己的博客要把他当做自己的一个浓缩知识树，你必须主动去维护他、更新它。之前我的博客就像一粒一粒种子，我把他们种下就不管了，如果你给他浇水，努力的把你的这些树苗养成一颗颗大树，总有一天你能看到一片茂密的森林</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>所以在写完博客之前，我干了一件事，就是重新把我的博客更新了一遍，首先做了几个改变的地方，第一个就是将hexo样式换成Next，第二个就是开放了评论以及阅读量，当然我也开通了打赏，大家如果觉得我写的东西能对你有帮助的话，那就请我喝杯奶茶。</p><p>当然这次改版我觉得最重要的一个地方就是，我重新把我的博客文章体系重新分了类，而且我自己也开始使用IDE来管理我的文章，我其实推荐大家学会在目录上就把你的文章进行分类，这样你能很快速的找到你想要的文章，也能在目录上对你知识树进行分类，尤其是像我已经写了近百篇博客。</p><p>前路漫漫，诸君加油！！！</p><h2 id="引用"><a href="#引用" class="headerlink" title="引用"></a>引用</h2><p><a href="http://www.ityouknow.com/java.html" target="_blank" rel="noopener">http://www.ityouknow.com/java.html</a></p><p><a href="https://bugstack.cn/" target="_blank" rel="noopener">https://bugstack.cn/</a></p><p><a href="https://www.javaboy.org/" target="_blank" rel="noopener">https://www.javaboy.org/</a></p>]]></content>
      
      
      <categories>
          
          <category> 随想 </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>面试杂思</title>
      <link href="2021/03/07/summary/%E9%9D%A2%E8%AF%95%E6%9D%82%E6%80%9D/"/>
      <url>2021/03/07/summary/%E9%9D%A2%E8%AF%95%E6%9D%82%E6%80%9D/</url>
      
        <content type="html"><![CDATA[<blockquote><p>最近在面试，面试之前总觉得有一丝焦虑</p></blockquote><p>我不知道这丝焦虑是不是学生时代带给我的，对于我来说，这种焦虑是害怕自己不知道这个知识点而很囧。或者对于我来说，我自己一直对自己不够自信。为什么呢，因为对于我来说，我自己一直在干的很杂，相比于其他人来说，我算是一个真“全干”工程师</p><h2 id="原因"><a href="#原因" class="headerlink" title="原因"></a>原因</h2><p>我在思考为什么会这样，我觉得第一点很重要的是我缺乏一个明确的<strong>职业规划</strong></p><p>之所以我会变成“全干”工程师，是因为我一直在“瞎”折腾。为什么会这样，主要是因为好奇心，每个人都有好奇心，区别在于有些人好奇心强非得把这个东西搞懂然后再放手，有些人好奇心弱一会就结束了。相比而来，我是一个好奇心非常重的人</p><p>我觉得这种好奇心可能是在我上小学的时候就养成了。为什么这么说，你能想到一个十二岁左右小屁孩捧着一本右脑记忆的书，把圆周率后面五百位完整的记下来，还能完整的记忆一副扑克吗。</p><p>现在这种右脑记忆培训班在中国现在也挺流行的，但是在十几年前，图书馆里面只有几本这样的书。我觉得这是我第一次开始折腾吧，虽然最终我没能应用在我的学习上（因为我读的理科）。但是这段经历现在回头看还是得给自己竖个大拇指，现在十几岁的小孩只知道玩手机，看电视，玩游戏。当年的我在没人教导的情况下，自己去钻研学习记忆方法，而且直到我上高三，六年时间内我都没有放弃。我高三的时候用它来背古诗，读一遍就背出来，可惜背诵高考只占8分，最终也没能派上啥用场。</p><p>我人生中第二个折腾就是在高一的时候，我不记得我是听老师推荐还是自己在网上搜怎么学好英语，反正那个时候是新概念英语比较火，那个时候不知道是李阳还是谁鼓吹着学好英语就是听和背，大量重复的听和读，等你读到几百遍的时候你就自然而然的背下来了，我那个时候在睡觉的时候都听着新概念的英语文章，就是重复的听一篇文章还是每天重复的读，当你自然而然的背下来的时候就换下一篇文章背。我记得我当初买的新概念英语三，用这个方法背了大概20篇，有点后悔当初没有继续背下去，如果把新概念3和4全本背完，现在英语应该流利到丝滑境界吧。</p><p>当然现在捡起来其实也可以，其实自己毕业后也一直在背单词，但是其实效果也不是很明显。现在看来，自己现在会不满意自己的现状也是有原因的，自己会放弃一些好的习惯。</p><p>其实写之前自己也没意识到自己放弃了什么，因为人无时无刻不在做选择，当初放弃的原因是没有看到效果，相比于虎头虎脑的背，还不如多刷几道题（最终我高考英语考了130分），从分数上看我是挣了的，但是从长远的角度上来看，我是亏了的，我只是拿到了分数，然而那种方法能给我带来一种更长远的回报，好比短期投资和长期投资。长期投资最让人恐慌的是很难在短期内看到效果，必须要有一种信念让你坚持下去。</p><p>所以其实对于我来说，目前我就需要的是一种坚定的信念，首先我得明确自己的规划，然后把这些规划慢慢给他落实掉。</p><h2 id="反思"><a href="#反思" class="headerlink" title="反思"></a>反思</h2><p>其实回头看看，从大学开始到现在七年时间自己一共尝试了五个方向，按照时间排序</p><ul><li>后端</li><li>机器学习</li><li>爬虫</li><li>数据分析</li><li>大数据</li></ul><p>每个方向其实自己都钻研成中级和高级之间的实力，等我走完这一个圈后，我发现我更喜欢做后端，我突然发现我好像走了一个圈，最后又回到了原点。有时候在想，好像自己真的在浪费时间，假如自己一直在自己喜欢的圈子里努力，那么自己的实力目前也不会之停留在中级和高级之间了。</p><p>但是回头一想，其实自己也不去尝试怎么知道自己最后喜欢的是什么，没有谁天生就喜欢某一行，放牛郎的孩子没办法一辈子只能放牛，作为新一代的人，我很幸运的生活在这个互联网时代，我可以选择自己喜欢做的事，其实我也很感激毕业后呆的这家公司，虽然待遇上比不上大部分互联网人。但是在这里我没有压力没有开会，每天都在思考怎么学，怎么用。</p><p>这几年就像交了5个朋友，每个朋友关系都不是很亲密，所以我会感到“孤独”，感到焦虑。</p><p>要打破这个也很简单，选择自己真正想交的朋友做真心朋友，这样能解决你的焦虑和孤独</p><p>其实有时候觉得做技术必须学点心理学和经济学。为什么呢。因为做到最后你会发现，99.99%的问题都能通过时间来解决，我们每天都在做选择，这个选择是有代价的，其中最大的代价就是时间，所以如何把人生价值最大化，最重要的就是规划好你的时间</p><p>从大的层次上来讲，你要给自己做一个长期规划，这个一般是以年来做计划，第二个就是中期规划，这个一般以月来做单位，还有一个就是短期的，一般是以天来。</p><h2 id="术和道"><a href="#术和道" class="headerlink" title="术和道"></a>术和道</h2><p>其实上面讲的做规划很早之前自己就了解过，但是之前没有意识到这个重要性，年轻的时候不珍惜时间，对于任何问题不管是三七二十一搞定再说，所以以前会通宵解决一下很小的bug。虽然成就感很高，但是其实从后面角度上来看，得不偿失。</p><p>我不是说你碰到bug就放弃，而是你要去想自己为什么会碰到通宵才能解决的bug。自己的规划是不是出了问题，是自己哪个地方能力有欠缺。是debug能力，还是编程能力不足，或者没有熟悉某些工具帮助自己快速定位等等。</p><p>其实我自己也在开始尝试实践这种方法，我第一次了解是在知乎上看到一个硬件工程师在分享自己在家办公如何让自己有效的工作12个小时，然后我自己开始用Microsoft的TODO来帮助我自己实践这个。</p><p>下面是我这些天实践的心得：</p><ol><li>规划是非常严肃，你不能抱着一个完成任务的心态去制定它，你要首先把自己想做的事列出来，然后你得给他排序，对于很重要的事是必须要完成的</li><li>规划完成之后会有一种成就感，对于你历史规划，你也可以定时复盘，看看自己哪些地方可以做到更好</li><li>长期规划非常有必要，你可以每天开始规划之前，或者完成了当天规划之后再思考是否长期规划得做适当的调整，比如加快某项，或者你觉得某项还有更多细节需要深追</li></ol><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>我觉得最重要一点就是复盘吧，你把每天自己完成的TODO，以及这个TODO对你长期规划的影响在脑海里面思考计量，这样你才能以最快速度的接近你的目标。</p>]]></content>
      
      
      <categories>
          
          <category> 随想 </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>浅析Spring</title>
      <link href="2021/01/07/java/springboot/%E6%B5%85%E6%9E%90Spring/"/>
      <url>2021/01/07/java/springboot/%E6%B5%85%E6%9E%90Spring/</url>
      
        <content type="html"><![CDATA[<blockquote><p>Spring核心就是IoC（依赖注入）AoP（面向切面）本篇就基于一个开源项目 tiny-spring 来分析Spring框架到底给我们提供了什么东西</p></blockquote><p>依赖注入是什么呢，说起来非常简单就是系统帮你把依赖组装好，我们来举个简单例子</p><pre><code>class People { Hand hand;};</code></pre><p>上面是一个非常简单的类，Spring把这个类叫做Bean，这个People 的 Bean 依赖一个Hand的类，我们来看一下这个Hand类</p><pre><code>class Hand { Finger finger; int num};</code></pre><p>  上面的Hand又依赖一个Finger类，而且提供了一个属性：手的num（数目），假如你要使用这个People类，你得新建3个类，而且得把代码写死，而Spring就是通过xml或者注解来帮你把这些类自动创建，所以才叫依赖注入，其实原理也非常简单</p><p>为啥要提供这个功能呢，因为我们希望通过配置这些xml就能实现不同的xml来实现不同的功能类，而不用修改代码重新编译，你可以理解为就是代码是高度松耦合，我们可以提供很多相同接口的组件给用户用，用户用的时候只需要修改配置文件就能实现灵活组合不同模块的功能</p><p>我们知道对于一个HTTP服务器，我们希望调用这个接口方法之前能对用户发的东西做一个核验，就像拦截器一样，但是Java里面怎么实现呢，就是调用一个方法之前调用一个特定方法，我们有两种方法，第一种就是定死类名，在调用所有方法之前就调用这个东西，第二张就是重新继承这个方法，在调用父方法之前或者之后在调用我们想调用的方法</p><p>显然第一种太蠢了，我们得先判断这个方法不存在，第二种可以使用Java Proxy类来实现（当然有局限只能是接口，可以用动态生成的类比如Cglib来做），我们而且可以把那个想注入的方法也看作一个特殊的bean（默认在普通的bean之前就做好），这样最终我们把普通的bean使用IoC容器给初始化，然后在用代理类把这个bean继承出来</p><p>这样我们就实现了一个高度控制的类方法，而Spring最主要就是要组装一个Servlet类来处理不同路径下面的请求，网络处理那些事情都交给Tomcat来处理了，所以其实Spring的职责也很简单，就是组装，最后根据组装模块来响应Tomcat发过来的request，返回一个response,因为Spring需要生成很多类，而且假如你使用为了支持注解，Spring还得使用反射来获取所有类的注解，反射速度也是非常慢的，所有Spring启动一般需要一点时间，甚至比其他框架比如Python还要慢，不过这些类的生成都是一次性的（不是单例除外），生成完之后就可以直接使用这些生成好的类的方法来响应Tomcat请求了</p><h2 id="资料"><a href="#资料" class="headerlink" title="资料"></a>资料</h2><p><a href="https://github.com/code4craft/tiny-spring/blob/master/changelog.md" target="_blank" rel="noopener">https://github.com/code4craft/tiny-spring/blob/master/changelog.md</a></p>]]></content>
      
      
      <categories>
          
          <category> Java </category>
          
      </categories>
      
      
        <tags>
            
            <tag> SpringBoot </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>技术如何深入</title>
      <link href="2021/01/07/summary/%E6%8A%80%E6%9C%AF%E5%A6%82%E4%BD%95%E6%B7%B1%E5%85%A5/"/>
      <url>2021/01/07/summary/%E6%8A%80%E6%9C%AF%E5%A6%82%E4%BD%95%E6%B7%B1%E5%85%A5/</url>
      
        <content type="html"><![CDATA[<blockquote><p>为什么会有这篇博客呢，因为随着你阅历的增长，以及各种你即将面临的问题，比如婚姻、跳槽这种，你会有一种焦虑感，你一直在原地踏步，你怎么面对你的未来呢</p></blockquote><p>所以这篇博客其实很大程度是一种反思</p><p>首先要明确一点就是你是一个什么样的人，自我反思，其实我算一个矛盾的人，做事情急着做完，但是要实现一些我自己的想法的时候又很拖延</p><p>所以其实阻止我继续前进的一个东西就是拖延，其实毫不自夸的来讲我是一个解决问题能力很强的人，但是我不是一个行动力很强的人。</p><p>有的时候我在想，我就像一只牛，假如给我一个红旗，我会卯足了劲往那个地方去，吃饭想，睡觉想，无时无刻不在思考这个。但假如不给我目标，我就像一只猫，只想安静的躺着</p><p>就是这种性格，导致我现在什么都懂，但是什么都不精</p><p>所以技术深入也非常简单，就是找到几个目标，一直钻，但是钻完了你还得思考，下一步往哪里钻，钻这个收获了什么，必须抛弃以前那种，完成了就结束了</p><p>说完了怎么深入，现在来谈谈最主要的这么找目标。我也很迷茫，查找了很多资料，看看前人怎么做的，但是他们只是给了很多知识给你，让你去了解，当然在新手阶段，这个挺实用，但是对于那些懂了的来说，这些知识点都知道，只是不精</p><p>怎么才能精呢？这个问题我想了很久，一直没有答案。后来我回顾我技术生涯，我有了一个比较简单的解决方法</p><p>其实这个好多前辈也说过，就是想深一点</p><p>其实技术这个东西基本功能大家都会，hello world，谁都会写。</p><p>用做网站来打比方，你做一个网站，其实新手也可以做，你设计的时候可能就想着照顾好你自己这个用户就好了，各种框架一把嗦。但是假如你想深一点，假设你的1万个用户用你的网站怎么办，或者假如我们要新增一些东西怎么办</p><p>就带着这些想法，去大胆的写你的代码，去验证你的想法，在实现你的目的去看看大家怎么解决的，然后继续学习你能不能用他们的想法做</p><p>所以概括起来也很简单：</p><ul><li>要“多虑”，想事情想深一点</li><li>要“大胆”，想到就大胆的去做</li><li>要“虚心”，做完不能沾沾自喜，去看看别人怎么做的</li><li>要“总结”，要把你做的这方面形成一个系统</li></ul><p>其实仔细回头看，自己其实也有大部分都有的拖延症，其实很早就知道该怎么做，但是总是拖着，被历史的惯性拖着</p><p>这里分享一个 中国比特币第一人 李笑来的一个技巧，就是他以前背单词也是，知道背单词对自己有好处，但是总是拖延，后面他就给自己暗示，背一个单词10块钱，就是这种激励让他打败拖延症，成为新东方的一名非常强的英语老师。</p><p>⛽️</p>]]></content>
      
      
      <categories>
          
          <category> 随想 </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>浅析微服务</title>
      <link href="2021/01/07/java/%E6%B5%85%E6%9E%90%E5%BE%AE%E6%9C%8D%E5%8A%A1/"/>
      <url>2021/01/07/java/%E6%B5%85%E6%9E%90%E5%BE%AE%E6%9C%8D%E5%8A%A1/</url>
      
        <content type="html"><![CDATA[<blockquote><p>这篇博客主要是从web技术发展来探索微服务的起源</p></blockquote><p>要想了解微服务是什么得从web框架出来之后开始讲起，大部分可能不知道微服务，一定知道写web服务的框架，懂Java的可以用Spring Boot一把梭，懂Python的Flask、Django、Tornado也写的飞起</p><p>微服务也就是在这些框架出来后才慢慢出现的，首先我们就不谈MVC模式的兴起（主要就是把前端和后端解耦合），我们谈谈在MVC后要面临的问题</p><p>第一个就是接口越来越多，用户也越来越多，面临的第一个问题就是某些接口请求非常频繁，面临第二个问题就是某些接口我想升级怎么办，我们知道使用上面那些框架你要是想修改接口得把代码重新发布一下，然而线上就得暂停服务一会等服务升级</p><p>针对上面的问题，就开启了微服务的热潮，解决方法和MVC一样，我们把后端和后端进行解耦合，一个后端系统专门负责认证，一个后端系统专门负责订单，两者之间使用token来确认同一个用户，登录的请求接口不是很频繁，就用2台机子，订单非常频繁双十一的时候还会爆炸，就用10台机子，实现上非常简单，就是把在每台机子上面启动单独后端程序，再使用nginx作为负载均衡这样流量就均匀的打到每台机子上面去了，这样不管你是用Java还是Python都OK</p><p>这种解决方案能扛起很高的并发，但是还是有个问题，升级起来非常麻烦，要是想升级系统，得把每台机子的后端程序单独升级，管理起来非常麻烦</p><p>然后大家开始研究，大家把接口抽象起来，其实作为用户来说，他只是想调这个接口，能不能使用一个中间层把用户想调的接口和后端真正的提供的服务连接起来，这个就是中间件兴起的源头</p><p>让我们想想中间件的职责是什么，其实想想也很简单就是一个后端接口管理系统，能够实现注册接口的功能，并且打通接口和后端的接口，当然说起来简单，但是要实现一个高可用的中间件也是不容易的，具体可以去看看Spring Cloud、Dubbo这些中间件框架</p><p>我们想想这些中间件框架优缺点，优点也很明确，就是性能非常强，每个小模块只负责自己的接口，当业务量大起来的时候能迅速启动上万个进程来分流大流量，但是缺点也有就是只限定了自己的编程语言（Spring Cloud和Dobbo这些只能用Java写），而且随着接口越来越多，管理也越来越麻烦，调用链也越来越长了，有的时候你想加一个接口得涉及很多道调用链，先去请求用户信息，再去查询用户银行卡信息，再去查询商品信息，虽然分模块很爽，但是有些东西越分越累，模块越来越多，实现功能也越来越散</p><p>模块越来越多带来的一个副作用是测试调试越来越难了，而且随着摩尔定律，我们现在的内存越来越大了、CPU也越来越多了，以前写微服务为的就是省点内存，现在大家发现其实他也没有那么省内存，为了保证服务高可用，我们得用上百台机器来搭一个集群，但是大部分时间，业务量没有那么大，其实大部分时间那么大内存都放在那里浪费了，而且当业务量万一突然起来了，然而固定集群却没法承受这么大流量</p><p>后面随着Docker、K8S的兴起，大家慢慢的发现容器化能够解决上面的问题，当业务量小的时候启动少量容器，业务量大的时候启动多一点容器，而且可以实现动态扩容，所以这时候提出了一个Service Mesh模式</p><p>什么是Service Mesh，其实就是我们前面提到的nginx的负载均衡方案类似，就是使用一个nginx来把请求打在后端上面，但是不同点在于，原来nginx是写死在配置里面的，现在k8s里面的nginx（目前主流的方案是Istio）把接过来的请求打到容器网络上面去，而且当流量大的时候会自动新建新的实例来负载新的请求</p><p>兜兜转转我们又回到了最初的样子，你也不用学习什么后端中间件使用，你只要会写一个提供web接口的服务，打包成个容器，在k8s配置一下，轻轻松松就能扛着上千万并发，扛不住加实例就够了</p><p>现在我们来谈谈Service Mesh的原理，其实很简单，就是蚁群的思想，每一个容器都是一只小蚂蚁，功能齐全啥都能干，一只蚂蚁搬不动一座大山，但是上亿只蚂蚁可以搬的起，Service Mesh的思想有点像大数据分而治之的理论，当你的容器能服务1千人，那你1万个容器就能服务1亿个人，所以对于开发者来说，你只需要写好一个能服务1千个人的小web，你不需要去考虑当你要服务1万个人的时候，代码需要怎么改动，对于开发者来说，测试和开发都变得异常轻松，自己也不用去写原来的Dubbo那种大体量代码，你所有的接口都在一个项目中，测试开发都变得异常轻松，而且你也不用思考怎么使用Java调用Python的机器学习框架等等，你可以用Python写，用Go写，甚至可以用C来写，容器不在拘束于语言，你只要能对外提供服务就好了</p><p>总之在K8S容器化兴起之后，我们也不用再怎么思考如何解耦合代码，套上各种中间件把我们各个接口都打散，实现一套高并发系统，我们只需要写一个网站，能服务上千的用户，当我们业务量大起来的时候交给K8S，让它帮我们启动上万个相同网站，把流量给消化掉，所以在一定的程度上，在现在技术发展下，高并发的门槛越来越低，原来一个高并发专家得学会好多框架，dubbo各种组件，现在随便叫来一个小白随便把它从网上复制的代码拷拷，搭起来一个小网站，放到K8S上面，马上就可以服务上千万甚至上亿用户</p><p>所以说技术发展真的是快啊，记得以前一个架构师的道路是慢慢从学习各种nginx负载均衡Redis集群搭建，搭建起一套复杂的系统，你得去学习各种组件使用，现在呢，一个小白，网上拷拷代码，使用上内存作为缓存，使用MySQL当数据库，并发也就上个千，然后数据库配置一下把内存换成redis，MySQL换成Kudu，放进K8S里面并发就能上千万，高并发问题就解决了，原来一个百万年薪才能做的高并发技术专家在新的技术降纬打击下一个月薪1万的菜鸟也能搞定了</p><p>总而言之，高并发随着技术发展会越来越简单，微服务的提出有人说就是干掉架构师，所以有时候也会想，随着技术的发展，是不是不会有工程师这个职位了，的确技术在不断发展，但是只要抱着一个不断学习的心也就不怕了，技术不断推成出新，但是本质是不变的，所以也不用害怕。</p><h2 id="资料"><a href="#资料" class="headerlink" title="资料"></a>资料</h2><p><a href="https://philcalcado.com/2017/08/03/pattern_service_mesh.html" target="_blank" rel="noopener">https://philcalcado.com/2017/08/03/pattern_service_mesh.html</a></p>]]></content>
      
      
      <categories>
          
          <category> Java </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>Tomcat高并发浅析</title>
      <link href="2020/12/31/software/http/Tomcat%E9%AB%98%E5%B9%B6%E5%8F%91%E6%B5%85%E6%9E%90/"/>
      <url>2020/12/31/software/http/Tomcat%E9%AB%98%E5%B9%B6%E5%8F%91%E6%B5%85%E6%9E%90/</url>
      
        <content type="html"><![CDATA[<blockquote><p>Tomcat作为Java老牌web服务器，所以研究Tomcat就能搞懂高并发，本文是查阅大量资料总结的自己对于web高并发服务器的理解，有可能部分理解是错误的，欢迎来纠错，本文尽量不使用代码，网上查到的都是一大段一大段代码，让人看的头痛</p></blockquote><p>一个最简单的web服务器长啥样呢，就是一个echo服务期</p><ul><li>监听端口</li><li>新TCP请求进来，建立TCP链接</li><li>向TCP链接发送响应</li></ul><p>所以我们扩展一下，一台高并发服务器最重要的就是接受成千上万的新请求，以及管理已经建立的请求，虽然说起来很简单，虽然说起来很简单，但是做起来很复杂</p><p>首先我们从用户端看一次完整的HTTP请求是咋样的，我们用吃饭来打比方，首先用户先想好他去哪里吃饭（构建url，比如abc.com)，然后他通过百度去查这个饭店在哪（查询DNS abc.com 的IP地址是什么），百度告诉他这家餐馆搬到他家附件2公里外（得到IP地址 127.0.0.1），然后用户就按照地图找到了这家店，然后他想了半天决定吃鱼（发起HTTP请求<code>/fish</code>）</p><p>现在用户已经决定好了他要干什么了，接下来就是看这家餐馆的操作了，首先用户会发起一个TCP请求建立连接，因为他得确定这家店今天开不开张，不开张人家也不会卖他鱼吃，接下来我们来看看Tomcat是怎么把这家餐馆给经营好的</p><p>首先他要根据他自己后厨的规模，也就是同时可以做多少个菜（并发线程数），以及做一个菜要花多少时间（TPS性能），来决定他餐馆可以放多少椅子（最大连接数），因为这个餐馆一直有人吃饱就走，所以他也特意搞了个叫号系统，允许顾客来了排个队等一等（也就是Tomcat的排队队列来控制）</p><p>那我们来分几种情况来讨论一下，第一种今天没多少顾客，那么大家来了之后就有椅子做（建立好TCP链接），第二种做满了但是门口还有地方站一站（也建立链接TCP放进排队队列），最后一种是门口都站不下了，为了不影响其他顾客，我们只能说客满了，您不一定能吃上，您去别处看看吧，那么用户就会得到一个拒绝连接的错误（Connection refused）</p><p>接下来我们再看看坐在店里的顾客了（建立好了TCP连接），这个时候顾客都知道今天能吃上就安安静静的等了，餐馆呢就把顾客点好菜发给后厨（Tomcat把HTTP请求发给Servlet容器），有的菜烧得快有的菜烧得慢（每个请求耗时不同），菜好了就端上去了，客户呢觉得吃饱了那就走了（关闭TCP连接），觉得没吃饱就继续叫（继续发送HTTP请求），假如有时候叫了一道特别难的菜，后厨做了很久都没端上来，假如你不急那就慢慢等，假如你急的化那你可以直接走（抛出一个Read timed out错误）</p><p>现在我们总结一下，从用户角度上来看，他碰到的问题就是两种，第一种就是餐馆人太多了，没法吃（拒绝连接），第二种就是餐馆上菜太慢（读取超时）。从餐馆角度上来看，为了服务好用户他首先得安排好足够的椅子，第二个就是要让后台上菜速度要快，上菜快可以通过增加厨师（增大最大线程数），你也可以针对做菜某些步骤慢的地方多派人手（SQL调优、缓存优化等等）</p><p>PS: 这篇文章还有一个重要的问题没有提，用户进了餐馆（建立了TCP连接），他什么时候叫菜（发送HTTP请求）我们是不知道，我们怎么来实现一个高效的系统来服务客户，来满足成千上万的顾客有求必应，我会在另外一篇博客来专门谈这个东西【Select和epoll浅析】</p><h3 id="资料"><a href="#资料" class="headerlink" title="资料"></a>资料</h3><p><a href="https://qiankunli.github.io/2019/11/26/tomcat_source.html" target="_blank" rel="noopener">https://qiankunli.github.io/2019/11/26/tomcat_source.html</a></p>]]></content>
      
      
      <categories>
          
          <category> 软件 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> HTTP </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Select和epoll浅析</title>
      <link href="2020/12/30/software/http/Select%E5%92%8Cepoll%E6%B5%85%E6%9E%90/"/>
      <url>2020/12/30/software/http/Select%E5%92%8Cepoll%E6%B5%85%E6%9E%90/</url>
      
        <content type="html"><![CDATA[<blockquote><p>这篇文章是【Tomcat高并发浅析】的姊妹篇，专门通过分析Select和epoll两个框架来介绍如何实现高并发</p></blockquote><h1 id="起因"><a href="#起因" class="headerlink" title="起因"></a>起因</h1><p>我们知道建立TCP连接之后，就是创建了Socket链接，服务器得监控用户所有的Socket，以防用户发送请求过来</p><p>首先最简单就是用个for循环，遍历每个Socket链接看看有没有数据，如果没有就进程休眠一会再来，这个就非常低效，得一直检查一直检查，所以操作系统就开发了一个接口，因为网卡数据是他控制的，首先他让进程睡觉，只要操作系统接受到了网卡数据，根据网卡数据请求的TCP上面的端口找到对应的进程，然后唤醒它，进程就开始遍历每个Socket链接，里面其中一个一定会有数据，因为操作系统已经明确告诉它了</p><p>这个就是select框架，这个实现起来特别简单，不需要加很多东西就能实现，但是有个缺点就是，进程醒来之后他不知道哪个socket链接有数据，所以他得遍历所有的，假如有成千上万的Socket链接，那遍历一遍得好久，所以操作系统就限制了最多你可以监控1024个Socket接口，所有假如你使用select作为监控，你只可以同时监听1024个socket，意味着你同时只能和1024个用户建立连接</p><p>这样看起来真的太低效了，当然也不是毫无优点，假如你用户少，而且都是活跃用户，这样操作系统叫醒一次你就可以同时处理很多用户了，但是假如你用户很多，那遍历耗费时间也太长了</p><p>所以在Linux内核2.4提供了epoll框架，他终于开放自己的事件驱动核心，专门给他写了个函数，首先他用红黑树存贮了所有Socket连接以及他的回调函数，这样当网卡数据来了之后，他直接去回调那个函数了，这样进程不需要遍历了，直接执行函数，当然也有一定的代价，就是存贮这些Socket连接和回调地址，但是相对于的解除了1024的限制，第二直接找到有数据的Socket，读取数据一步到位，具体怎么实现我就不详细讲了，有兴趣的可以去查阅相关资料</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>服务器之所以很轻松的实现上万Socket监控靠的是操作系统的帮助，我们可以理解各种并发都是事件，操作系统就是事件并发高手，所以借助操作系统我们能够很轻松的实现管理看起来非常复杂的事情</p>]]></content>
      
      
      <categories>
          
          <category> 软件 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> HTTP </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>技术虾虾看</title>
      <link href="2020/12/24/summary/%E6%8A%80%E6%9C%AF%E8%99%BE%E8%99%BE%E7%9C%8B/"/>
      <url>2020/12/24/summary/%E6%8A%80%E6%9C%AF%E8%99%BE%E8%99%BE%E7%9C%8B/</url>
      
        <content type="html"><![CDATA[<p>仔细回顾我这7年的技术生涯，从高三的时候开始就自学C语言，那时候不知道怎么回事，高考发挥的不是很好，总想着既然高考失败了，那就在大学赢回来，相比于其他大学生来说，我上大学不是天天游戏恋爱，反而是每天就研究IT技术</p><p>当时我记得自己刚从高考完，不知道怎么才能在大学赢回来，就去图书馆看看有没有什么书可以看看提前学，因为当时对互联网仅限于看个电影玩个游戏啥的，也是”猿”分，当时就找到一本C语言，看起来好像大学要学这个，反正咋当时也是啥也不懂</p><p>拿到书就开始看起来，在那本书上我慢慢了解啥是程序，基本上把那本书都看完了，把上面的程序在电脑上打起来了，当时很好奇，书上说C语言很牛逼，但是我学完好像就只会打个hello world，算个算术，好像也没啥了不起。</p><p>当时特别想在大学赢回来，就去搜怎么把大学过好，当时很多网站都说要加入个社团，不要整天打游戏，我就去搜南昌大学有啥社团，我挑了一会，什么学生会啥的，我这个人从小就害羞，可能也不想去，然后后面搜到南昌大学家园网，然后我想这个是搞电脑技术的，我也喜欢电脑，要不加进去看看，然后就填了申请单，后面也就耐心等待我的大学生活了</p><p>军训期间就开始我们称作“百团大战”的东西，就是各个社团抢新生，相比别的部门门可罗雀，家园网招聘算是万一挑一了，我记得当初笔试好像有四五百人，后面每轮笔试面试，最后才录取10几个人，我也算运气比较好，而且当初学了C语言，还有家园网有个大四学长和我同名，我进了社团后大家就叫他大张伦了哈哈</p><p>家园网算是我大学第一个平台吧，里面的学长学姐都挺好的，他们不像高中的老师一样每天给你上课，更像是“老板”，给你一个任务，然后你自己去学，自己去完成，这个完全不和高中一样，高中就是老师教，出卷子，考试，最后再告诉你错在哪了，在社团没有老师指出你的错误，也没有老师教你怎么做，你做的东西也没有对错可言，只有功能能用就OK，当然要符合一些代码规范啥的</p><p>记得当初学的算是又开心又辛苦，开心是好像自己的东西做的东西还不错，辛苦是有时候碰到一个bug，不知道要尝试多少次，查多少资料也找不到，在社团真正的教会了我“自学”这个词，慢慢的自己懂了什么是HTTP，什么是session，什么是cookie，也知道前端后端各种划分，随着做的项目越来越多，慢慢就迷茫了，未来自己做什么，后端好像就是CRUD，难道要CRUD一辈子吗</p><p>所以我在做了两年后端的时候开始准备去接触别的，经过了两年的“社团”鞭打，我已经具备了强大的自学能力了，大学还是舒服，图书馆的书又多，借书而且还不花钱，我就开始漫漫“搬书”之路，当时真的是有种“无敌是多么寂寞”的感觉，一两天一本书，开始Android、数据挖掘、机器学习、爬虫各个方向猛学，但是学完之后又迷茫了，好像都不难，到底选哪个了</p><p>当时各个东西都学习一点，最后去网上搜大牛的成长经历，他们都告诉我，要先有广度，再有深度，也就是要朝着一个方向一直努力，直到你能够成为这个领域的大牛</p><p>记得第一个选择的方向是计算机底层，也就是大牛们说的屠龙之技，深入操作系统底层去学习计算机程序背后的秘密，当时把一个mit的课程给上完了，也对操作系统有一定的理解了。但是学完后更迷茫了，学会了屠龙记，没有龙怎么办</p><p>所以又开始第二次转向，数据挖掘，这个坚持了1年，打了不少比赛，没取得什么好成绩（最好就是一个第4名），当时想了好久自己真的喜欢这个吗，相比于之前写后端的一刀斩，数据挖掘就是对数据翻炒，好像我更喜欢写代码，而不是写sql，不知道是读了很多编程大牛的书之后，我喜欢上把自己代码不断精简优化，然而数据挖掘就是在一团糟的东西里面理出头绪，写着写着我又把数据挖掘变成代码挖掘，我更喜欢把用代码生成代码，感觉是在歪路上越走越远</p><p>所以后面我就处于半放弃阶段，我想写代码，但是我又不想放弃数据挖掘，因为很多文章都告诉我这个一个非常有“钱”途的职业，所以我大三开始就搞爬虫去了，因为我听他们说搞爬虫又可以搞数据又可以搞技术</p><p>写爬虫还是挺开心的，但是写多了之后又开始迷茫，爬虫的意义是什么，本来别人的网站的东西让你给免费获取掉了，这算啥事，但是后面想想人家谷歌也是爬虫起家，职业并没有贵贱之分，但是写多了爬虫又开始迷茫了，数据我有了，但是这个数据怎么来用呢，数据最后你想要干什么，目前似乎爬虫在国内似乎大部分都“不务正业”，搜索引擎暂且不提，目前做的比较大的是抢票、抢课、薅羊毛等，大部分都是有种走歪路，人家网站本来想给你一个页面来公平竞争，但是你依靠电脑“作弊”，有点像用技术来作恶</p><p>当然也有些好的爬虫，给你解决一些本来人工做很费事的事情，比如说那些摸鱼网（整合所有的网站的热点）那些，但是基本上所有爬虫都是处于一种偷偷摸摸的状态，你有爬虫就有反爬虫，人家网站本来就不想让你爬取数据，从法律角度上讲，爬虫是“贼”，拿别人的家数据，当然目前大部分爬虫除了爬取个人隐私（杭州的魔蝎科技）的还没被国家整治，目前大家对爬虫的定义还是很模糊的，目前国家也不太愿整治这些爬虫，因为大部分爬虫基本上都不是“害虫”（窃取人隐私去卖的除外），用机器抢个票到底是坏的吗，从道德层次上来看说是坏的，你破坏了公正，所以我们只能用道德层次上面来谴责</p><p>其实大部分人对爬虫的概念是和反爬虫联系起来的，认为一个是“贼”，一个是“警察”，而且大部分只有小公司才会有爬虫，大公司数据都多的处理不过来哪有时间来做这个，所以其实爬虫是小公司弯道超车的一个工具，就像抢票一样，抢大公司的资源，当然除了谷歌这种搜索引擎公司成功的把被人的数据做到被所有人都多</p><p>一不小心又扯到别的地方去了，总而言之，爬虫是比较容易入手的，但是难度大在于你怎么实现一个稳定分布式强大的系统，但是让我头疼的是随着越来越多企业在搭建自己的护城河，对自己数据越来越重视，反爬虫的措施越来越严，虽然爬虫的手段也在升级，但是随着升级，付出的成本也越来越大了。总而言之，爬虫假如只是为了获取某个公司的数据，那为何不如加入某个公司，直接拿到数据来玩，爬虫和反爬就是无意义的浪费</p><p>所以我后面对爬虫的技能点越来越不想点了，我开始有意识的去了解怎么去用数据，所以我虽然一开始进公司是做爬虫相关的，但是基本过了几个月后我就一直在做数据方面的探索</p><p>我第一个探索是大数据方面，其实这个在毕业前就往这个方向发展了（在家园网老司机的提点下），因为当时做了很长一段时间爬虫和数据挖掘了，但是数据挖掘做到最后发现还是要么直接上刺刀（算法方向），要么拼运气（数据分析），这两个方向我都学了学，算法方向，虽然把机器学习算法原理搞通特别舒服，但是最后我不知道怎么去更进一步，我更喜欢工程方向，数据分析就是拼的是细心，比如写SQL，然而我更喜欢写代码</p><p>大数据方向我算是摸石头过河，因为我们公司过小，根本用不上大数据，基本上花了小半年把大数据各个组件都把玩了一遍，然后就迷茫了，接下来怎么做呢，有人说深读源码，我就挑了Flink看，也自己提了几个pull request，但是我又迷茫了，因为我基本上碰到的都是一些小问题，比如说断电这些，我没有那个经验去面对真正大企业的问题</p><p>正当我满世界去找真正的大项目练手之时，正好旁边室友问我搞不搞大数据，我就说好啊，结果我就接了一个私活，这是我真正的面对大数据，这是我第一次真正的面对千亿的大数据处理，这个项目断断续续写了半年，写代码时间不长，大部分是远程调试，通过这个项目我自己也真正的了解了啥是内存不足，以前处理小数据你根本不会遇到这个问题，当你的数据超过几百亿，你单机内存根本处理不了那么多数据，你只能将数据分片均匀的分到每台机子上面去</p><p>所以大数据核心就是“分而治之”，你还是用处理小数据的方法就对付它就好了，只要把数据分成小片，处理小数据的主流方法就是写SQL，所以做大数据做到最后面就是写SQL</p><p>所以我意识到最终我还是要去写SQL，正好这时候我们数据部门人走光了，我这时候就开始接手以前的数据工作，一开始我还想着做大数据，不想做纯SQL，但是等我学到后面我意识到好像最后大数据还是要写SQL，所以我就开始认真的学SQL，开始了解数据仓库，开始了解数据分层，慢慢的我的SQL越来越短，越来越快，但是我还是不喜欢这个</p><p>因为我喜欢写代码那种优雅感，喜欢写代码那种有测试，SQL是个不完整的语言，它虽然能够很多事，但是没办法形成模块，所以SQL代码才会有几千行代码出现，我喜欢模块化的那种感觉，不需要复制粘贴，当然我自己也做了很多尝试，但是虽然能够解决一些重复问题，但是还是不够优雅</p><p>但是做了一段时间的数据仓库之后，我还是不知道做啥，所以我开始尝试做数据模型，最终做出了一个模型来发优惠券，刚开始是挺开心的，但是后面业务停滞就没用了，后面回头想想，也去网上查了查啥是算法工程师，也知道算法工程师大概的工作流程，发现这种生活也不是我喜欢的（虽然工资是真的高），毕竟现在其实好的模型的确能帮大公司省下或者挣很多钱，但是这行不像工程师，他要需要一种数据分析的能力加机器学习的能力（其实我感觉最终机器学习部分可能大部分都是调包侠），本质还是写SQL</p><p>所以兜兜转转这么久，基本上大部分工作（除了测试产品运营这些非技术工作）我都接触了，现在回头看看，这些职位可以分成三种，一种就是数据分析（写SQL），另一种就是写代码，另外一种就是人工（运维这种），这些职位我最喜欢的就是写代码，然而最纯粹的写代码就是做后端</p><p>也就是常说的CRUD，其实一开始自己不想做后端别人说后端每天干的事情是重复的，没有成长就是CRUD，所以我才去尝试别的职业，但是走了一圈发现其实，别的工作重复性更多，相比于其他工作，我看起来更喜欢做后端，因为后端大部分都是纯代码的</p><p>而且回头想想，我在后端方向的确只懂一点点，基本上只处于会用阶段，有时候想想假如一开始就专注于后端会怎么样，但是想想去试试其他职业也不错，不试试怎么知道会有喜欢的呢，而且我也是在学的过程中才慢慢发现自己的喜好的，而且尝试了其他职业才能更好的定下来</p><p>一不小心说了这么多，其实尝试了这么多方向，自己大部分都是一知半解，未来怎么样我也不知道，所以其实毕业之后一直在找一个方向定下来，两年前是大数据方向，学了两年了，突然发现可能“钱”途更好吧，但是还是找一个喜欢的，现在看来是代码更喜欢，虽然后端CRUD比较多，但是在学大数据工程中发现吧好多大数据组件人家都提供web端，而且比如Cloudera公司，就是卖一个大数据管理后台就卖出了几百亿美元的身家，所以不要小瞧CRUD，以前做CRUD的时候，总觉得这个很low，现在看来是自己壶子里面的“水”太满，得倒出来点。</p><p>接下来的自己主攻的方向就是后端了，大概就是架构师方向前进吧，虽然说自己已经差不多有7年经验了吧，但是在后端方向可能也属于那种入门级选手吧，接下来就是慢慢提升自己。</p>]]></content>
      
      
      <categories>
          
          <category> 随想 </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>大数据架构小结</title>
      <link href="2020/12/21/bigdata/%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%9E%B6%E6%9E%84%E5%B0%8F%E7%BB%93/"/>
      <url>2020/12/21/bigdata/%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%9E%B6%E6%9E%84%E5%B0%8F%E7%BB%93/</url>
      
        <content type="html"><![CDATA[<blockquote><p>首先说说什么是大数据，最简单用数据量为单位，大于1亿就算大的，因为小于1亿就在数据库做就可以了，所以大数据是公司数据大到数据库处理不了的时候才要考虑的事情，小公司MySQL优化一下索引就可以了</p></blockquote><p>那怎么解决了，Google给了我们三篇论文作为解决方法，这个也是大数据的基石，所有后面才有Hadoop的横空出世，我们也知道了解决这个问题的方法，就是对症下药，你是大数据，我就是“大”机器，以前在一台服务器就可以处理，现在上十台、一百台</p><p>但是又出来一个问题，怎么管理，搭建过简单的4节点的机器就知道，那些大数据组件配置虽然不负责，但是很繁琐，关键最坑的事，你一开始并不知道怎么配置最好，你大部分时间都浪费在修改配置，更新所有组件配置，重启，最坑的是依赖关，比如Hbase依赖Zookeeper，你更新了Zookeeper，HBase你也得重启。而且最麻烦的是你用的那些开源大数据组件更新频繁，你很多时候要踩很多坑才能搭一个简单的大数据架构</p><p>所有就有了Cloudera和Hortonworks（被Cloudera合并了）这些提供一整套解决方法的公司，他们能提供什么了，就是管理这个大数据架构的一套系统，通过这个系统，你很容易就搭建起来一个可用的大数据集群</p><p>但是一个悲伤的消息就是Cloudera也要恰饭了，6.3.3之后不会提供免费的版本供小公司使用，对于大公司自己可能会考虑做一个类似的系统（其实也不难，就是一套web管理系统以及专业的解决方案）。但是对于小公司来说，做一个这样的系统成本太高了，但是目前来看6.3.2还算良心，支持到2022年，虽然节点最多只能100个，但是超过100个，说明你公司应该负的起钱买新版了</p><p>CDH是Cloudera提供给大公司的大数据一套架构，接下来我们就剖析一下Cloudera为什么能凭这一套解决方案支撑起百亿美元的市值</p><p>这里我就不介绍这些基本组件了，网上都有，我就谈谈这些组件是怎么解决大数据这个问题的</p><p>首先我们得知道我们要搞大数据的原因，很简单我们想从大数据中得到我们想要的结果，比如淘宝双十一大屏，我们就想知道淘宝双十一卖了多少钱，最后只要一个数字就好了。</p><p>接下来我们就以最简单的求和举例子来思考Cloudera提供的CDH来处理这个问题，第一步是存第二步是算</p><p>首先是存，假如你数据是具有唯一性的，你就存到HBase，用Row-key来区分，假如你不具有唯一性那你就存到Hadoop里面去，这是最基本的，也是Google三篇论文中的两个</p><p>接下来就是算，MR也就是Google三篇论文中的Map-Reduce理论。由于MR写起来非常复杂，所以就有Hive把SQL转换成MR程序</p><p>这三板斧下来连Google那样的大体量都能解决，但是这个方法有个弊端就是慢，很多不必要的东西比如数据落盘这些耗时间的东西拖慢了整体的速度，而且大数据是这样万一，必须要等所有的步骤都走完才能得到最终结果，所以就是最慢的那一个步骤拖慢了整体速度</p><p>MR的产生是因为内存放不了所有数据才把中间数据落盘，所以假如你内存比磁盘还要大，那在内存里面跑就可以了，Spark就是这样产生的，用内存来加速计算，Spark就像一个万能工具箱一样，你可以直接用它读取各种数据源，来各种折腾，它提供了一整套的框架来帮你做这些，但是对于企业来说懂Spark的人少，懂SQL的多</p><p>所以Cloudera自己开发了一个框架impala来方便那些懂SQL的人直接来进行数据计算，这个方案好是好，但是有个问题，impala一开始只能读取Hive表上的数据，由于Hive存在一个问题就是它由于存贮在Hadoop上所以她只能新增不能删除</p><p>所以impala只能在离线大数据上做分析，你要全量的数据得把最新的数据覆盖掉后才能开始计算，所以他们又做了一个类似HBase的数据库Kudu，基本上融合了HBase 快速新增更新删除和Hive的SQL优好的两个，这样我们就可以用impala读取最新Kudu数据做实时大数据分析了，而且速度要比Spark还快</p><p>当然也不是说impala一定是一个万能手术刀，能解决所有大数据问题，大数据这个东西吧特别有意思，我们永远也找不到一个完美的解决方法，只能找到一个相对好的解决方案。</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>以前我刚开始了解大数据的时候，我一直觉得哪个强就用那个，其实恰恰相反，每个大数据组件都有其存在的意义，面对大数据，我们不需要一套方案跑通所有，而是找一个最优雅的方法来解决特定问题，其实大数据有很多开源的框架，这些开源组件都有其特定的能力去解决他们遇到的问题，等真正有一天一套解决方案能解决所有问题，那应该大部分软件工程师都应该要失业了</p>]]></content>
      
      
      <categories>
          
          <category> 大数据 </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>八月杂感</title>
      <link href="2020/08/22/summary/%E5%85%AB%E6%9C%88%E6%9D%82%E6%84%9F/"/>
      <url>2020/08/22/summary/%E5%85%AB%E6%9C%88%E6%9D%82%E6%84%9F/</url>
      
        <content type="html"><![CDATA[<blockquote><p>一眨眼暑假就要过完了，这段博客又停下来了，倒不是因为我停下来学东西，而是我又忘了去归纳总结自己学的东西，总是一个鲁莽的心态去学，总感觉停下来会阻挡自己学习的脚步，但是其实写博客也是学习，把自己的学到的知识转换成自己的东西，写出来的过程就是帮自己的过程，其实这个方法也是一个非常牛逼的学习的方法叫做费尔曼学习法</p></blockquote><p>但是我这个人有点啰嗦，写的东西总是东一句西一句，没有一针见血，这个在接下来的博客里面也要改掉，尽量不要写废话</p><p>接下来说一下我最近的学习方向，其实主要就是金融方向，主要设计到金融、股票、量化等等。牵涉到很多金融知识</p><p>这里我再谈一下为什么我会去想精通业务，其实我一开始接触编程，我一直把自己往框架、底层方向搞（俗称造轮子），忽视了业务，一直觉得业务就是增删查改。但是其实本末倒置。</p><p>所以去年到今年，我一直在犯错误，我一直想造一个轮子出来，但是没有灵魂，再好看的轮子也没有用，所以现在我从头开始，把金融知识掌握透，这个过程会很难，但是应该会很有趣</p>]]></content>
      
      
      <categories>
          
          <category> 随想 </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>5月杂感</title>
      <link href="2020/05/14/summary/5%E6%9C%88%E6%9D%82%E6%84%9F/"/>
      <url>2020/05/14/summary/5%E6%9C%88%E6%9D%82%E6%84%9F/</url>
      
        <content type="html"><![CDATA[<blockquote><p>不知不觉2020已经过了快一半了，2020的确是个魔幻的一年，两年都没生过病的我却在疫情开始的时候发了个烧，幸好一天就退了，但是也是魔幻</p></blockquote><p>这次疫情的确影响颇深，我也花了一段时间才慢慢从担心确诊人数到开始安之若素，现在回头看看专心做事能消除很多恐惧和不安</p><p>现在回头看看，自己毕业也快两年了，加上我在大学的四年，我在编程这行已经呆了六年了。回头看看我这六年经历</p><p>在大学四年是入门区，开始接触编程，像一块海绵拼命的汲取知识，慢慢的扩展了自己的广度，但是当我到第三年的时候我就发现自己陷入了一个迷茫区，自己越来越“膨胀”，学了很多东西，感觉编程也不是很难，只要自己想花时间就能学会，这个时候自己就像一个稚童编程就像玩具，每次拿到一个新的玩具（框架、软件），玩一会会基本操作之后就丢了</p><p>接着到了工作时候变成了一个熟练工，在业务的驱动下做了很多很多项目，但是都是新瓶装旧酒，看起来自己编程技术过硬什么问题都能解决，但是其实这个时候只是会一些基本操作而已</p><p>其实在熟练工这个阶段是最难熬的，自己能清楚自己其实只是会编程，但是不是精通，这个时候段我也看了很多编程书，在实际工作中也开始运用大师们的智慧，但是自己对编程越来越迷惑了</p><p>在看了很多书之后，知道了各种编程模式，各种编码规范，自己实际编程的时候越来来畏手畏脚了，总感觉自己写的代码就是shit，越来越怕写垃圾代码，每次编程都在考虑自己的编码是否符合规范是否优美，虽然想了很多，但是最后回头看还是写的一地鸡毛，最后在逐渐自责中慢慢的淡忘编程的条条框框，开始野蛮生长模式</p><p>这种情况直到自己开始在自己项目开发近一年才开始改变，项目从设计原型到所以的设计稿出来全部都是自己的主意，项目从v0.1开始开发，慢慢升级到v1、v2、v3，每个版本升级都是几乎全部重写，这个项目特别有趣，基本上都是自己一直在催促自己，把所有功能都加紧上线，测试能不写就不写，代码能跑就行，终于重要所有的功能都开发完成了，在这过程中由于各种不规范编程，花了无数时间debug，由于是自己的项目，感觉写的这么烂都对不起自己，所有所有的代码又开始重构，在重构的过程发现干脆重写一个算了，基本上每一行代码都被千锤百炼，直到目前还是有很多问题</p><p>但是这个过程中，自己似乎领悟到了一点别的东西，好像编程大师说的那些东西都可以用，当把那些东西稍微的应用到自己的项目中的时候，突然感悟到了点什么</p><p>其实在做项目的时候我们都是为了完成功能而写的代码，从结构上他们完全围绕这业务，以素描来打比方，我们想画一幅人，我们为了快速完成一个功能，我们可能会先把一个脚画出来，一般业务不明朗的情况下我们都会把这个脚画在图片最中心，这样最快，但是当我们开始画头的时候，完了图片不够大，假如不想搽掉重来的话，你只能把一个人设计成一个矮子了</p><p>这就是我们大部分也包括我在平常工作中遇到的情况，这样的情况会造成一个很恶劣的影响，随着项目越来越大，你就越难纠正，所以我领悟到第一个就是”勇气“，敢于把你代码给重构掉</p><p>当然还有重要的一点就是快速的勇气，其实重构多了你自己会开始畏手畏脚，每次你添加新功能的时候你害怕写垃圾代码或者害怕自己考虑的不够全面为了未来不可能的事写了很多代码，其实我们必须要有写脏代码的勇气，而且我们对这些代码写的越快越好，这样你把他全删了你才不会伤心。</p><p>只有开始习惯重构你才发现，把代码写好是一个多么艰难的事情也是多么有趣的事情，终于明白大师为什么是大师，大师能用的工具框架我能懂都能用，但是大师通过这些工具搭建起来的项目我们却只能望其项背。其实越重构你就会越钦佩大师们总结的经验，他们是如此准确如此实用，虽然我很早之前就看过了解过，但只有踩了无数的bug之后才能发现并理解这些。</p><p>虽然目前我只是一个熟练工，但是我似乎发现了前进的突破的方向。以前一直觉得编程很简单，现在才知道编程没那么简单，但是有那么有趣了。</p>]]></content>
      
      
      <categories>
          
          <category> 随想 </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>深夜有感</title>
      <link href="2020/01/07/summary/%E6%B7%B1%E5%A4%9C%E6%9C%89%E6%84%9F/"/>
      <url>2020/01/07/summary/%E6%B7%B1%E5%A4%9C%E6%9C%89%E6%84%9F/</url>
      
        <content type="html"><![CDATA[<blockquote><p>去年的年终总结还没有写，2020就来了，已经很久没有写博客，原因有很多吧，这里也不多提了，今天突然想写主要是突然内心有些感想吧</p></blockquote><p>大学寝室四个毕业后两个考上了研究生，两个出来工作，今天那个工作的那个告诉我，他和班里的一个同学今年三月份辞职开始全职考研</p><p>听到这个消息的时候我挺震惊的，让我重新开始思考“考研”这件事</p><p>假如我回到毕业前，我的对考研的态度是不反对别人考，但是自己不想考，因为我觉得研究生的最大作用就是一个敲门砖让我获得更好的工作，我希望通过自己的努力去达到这个目标而不是去白白浪费教育资源</p><p>但是毕业一年半了，我逐渐变得疑惑了，因为我确确实实感受到了一种无力感，过去一年我无论是工作还是生活都很拼命，但是感觉就像一个拳头重重的打在空气中，一点反应都没有，虽然经常会因为自己做的事情感到自豪，但是似乎害怕自己所付出的努力只是镜中月水中花</p><p>有时候跟同事聊天，他们总会说你还年轻，但其实在内心我却已经非常“苍老”，在学过很多技术很多未知的东西发现，无论是技术还是产品，没有什么是不能实现的只要肯花时间，以前刚出茅庐的时候，总是一种我全部都要的心态，无论是多难的技术多么难实现的功能，只要自己觉得炫酷就一定要去搞懂一定要去实现</p><p>当你排除所有的Bug，消除心中所有的疑惑回头再看好像那个炫酷的技术，那个困扰你很多天的bug只是一个小小的知识点，想想自己当初发了九牛二虎的力量排除到凌晨3点没想到却是那么一个小小的地方出了问题</p><p>经历的多了就发现，重要的不是你会做什么，而是你选择做什么，自己已经不在年轻了，自己做的每个决定都会影响到未来，还是年轻的时候好，只管拼命往前冲，现在“老”了，做事总是畏手畏脚，因为自己已经有经验了，不再是摸着石头过河，总是望着河边，想着到底到底是游过去还是像大部分人一样坐船过去</p><p>也是很巧，过马路的时候绿灯亮了，在我这边只有我一个人要过去，对面马路却一群人要过来，当时瞬间感受到了一种孤独的东西</p><p>在这条人生的斑马线上我可以选择和大部分人一样走上一条康庄大道，路上非常平坦，而且在这条道路上我不会感受到孤独，或者我选择一个人孤独倔强的走下去，虽然路的终点迎接我的不一定是鲜花和掌声，但是当我回过头看看过去的自己的回忆，会感叹这辈子还是很能折腾的哈哈，没白过。</p><p>其实现在有的时候回头看看自己写的博客，还是有一点点自豪的，有的时候甚至会问自己是不是请人代笔的，怎么会写出这么有深度的文章（不要脸哈哈）。过去的2019年自己折腾了很多，可能有些看起来是毫无意义的任性，但是我相信自己每一次的折腾都没有白费，可能现在看来没有价值，但是未来能掀起多大浪花还不知道呢。加油吧，2020！！！</p>]]></content>
      
      
      <categories>
          
          <category> 随想 </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>Flink批和流处理的思考</title>
      <link href="2019/10/16/software/Flink%E6%89%B9%E5%92%8C%E6%B5%81%E5%A4%84%E7%90%86%E7%9A%84%E6%80%9D%E8%80%83/"/>
      <url>2019/10/16/software/Flink%E6%89%B9%E5%92%8C%E6%B5%81%E5%A4%84%E7%90%86%E7%9A%84%E6%80%9D%E8%80%83/</url>
      
        <content type="html"><![CDATA[<blockquote><p>随着2019年8月份，Flink1.9.0正式发布，Flink并入Blink代码，开始同时支持批和流的处理，于此同时，批量同时处理所遇到的问题也就产生了</p></blockquote><p>首先我们要知道，什么是批，什么是流，批好比一瓶水，我们看到到它在那里，流就像雨水，我们只能看着它慢慢的从天上掉下来，什么时候掉完我们不知道</p><p>批处理解决我们非常好理解，分而治之即可，然而对于流数据的解决该如何呢，当然也是分而治之，但是我们分的是时间，所以对于流数据最重要的第一是不能让历史数据沉淀下来</p><p>我们回到现实中，理想状态我们处理数据总希望数据全部都能获取到，由于数据库三范式的存在，我们的数据不是完整的，比如消费记录只会存一个id代表用户，而我们希望统计用户记录的年龄分布就不能只通过记录表来实现</p><p>所以我们第一个挑战就是<code>Join</code>，把多个维度表汇集到一起，什么是<code>Join</code>呢，就是把不同的表进行分组，我们这里就不讨论大表和大表的<code>Join</code>，因为这个不是<code>Flink</code>的强项，对于这个来说，我只能说<code>MapReduce</code>欢迎您。</p><p>我们就讨论一种情况，大表处理表，小表配置表，因为这代表实际大数据处理的中最常见的情况，由于Flink的批流一致的支持，我们现在处理<code>Join</code>有两张情况</p><p>第一种是批流进行<code>Join</code>，第二种是流和流（双流）<code>Join</code>，当然由于Flink底层其实把批看做一种特殊的流（无后续增加），我们也可以看成一种</p><p>对于批和流的<code>Join</code>，Flink的优化是将批变成<code>State</code>，这样流只需要进行内存匹配而不需要每次进行<code>Join</code>的时候在读取一遍批处理数据，这个方式来说最简单，但是也有一个问题，我们的配置表变成的“一次性”的了，假如我们想更新配置表，但是流计算察觉不到变化也就实现不了一个动态的更新</p><p>接下来我们在看看流和流的<code>Join</code>，对于最简单的双流<code>Join</code>来说，为了维持流数据的更新，必须存贮两个流的所以历史数据，对于小表来说存储耗费不了内存，但是大表来说就不一样的，所以双流<code>Join</code>不适合大表和小表的<code>Join</code>，但是统计的话可以，<code>Flink</code>针对一些东西做了优化不会存储所以历史数据，只保留一个统计<code>State</code>，会根据更新动态来修改，那么大表和小表就没法实现双流<code>Join</code>了吗</p><p>当然不是，我们想想为什么大表小表会保留历史数据才能维持结果正确，我们思考一个例子，我们有很多不同种类的商品，然后我们有一个本子记录客人想买的商品（实时更新），为了做到正确匹配客人像要的商品，我们必须记住我们所以种类的商品，当有个客人想买的时候才能匹配上，假如你忘了你们有什么匹配的商品，那么这个客人就买不到了，因为匹配不上</p><p>这种情况是必须要记住所以种类的，但是我们大部分情况是这样的，我们的大表里面记录了系统某个指标，我们的配置表里面保存了我们想监听的指标，当我们更新我们的配置表的时候代表从这个时刻起我们对某个指标感兴趣，对于历史的数据我们其实是不感兴趣的，所以，你完全可以“忘掉”原来的大表历史数据，<code>Flink</code>对于这个功能使用了<code>Temporal Join</code>来支持</p><p>例如：</p><pre><code>select * from source a, lateral table (ConfigTable(a.protime)) bwhere a.id = b.id;</code></pre><p>我们只要注册<code>ConfigTable</code>到一个流表上就代表告诉<code>Flink</code>历史数据可以不用记住了，我只关心现在的最新配置能不能匹配上，这样就解决了对大表的时间分片</p>]]></content>
      
      
      <categories>
          
          <category> 软件 </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>Apache Atlas 2.0.0部署实践</title>
      <link href="2019/10/12/software/ApacheAtlas%E9%83%A8%E7%BD%B2%E5%AE%9E%E8%B7%B5/"/>
      <url>2019/10/12/software/ApacheAtlas%E9%83%A8%E7%BD%B2%E5%AE%9E%E8%B7%B5/</url>
      
        <content type="html"><![CDATA[<blockquote><p>Atlas 是一个可扩展和可扩展的核心基础治理服务集 - 使企业能够有效地和高效地满足 Hadoop 中的合规性要求，并允许与整个企业数据生态系统的集成。[来自百科]</p></blockquote><p><img src="/images/apache-atlas-formwork.png" alt="架构图-来源网络"></p><p>现在我们就基于最新版2.0.0介绍一下部署细节</p><h2 id="Server安装"><a href="#Server安装" class="headerlink" title="Server安装"></a>Server安装</h2><ol><li>源码安装</li></ol><blockquote><p>首先在<a href="https://www.apache.org/dyn/closer.cgi/atlas/2.0.0/apache-atlas-2.0.0-sources.tar.gz" target="_blank" rel="noopener">https://www.apache.org/dyn/closer.cgi/atlas/2.0.0/apache-atlas-2.0.0-sources.tar.gz</a>下载2.0.0源码</p></blockquote><p>解压源码</p><ol start="2"><li>选择Atlas架构</li></ol><p>Atlas支持多种架构作为后端</p><ul><li>HBase + Solr</li><li>Cassandra + Solr</li></ul><p>你可以选择多种，这里我们采用集成<code>HBase + Solr</code>方式编译</p><pre><code>mvn clean -DskipTests package -Pdist,embedded-hbase-solr</code></pre><p>执行代码即可（推荐使用阿里云的maven源加速编译）</p><ol start="3"><li>修改环境变量</li></ol><p>编译完之后在<code>/distro/target</code>下面有很多tar.gz包，我们需要的是<code>apache-atlas-2.0.0-server.tar.gz</code>包，解压到当前目录</p><p>3.1 修改配置文件<code>conf/atlas-env.sh</code></p><pre><code>export JAVA_HOME=/your/java/installexport MANAGE_LOCAL_HBASE=falseexport MANAGE_LOCAL_SOLR=false</code></pre><p>我们设定<code>Solr</code>和<code>HBase</code>手动开启，方便我们发现哪个部分启动异常</p><p>3.2 修改admin密码：</p><p>系统默认会生成一个密码给我们，但是官网我也没看到说这个密码，所以我们自己生成一个，然后修改上去</p><pre><code>echo -n &quot;password&quot; | sha256sum</code></pre><p>使用上面命令生成一个<code>sha256</code>加密字符（你可以把password改成你想要的密码），复制生成的字符串（不需要<code>-</code>），例如<code>5e884898da28047151d0e56f8dc6292773603d0d6aabbdd62a11ef721d1542d8</code><br>修改<code>conf/users-credentials.properties</code> 改成</p><pre><code>admin=ADMIN::5e884898da28047151d0e56f8dc6292773603d0d6aabbdd62a11ef721d1542d8</code></pre><p>3.3 修改HBase配置(需要提前安装好java和Zookeeper和Hadoop）</p><p>进入<code>hbase</code>目录夹</p><p>修改 <code>conf/hbase-env.sh</code></p><pre><code>export JAVA_HOME=/your/java/installexport HBASE_MANAGES_ZK=false</code></pre><p>复制Hadoop配置到HBase中</p><pre><code>cp $HADOOP_HOME/etc/hadoop/core-site.xml $HBASE_HOME/conf/cp $HADOOP_HOME/etc/hadoop/hdfs-site.xml $HBASE_HOME/conf/</code></pre><p>在<code>hbase-site.xml</code>中加入</p><pre><code>      &lt;property&gt;         &lt;name&gt;hbase.cluster.distributed&lt;/name&gt;         &lt;value&gt;true&lt;/value&gt;    &lt;/property&gt;             &lt;property&gt;        &lt;name&gt;hbase.rootdir&lt;/name&gt;         &lt;value&gt;/hbase&lt;/value&gt;     &lt;/property&gt;    &lt;property&gt;    &lt;name&gt;hbase.zookeeper.quorum&lt;/name&gt;    &lt;value&gt;localhost&lt;/value&gt;    &lt;/property&gt;    &lt;property&gt;    &lt;name&gt;hbase.zookeeper.property.clientPort&lt;/name&gt;&lt;value&gt;2181&lt;/value&gt;    &lt;/property&gt;</code></pre><p>启动安装好的Zookeeper，使用<code>./bin/start-hbase.sh</code>启动<code>HBase</code></p><p>使用<code>jps</code>应该能看到<code>HMaster</code>和<code>HRegionServer</code>启动了</p><p>测试HBase安装是否完成，使用<code>./bin/hbase shell</code> 进入<code>HBase</code> 命令行，如果<code>status</code>命令返回正确的话，那么你的HBase就安装好了</p><p>3.4 启动Solr</p><p>进入<code>solr</code>目录，启动<code>solr</code></p><pre><code>./bin/solr -c -z localhost:2181 -p 8983</code></pre><p>打开<code>http://localhost:8983/solr/#/</code>如果能看到正常页面，那么Solr就启动好了</p><p>在<code>apache-atlas-2.0.0</code>目录下创建索引</p><pre><code>./solr/bin/solr create -c vertex_index -d conf/solr -shards 1 -replicationFactor 1./solr/bin/solr create -c edge_index -d conf/solr -shards 1 -replicationFactor 1./solr/bin/solr create -c fulltext_index -d conf/solr -shards 1 -replicationFactor 1</code></pre><p>3.5 启动Atlas</p><p>在<code>apache-atlas-2.0.0</code>目录下启动Atlas</p><p>使用<code>bin/atlas_start.py</code>  或者 <code>/usr/bin/python2.7 bin/atlas_start.py</code></p><p>PS：第一次启动比较慢，如果前面的HBase和Solr都安装好了的话，一般都没有什么大问题，可以查看<code>logs/application.log</code>查看系统运行情况，等到初始化完成后打开<code>localhost:21000</code>使用<code>admin:password</code>即可登录上去</p><p><img src="/images/apache-atlas-index.png" alt="登录后的页面"></p><p>当然我们现在系统空空如也，现在我们来使用Hook导入数据到Atlas里面去吧<br>我测试通过的版本是：Hadoop2.8.1 + Zookeeper 3.4.10 ，其他的都是用的默认Atlas 集成的版本</p><h2 id="Hook安装"><a href="#Hook安装" class="headerlink" title="Hook安装"></a>Hook安装</h2><p><code>Atlas</code>最强大的的地方就是能够把Hive，Sqoop，Kafka这些大数据组件的血缘关系给自动抽取出来，所以钩子的安装至关重要</p><h3 id="Sqoop钩子"><a href="#Sqoop钩子" class="headerlink" title="Sqoop钩子"></a>Sqoop钩子</h3><blockquote><p>Sqoop 我用的是<code>1.4.7</code>版本</p></blockquote><ul><li>配置Sqoop钩子</li></ul><p>首先在<code>conf/sqoop-site.xml</code>中添加</p><pre><code> &lt;property&gt;  &lt;name&gt;sqoop.job.data.publish.class&lt;/name&gt;  &lt;value&gt;org.apache.atlas.sqoop.hook.SqoopHook&lt;/value&gt;&lt;/property&gt;</code></pre><ul><li>复制必要的包</li></ul><p>解压<code>distro/target</code>的<code>apache-atlas-2.0.0-sqoop-hook.tar.gz</code>，复制<code>apache-atlas-2.0.0-sqoop-hook/apache-atlas-sqoop-hook-2.0.0/hook/sqoop/</code>目录到 <code>&lt;atlas package&gt;/hook/sqoop</code></p><p>如：</p><pre><code>cd /where/your/untar/atlascp -r ../../apache-atlas-2.0.0-sqoop-hook/apache-atlas-sqoop-hook-2.0.0/hook .</code></pre><p>创建软链接<code>&lt;atlas-conf&gt;/atlas-application.properties</code>到<code>&lt;sqoop-conf&gt;/</code></p><p>如：</p><pre><code>ln -s ln -s /home/zhanglun/github/apache-atlas-sources-2.0.0/distro/target/apache-atlas-2.0.0-server/apache-atlas-2.0.0/conf/atlas-application.properties /opt/sqoop-1.4.7.bin__hadoop-2.6.0/conf</code></pre><p>将<code>&lt;atlas package&gt;/hook/sqoop/*.jar</code> 复制到<code>sqoop</code> <code>lib</code>目录</p><p>如：</p><pre><code>cp hook/sqoop/*.jar /opt/sqoop-1.4.7.bin__hadoop-2.6.0/libcp hook/sqoop/atlas-sqoop-plugin-impl/*.jar /opt/sqoop-1.4.7.bin__hadoop-2.6.0/lib</code></pre><ul><li>测试Sqoop 导入Hive中</li></ul><p>sqoop import –connect jdbc:mysql://localhost:3306/sqoop<br>–username root<br>-P<br>–split-by id<br>–table root<br>–hive-import<br>–create-hive-table<br>–hive-table db.auth_user</p><p>不出意外应该会报错 </p><pre><code>Caused by: java.lang.NullPointerExceptionat org.apache.atlas.hook.AtlasHook</code></pre><p>因为我们还没有配置好Sqoop钩子，接下来我们来配置Sqoop钩子</p><ul><li>配置Atlas</li></ul><p>前面我们创建了软链接，现在我们只要修改<code>conf/atlas-application.properties</code>这个配置即可</p><p>首先我们得配置关闭<code>Kafka</code>作为发送消息缓冲，因为Atlas默认使用<code>Kafka</code>作为消息缓冲，然后我们修改下面的配置（这个后期可以打开，你再配置好kafka的地址）</p><pre><code>atlas.notification.embedded=false  # 不往kafka里面发送atlas.graph.index.search.backend=solr5 </code></pre><ul><li><p>异常一： Caused by: java.lang.ClassNotFoundException: org.json.JSONObject</p><p>包缺失，下载<a href="http://www.java2s.com/Code/Jar/j/Downloadjavajsonjar.htm" target="_blank" rel="noopener">java-json.jar</a> 到<code>Sqoop</code>文件夹</p><ul><li>异常二：<code>Import failed: java.io.IOException: java.lang.ClassNotFoundException: org.apache.hadoop.hive.conf.HiveConf Could not load org.apache.hadoop.hive.conf.HiveConf. Make sure HIVE_CONF_DIR is set correctly</code></li></ul></li></ul><p>环境变量没有设置对，设置<code>HIVE_CONF_DIR</code>在<code>conf/sqoop-env.sh</code>（确保<code>HADOOP_HOME</code>和<code>HIVE_HOME</code>不是空值）</p><pre><code>export HADOOP_CLASSPATH=&quot;`$HADOOP_HOME/bin/hadoop classpath`:$HIVE_HOME/lib/*&quot;</code></pre><ul><li>异常三： <code>Error when removing metric from org.apache.kafka.common.metrics.JmxReporterjava.security.AccessControlException: access denied (&quot;javax.management.MBeanTrustPermission&quot; &quot;register&quot;)</code></li></ul><p>根据<a href="https://stackoverflow.com/questions/12195868/java-security-accesscontrolexception-when-using-ant-but-runs-ok-when-invoking-j" target="_blank" rel="noopener">stackoverflow</a> 解决</p><ul><li>异常四：<code>java.lang.NoSuchMethodError: com.fasterxml.jackson.databind.ObjectMapper.readerFor</code></li></ul><p>Hive包版本和Sqoop包版本冲突（我的Hive版本是2.3.4），可以先备份<code>Sqoop</code>的lib，文件再进行下面操作：</p><pre><code>cp -r lib lib_backrm lib/jackson-*cp $HIVE_HOME/lib/jackson-* lib/</code></pre><ul><li>异常五：<code>Connection to node -1 could not be established</code></li></ul><p>你在<code>conf/atlas-application.properties</code>没有修改<code>atlas.notification.embedded</code>成false，那么你必须配置好<code>kafka</code>地址</p><pre><code>atlas.kafka.zookeeper.connect=localhost:2181atlas.kafka.bootstrap.servers=localhost:9092</code></pre><p>PS：每次出现异常，你必须先删掉Hadoop上面的文件，再执行导入，你可以直接安装我的流程进行修复，因为这些都是我在配置的时候顺序出现的问题，走到这里我们就配置好了<code>Sqoop</code>和<code>Hive</code>的导入Hook，如果运行成功，你会看到下面界面</p><p><img src="images/apache-atlas-hive-table.png" alt="Sqoop导入Hive血缘关系"></p><p>接下来我们配置Hive钩子来导入Hive中的表</p><h3 id="Hive-钩子"><a href="#Hive-钩子" class="headerlink" title="Hive 钩子"></a>Hive 钩子</h3><ul><li>配置hive-site.xml</li></ul><p>在里面加入</p><pre><code>&lt;property&gt;    &lt;name&gt;hive.exec.post.hooks&lt;/name&gt;      &lt;value&gt;org.apache.atlas.hive.hook.HiveHook&lt;/value&gt;  &lt;/property&gt;</code></pre><ul><li>解压 hive-hook包</li></ul><p>如：<code>tar xzvf apache-atlas-2.0.0-hive-hook.tar.gz</code></p><ul><li>复制到atlas中</li></ul><p>如：<code>cp -r ../../apache-atlas-2.0.0-hive-hook/apache-atlas-hive-hook-2.0.0/hook/hive hook/</code></p><ul><li>配置Hive环境变量</li></ul><p>在<code>hive-env.sh</code>中加入<code>&#39;export HIVE_AUX_JARS_PATH=&lt;atlas package&gt;/hook/hiv</code></p><ul><li>给创建软链接</li></ul><p>像前面一样创建一个<code>atlas-application.properties</code>软链接到<code>hive/conf</code>目录下<br>如：<code>ln -s /home/zhanglun/github/apache-atlas-sources-2.0.0/distro/target/apache-atlas-2.0.0-server/apache-atlas-2.0.0/conf/atlas-application.properties /opt/apache-hive-2.3.4-bin/conf</code></p><ul><li>复制Hive包到Hook</li></ul><p><code>import-hive.sh</code>依赖<code>Hive</code>的<code>jackson</code>一些包（报<code>java.lang.NoSuchMethodError: com.fasterxml.jackson.databind.util.BeanUtil.okNameForGetter</code>错误），把Hive的依赖包复制到钩子的包目录下面<br>如：<code>cp $HIVE_HOME/lib/jackson-* ../hook/hive/atlas-hive-plugin-impl/</code></p><p>现在我们尝试执行<code>hook-bin/import-hive.sh</code>(在<code>apache-atlas-2.0.0-hive-hook/apache-atlas-hive-hook-2.0.0</code>目录下）</p><p><img src="/images/apache-atlas-hive-hook.png" alt="hive导入的表"></p><p>现在Atlas里面有两张表，不过一张是Sqoop导入的，一张是Hive导入的，查看Hive导入的血缘关系时候我们发现，他只有自己的一张表（源表）</p><p>其他Kafka和Storm的钩子比较简单我就不介绍详细过程了</p>]]></content>
      
      
      <categories>
          
          <category> 软件 </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>FlinkSQL Client实战</title>
      <link href="2019/10/12/software/FlinkSQLClient%E5%AE%9E%E6%88%98/"/>
      <url>2019/10/12/software/FlinkSQLClient%E5%AE%9E%E6%88%98/</url>
      
        <content type="html"><![CDATA[<blockquote><p>Flink SQL Client 是一个帮助用户直接运行SQL，而不要进行编译运行的工具，首先他支持多个数据源，在Flink1.9.0开始支持了Hive，并且在Flink 1.10.0时候发布了企业级Hive支持，这样就把批处理和流计算结合了起来，这篇博客详解了Hive在Flink SQL Client中的安装和使用，以及探索在上面做的一些复杂开发实践</p></blockquote><h2 id="Hive安装"><a href="#Hive安装" class="headerlink" title="Hive安装"></a>Hive安装</h2><p><del>依赖的<code>jar</code>包如下</del></p><pre><code>antlr4-runtime-4.5.jar        flink-connector-hive_2.11-1.9.0.jarantlr-runtime-3.5.2.jar        flink-hadoop-compatibility_2.11-1.9.0.jardatanucleus-api-jdo-4.2.4.jar  hive-exec-2.3.4.jardatanucleus-api-jdo-5.2.2.jar javax.jdo-3.2.0-m3.jardatanucleus-core-4.1.17.jar   datanucleus-rdbms-4.1.9.jar    flink-shaded-hadoop-2-uber-2.7.5-8.0.jar</code></pre><p><del>将这些复制到<code>flink/lib</code>目录下面即可，下面是下载链接</del><br><del><a href="https://drive.google.com/file/d/1hRit-IsX7zvkHloUg5S36czWKvpBgBtN/view?usp=sharing" target="_blank" rel="noopener">1.10.0 flink hive jar Google Driver 下载地址</a></del><br><del><a href="https://drive.google.com/file/d/1WDFFa6ePkIu4Wft_0jEEhS7ekto5_DSo/view?usp=sharing" target="_blank" rel="noopener">1.9.0 flink hive jar Google Driver下载地址</a></del><br><del>百度云提取码： <a href="https://pan.baidu.com/s/1EA8YRUKjC7OzhJ3MPA0GlA" target="_blank" rel="noopener">1.9.0 flink hive 百度云下载地址</a></del><br> <del>百度云提取码: e9c7 链接:  <a href="https://pan.baidu.com/s/1WFH4T7AiV31PrptTRTqzJA" target="_blank" rel="noopener">1.10.0 flink hive 百度云下载地址</a></del></p><h2 id="Hive使用中遇到的问题"><a href="#Hive使用中遇到的问题" class="headerlink" title="Hive使用中遇到的问题"></a>Hive使用中遇到的问题</h2><p><del>1. 目前只支持2.3.4和1.2.1</del><br><del>2. 支持的读取的hive类型有限，时间类型只支持1个<code>Date</code></del><br><del>PS: Sqoop导入数据库表到Hive的时候，只能将<code>DateTime</code>和<code>TimeStamp</code>设置为<code>String</code>，否则无法在Client中使用</del><br><del>3. 跨越<code>Catalog</code>读取表的时候Hive不能存放在<code>default</code>（默认）数据库中，否则会解析异常</del></p><p><del>例如：想从Hive默认数据库中获取表<code>a</code>，使用<code>insert b select * from hive_catalog.default.a</code>会解析失败</del></p><p><del>4. Hive不支持写入分区表，也不支持Overwrite，只支持append模式</del></p><p>目前flink-1.10已经支持多个Hive版本并且修复了上面所遇到的问题，但是目前在<code>SQL Client</code>里面使用较为麻烦，需要自己安装缺少的jar包(而且版本不兼容的话会出现各种奇奇怪怪的bug），所以我写了一个项目把所以的依赖打包到一起这样只要一行命令就可以生成所需要的依赖，而且支持多个<code>Hive</code>版本</p><p>下面是这个项目的github地址</p><p><a href="https://github.com/mrzhangboss/flink-hive-dependence.git" target="_blank" rel="noopener">Flink SQL Client Hive 依赖生成器</a></p><hr><p>未完待续</p>]]></content>
      
      
      <categories>
          
          <category> 软件 </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>如何真正精通</title>
      <link href="2019/10/05/summary/%E5%A6%82%E4%BD%95%E7%9C%9F%E6%AD%A3%E7%B2%BE%E9%80%9A/"/>
      <url>2019/10/05/summary/%E5%A6%82%E4%BD%95%E7%9C%9F%E6%AD%A3%E7%B2%BE%E9%80%9A/</url>
      
        <content type="html"><![CDATA[<h2 id="引言"><a href="#引言" class="headerlink" title="引言"></a>引言</h2><blockquote><p>写这篇博客是因为自己从去年就开始学习大数据，但是当自己入了门之后，虽然懂得一点皮毛，但是总觉得自己没有真正掌握这个，尝试了很多方法（做了不少项目，读了几个月源代码），但是总感觉不得要领，或者来说进步缓慢，下面是自己的一些思考</p></blockquote><p>进步最快的是做项目，但是做项目其实也有下面的问题：</p><ul><li>项目参杂业务</li><li>项目只涉及到大数据某个部分</li></ul><p>比如以我前面数据转存HBase的项目为例，我遇到的问题是Spark内存不足，以及优化HBase存贮，在解决问题的过程中我的确查了不少资料，学了不少东西，但是通过这个项目之后我只能说我对上面的问题有一定的经验，谈不上精通</p><p>从理论上讲你想真正精通就是不停的使用它，但是对于每个人来说，并不是都有机会去在每个项目中应用它，比如我上个项目Flink SQL管理系统，虽然它是跟Flink有关，但是它大部分都是和前端以及动态编译有关，随着项目越来越多你会发现你越来越熟练某个东西，但是不是精通</p><p>那怎么才能真正的精通某个东西呢，接下来我就下面几点谈谈我的体会</p><h2 id="心态"><a href="#心态" class="headerlink" title="心态"></a>心态</h2><p>心态这个东西非常的重要，一开始我接触大数据，我飞快的过了一遍大数据的各个软件，把tutorial都运行了一遍，整个人是一种完成任务的心态把所以大数据组件给过了一遍，做完之后感觉大数据也就那么回事，但是其实你只是会运行几个软件而已，就好比一个会用遥控器的猴子，你会看电视不是你有多聪明，而是发明遥控器的人的聪明才让你能那么轻松的看电视</p><p>所以我们要想深入精通某个东西第一个就是摆正自己的心态，心态这个东西怎么端正呢，其实我觉得第一件事就是让自己变成一个“演员”</p><p>这里我插一句，其实我们这一代虽然压力很大，但是有很大一部分年轻人比如我会有一种“佛系”心态，无欲无求，这种心态可能会让你心态变得更加平静，你不会思考那么多问题（为什么我工资没有他高、为什么我买不起房），但是这种心态也会让你丧失变得更加优秀的可能，比如前面我们提到我们要想精通必须端正心态，把这件事放在你心上，作为一个“佛系”看到这可能会想，为什么要把它放到心上，不就是精通嘛多花点时间就可以了</p><p>这种心态就像在一间教室了，你和你同学都在认真听课，你同学被叫起来回答问题，你的心态非常平静，老师提出的问题你根本不会用心去想，你内心只有这个起来讲的傻蛋会怎么出丑，而对于站起来回答问题的那个同学他会精神高度紧张，他会用100%的精力会放在这个“问题”上</p><p>我们回到前面，前面我们提了“演员”这个概念，我们端正心态的具体方法就是把自己变成一个“演员”，或许学这个东西对于你来说只是一个给自己充一下电，在你心中，玩一把LOL、刷一会抖音和充一下电的地位是一样。</p><p><em>所以第一步就是“假装”你非常热爱这个东西</em></p><p>为什么要说是假装呢，就以我们自己举例，我们从小到大爱过学习吗？我敢说一万个里面只有一个会说我好爱学习，那个人很有可能是如果他不学习他爸就叫他下地干活，其实我们从小到大一直在被强迫着学习，如果没有老师布置作业我们很有可能一下课就把书包丢到看不到的地方，打开电视机，假如没有考试要求我们背诵课文，我们可能不会把一段又丑又长的文章读了一遍又一遍，等我们到了社会，我们又被领导用工资压着我们做一些内心厌据的工作</p><p>我还好毕业之后从事的是我喜欢做的，所以我经常下班之后会呆在电脑旁，与之相反我有一个建筑朋友，他也是做电脑设计这方面的，他很好奇我每天下班都待在电脑旁边不打游戏就坐在那编程，对于他来说，他下班之后再也不想打开电脑，因为他觉得平常在公司已经非常辛苦了，为什么还要折磨自己。对于他来说工作只是一个维持生活的手段</p><p>其实对于我来说，编程也没有上升到热爱那一个层次，只是比游戏稍微高那么一点而已，这也可以解释有的时候我敲着敲着代码敲累了又打开了虎牙看起直播来了，热爱这个东西就好比打游戏，你越打越开心，而不会越来越累</p><p><em>所以其实对于我们大部分年轻人来说，第一件事就是把“佛系”心态收起来，变成一个“演员”，假装我们热爱这个东西</em></p><p>其实李笑来也提到过类似观点，他提过他是学习英语的窍门，他“假装”他背一个单词可以挣5块钱，所以他单词越背越多，最后英语也越来越好，对于我们每个人来说如果你真的想精通某个东西，找到一种能够刺激你的方法去“热爱”它</p><p>插一句，为什么我在这个心态这里用了很多“演员”、“假装”这些词汇，因为从小到大我没有看到一个人真正的热爱某个东西，无论是工作还是学习大家似乎都是一种“妥协”的态度，因为钱而工作，因为击杀的快感而打游戏，很少能看到一个人因为热爱而沉浸在一件事上一辈子，这也可以解释为什么大师那么少，大部分太“博爱”，无法选择出自己最喜爱的</p><p>把人生当做登山，我们面前有很多座山，大部分人登了一会就觉得这座山不是自己的菜，厌倦了，从头开始换另一座山登，而对于我们来说，最好的方法是登了一会虽然觉得厌倦了，“假装”热爱，拼命往上登，到了山顶，视野广阔才能真正的看到自己想要的，这个时候不是随便换个山登，而是“跳”到那座山去。</p><p>大道理我们也不多说了，现在我们面前只有一个朱丽叶，接下来我们谈谈一些我觉得很重要的“术”来帮助我们维持这个心态</p><h2 id="精通的手段"><a href="#精通的手段" class="headerlink" title="精通的手段"></a>精通的手段</h2><h3 id="问自己问题"><a href="#问自己问题" class="headerlink" title="问自己问题"></a>问自己问题</h3><p>帮住我们精通某个东西的最重要的方法的是问问题，比如你想精通Hadoop，这个时候你就问自己，什么是Hadoop，为什么叫Hadoop，它有啥用….</p><p>这些问题你可以记录下来，在回答过程中你必须要像对待孩子一样对待这些问题，你要充分激发你的求知欲，其实这个同打游戏是一个道理，假如你喜欢打LOL，你会去尝试各种符文天赋，尝试各种英雄各种套路，你拿到一个盲僧你总会问自己，盲僧装备啥符文最强，R闪会不会撞墙。</p><p>问自己问题还有一个关键就是：打破砂锅问到底。接着Hadoop那个问题，当你回答到Hadoop的组成的时候，提到了Namenode，然后在问自己Namenode是什么，如果文件过多会怎么样，高可用和Namenode有什么关系，如果Namenode挂掉会怎么样….</p><p>问问题的过程不但是是一个不断了解的过程，也是一个学习的工程，你在学习的过程中遇到的问题越多说明你的短板也越多，我们就像一个修补匠不断对一艘船修修补补，最终它能坚固的如军舰一般</p><h3 id="系统总结"><a href="#系统总结" class="headerlink" title="系统总结"></a>系统总结</h3><p>不断的提出新的问题就像不断的在一个树枝上分叉，但是假如你一直问下去，你有可能会在寻找问题的过程中迷失自己，或者偏离主航道，比如问大数据最后深入到硬件最后深入到化学方向去了，所以一个系统的归纳总结也非常有帮助</p><p>方法有很多，你可以像我一样写博客，画画思维导图，甚至手写都可以，一切能够帮助你总结归纳都可以</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>这篇博客简单的介绍了一下我对于怎么精通的个人体会，我个人也还在路上，之所以写这篇也是自己在摸索的过程中磕磕碰碰走了很多弯路，这里谈一下对以前学习过程中的反思吧，我一开始心态就不对，飞快的过了大数据的组件，然后开始做了些项目，但是做完之后也只是熟练的编程而已，接着开始研究源代码，虽然代码都能看懂，但是不知道怎么去看，好像是为了看源代码而看源代码而已，自己对整个系统一窍不通</p><p>后面突然看到一下大数据的面试题，我尝试回答但是好像都模模糊糊，这个时候我才意识到我的问题的关键：“心态”。我自己一直没有摆正自己的心态，我只是把他当做一个工具来用了，就好比电视遥控器，我只是一只会按遥控器的monkey而已。摆正心态说难也难说容易也容易，假如明天马云跟我说你把Hadoop搞精通我给你1000万，那我心态不需要“假装”，我们需要一种手段帮助我们端正心态，每个人可以采取不同的手段来帮助自己，对于我这个佛系青年来说，上面的方法是最好的。</p><p>其实我记得好像小学的时候我就有过这种思考，当初我妈和我姐聊天说为什么同一个妈生的，一个成绩好一个成绩差，我当时直接就说了假如你想成绩好那必须要热爱它，后面我也用这个办法帮我自己学习一些东西，但是苦于没有想到怎么精通的“术”，所以虽然一开始保持100分的热情但是坚持坚持着就放弃了，所以心态不但重要，后面的精通的“术”也非常重要，就像太极，一阴一阳，光有热情没有行动没有用，只有行动没有热情，你也领悟不到精髓。</p><p>PS：其实回头想想这篇博客也非常简单，很多前辈或者先人已经提出来过，不外乎是：端正心态、多问问题、多总结。我自己一开始也想过别人都写过，自己何必多此一举，但是想想虽然这些问题都是老生常谈，但是我们来说，我们都是第一次做人，每个人都是不一样，对于我来说，端正心态怎么端，为什么要端正，端正后干什么这些都是不一样。其实我这篇博客最主要的想谈一下心态，就以我们健身房为例，一遍健身一遍玩手机的都是没有什么肌肉的，那些肌肉大的都是训练的时候拼尽全力，假如你抱着来健身房玩一玩的心态，你永远没法把你的肌肉练出来，而那些为每次动作拼尽全力的人他们的肌肉是最强壮的。</p>]]></content>
      
      
      <categories>
          
          <category> 随想 </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>推荐系统微博数据实战</title>
      <link href="2019/05/05/ai/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E5%BE%AE%E5%8D%9A%E6%95%B0%E6%8D%AE%E5%AE%9E%E6%88%98/"/>
      <url>2019/05/05/ai/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E5%BE%AE%E5%8D%9A%E6%95%B0%E6%8D%AE%E5%AE%9E%E6%88%98/</url>
      
        <content type="html"><![CDATA[<blockquote><p>虽然自己刚解决掉一个TB级数据导入“大项目”，但是感觉自己对大数据流程还是缺乏一定对认识，所以想通过一个完整对项目体会一下大数据如何落地再实际产出对，正好@志斌 提供了一个硬盘给我，里面有12年微博对数据，数据量在3TB左右，微博量为65亿，所以就借助这个数据来捣鼓一下推荐系统，我把自己捣鼓的过程写下来，希望能够给其他想了解推荐系统的人一点帮助。</p></blockquote><h2 id="什么是推荐系统"><a href="#什么是推荐系统" class="headerlink" title="什么是推荐系统"></a>什么是推荐系统</h2><p>其实从名字上面我们就知道，我们其实就是要做一件事，“推荐”，并把这个东西做成一个系统，推荐很好理解，但是怎么来做呢，我们拿买东西作为例子，我很喜欢吃苹果，所以我经常去超市买苹果，售货员大妈知道我喜欢吃苹果，所以经常推荐我一些好吃苹果，这个就是推荐，他是根据我们历史的消费记录来给我们推荐的</p><p>这算一种非常简单的推荐系统，你喜欢啥我就给你推荐啥，但是有一个问题，假如你喜欢Apple，你买了一个IPhone X，但是如果我再给你推荐IPhone X其实你不会买的（当然也有可能，但是可能性非常低），那一个“高级”的推荐系统应该怎么做呢，你买了IPhone X，你就很有可能会买AirPod、会买IPod、IMac这些，你脑子里面可能会这样想，做一个归类把一类商品都归到一起，比如IPhone，Mac这些，然后只要他买了其中一个，我们就推荐其他，当然这是最容易想到的一个方法，但是这个方法成本太高而且只适用了Apple产品，假如你买小米，你的手机可能是小米的，但是你的贴膜可能不是小米的。</p><p>得亏于大数据时代的到来，我们记录了很多用户的购买记录，这就相当于我们知道这些商品存在一些联系，假如我们能把这种联系找出来，再用一些算法搅拌一下数据，我们就能产生香喷喷的“推荐数据”</p><h2 id="推荐系统原理"><a href="#推荐系统原理" class="headerlink" title="推荐系统原理"></a>推荐系统原理</h2><p>首先我们来看一下我们有什么数据，最主要的就三张表，第一张用户表（User表），第二张商品表（Item表），第三张用户和商品交互记录表，这个交互记录可能是评分可能是浏览记录可能是购买记录，我们简称为Rank表</p><p>我们把这三个维度组合起来就是一个二维数组，我们可以用一个矩阵来代表这个，如下面所示</p><p>$$\begin{bmatrix} 5 &amp; 4 &amp; 2 &amp; 1 &amp; 0 \\ 4 &amp; 3 &amp; 1 &amp; 2 &amp; 5 \\ 1 &amp; 1 &amp; 5 &amp; 0 &amp; 2 \end{bmatrix}$$</p><p>有三个用户，5件商品，我们从上面伪造的数据可以看出，用户1、2品味很近，而用户三品味与他们完全不同，所以对于最后一件商品，虽然用户1没有评价或者没有购买，我们可以预测他的评价和用户2 差不多，所以我们可以把东西给推荐给用户1，这个是从我们直觉上来感受的，但是计算机可是没有知觉的，我们必须把这些东西量化才能实现智能推荐</p><p>我们用一个欧几里德距离或者皮尔逊相关度来衡量这种品味，其实你可以把它想象为一个N维空间的上每个点的距离，这里我们就不谈公式，我们直接给结果，对于每个用户来说，自己跟自己的品味是100%相近，其他人则有的相同，有的不相同，所以对于用户1，我们可以算出一个“品味值”（我们其实可以不跟自己算因为绝对是1，但是在大型矩阵运算的过程中，其实少算一个比多算一个要划得来）</p><p>$$\begin{bmatrix} 1 &amp; 0.8 &amp; 0.1 \end{bmatrix}$$</p><p>我们得到上面的品味值（加权值），所以对于最后一个商品，我们把其他用户评价与他们与用户一的值相乘，我们就把他们对数据的评价可靠性给量化下来了，所以就可以下一步排序输出我们的推荐。</p><p>我们知道现在的大数据强大在于能给所有用户都推荐他们想要的数据，但是从我们上面的描述来看，我们为了获取一个用户的品味值就得对数据进行大量运算，如果一个用户要1s的，一亿个用户就得要3年才能算完，所以我们得将我们的for循环改成矩阵运算，才能通过大数据将所以用户的推荐给算出来</p><p>现在我们来谈谈怎么用矩阵运算来加速这个过程，我们用$$R_{UI}$$ 来代替上面的评价矩阵，我们首先要把$$R_{UI}$$ 给倒置，然后让两个做矩阵乘法，得到”品味矩阵“ $$W_{UU}$$</p><p>$$R_{UI}$$ * $$R_{UI}^T $$ = $$W_{UU}$$</p><p>这个$$W_{UU}$$ 是什么呢，他是一个行数为用户总数，列数也为用户总数的方阵，每一行代表，第i个用户，他与其他用户的品味差距，有了这个我们可以将$$W_{UU}$$ 与 $$R_{UI}$$ 做一个矩阵乘法，这样我们就得到一个$$V_{UI}$$矩阵，这个矩阵就是我们对$$R_{UI}$$的一个预测值，我们就把一些用户没有评价的值给预测出现，接下来就对每一行先做一个过滤，取出用户没有购买的，然后把没有购买的按照大小排序输出</p><p>PS：也可以用点乘一下矩阵进行过滤，就是将没有将用户与商品购买记录生产一个UI矩阵，购买过置0，每购买过置1，这样点乘后购买过无论分多高都变成0了，把0过滤就ok了</p><p>我们来总结一下上面用到的矩阵公式</p><ol><li>$$R_{UI}$$ * $$R_{UI}^T $$ = $$W_{UU}$$</li><li>$$W_{UU}$$ * $$R_{UI}$$ = $$V_{UI}$$</li></ol><p> 初略一看没有啥问题，但是你要想一下假如用户数非常大，有一千万（很容易达到，QQ微信都10几亿了），那个$$W_{UU}$$ 矩阵的大小有 1千万 * 一千万 = 1e+14 ，我们根本没有足够的内存来存放这个矩阵，而且有一个问题，就是其实假如我们有一千万件商品，但是一个用户一辈子可能就买了几十件，这是一个巨大的稀疏矩阵</p><p>所以虽然我们这个矩阵算法很高效，能通过两次运算就把用户推荐给全部输出，但是假如用户量多或者商品多第一内存不够，第二个矩阵太稀疏了，在没有大数据之前或者计算机还没有现在这么强的时候，大家的解决方法是对数据进行抽样，把大数据分解成为小数据，但是大数据的魅力就是使用全部数据而不是部分，所以我们要使用一些抽象的算法来帮助我们把这个问题给解决了</p><p>首先我们还是要用所以的用户数据，所以的商品数据，但是现在我们做一个抽象，我们假设用户身上有几十种特征来决定他们的品味（比如年龄，性别，性格等），我们直接跳过前面两个计算步骤，我们假设用户特征矩阵$$U_{UP}$$ (每个用户有P个特征，决定他们品味)，对于商品，我们假设就是因为这些特征导致的一个商品特征矩阵$$I_{PI}$$</p><p>最后我们假设，我们这个$$U_{UP}$$ 矩阵乘法$$I_{PI}$$就是 我们上面得到的$$V_{UI}$$ ，也就是公式为</p><p>$$U_{UP}$$ *$$I_{PI}$$ = $$V_{UI}$$ </p><p>假如我们能计算出来这个 $$U_{UP}$$和 $$I_{PI}$$，那么我们就基本上算出所以用户的评价了，这个就是ALS的原理，我们利用这个抽象能将原来的亿亿相乘的计算，改成亿与常数P的相乘的计算</p><p>当然具体的原理没有这么简单，涉及到很多矩阵运算的知识，我这里就不多说了，具体可以看下面给的论文《Collaborative Filtering for Implicit Feedback Datasets》，Spark的Mlib中的ALS实现就是基于这篇论文的，也可以参考一下Python的实现，公式推导比较复杂，但是实现还是非常好理解的</p><h2 id="Python实战"><a href="#Python实战" class="headerlink" title="Python实战"></a>Python实战</h2><p>首先我们来先试试使用Python来快速搭建我们的模型出来</p><pre><code>{&apos;account&apos;: &apos;1920286160&apos;, &apos;content&apos;: &apos;你的笑容灿烂了一整个夏天!&apos;, &apos;source&apos;: &apos;http://wp1.sina.cn/wap240/5d738637jw1dpmonay1thj.jpg&apos;, &apos;time&apos;: &apos;2012-02-01 13:48:39&apos;, &apos;from&apos;: &apos;网页版&apos;, &apos;comment&apos;: 0, &apos;forward&apos;: 0, &apos;forward_account&apos;: &apos;1567852087&apos;, &apos;forward_content&apos;: &apos;民浩君！！ &apos;,&apos;forward_id&apos;: &apos;xdasfd21x-s&apos;， &apos;forword_nickname&apos;: &apos;全球流行风尚&apos;, &apos;is_forward_followed&apos;: &apos;0&apos;}</code></pre><p>我们原始数据全部都是这种字符串，我们没有用户点赞、浏览数据，我们只有用户所发的所以微博数据，而且假如后面有<code>forward_account</code>的值的话，说明这条微博是用户转发的，所以这里我们把用户转发当做一个用户“喜欢”这篇微博，我们系统就是要推荐更多微博给用户“转发”，所以我们把用户账号，和转发ID提取出来当做一次“记录”</p><p>我们把数据全部变成，<code>user-id：forward_id</code>，字符对，然后我们导入到一个<code>pandas</code> 的<code>DataFrame</code>里面去</p><p>我们这里使用一个Python 实现的ALS包<code>implicit</code>，只需要把我们生成的<code>DataFrame</code>转换成为一个压缩矩阵OK了，但是这里要注意的一点是我们所以的用户id和转发id都得变成Int型整数，最好的做法使用</p><pre><code>df.user.astype(&apos;category&apos;).cat.codes</code></pre><p>比如上面，我们把<code>user</code>这个字符型数字转换成一个整数序号，这样我们得到的就是每个<code>user</code>id的序号</p><p>完整的代码为下面</p><pre><code>origin_data = [(x[&apos;forward_id&apos;], x[&apos;account&apos;], 1) for x in data if x.get(&apos;forward_id&apos;)] # data  为json数组df = pd.DataFrame(origin_data, columns=[&apos;item&apos;, &apos;user&apos;, &apos;num&apos;])df[&apos;item&apos;] =  df.item.astype(&apos;category&apos;).cat.codes.astype(&apos;int64&apos;)df[&apos;user&apos;] = df.user.astype(&apos;category&apos;).cat.codes.astype(&apos;int64&apos;)from scipy import sparse# convert df to martrixitem_user_data = sparse.csr_matrix(df.values)# trainimport implicitmodel = implicit.als.AlternatingLeastSquares(factors=50)model.fit(item_user_data)# recommend items for a useruser_items = item_user_data.T.tocsr()# recommend usermodel.recommend(1, user_items=user_items, N = 5)</code></pre><p>只要简单的几行代码我们就实现了一个协同过滤的算法实现，从Python代码我们可以知道，最关键的地方就是生成训练数据，然后其他都交给算法了，接下来我们来尝试实现一个使用Spark加载全部数据，来实现对几十亿微博数据的推荐</p><h2 id="Spark实战"><a href="#Spark实战" class="headerlink" title="Spark实战"></a>Spark实战</h2><p>在前面我们把3个T的微博数据的用户转发数据抽取出来，最后数据量在35亿左右，数据量在100G左右，这个数据量还是有的大的，首先内存肯定放不下，假如我们使用前面Python的方式来实现的话，我们的内存至少要200个G，所以我们得使用Spark的内存加磁盘运算来实现这个推荐系统</p><h3 id="生成用户和转发ID的序列值"><a href="#生成用户和转发ID的序列值" class="headerlink" title="生成用户和转发ID的序列值"></a>生成用户和转发ID的序列值</h3><p>前面Python的实现100个G的数据排序，基本的原理就是Hash表进行排序，然而我们内存不足以放下这100G的数据，所以我们要使用MapReduce来“消化”这个数据，所以我们要借助Hive来进行一个排序，一方面我们可以生成用户和物品的唯一ID，等我们产生推荐结果后也可以通过HIVE来生成对应的数据</p><p>第一步我们将数据导入到HIVE中</p><blockquote><p>首先创表</p></blockquote><pre><code>create table record(item string, account int, time int) row format delimited fields terminated by &apos;,&apos;   LINES TERMINATED BY &apos;\n&apos;  stored as textfile;</code></pre><p>这里我还额外导入了一个时间进去，为的是方便以后做线上预测</p><p>然后我们把文件导入进去（文件存贮在<code>/weibo/all_data.txt</code>中）</p><pre><code>load data  local inpath &apos;/weibo/all_data.txt&apos; into table record;</code></pre><p>接下来我们生成用户表和物品表</p><pre><code>create table items as select row_number() over (order by a.item) as id ,a.item from (select distinct item from record) a ;  create table users as select row_number() over (order by a.account) as id ,a.account from (select distinct account from record) a ;</code></pre><p>我们使用HIVE的<code>row_number</code>函数来生成一个唯一序列自增值，每一个值对应一个用户，最后我们生成我们训练表，最好使用<code>HIVE2</code>来进行导入，因为HIVE1的<code>row_number</code>会存贮所以的id值，由于我们数据量有35个亿，内存会爆炸，<code>HIVE2</code>优化了使用一个流来存贮</p><pre><code>  create table train as select account_id, item_id, count(*) as num from (select a.id as item_id, b.id as account_id from  record left join items a on record.item = a.item left join users b on record.account = b.account) a group by item_id, account_id ;</code></pre><p>接下来我们在Spark里面调用我们Mlib的<code>ALS</code>模型，首先我们从HIVE中获取数据</p><pre><code>val df = sql(&quot;select * from train&quot;)</code></pre><p>然后我们从将数据导入ALS模型进行训练，同Python比较类似，我就直接给出代码</p><pre><code>val ratings = df.map(row =&gt; Rating(row.getAs[Int](&quot;item_id&quot;).toInt, row.getAs[Int](&quot;account_id&quot;).toInt, row.getAs[Long](&quot;num&quot;).toDouble)).rddval rank = 10val numIterations = 50val model = ALS.train(ratings, rank, numIterations, 0.01)val usersProducts = ratings.map { case Rating(user, product, rate) =&gt;  (user, product)}val predictions =  model.predict(usersProducts).map { case Rating(user, product, rate) =&gt;    ((user, product), rate)  }val ratesAndPreds = ratings.map { case Rating(user, product, rate) =&gt;  ((user, product), rate)}.join(predictions)val MSE = ratesAndPreds.map { case ((user, product), (r1, r2)) =&gt;  val err = (r1 - r2)  err * err}.mean()println(s&quot;Mean Squared Error = $MSE&quot;)</code></pre><p>最终我们这35亿数据用时4个半小时跑完模型并计算出MSE （均方误差），当然由于我的核心比较小，假如在一个大集群上面的话速度会快很多，值得注意的是，在Spark运行这35亿数据的时候，物理磁盘占用达到800多个G，所以在哪里省下来（内存）的就得从哪里还回来（磁盘）O(∩_∩)O哈哈~</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>我们这次只是完成了一个非常简陋的推荐系统，只是具备简单的推荐功能，我们还没有考虑到一些“冷启动”问题，一个“智能”的推荐系统需要考虑到很多方面，我的推荐系统还有很多地方需要完善，但是从我的这个Demo里面也可以看到其实大数据也是非常“简单”的，由于数据量太大，你只能使用最简洁的代码来训练你的模型，你的算法要最大可能性满足分布式要求，推荐系统最核心的地方还是如何将你的推荐算法变成一个优秀的分布式算法，我们这篇博客只是走马观花的谈了一下它的原理，看看有空好好研究一下Spark分布式算法是怎么实现的。</p><h2 id="资料"><a href="#资料" class="headerlink" title="资料"></a>资料</h2><p><a href="https://blog.csdn.net/antkillerfarm/article/details/53734658" target="_blank" rel="noopener">ALS原理介绍</a></p><p><a href="https://yq.aliyun.com/articles/684195" target="_blank" rel="noopener">Python实现ALS</a></p><p><a href="http://yifanhu.net/PUB/cf.pdf" target="_blank" rel="noopener">Spark Mlib ALS 论文</a></p>]]></content>
      
      
      <categories>
          
          <category> 人工智能 </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>TB级数据存贮经验总结</title>
      <link href="2019/04/13/bigdata/TB%E7%BA%A7%E6%95%B0%E6%8D%AE%E5%AD%98%E8%B4%AE%E7%BB%8F%E9%AA%8C%E6%80%BB%E7%BB%93/"/>
      <url>2019/04/13/bigdata/TB%E7%BA%A7%E6%95%B0%E6%8D%AE%E5%AD%98%E8%B4%AE%E7%BB%8F%E9%AA%8C%E6%80%BB%E7%BB%93/</url>
      
        <content type="html"><![CDATA[<blockquote><p>项目背景</p></blockquote><p>这个项目是深圳一家证卷公司一个TB级日志离线导入项目，当数据达到10T左右的时候，他们的存贮架构以及检索系统直接奔溃，在朋友的推荐下，我负责对这个项目进行整体的重构以及优化，对于我一个大数据新手来说这是一个挑战也是一个学习的机会，最终幸不辱命最终完成了系统的优化，由原来的48小时导入优化至7个小时，并且提供亚秒级的查询检索，下面就是我对这个项目的总结</p><h2 id="历史方案介绍"><a href="#历史方案介绍" class="headerlink" title="历史方案介绍"></a>历史方案介绍</h2><blockquote><p>集群架构</p></blockquote><p>集群由8台高性能服务器组成，其中两台负责接收历史日志包，两台提供ES和Spark服务，其他四台提供HDFS存贮服务，总存贮量在200TB，其中我们导入主要在两台高性能服务器上面，每台内存256GB，总计512G内存</p><blockquote><p>历史方案</p></blockquote><p>原来的解决方案是日志被解析完成之后导入两份数据分别存贮到ElasticSearch（下面简称ES）和Kafka之中，其中存贮到Kafka之中的数据会被Spark读取，然后写入到HBase</p><h2 id="架构优化"><a href="#架构优化" class="headerlink" title="架构优化"></a>架构优化</h2><h3 id="读取和存贮优化"><a href="#读取和存贮优化" class="headerlink" title="读取和存贮优化"></a>读取和存贮优化</h3><p>历史解决方案对日志数据的存贮是一个并行化的，但是由于项目对数据准确性要求严格，假如其中ES导入或者HBase导入失败之后，所以的数据都得重新生成，但是原来架构不提供删除HBase数据的功能（能实现但是不能检测是否删除），所以在读取方面我丢弃使用Kafka并行导入的方案，待ES导入完成并且校对完成后再从ES中获取数据导入到HBase中</p><p>原来的导入是直接使用HBase提供的Put方案，单个插入到HBase当中，由于我们一次导入的量多的话大概在10个亿左右，虽然HBase能够承受住压力但是由于行键设计导致会造成region的”脑裂”，原来解决方案为了避免这个现象进行了region的预分配，由于这个预分配导致了另外的一个Bug，后面会提到。所以为了提高导入效率和避免脑裂，我采用了HBase的Bulk Load将数据直接”存”至HBase</p><hr><p>回头看看，其实大数据解决方案非常简单，当我刚接触项目的时候，在看原来的解决方案，惊讶的发现他们的代码其实非常简单，就像教程里面的demo一样，但是随着在实际接触这个”大”数据的过程中，大数据考验的是你代码的”结实”，能不能扛的住，什么花里胡哨的骚操作都没有，所以大数据的代码非常简洁质朴，如果说那些优雅的代码是面对对象的话，那么大数据的代码就是面对数据的。所以我接下来也不谈大数据框架，谈一谈我在这个项目中学到的大数据处理经验</p><h2 id="提高处理速度"><a href="#提高处理速度" class="headerlink" title="提高处理速度"></a>提高处理速度</h2><blockquote><p>读取速度</p></blockquote><p>大数据处理最核心的点就是分而治之，也就是我们常说的<code>MapReduce</code>中<code>Map</code>，无论数据有多大，只要我们分的够小，每一份我们计算机都很很快处理，那么我同时运行这些，意味着我就能在一个很短的时间内跑一个巨大的数据。</p><p>由于我们是从ES中读取数据，ES的Map的个数是由分片（shards）来控制的ES的分片默认是十片（5个原始5个备份），假如我们采用默认设置，我们读取数据Map的大小最大为10（具体跟你给的Container大小和每个核心数的大小有关，给足的话就是10），这里要谈一下这个项目中的分片设计，分片这个东西对于每一个Index来说，不是越多越好，越多读取速度快，但是写入慢，反之。所以这个项目使用别名的方法来保证分片不会膨胀，当一个Index数据大于20GB的时候，会自动新建一个别的Index，比如原来是<code>es _index_0</code>，写满20G后，再新建一个 <code>es_index_1</code>，然后把新数据导入到里面去，然后每个Index固定分片为4片，这样所有的数据会均匀的分布到ES中去</p><p>这种解决方法带来一个问题就是，我们获取数据的时候只能通过<code>es_index_*</code>，而且我们的ES中每次导入的<code>Map.size</code>只能是4，所以无论给<code>Spark</code>任务多少个<code>Container</code>多少个核心，他的Task任务最大也就是4个，一开始我一直在琢磨是否能够提高这个Task来加速数据读取，我尝试使用ES时间索引把数据切分成多份，并行跑多个Spark任务，但是无论我并行多少个，从ES一个月导出来时间总和单个Spark任务读取速度一致</p><p>出现这个问题的原因主要是磁盘的性能关系，由于Bulk Load有一个很重要的部分就是Shuffle排序，所以数据在导入HBase之前一定会落盘，所以读取由两个条件限制，ES的获取速度和磁盘写入速度，由于ES读取速度也跟磁盘读取速度有关，所以读取基本上由磁盘的性能有关，由于集群Spark跑在机械硬盘上，除非换SSD否则在软件上优化无法取得较大的收益</p><p>所以我放弃在读取上面优化，接下来介绍我在处理上面的优化</p><blockquote><p>处理速度</p></blockquote><p>其实导入到HBase的原理非常简单，只要使用Spark生成一个HFile然后调用HRegion将数据导入到HBase中去，HFile你可以看着一个排序好的文件，但是如何将1TB数据排序并在内存中生成HFile文件（可能比1TB小如果采用压缩的话，不压缩比原始文件要大得多）这个步骤就很复杂了</p><p>Spark作为大数据界的神兵利器，处理PB级的数据也不在话下，但是怎么处理呢，简单来说就是三个和尚挑水，现在缸很大，但是和尚很多，如果要像迅速把缸里的水挑完，那就让很多和尚通力合作，同时去挑，但是如果里面10个和尚，9个是不干活，只有一个挑水，那么这个任务就花很长时间来完成，这个就是我们说的数据倾斜</p><p>所以其实提高处理速度的方法只有一个把任务均分给CPU每个核心，接下来我就这个项目来谈一谈我遇到的数据倾斜问题</p><p>首先我们要知道一个前面我们讲了Spark作为大数据上的神兵利器，他不会假设用户内存能装下所有的数据，但是用户的磁盘一定能装下所有的数据，我们其实要Spark做的最主要的一件事就是对数据的RowKey（HBase的行键）进行排序，然后把排好序的文件导入到HBase中，排序这个东西最主要的是要把所有的数据都放到一起排一遍，也就是说1个T的数据至少都得放到内存或者磁盘里面来进行排序</p><p>我们接下来缕缕Spark处理数据的流程，其实你可以在UI界面上看到所以的stage，前面读取数据算一个Map stage，接下来我们要完成，排序的stage，还有保存数据的stage，按照正常流程，每个stage都会做一个Shuffle，也就是理论上我们总共要做两次Shuffle，然而TB级数据的Shuffle在机械硬盘上的速度非常慢（相比SSD），所以我们就的优化Shuffle过程</p><p>我们先来谈一下一个正常大数据的排序怎么样，我们用打擂台来打比方，首先我们将所有选手分成10个小组，等小组赛打完，所以等小组都分好名次，然后我们搞一个10个人的擂台，小组第一名全部上擂台打比赛，打完决定出第一名，第一名成为擂主出擂台，然后这个时候再从其他其他小组中找一个积分最高的上来，继续打擂台，打完再找出第一名，继续上面的循环，最后每个人都出了擂台，名次也排练出来了</p><p>这个排序方法是Spark早期使用的方法，这个方法可以面对超大数据，假设数据有10个T，我只有1M内存，我也能完成数据的排序，只是每次排1M，然后写文件，这种方式可以面对海量数据，但是有一个弊端，在排完序后每次必须要把所有的文件全部都打开读取”第一名”，也就是说我们做完一次Map之后，还的进行一次Reduce</p><p>那怎么来去掉这个Reduce呢，考虑这种情况，我们在分组的时候，由一个”裁判”把每个人公正的计算他原来的历史积分在什么档次，然后把他分在那个档次的组，等所有组打完，排名就出来了，你属于哪个档第几名，也就知道你的排名了</p><p>这个方法就是预先评估你的组数，但是这个也有一个问题，假如选手水平层次不齐，强者太少，弱者太多，那么有些组非常多的人打比赛，有的组没有人打比赛，这样，最后总耗时是那个人最多的小组的时间</p><p>接下来我们回到上面的Shuffle过程，如果我们把那个排序过程使用上面的预估方法，那么我们就能在减少一次Shuffle过程，但是这个也带来了一个风险，就是预估如果失败（HBase会加载HFile失败）或者预估不均会导致运行异常</p><p>所以我采用Spark提供的<code>sortWithinPartitions</code>方法来ES数据进行一个分区并排序，分区的方法是通过账号ID来进行，很不幸线上数据部分账号异常，一个月某些账号占了总数的40%左右，这个就导致我们预估不均，那如何来解决这个问题呢，解决方案由很多，由于我们数据只有少量资金账户异常，所以我们采用最简单的方案，分多批次跑，每个任务分剔除大账户的部分，其他的任务就是单个大账户单独跑，得益ES强大的统计功能，只需要执行几次HTTP请求就能完成设计</p><hr><p>至此整个数据优化导入部分就说完了，当然还有其他优化的地方没有讲，比如使用堆外内存加速排序，使用更大的Shuffle cache优化写入，切分更小的块避免磁盘split …..</p><p>PS：亚秒级的查询主要通过Spark一个聚合生成HBase一张小表来实现，当然也可以使用MySql这些关系型数据库</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>记得我以前去面试大数据岗位的时候，我问面试官怎么才能提高自己大数据能力，他说必须要通过实践才能体会，经过这次项目我也感受到了，只有真正的面对大数据，你才能提高自己的大数据能力，小数据一下子就跑完了，完全体会不到内存OOM的”乐趣”，大数据的难点在于了解数据如何被处理，以及对大数据各个组件“协作”对经验。</p>]]></content>
      
      
      <categories>
          
          <category> 大数据 </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>重构-总结</title>
      <link href="2019/03/17/summary/book/%E9%87%8D%E6%9E%84-%E6%80%BB%E7%BB%93/"/>
      <url>2019/03/17/summary/book/%E9%87%8D%E6%9E%84-%E6%80%BB%E7%BB%93/</url>
      
        <content type="html"><![CDATA[<p><img src="/images/重构-思维导图.png" alt="重构思维导图"></p>]]></content>
      
      
      <categories>
          
          <category> 随想 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 读后感 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>代码整洁之道-总结</title>
      <link href="2019/03/13/summary/book/%E4%BB%A3%E7%A0%81%E6%95%B4%E6%B4%81%E4%B9%8B%E9%81%93-%E6%80%BB%E7%BB%93/"/>
      <url>2019/03/13/summary/book/%E4%BB%A3%E7%A0%81%E6%95%B4%E6%B4%81%E4%B9%8B%E9%81%93-%E6%80%BB%E7%BB%93/</url>
      
        <content type="html"><![CDATA[<blockquote><p>很早就听说过思维导图，但是怎么去画，怎么让它帮助你去学习，我以前不得其道，以为就像记老师讲的笔记一样，尝试几次后感觉无用就弃之高阁了，这次在知乎上看到一篇回答感觉很是在理，就实践了一番</p></blockquote><p>下面是原答案<a href="https://www.zhihu.com/question/20273625/answer/531743073" target="_blank" rel="noopener">思维导图真的有效吗？ - YJango的回答 - 知乎</a></p><p>我按照这个方法重新把&lt;&lt;代码整洁之道&gt;&gt;读了一篇，受益匪浅，下面是我的思维导图</p><p><img src="/images/代码整洁之道-思维导图.png" alt="思维导图"></p><p>一千个人有一千个哈莫雷特，所以我就不写自己的看法了</p>]]></content>
      
      
      <categories>
          
          <category> 随想 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 读后感 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>如何在Ubuntu上发布Scala Jar包到Maven中央仓库</title>
      <link href="2019/01/13/java/%E5%A6%82%E4%BD%95%E5%9C%A8Ubuntu%E4%B8%8A%E5%8F%91%E5%B8%83ScalaJar%E5%8C%85%E5%88%B0Maven%E4%B8%AD%E5%A4%AE%E4%BB%93%E5%BA%93/"/>
      <url>2019/01/13/java/%E5%A6%82%E4%BD%95%E5%9C%A8Ubuntu%E4%B8%8A%E5%8F%91%E5%B8%83ScalaJar%E5%8C%85%E5%88%B0Maven%E4%B8%AD%E5%A4%AE%E4%BB%93%E5%BA%93/</url>
      
        <content type="html"><![CDATA[<blockquote><p>由于网上的教程大多数是<code>Windows</code>下的，而且都是介绍怎么打包<code>Java</code>的<code>Jar</code>包，关于<code>Scala</code>相关的比较少，因此我踩了不少坑才成功的把包发布到<code>Maven</code>中央仓库，你们可以才中央仓库里面搜到<a href="https://search.maven.org/artifact/me.zhanglun.ahocorasick/trie/1.0.5/jar" target="_blank" rel="noopener">我的包</a></p></blockquote><p>如何你想直接使用<code>ctrl-c</code> + <code>ctrl-v</code>那么你直接去我的github上的<a href="https://github.com/mrzhangboss/ahocorasick/tree/MiniDeployToSonatypeDemo" target="_blank" rel="noopener">最小化打包版本</a>把<code>pom.xml</code>里面的<code>build</code>给复制下来就可以了（前提是你已经把<code>gpg</code>给配置好了）。</p><p>PS：因为这篇主要是介绍步骤所以就用中文写了</p><h2 id="第一步：选择用什么打包"><a href="#第一步：选择用什么打包" class="headerlink" title="第一步：选择用什么打包"></a>第一步：选择用什么打包</h2><p>由于<code>Scala</code>既可以选择用<code>SBT</code>打包，又可以用<code>maven</code>打包，所以你要先选择使用什么来打包，一开始我使用<code>SBT</code>来尝试进行打包上传，但是一直卡在没法配置好<code>SBT</code>的<code>gpg</code>插件（这个主要是对你的文件进行签名），我甚至到<code>Github</code>去看别人的项目是怎么配置的，但是我搜到的<code>Scala</code>项目大部分都没有选择把包发到<code>sonatype</code>去，有的是自己搭建中央仓库有的是根本没有提供中央仓库下载的。虽然我很想用<code>SBT</code>把<code>Scala</code>打包（毕竟是专门给<code>Scala</code>用的），但是那个<a href="https://www.scala-sbt.org/sbt-pgp/" target="_blank" rel="noopener">插件</a> 实在是卡的死死的，而且文档太少了，官网上的SBT似乎一直是用的0.13.5+，但是我用的是1.2+，在选择低版本还是选择换一种打包方式面前我选择听从大牛的意见，选择使用<code>maven</code>进行对<code>Scala</code>打包（毕竟<code>Scala</code>杀手级应用<code>Spark</code>也是用的<code>maven</code>进行打包）</p><p>而且<code>SBT</code>有一个特别困扰我的地方，虽然我一直用阿里源，但是<code>SBT</code>更新依赖的速度实在是太慢了，无论是新建项目还是增加第三方包，相比<code>maven</code>它的速度都特别慢</p><p>当然我做不到不代表大家做不到，如果有人能够搞定用<code>Maven</code>打包，记得在<code>Github</code>上发个<code>issue</code>告诉我</p><h2 id="第二步：在中央仓库上面新建项目"><a href="#第二步：在中央仓库上面新建项目" class="headerlink" title="第二步：在中央仓库上面新建项目"></a>第二步：在中央仓库上面新建项目</h2><p>为了将包发到<code>maven</code>中央库，我们得借助【sonatype]<a href="https://issues.sonatype.org" target="_blank" rel="noopener">4</a>来帮我们上传，首先你得去上面注册一个用户</p><p>相比于<code>Python</code>发包，<code>Java</code>有一个<code>group</code>的概念，也就是说每个包都属于不同的组织，比如说Java的核心包<code>java.lang</code>也可以看做是一个组织，在<code>Pypi</code>发包你只要不重名就行，但是在<code>maven</code>中央库发包，你可以重名但是不能重复组织名，由于我有个<code>zhanglun.me</code>的域名，所以我就新建了一个<code>me.zhanglun.ahocorasick</code>组织，你也可以理解组织就是一个网址，我新建了一个<code>ahocorasick.zhanglun.me</code>的网址，由于名字我们没法规定重名，但是网址可以，所以在<code>sonatype</code>上新建一个项目后你得要有帮你审核一下这个域名是否是你的（如果你使用<code>com.github.xxx</code>来作为域名的话那就不要审核了）</p><p>随便找篇<a href="https://www.sojson.com/blog/250.html" target="_blank" rel="noopener">教程</a>新建完项目，等你的项目变成<code>Resolved</code>，接下来我们就来配置上传的秘钥</p><h2 id="第三步：-上传配置"><a href="#第三步：-上传配置" class="headerlink" title="第三步： 上传配置"></a>第三步： 上传配置</h2><p>相比于<code>Python</code>直接将打包好的egg包发到<code>pypi</code>，<code>sonatype</code>需要你对你上传的文件都签名以验证安全，怎么签名呢，借助一个开源的<a href="https://www.gnupg.org/" target="_blank" rel="noopener">GnuPG</a>，我们只要自己制作一个秘钥，然后上传到秘钥服务器，然后我们就可以用这个秘钥来对我们的文件进行签名（会产生一个<code>.asc</code> 文件，里面是签名值）</p><p>所以关键就是把这个秘钥产生并且上传到秘钥服务器，在<code>Ubuntu</code>上有两个版本的<code>GnuPG</code>，一个是1.0版一个是2.0版，我试过使用1.0版但是没法传到秘钥服务器上面，所以大家还是最好使用2.0来生成秘钥和上传到服务器。</p><p>首先生成key，全部选择默认进行，填上你的个人信息，最后就会生成一个key</p><pre><code>gpg2 --gen-key </code></pre><p>里面会让你输入一个口令，你记住就行，等你上传的时候会弹出一个框让你输入这个口令</p><p>生成完后使用下面命令列出key</p><pre><code>   gpg2 --list-key/home/zhanglun/.gnupg/pubring.gpg---------------------------------pub   rsa2048/47DC71B6 2019-01-12 [SC]uid         [ultimate] zhanglun &lt;zhanglun.me@gmail.com&gt;sub   rsa2048/985EE474 2019-01-12 [E]pub   rsa2048/D0516023 2019-01-12 [SC]uid         [ultimate] zhanglun &lt;zhanglun.me@gmail.com&gt;sub   rsa2048/EE34357C 2019-01-12 [E]</code></pre><p>我生成好几个没有关系，随便找到一行pub ，例如第三行的，<code>47DC71B6</code>就是你得公钥</p><p>使用下面命令把秘钥发到秘钥服务器（keyserver你可以在网上随便找一个，只有能用就会同步到全球）</p><pre><code>gpg2 --keyserver pool.sks-keyservers.net --send-keys  47DC71B6</code></pre><p>接下来我们就要配置<code>maven</code>来使用这个秘钥，首先在pom.xml里面加上这个插件</p><pre><code>&lt;plugin&gt;    &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt;    &lt;artifactId&gt;maven-gpg-plugin&lt;/artifactId&gt;    &lt;version&gt;${version.maven.plugin.gpg}&lt;/version&gt;    &lt;executions&gt;        &lt;execution&gt;            &lt;id&gt;sign-artifacts&lt;/id&gt;            &lt;phase&gt;verify&lt;/phase&gt;            &lt;goals&gt;                &lt;goal&gt;sign&lt;/goal&gt;            &lt;/goals&gt;        &lt;/execution&gt;    &lt;/executions&gt;&lt;/plugin&gt;</code></pre><p>它能自动帮你使用<code>gpg</code>来进行签名（默认使用<code>gpg</code>待会我们得改成<code>gpg2</code>，否则会报错），这里要注意一点假如你计算机上面有很多秘钥，如果你不指定那个秘钥来进行签名那也会报错，我们要在<code>~/.m2/settings.xml</code> 上配置一下（注意不是你项目目录下面）</p><p>在<code>settings.xml</code>的<code>settings/profiles</code>节点下面下面加上配置</p><pre><code>&lt;settings&gt;      &lt;profiles&gt;            ....            &lt;profile&gt;            &lt;id&gt;oss&lt;/id&gt;            &lt;activation&gt;            &lt;activeByDefault&gt;true&lt;/activeByDefault&gt;            &lt;/activation&gt;            &lt;properties&gt;            &lt;gpg.executable&gt;gpg2&lt;/gpg.executable&gt;            &lt;gpg.keyname&gt;47DC71B6&lt;/gpg.keyname&gt;            &lt;/properties&gt;            &lt;/profile&gt;             ..... &lt;/profiles&gt;&lt;/settings&gt;</code></pre><p><code>id</code>就是这个配置的名字，<code>gpg.keyname</code>就是秘钥的名字，<code>gpg.executable</code>就是选择使用<code>gpg2</code>（默认使用1.0版本也就是<code>gpg</code>）</p><p>配置完这个你可以<code>mvn install</code>一下，你可以看到生成的<code>jar</code>都已经有一个<code>.asc</code>文件，接下来就是配置<code>sonatype</code>的账号密码，在<code>pom.xml</code>配置文件里面加上一个</p><pre><code>&lt;distributionManagement&gt;    &lt;snapshotRepository&gt;        &lt;id&gt;oss&lt;/id&gt;        &lt;url&gt;https://oss.sonatype.org/content/repositories/snapshots&lt;/url&gt;    &lt;/snapshotRepository&gt;    &lt;site&gt;        &lt;id&gt;master&lt;/id&gt;        &lt;name&gt;Efficient longest keyword string matching&lt;/name&gt;        &lt;url&gt;git@github.com:mrzhangboss/ahocorasick.git&lt;/url&gt;    &lt;/site&gt;    &lt;repository&gt;        &lt;id&gt;oss&lt;/id&gt;        &lt;url&gt;https://oss.sonatype.org/service/local/staging/deploy/maven2/&lt;/url&gt;    &lt;/repository&gt;&lt;/distributionManagement&gt;</code></pre><p>在这里我们指定了用<code>oss</code>这个作为上传账号，接下啦我们在<code>~/.m2/settings.xml</code> 加上<code>sonatype</code>的用户名和密码</p><pre><code>&lt;settings&gt;        &lt;servers&gt;            ....            &lt;server&gt;              &lt;id&gt;oss&lt;/id&gt;              &lt;username&gt;your usename&lt;/username&gt;              &lt;password&gt;your password&lt;/password&gt;            &lt;/server&gt;            ....  &lt;/servers&gt;&lt;/settings&gt;</code></pre><p>在<code>settings/servers</code>节点下加入一个<code>server</code>节点，填上你的账号密码就ok了，在这里你就可以使用<code>mvn clean deploy</code>进行上传了。</p><h2 id="第四部：发布配置"><a href="#第四部：发布配置" class="headerlink" title="第四部：发布配置"></a>第四部：发布配置</h2><p>前面我们已经能够上传，但是我们现在上传的时候假如我们的版本号上面有<code>SNAPSHOT</code>这个的话，我们是不能把他发布出去的，带有那个代表那个只是实验性质，虽然我们能够把它上传上去但是不能在<code>maven</code>中央仓库里面看到。</p><p>当然这个只是一个要求，为了能够上传你还必须满足两个条件，第一个要把源文件上传上去，第二个就是你的文档上传上去，而且<code>sonatype</code>采用了一个工作流的概念，你要上传中央库必须要进过 上传release-&gt; 关闭release -&gt; 发布release，当然所以的一切都可以用插件来完成，接下啦我们先介绍两个很重要的插件，打包源文件和打包文档。</p><p>由于我们要打包<code>Scala</code>项目，所以第一步就是把<code>Scala</code>编译成<code>Java</code>字节码，这一步我们得借助<code>scala-maven-plugin</code>来完成</p><p>我们在在<code>build</code>里面加入下面的插件</p><pre><code>&lt;plugin&gt;    &lt;groupId&gt;net.alchim31.maven&lt;/groupId&gt;    &lt;artifactId&gt;scala-maven-plugin&lt;/artifactId&gt;    &lt;version&gt;${version.scala.maven.plugin}&lt;/version&gt;    &lt;executions&gt;        &lt;execution&gt;            &lt;goals&gt;                &lt;goal&gt;compile&lt;/goal&gt;                &lt;goal&gt;testCompile&lt;/goal&gt;            &lt;/goals&gt;        &lt;/execution&gt;        &lt;execution&gt;            &lt;id&gt;attach-javadocs&lt;/id&gt;            &lt;goals&gt;                &lt;goal&gt;doc-jar&lt;/goal&gt;            &lt;/goals&gt;        &lt;/execution&gt;    &lt;/executions&gt;&lt;/plugin&gt;</code></pre><p>它带了连个命令一个是<code>compile + testCompile</code>（编译），一个是<code>attach-javadocs</code>（打包<code>scaladoc</code>），加入这个插件后你可以试一下<code>mvn install</code> 它会帮你把<code>javadoc.jar</code>生成出来。</p><p>添加<code>source</code>文件的插件就简单了，<code>Java</code>和<code>Scala</code>都可以使用这个插件来生成<code>jar</code>文件（只是简单的复制文件而已）</p><p>加入下面这个插件就能实现打包源文件了</p><pre><code>&lt;plugin&gt;    &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt;    &lt;artifactId&gt;maven-source-plugin&lt;/artifactId&gt;    &lt;executions&gt;        &lt;execution&gt;            &lt;id&gt;attach-sources&lt;/id&gt;            &lt;goals&gt;                &lt;goal&gt;jar&lt;/goal&gt;            &lt;/goals&gt;        &lt;/execution&gt;    &lt;/executions&gt;&lt;/plugin&gt;</code></pre><p>加上这两个插件你就可以发布<code>release</code>版本了（第一次你得要跟工作人员comment一下才能上传到中央库，以后就不要了）。现在你就可以只要执行<code>mvn clean deploy</code>就能在<a href="https://oss.sonatype.org/#welcome" target="_blank" rel="noopener">管理页面</a>的Staging Repositories下面看到你构件，你只要选择最后一个，然后点击<code>close</code>，<code>close</code>结束之后在点击<code>release</code>就能在<code>maven</code>中央库看到你的开源包了，是不是很激动</p><p>但是每次都要登录上面去点击很繁琐，所以你可以安装下面的插件帮你直接发布，不需要登录网站</p><p>加上下面的插件就能自动帮你发布了</p><pre><code>&lt;plugin&gt;    &lt;groupId&gt;org.sonatype.plugins&lt;/groupId&gt;    &lt;artifactId&gt;nexus-staging-maven-plugin&lt;/artifactId&gt;    &lt;version&gt;${version.maven.plugin.nexus.staging}&lt;/version&gt;    &lt;extensions&gt;true&lt;/extensions&gt;    &lt;configuration&gt;        &lt;serverId&gt;oss&lt;/serverId&gt;        &lt;nexusUrl&gt;https://oss.sonatype.org/&lt;/nexusUrl&gt;        &lt;autoReleaseAfterClose&gt;true&lt;/autoReleaseAfterClose&gt;    &lt;/configuration&gt;&lt;/plugin&gt;</code></pre><p>PS：我在<a href="https://github.com/mrzhangboss/ahocorasick/tree/MiniDeployToSonatypeDemo" target="_blank" rel="noopener">项目</a>里面把这个插件注释掉了，你们可以把注释取消</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>这篇博文主要是把发布涉及到的库以及知识点简单的介绍了一下，其实发布并不难，很多时候我们只是卡在某一步上，只要我们知道原理，就能把钉子拔掉，顺利上路，在这里顺便推广一下我的项目，这个项目基于<code>Aho-corasick</code>自动匹配关键词，使用统计方法来对输出最匹配路径，项目展示的<a href="http://ahocorasick.zhanglun.me/" target="_blank" rel="noopener">Demo</a>在<a href="http://ahocorasick.zhanglun.me/" target="_blank" rel="noopener">http://ahocorasick.zhanglun.me/</a>  ，随意输入地点能够迅速匹配出相关城市，比如打入“樟树”能够找到“江西省-宜春市-樟树市”，假如你服务器内存够的话还可以根据村委会名字来寻找到你的地点，而且搜索的速度只和你的地点名长度有关，不会随着关键词的增长而变慢</p><h2 id="引用"><a href="#引用" class="headerlink" title="引用"></a>引用</h2><p><a href="https://stackoverflow.com/questions/5901378/what-exactly-is-a-maven-snapshot-and-why-do-we-need-it/5907727" target="_blank" rel="noopener">SNAPSHOT</a></p>]]></content>
      
      
      <categories>
          
          <category> Java </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>Big Data Questions</title>
      <link href="2018/11/07/bigdata/BigDataQuestions/"/>
      <url>2018/11/07/bigdata/BigDataQuestions/</url>
      
        <content type="html"><![CDATA[<blockquote><p>Recently I found that I’m kind of over talking when I speak with others, so I want to train my conclusion of solving problem. Here are some problems I meet when I read books of Big Data.</p></blockquote><ul><li>No 1. Why <code>Spark</code> does not use map-reduce?</li></ul><p>Disk reading is too slow to complete big data analyze.Map-reduce is a shuffle which meanings parallizing.<code>Spark</code> just use memory to complete parallizing a huge problem.</p><ul><li>No 2. How Hadoop to append data?</li></ul><p>Just use </p>]]></content>
      
      
      <categories>
          
          <category> 大数据 </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>Learning Scala From Java</title>
      <link href="2018/10/20/java/LearningScalaFromJava/"/>
      <url>2018/10/20/java/LearningScalaFromJava/</url>
      
        <content type="html"><![CDATA[<blockquote><p>Recently I had finshed reading one book : &lt;&lt; Scala for The Impatient&gt;&gt;, I found a lot intrest thing during reading this book.Cause I had known <code>Python</code> and <code>Java</code> before, I can see  mixed fetures of <code>Scala</code> between <code>Python</code> and <code>Java</code>.This  blog is my thought  of studying a new language from other learned language.</p></blockquote><h2 id="Introudction"><a href="#Introudction" class="headerlink" title="Introudction"></a>Introudction</h2><p>What is <code>Scala</code>, <code>Scala</code> is a new lanuage ? No technically <code>Scala</code> is <code>Java</code>. They both create <code>*.class</code> file, and run on <code>JVM</code>.The most different between them: <code>Scala</code> use <code>.scala</code> as suffix and <code>Java</code> use <code>.java</code> as suffix.</p><p>The difference of two lanuage is the syntax which deeply affectting your coding style.This blog will introuce the most different of two lanuage and we will find out why is it.</p><h2 id="Stronger-Object-Oriented"><a href="#Stronger-Object-Oriented" class="headerlink" title="Stronger Object-Oriented"></a>Stronger Object-Oriented</h2><p><code>Scala</code> and <code>Java</code> both generate <code>Java bytecode</code>(we will call it <code>bytecode</code> later) file, and let it run on the <code>JVM</code>.The <code>bytecode</code> is totally a <code>object-oriented</code> format.</p><p>The basic structure of <code>bytecode</code> is <code>class</code>, but we will see something wrong in <code>Java</code>.For examples.In <code>Java</code> library, they give us two type <code>int</code> and <code>Integer</code>. One is <code>C</code> level data type, Other is a really <code>Object Class</code>.This is a huge problem, do we really need a <code>Object Class</code> like <code>Integer</code>, this <code>Class</code> make a <code>C</code> level data type not so effective (It’s huge than <code>int</code>).The only reason we need it for <code>collection</code> data type like <code>List</code>,<code>Set</code>,<code>Map</code> and so on can only load Class Object.So we not only make it ugly in <code>Java</code> but also we waste lot of time  <code>packing</code> and <code>unpacking</code> between Class-Object and C-level Object.</p><p>This problem is fixed or improve by <code>Scala</code>.In <code>Scala</code> libray, there are no <code>int</code> or <code>long</code> or <code>char</code> anymore.There are all to be <code>Class</code>.And  in <code>Scala</code> offical collection (<code>scala.collection</code>) we can load this Class-Object as a c-level data type undergroud. This trick is done by <code>Manifest</code> type, using <code>reflect</code> to make the collections to save base type like <code>int</code>，<code>char</code> and so on as its’ elements.</p><p>In this way, we not only delete c-level data type, but also delete the un-object-oriented part in a object-oriented lanuage.This is improvement of <code>Scala</code>.We needn’t <code>pack</code>  and <code>unpack</code> again and again.But we share the speed of <code>C</code> level data type without touch it.</p><h2 id="Enhance-Function"><a href="#Enhance-Function" class="headerlink" title="Enhance Function"></a>Enhance Function</h2><p><code>Java8</code> support <code>lambda</code> fuction in 2014,so in a long time if you want use something like <code>function</code>, you can use a anonymous class which implemented some interface.<br>eg:</p><pre><code>Thread thread = new Thread(new Runnable() {    @Override    public void run() {        // do something         }});</code></pre><p>You can see if we want run a thread we must initialize one object.Actually you only need the <code>run</code> function.</p><p>Let’s see what <code>Python</code> do with function.If you want to call one object as function you only need give a object <code>__call__</code> function.Then you can call as <code>obj(*arg, **kwds)</code>.The <code>Python</code> will transform it to <code>obj.__call__(*arg, **kwds)</code>.</p><p>We can learn it from <code>Python</code>, if <code>Java</code> want to call <code>obj</code> as <code>function</code> so he need use some unnified rule.So when you can write just function and <code>Java</code> will turn it to some Interface.</p><p><code>Scala</code>build 23 generic <code>Class</code> from <code>Function0</code> to <code>Function22</code> to help us write our <code>function</code> as a object.By the way, in <code>Java</code>, you maybe use <code>avoid</code> in a <code>method</code> to announcing no nedd to return.In <code>Scala</code>, you must have a <code>return</code>, if you really don’t need one, just return <code>Unit</code> which is same as <code>void</code>.</p><p>Now you can use <code>fuction</code> as a veriables now in <code>Scala</code></p><pre><code>def rInt(): Int = 1val k: () =&gt; Int = rInt _</code></pre><p>In upper, we use fuction <code>rInt</code> as a virable of <code>k</code>, <code>() =&gt; Int</code> is tell the <code>Scala</code> the type of <code>function</code>.So when call some thing like <code>k()</code>, it will tranceform to <code>k.applay()</code>.</p><p><code>Scala</code>  build a rule for us like <code>HTTP</code> to <code>Internet</code>.Maybe this is not so meaning just building 23 <code>trait</code> (like <code>interface</code> in <code>Java</code>).But you will a huge power when on the basement.It kind like <code>HTTP</code>, thanks to it, the <code>Internet</code> give a amazing world to us.</p><h2 id="Do-More"><a href="#Do-More" class="headerlink" title="Do More"></a>Do More</h2><p><code>Scala</code> is shorter than <code>Java</code>.In my word, I think <code>Java</code> is kind of wordy.If you want to <code>print</code> some word, you nedd use <code>System.out.println</code>, maybe you can <code>static import</code> to reduce it(use <code>import static System.out</code>).Also there are too mush strict rule in <code>Java</code>.Such as: Only one public class in a file.Your package need use same physcial address.etc.</p><p><code>Java</code> is strict lanuage, you aren’t trust by the compiler.While <code>Scala</code> give us more freedoom.You can save a lot of time in it.</p><p>Let us see what <code>Scala</code> do for us.</p><ul><li>add varies and method in builder</li></ul><p>just use one line like </p><pre><code>class T(var name:String)</code></pre><p>it’s equal </p><pre><code>class T{private String name;public String getName(){return this.name;}public String setName(String name){this.name = name;}public T(String name) {this.name = name;}}</code></pre><ul><li>saving brackets</li></ul><p>if a fuction use no params, you can save brackets.It maybe confused if you want a <code>function</code> not just call them.Just add <code>_</code> after your function.<code>Scala</code> will know you just need a function.</p><ul><li>use symbol as your function</li></ul><p>In <code>Java</code>, we are not allowed to use <code>*</code>,<code>/</code>，<code>+=</code> as your class method, <code>Scala</code> open it, you can what you like.Sometimes, <code>+</code> will be more clearly than just <code>add</code>.</p><ul><li>add your patch to other library</li></ul><p>In the your kingdom of <code>Scala</code>, you can simple add any patch to other class without recompiling <code>Java</code> again.</p><pre><code>implicit def addKing(w:String) = new {  def king()= println(w + &quot; is my king&quot;)}def say = &quot;Scala&quot; king</code></pre><p>just add one rule use <code>implicit</code> to tell <code>Scala</code>, you can add <code>String</code> a new method <code>king</code>.if you run <code>say</code>, it will print <code>Scala is my king</code>.</p><p>Compare <code>Java</code>, <code>Scala</code> more like a human, he will think a lot for your code.If <code>String</code> have no method call <code>king</code>, he will look up in his scope, is there any definenation of a convertion to a new <code>Class</code> which have a method call <code>king</code>.If he find one, he will do it for you, convert the <code>String</code> to the new <code>Class</code> and call the method again.</p><hr><h2 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h2><p>There are so many secret in <code>Scala</code>, if you realy want to know how <code>Scala</code> do for us, just use <code>scalac</code> to compile to <code>.class</code> file, and use <code>javap</code> to tranceform to <code>Java</code> code.That will help you a lot.There are  also a intresting part in new power <code>switch</code>  in <code>Scala</code>, it’s too important to write a new blog to introducing it.</p><p><code>Scala</code> is a intersting lanuage, if you want to know more about it, you can read the <code>Scala</code> library code .It’s really perfect.</p>]]></content>
      
      
      <categories>
          
          <category> Java </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>Build Hadoop Cluster in One Computer</title>
      <link href="2018/10/14/software/BuildHadoopClusterinOneComputer/"/>
      <url>2018/10/14/software/BuildHadoopClusterinOneComputer/</url>
      
        <content type="html"><![CDATA[<blockquote><p>If you are hadoop novice, I strongly suggest you beginning your study from single node building,you can learn from this <a href="https://www.tutorialspoint.com/hadoop/" target="_blank" rel="noopener">website</a>, after you having finshed build one single node, then you can reading my blog to learn how to run a N-node clusters just in your computer.</p></blockquote><h2 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h2><p>This blog is introduce using one computer to build a N-node clusters.I suggest you use ubuntu to build. You can also use Windows, but you’d better install virtualbox to install one desktop ubuntu  as your base server.In this blog, we will try two different way to build hadoop clusters in one computer.</p><h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><p>Before you start learning, you can download these required softwares from Intelnet.</p><ul><li>JDK8(optional)</li></ul><p>we can also install it by apt tool, but may be slow in China.So you’d better download it from website.</p><p><a href="https://www.oracle.com/technetwork/java/javase/downloads/jdk8-downloads-2133151.html" target="_blank" rel="noopener">https://www.oracle.com/technetwork/java/javase/downloads/jdk8-downloads-2133151.html</a></p><p>choose “jdk-8u181-linux-x64.tar.gz” to download.You can alse install in your master computer later, you can read from this <a href="https://websiteforstudents.com/how-to-install-oracle-java-jdk8-on-ubuntu-16-04-17-10-18-04-desktops/" target="_blank" rel="noopener">blog</a></p><ul><li>Hadoop(2.85)</li></ul><p>I choose latest 2.85 version, you can download from this website.</p><p><a href="https://www.apache.org/dyn/closer.cgi/hadoop/common/hadoop-2.8.5/hadoop-2.8.5-src.tar.gz" target="_blank" rel="noopener">https://www.apache.org/dyn/closer.cgi/hadoop/common/hadoop-2.8.5/hadoop-2.8.5-src.tar.gz</a></p><ul><li>Ubuntu Image</li></ul><p>In this trip, we choose Ubuntu16.04 server for build clusters.You can use 163 mirrors to speed up your download.</p><p><a href="http://mirrors.163.com/ubuntu-releases/16.04.5/ubuntu-16.04.5-server-amd64.iso" target="_blank" rel="noopener">http://mirrors.163.com/ubuntu-releases/16.04.5/ubuntu-16.04.5-server-amd64.iso</a></p><ul><li><p>Virtualbox</p><p>we need <code>virtualbox</code> to create our clusters. It’s easy for you to install virtualbox in ubuntu. You can read this <a href="https://tecadmin.net/install-oracle-virtualbox-on-ubuntu/" target="_blank" rel="noopener">article</a> to install virtualbox-5.2</p><ul><li>Docker</li></ul></li></ul><p>we will try use docker build our clusters, it’s easy install in ubuntu.The install tutorials is <a href="https://docs.docker.com/install/linux/docker-ce/ubuntu/#uninstall-old-versions" target="_blank" rel="noopener">https://docs.docker.com/install/linux/docker-ce/ubuntu/#uninstall-old-versions</a></p><h2 id="Clusters-On-VirtualBox"><a href="#Clusters-On-VirtualBox" class="headerlink" title="Clusters On VirtualBox"></a>Clusters On VirtualBox</h2><p>Now I assuming you’re working on a ubuntu16.04 desktop OS.Now let’s begining our trip.</p><p>First,let’s init a master, after we install required software in master, we can use virtualbox <code>clone</code> function to easy to build slave.</p><h3 id="Build-Ubuntu-VMS"><a href="#Build-Ubuntu-VMS" class="headerlink" title="Build Ubuntu VMS"></a>Build Ubuntu VMS</h3><ul><li>new a machine named master</li></ul><p><img src="/images/vm-create1.png" alt></p><ul><li>Choose 2G RAM, VDI </li></ul><p>then run this image, load the iso file you downloaded.Pay attention to make true install ssh server( Or you can install after installing os by <code>apt</code>)</p><blockquote><p>Before we start install hadoop and java skd, let me tell you something about the internet require.</p></blockquote><p>For our clusters runing, we need a connected internet between master and slaves.If we have many computers, it’s simple, we just need they both have public IP or private IP in one LAN.But if we just in one computer, how can we have independent IP for our master and slaves.</p><p>This is why we install <code>virtualbox</code>, <code>virtualbox</code> provide our independent computers in just one computer.Moreover, it can provide a simulate NIC for each computer.By using that, each computer can have they own private IP in LAN. </p><p>So the key our cluster running is the <code>bridge</code></p><p><img src="/images/vm-net-adapter.png" alt></p><p>we need choose the <code>bridged adapter</code> to make master and slaves just in same LAN.Pay attention to make true you need choose your real NIC.In ubuntu you just run <code>ifconfig</code> and find out have one line <code>inet addr:192.168.1.12</code> .Usually it’s <code>eth0</code> in ubuntu.</p><p>When you have finished OS installment.You can login in and start installing hadoop clusters.</p><blockquote><p>Step 1. Configure Static IP</p></blockquote><p>In your virtual machine, your IP is changeable when reboot.Because ubuntu use DHCP for init your IP from gateway.We need make true our  master and slaves have changeless IP to protect their connection.</p><p>To do this, first you need make true your installment is ok. Try <code>ping  baidu.com</code> to check you connected Internet or not.Then we need know our gateway address.Try run <code>route</code> in shell, you can find a table, in the row <code>Gateway</code>, you can find one or more static IP like <code>192.168.0.1</code> , this is your gateway.Now we open our internet settings.</p><pre><code>cat /etc/network/interfaces    </code></pre><p>you can see something like this</p><pre><code>auto eth0iface eth0 inet dhcp</code></pre><p><code>eth0</code> is your NIC(yours maybe different). and we use <code>dhcp</code> to get IP. Now we need change it to static.</p><pre><code>auto eth0iface eth0 inet staticaddress 192.168.0.105netmask 255.255.255.0gateway 192.168.0.1</code></pre><p>PS: make true, you need change the <code>eth0</code> and <code>gateway</code> IP to yours.The address <code>IP</code> must be subnet of gateway under the control of netmast.eg, you can’t set you ip address to 10.1.1.1 if your gateway is 192.168.0.1.The easiest way is set by <code>dhcp</code> format.And just change the last number.If you still can’t connect the Internet.Try add one line <code>dns-nameservers 8.8.8.8</code> .</p><pre><code>ifdown eth0ifup  eth0</code></pre><p>now run upper commands in your vm(<code>eth0</code> need your NIC name).If run <code>ifconfig</code> again, you can see our IP address chage to <code>192.168.0.105</code> now!</p><blockquote><p>Step 2. Add Hostname alias</p></blockquote><p>Becase hadoop need hostname to identify their ID, so now we add <code>Hostname-IP</code> pair to smooth our connection.</p><p>Just edit <code>/etc/hosts/</code> and add three line below</p><pre><code>192.168.0.105 master192.168.0.104 slave1192.168.0.103 slave2</code></pre><blockquote><p>Step 3. Make SSH Login</p></blockquote><p>Becase hadoop need login by root with <code>SSH</code> , so we need make <code>root</code> can login in in ubuntu.Open <code>/etc/ssh/sshd_config</code> and change line <code>PermitRootLogin prohibit-password</code> to <code>PermitRootLogin yes</code>, then <code>service ssh restart</code> .</p><p>Also you need use your <code>sudo</code> to set password for root</p><pre><code>sudo passwd root</code></pre><p>now check you can login in with <code>root</code></p><pre><code>ssh root@127.0.0.1</code></pre><blockquote><p>Step 4. Set Hadoop Env</p></blockquote><p>First, we need install <code>JDK</code> for hadoop, now back to your host computer. And use <code>scp</code> to upload <code>JDK</code> to vm.You can add below to <code>/etc/hosts</code> in your host machine.</p><pre><code>192.168.0.105 master192.168.0.104 slave1192.168.0.103 slave2</code></pre><p>then you can easy upload your <code>JDK</code> and <code>Hadoop</code> to your vm(you need unpack this tar.gz file first)</p><pre><code>scp -r /path/your/jdk root@master:/usr/lib/jvm/java-8-oraclescp -r /path/your/hadooproot@master:/usr/local/hadoop</code></pre><p>PS: you can also install <code>Java8</code> by <code>apt</code></p><p>Now, we installed <code>JDK</code> and <code>Hadoop</code> in our VM.Then we back to VM and initialize our <code>Hadoop</code>.</p><ul><li>Set JDK Home</li></ul><p>edit <code>hadoop-env.sh</code>(in <code>/usr/local/hadoop/etc/hadoop/</code>) file add <code>export JAVA_HOME=/usr/lib/jvm/java-8-oracle</code> to tell <code>Hadoop</code> JDK local address.</p><ul><li>Set Core IP</li></ul><p>We need a boss to handle all employer.So edit <code>core-site.xml</code>(in <code>/usr/local/hadoop/etc/hadoop/</code>) and add a property in <code>configuration</code></p><pre><code>&lt;property&gt;    &lt;name&gt;fs.defaultFS&lt;/name&gt;    &lt;value&gt;hdfs://master:9000/&lt;/value&gt;&lt;/property&gt;</code></pre><p>each cluster will send heartbeat to <code>master:9000</code>.</p><ul><li>Set <code>HDFS</code> replication and file dir</li></ul><p>The hadoop basement is <code>HDFS</code>, edit <code>hdfs-site.xml</code> and add three property </p><pre><code>&lt;property&gt;        &lt;name&gt;dfs.replication&lt;/name&gt;        &lt;value&gt;2&lt;/value&gt;    &lt;/property&gt; &lt;property&gt;        &lt;name&gt;dfs.namenode.name.dir&lt;/name&gt;        &lt;value&gt;file:///root/hdfs/namenode&lt;/value&gt;    &lt;/property&gt;    &lt;property&gt;        &lt;name&gt;dfs.datanode.data.dir&lt;/name&gt;        &lt;value&gt;file:///root/hdfs/datanode&lt;/value&gt;    &lt;/property&gt;</code></pre><p>The <code>dfs.replication</code> meaning the backups of <code>HDFS</code>, <code>dfs.namenode.name.dir</code> and <code>dfs.datanode.data.dir</code> is optional.If you not set this, it will store under <code>/tmp</code> (when reboot ,it will delete).</p><ul><li>Set <code>Yarn</code> for <code>MapReduce</code></li></ul><p>In hadoop2, we use <code>Yarn</code> to manage our <code>MapReduce</code>, run <code>cp mapred-site.xml.template mapred-site.xml</code> and then add property to set <code>Yarn</code> as our <code>MapReduce</code> framework</p><pre><code>&lt;property&gt;    &lt;name&gt;mapreduce.framework.name&lt;/name&gt;    &lt;value&gt;yarn&lt;/value&gt;&lt;/property&gt;</code></pre><p>and we also need tell <code>yarn</code> the <code>master</code> of the clusters and our need open <code>MapReduce</code> <code>Shuffle</code> Fuction effective our <code>MapReduce</code>, edit <code>yarn-site.xml</code>, and add two property</p><pre><code>&lt;property&gt;    &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt;    &lt;value&gt;mapreduce_shuffle&lt;/value&gt;&lt;/property&gt;&lt;property&gt;    &lt;name&gt;yarn.resourcemanager.hostname&lt;/name&gt;    &lt;value&gt;master&lt;/value&gt;&lt;/property&gt;</code></pre><p><code>yarn.nodemanager.aux-services</code> open <code>shuffle</code>, and <code>yarn.resourcemanager.hostname</code> set <code>ResourceManager</code> hostname.</p><hr><p>Now we complete the base <code>Hadoop</code> settings, now we can try run <code>hadoop</code> on <code>master</code></p><pre><code>cd /usr/local/hadoop/bin/hadoop namenode -formatsbin/start-dfs.sh</code></pre><p>We try format our namenode, and start dfs server, now run <code>jps</code>, you  can see <code>NameNode</code> and <code>SecondaryNameNode</code> server started.</p><p>Now we try start <code>Yarn</code> to start <code>MapReduce FrameWork</code>.</p><pre><code>sbin/start-yarn.sh</code></pre><p>Now, rerun <code>jps</code>, you can see <code>ResourceManager</code> running.You can also try <code>netstat -tuplen|grep 8088</code>, you will find the <code>ResourceManager</code> open some tcp port like 8080,8031,8033,etc.And the <code>8088</code> is the website of  managing clusters.You can open <a href="http://master:8088" target="_blank" rel="noopener">http://master:8088</a> to see the <code>clusters</code> status.Now you can only see blank node in clusters, for we have not started one slave yet.</p><p>Congratulation, our master is starting, in the running, we need input our password when start, after complete all slave building, we can use ssh-key to autologin.</p><blockquote><p>Now let’s build our slaves.</p></blockquote><p>Use <code>virtualbox</code> clone function, we clone <code>master</code> to a new VM named <code>slave1</code>.</p><p>Because we clone every thing to the <code>slave1</code>, so we need close <code>master</code> and goto <code>slave1</code> change  its hostname and static IP make it to be a <code>slave</code></p><p>First we need do is rename the VM,edit <code>/etc/hostname</code> change it to <code>slave1</code>,  then we need do is setting <code>slave1</code> Static IP, we do like upper.Just replace IP to <code>192.168.0.104</code>, and then we reboot and start <code>master</code> and <code>slave1</code> at meatime.</p><p>Now let’s check master to start our <code>slave1</code>, in our <code>master</code> VM, we edit <code>/usr/local/hadoop/etc/hadoop/slaves</code> file, and one line </p><pre><code>slave1</code></pre><p>and make true you have add slaves’ hostname alias in <code>master</code> VM.<br>Then we try start our <code>Cluster</code></p><pre><code> cd /usr/local/hadoopbin/hadoop datanode -formatsbin/start-dfs.sh &amp;&amp; sbin/start-yarn.sh</code></pre><p>After running these command, check <a href="http://master:8088" target="_blank" rel="noopener">http://master:8088</a>  to find the master have one slave online named <code>slave1</code>.</p><p>PS: Now you can generate ssh-key for your login in slaves, just run <code>ssh-keygen -t rsa &amp;&amp; ssh-copy-id slave1</code>, you don’t need input your password to start your clusters.</p><p>Now we have one node clusters, if you want more, you can add more slaves repeatting upper produce.</p><p>After you build your N-Clusters , you can now run those commands to check the hadoop working or not.</p><pre><code># create input filesmkdir inputecho &quot;Hello Docker&quot; &gt;input/file2.txtecho &quot;Hello Hadoop&quot; &gt;input/file1.txt# create input directory on HDFShadoop fs -mkdir -p input# put input files to HDFShdfs dfs -put ./input/* input# run wordcount cd /usr/local/hadoop/binhadoop jar ../share/hadoop/mapreduce/sources/hadoop-mapreduce-examples-*-sources.jar org.apache.hadoop.examples.WordCount input output# print the input filesecho -e &quot;\ninput file1.txt:&quot;hdfs dfs -cat input/file1.txtecho -e &quot;\ninput file2.txt:&quot;hdfs dfs -cat input/file2.txt# print the output of wordcountecho -e &quot;\nwordcount output:&quot;hdfs dfs -cat output/part-r-00000</code></pre><p>PS: By the way, if you want to running this clusters for a long time, you can try use <code>vboxmanage</code> to manage the vm. You can simple use <code>vboxmanage startvm master --type headless</code> to start master background(change <code>master</code> to other VM name can start them too)</p><h3 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h3><p>The difficult of build a clusters in virtualbox is know how master and slaves connecting each other.If you set a right network, it’s easy to running the cluster.But there’re some problem in <code>virtualbox</code>, we can’t share our network in the host LAN with virtualbox <code>bridge</code>. So we will introduce you build clusters in <code>Docker</code> and we can run our clusters in a swarm clusters in a real envirment.</p><h2 id="Clusters-On-Docker"><a href="#Clusters-On-Docker" class="headerlink" title="Clusters On Docker"></a>Clusters On Docker</h2><p>Building clusters is much easily in docker, for docker provide a easy network bride in sigle computer or in a swarm clusters.</p><p>we use <code>kiwenlau/hadoop:1.0</code> image to our test(which hadoop version is 2.7).Just run</p><pre><code>sudo docker pull kiwenlau/hadoop:1.0</code></pre><p>After few minutes, we can have a hadoop images, now we need set our private LAN Net just use this(If you want to run a swarm clusters above many computers, just change <code>bridge</code> to <code>overlay</code>, powerful, isn’t it)</p><pre><code>sudo docker network create --driver=bridge hadoop</code></pre><p>Now let start our master server</p><pre><code>sudo docker run -itd \                --net=hadoop \                -p 50070:50070 \                -p 8088:8088 \                --name hadoop-master \                --hostname hadoop-master \                kiwenlau/hadoop:1.0 &amp;&gt; /dev/null</code></pre><p>In the command, we set the master hostname to <code>hadoop-master</code>.and we needn’t change <code>/etc/hosts</code> to add it like in <code>virtualbox</code>, docker will do it for us.</p><p>Now we start our slaves</p><pre><code>sudo docker run -itd \                --net=hadoop \                --name hadoop-slave1 \                --hostname hadoop-slave1 \                kiwenlau/hadoop:1.0 &amp;&gt; /dev/nullsudo docker run -itd \                --net=hadoop \                --name hadoop-slave2 \                --hostname hadoop-slave2 \                kiwenlau/hadoop:1.0 &amp;&gt; /dev/null</code></pre><p>After doing that, we have finshed all softwares build.Now just run<code>sudo docker exec -it hadoop-master bash</code> into <code>master</code>, and then start our clusters <code>bash start-hadoop.sh</code>.</p><p>Now you can enjoy your clusters in few minutes, open <a href="http://127.0.0.1:8088/" target="_blank" rel="noopener">http://127.0.0.1:8088/</a> to see our clusters running happily.</p><h2 id="Conclusion-1"><a href="#Conclusion-1" class="headerlink" title="Conclusion"></a>Conclusion</h2><p>After introducing two way to build a hadoop clusters, you will find it’s easy to build a clusters if you know how they work together.In a word, we kind of like using <code>Docker</code> to running hadoop clusters, we can easy move add more <code>Hadoop</code> slaves in just one command.Meantime we can use <code>bridge</code> or <code>overlay</code> network for us building a more safe hadoop clusters.</p><h2 id="References"><a href="#References" class="headerlink" title="References"></a>References</h2><p><a href="https://github.com/kiwenlau/hadoop-cluster-docker" target="_blank" rel="noopener">https://github.com/kiwenlau/hadoop-cluster-docker</a></p>]]></content>
      
      
      <categories>
          
          <category> 软件 </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>Starting Using English In My Blog</title>
      <link href="2018/10/13/useless/StartingUsingEnglishInMyBlog/"/>
      <url>2018/10/13/useless/StartingUsingEnglishInMyBlog/</url>
      
        <content type="html"><![CDATA[<blockquote><p>Happily, I decide to using English for my blog</p></blockquote><p>The reasons why I want to use English to write blog :</p><ul><li>I want to pratice my English writing </li><li>it’s convenient for some tec in IT</li><li>It’s kind of difficult for me in (<code>en</code>and <code>eng</code>,<code>in</code> and <code>ing</code>) as a southern Chinese which is deeply delay my write speed.</li></ul><p>It’s may be kind of difficult for me writing English in my blog, but I will try my best to avoiding stupied mistake.</p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>触摸Python的GIL</title>
      <link href="2018/09/15/python/%E8%A7%A6%E6%91%B8Python%E7%9A%84GIL/"/>
      <url>2018/09/15/python/%E8%A7%A6%E6%91%B8Python%E7%9A%84GIL/</url>
      
        <content type="html"><![CDATA[<blockquote><p>Python的<code>GIL</code>一直是被大家攻击其语言的一个弊端，每次在讨论语言特性的时候这点总是会被人们提起，但是这个东西好像就一个“污点”，大家都知道，但是大家都不了解为什么。本片博客就是好好的探索一下GIL，让我们不再畏惧它</p></blockquote><h2 id="引言"><a href="#引言" class="headerlink" title="引言"></a>引言</h2><blockquote><p>其实一开始并没有想到研究GIL，但是在研究<a href="/2018/09/12/如何让你的Python像C++一样快/">如何让你的Python更快</a>的过程中发现我们可以通过这种方式解决掉GIL，让我们的代码不被<code>Python</code>拖累</p></blockquote><p>这篇博客相比于上面的博客更注重于代码的讲解，我们通过使用<a href="https://github.com/pybind/pybind11" target="_blank" rel="noopener">pybind11</a>从一个<code>Python</code>调用<code>C++</code>的demo出发介绍如何让<code>Python</code>调用<code>C++</code>并且丢弃GIL</p><h2 id="GIL简介"><a href="#GIL简介" class="headerlink" title="GIL简介"></a>GIL简介</h2><p>首先我们要知道什么是<code>GIL</code>，为什么它会拖累<code>Python</code>，首先我们看一下<code>Python</code>历史，<code>Python</code>是Guido van Rossum 在1989年发布的，那个时候计算机的主频还没有达到1G，程序全部都是运行在单核计算机上面，直到2005年多核处理器才被Intel开发出来</p><p>多核处理器意味着什么呢，就好比一个工厂，你原来只有一个工人干活，现在有很多个了，一开始设计出来只是为了能在每个核心上跑不同的应用，但是随着大家对多核计算机的使用，大家发现有的时候计算器其实很空闲，大部分CPU都在休息，假如只在一个核上跑一个应用的话，那么其他CPU就浪费了，所以大家就开始设计怎么并行在多个CPU上跑同样的任务</p><p>现在我们来考虑一下怎么能让CPU力往一处使，我们用数据库来做比方，假设我们计算机上安装一个银行数据库，为了让这个“银行”能够服务更多的人，我们把对钱的操作（增删查改）放到每个CPU上运行。假如我们的顾客一个一个排着队来取钱存钱，我们每个CPU查询都是唯一的，存取也是唯一的，那么我们的“银行”就能正常工作</p><p>但是现实的环境往往不是这样的，顾客它可能会因为网络原因个人原因同时进行多个操作，假如它同时取1千万的两次操作（它账号只有1千万），每个CPU上的程序查询时候正好都是账号有一千万，然后依次进行数据的更新，最后我们发现用户的账号变成了0，但是用户却取了两千万出来，你的银行损失了一千万，所以并行任务最重要的就是数据共享</p><p>怎么解决这个共享问题呢，很简单加“锁”，我们给需要共享的东西上个锁，每次你想用的时候你就把锁锁上，然后对共享的东西进行操作，当有别人想动这个东西的时候，他一看哎呀有人在用，那我等会。这样就不会造成上面的冲突了，但是这个也造成了一个问题由于我上了一把锁，每次我们想操作的时候，必须去看一下这个锁有没有被人锁上，假如没有我就锁上，有就等待，这一来一去就会造成一个效率问题（感觉这个也是国企的通病，权利依次掌握在领导上，要想完成工作得不断的进行开“锁”、关“锁”，有时候还会造成“死锁”），所以并行的4个任务运行速度不一定是一个任务的四倍，所以我们经常看到一些库在运行说明里面双核速度会比单核加速一点几倍，之说以达不到双倍就是因为这些“锁”的存在</p><p>“锁”帮我们能让单任务拆分成子任务并行化加速，但是在一定程度上拖累了运行速度，我们回到<code>Python</code>，因为多核是在2005年才出现的，但是在并行化上面，一个比多核更早出现的概率就是：<code>线程</code>和<code>进程</code></p><p>在还没有多核处理器的时候，操作系统为了让程序并行化跑，就创造了进程和线程的概率。用通俗的话来讲，进程就是一家大工厂，而线程就是工人，为了提高生产力，我们可以开很多家工厂，当然我们也可以开一家工厂，招很多工人。但是线程这个东西相比于进程要消耗的少的多，因为它“原材料”都是从“工厂”里面拿的，假如说工厂少了几个工人还可以生产，但是上万个工人没有工厂他们也办法工作。</p><p>所以对于<code>Python</code>来说首先得支持线程和进程的概率，对于进程来说很简单，就是多开几家工厂(多开几个<code>Python</code>程序)罢了，但是对于线程来说，由于<code>Python</code>是一门脚本语言，它需要一个<code>解释器</code>来执行代码，我们知道这个解释器它可以当做大一个共享变量，假如在不同的线程里面用“锁”来限制一下的话，环境变量就会乱了套</p><p>所以<code>Python</code>对于线程的支持就是给他加一个锁，也就是我们俗称的<code>GIL</code>，由于在操作系统在运行单核的时候就支持线程，一个工人加一个锁其实也没有什么，无非就是多了一点开锁关锁的时间，所以<code>Python</code>在2005前一直没有<code>GIL</code>这个概率，到了2005大家发现<code>Python</code>使用多线程竟然只能使用一个核，完全浪费了其他核，因为虽然<code>Python</code>的线程可以分配到不同的核上运行，但是当他们运行的时候发现这个锁没有被释放，所以每个核上的线程都傻乎乎的在等待，结果最后查看效果多线程比单线程速度还慢（要等<code>GIL</code>释放）</p><p><code>Python</code>社区逐渐发现这个问题，他们也做了很多挽救工作，比如在线程睡觉（sleep）、等待连接的时候让线程主动释放<code>GIL</code>，这样就能让其他线程继续执行，但是对于纯粹的运算代码而不是IO密集代码总也避不开这个锁的存在，如果允许<code>GIL</code>释放，由于历史遗留问题很多代码都会乱了套（理论上其实就是需要重新修改锁的设计，可以参考MySQL的代码去掉<a href="https://dev.mysql.com/worklog/task/?id=8423" target="_blank" rel="noopener">“锁”</a>花了5年时间），考虑到<code>Python</code>本来就运行的慢，<code>Python</code>开发者觉得假如你觉得代码很慢，你可以放到<code>C/C++</code>里面执行，所以对于这个<code>GIL</code>就没有继续啃下去，而是把中心放在<code>Python</code>调用<code>C/C++</code>中，提供了一些很方便的方式让我们在<code>C/C++</code>中控制<code>GIL</code>的释放以及获取</p><p>所以我们接下来通过一个来学习<code>Python</code>调用<code>C++</code>代码，来了解<code>Python</code>如何调用<code>C++</code>，并且通过一些实验来验证线程、进程和<code>GIL</code></p><h2 id="测试GIL的存在"><a href="#测试GIL的存在" class="headerlink" title="测试GIL的存在"></a>测试GIL的存在</h2><p>首先我们要做的第一件事就是测试GIL的存在，现在基本上主流电脑都是多核CPU，所以我们这个实验可以很轻松的在多核下进行</p><p>首先我们得安装一些环境：<code>Python3</code>，<code>gcc</code>，<code>htop</code>（在Windows可以用下任务管理器代替）</p><p>首先我得提一下我的一个认识误区，在以前我不太清楚线程、进程与多核直接的关系的时候我有一个误区，我以为<code>C</code>能在单线程里面使用多核（我也不清楚为什么我会这么想，可能是因为了解很少），而<code>Python</code>却不能，后面通过我实验我才发现，无论是<code>C</code>和<code>Python</code>只要你的代码不使用线程、进程那么你的代码只能同时运行在同一个核上</p><p>怎么来测试呢，我们可以在<code>Python</code>的解释器里面输入</p><pre><code>while True:  pass</code></pre><p>然后我们打开<code>htop</code>，我们可以发现某一个<code>CPU</code>始终保持在100%（这个CPU可能会变化，因为操作系统控制每个进程切换CPU时间），假如你没有其他任务过多使用<code>CPU</code>的话，你其他的核心一直保持在很低的利用率，当你<code>ctrl-c</code>你的代码后，那个100%的CUP会立马降下来</p><p>然后你在编译一个<code>C</code>程序，使用<code>gcc a.c &amp;&amp; ./a.out</code>命令编译下面代码然后运行</p><pre><code>// a.cint main(){while(1){};}</code></pre><p>你会发现<code>C</code>也只能消耗一个CPU，这就印证了我们前面说过得，如果我们不主动使用线程或进程来，同时只能有一个在运行</p><p>接下来我们看看在多进程的基础上，使用<code>Python</code>来使用多核</p><pre><code>from concurrent.futures import ProcessPoolExecutordef f(a):    while 1:        passif __name__ == &apos;__main__&apos;:    pool = ProcessPoolExecutor()    pool.map(f, range(100))</code></pre><p>当我们运行上面代码的时候，我们会发现所有<code>CPU</code>会运行到100%，我们只要简单声明一个进程池（<code>ProcessPoolExecutor</code>），<code>Python</code>自动帮我们生成你CPU核数相同的进程，然后我们只要把任务分配到池中就能重复的并行化任务，把所有的核心都用起来。</p><p>然后我们来测试一下线程池，要使用<code>Python</code>线程池只需要初始化<code>ThreadPoolExecutor</code>就行</p><pre><code>from concurrent.futures import ThreadPoolExecutordef f(a):    while 1:        passif __name__ == &apos;__main__&apos;:    pool = ThreadPoolExecutor()    pool.map(f, range(100))</code></pre><p>我们从<code>htop</code>可以看到在<code>Python</code>线程中，只有一个能达到100%，这就是<code>GIL</code>的“威力”，它让我们多线程没有发挥多线程的力量，重复使用到多核CPU</p><p>接下来我们看看在<code>C++</code>里面使用多线程是否能够发挥多核的威力</p><pre><code>// run.cpp#include &lt;thread&gt;using namespace std;#define NUM_THREADS 50void f(){    while(1){};}void run_dead(){    std::thread threads[NUM_THREADS];    for(int i = 0; i &lt; NUM_THREADS; ++i)    {        threads[i] = std::thread(f);    }    for (int i = 0; i &lt; NUM_THREADS; ++i) {        threads[i].join();    }};int main(void){    run_dead();}</code></pre><p>我们使用<code>g++ -pthread -std=c++11 run.cpp &amp;&amp; ./a.out</code>运行上面的<code>C++</code>程序，我们在htop里面能够发现，<code>C++</code>的多线程能够完全发挥多核的威力</p><p>上面的程序都很简单，但是具备一个多线程运行的基本构造，我们可以修改我们的调用的子任务来完成实际的任务，当然你程序越复杂也涉及到了各种锁的使用，这里我们就不谈了</p><p>从上面的程序我们可以知道<code>C++</code>的多线程能够充分使用多核，而<code>Python</code>的不行，接下来我们就开始探索<code>Python</code>调用<code>C++</code></p><h2 id="Python调用C"><a href="#Python调用C" class="headerlink" title="Python调用C++"></a>Python调用C++</h2><p>在上面的<a href="/2018/09/12/如何让你的Python像C++一样快/">博客</a>我总结了<code>Python</code>调用<code>C++</code>的方式，总的来说<code>Cython</code>是控制能力最好的，效率也是最高的，但是由于存在一个学习新语言的难度，所以我这里就不提了，改天再写一篇关于<code>Cython</code>的博客，我们这里使用<a href="https://github.com/pybind/pybind11" target="_blank" rel="noopener">pybind11</a>这个库作为介绍</p><p>安装非常简单<code>pip install pybind11</code>就行，接下来我们使用github上这个<a href="https://github.com/pybind/python_example" target="_blank" rel="noopener">官方例子</a>做介绍，最后我们以一个实际的<code>C++</code>项目为例子，看看如何在实际的项目使用</p><p>首先我们先把项目下载下来</p><pre><code>git clone https://github.com/pybind/python_example.git</code></pre><p>然后我们新建一个环境（避免安装到我们系统的环境，方便删除）</p><pre><code>python -m venv venv</code></pre><p>PS: 当前<code>Python</code>版本默认为py3.5以上（你可以使用pyenv安装Python多个版本，目前我在自己使用Python版本，但主要使用3.6以上）</p><pre><code>source venv/bin/activate</code></pre><p>然后我们激活我们的环境，我们顺便安装一下我们接下来要安装的<code>Python</code>包</p><pre><code>pip install ipython</code></pre><p>然后我们进入项目<code>cd python_example</code>，假如你用<code>Pycharm</code>的话，你可以在项目目录下生成<code>venv</code>环境，然后在<code>Pycharm</code>里面打开会自动设定为默认环境</p><p>然后我们先测试一下代码可以不可以用</p><pre><code>pip install .</code></pre><p>假如我们安装成功了，恭喜你，我们的环境已经准备好了，打开<code>ipython</code>，我们先测试一下这个<code>C++</code>代码的速度</p><pre><code>In [1]: import python_exampleIn [2]: python_example.add(1, 1)Out[2]: 2</code></pre><p>很好，代码运行正常，就是一个简单的加法运算，我们测试一下平均时间</p><pre><code>In [3]: %timeit python_example.add(1, 1)313 ns ± 3.03 ns per loop (mean ± std. dev. of 7 runs, 1000000 loops each)</code></pre><p>很好，我们的<code>C</code>代码还是跑到很快，313纳秒就跑完了，接下来我们看看纯粹的<code>Python</code>代码速度</p><pre><code>In [4]: def add(a, b):   ...:     return a + b   ...:    ...: In [5]: %timeit add(1, 1)113 ns ± 9.04 ns per loop (mean ± std. dev. of 7 runs, 10000000 loops each)</code></pre><p>什么竟然比<code>C++</code>还要快，快了近3倍，记得我当时第一次运行出来的这个结果的时候的震惊，说好的快呢，你骗我。</p><p>接下来我们就来分析一下出现这个的原因，会不会是因为类型转换出现问题呢，因为<a href="https://github.com/pybind/pybind11" target="_blank" rel="noopener">pyblind11</a>使用了很多自动转换的技术来帮我们转换，我们看看原函数（在src/main.cpp)</p><pre><code>int add(int i, int j) {    return i + j;}</code></pre><p>首先<code>Python</code>调用它，要把第一个参数由<code>Python</code>的<code>int</code>对象转换成<code>C++</code>的<code>int</code>基本类型，<code>C++</code>运行完之后，又得转换将<code>C++</code>基本<code>int</code>类型转换成<code>Python</code>的<code>int</code>对象，这一来一回就得多花三个操作，为了验证我们猜想，我们插入一个<code>nothing</code>函数在<code>add</code>函数后面</p><pre><code>void nothing(){}</code></pre><p>然后模仿<code>m.def</code>仿照写一行插入<code>nothing</code>函数（你会发现语法特别简单，这也是我喜欢<a href="https://github.com/pybind/pybind11" target="_blank" rel="noopener">pyblind11</a>的原因）</p><pre><code>m.def(&quot;nothing&quot;, &amp;nothing, R&quot;pbdoc(        do nothing    )pbdoc&quot;);</code></pre><p>接下来我们安装一下我们的新库<code>pip install .</code></p><p>然后我们再开一个新的<code>ipython</code>（你可以用<code>importlib</code>来重新加载库）</p><pre><code>In [1]: import python_exampleIn [2]: %timeit python_example.nothing()125 ns ± 0.6 ns per loop (mean ± std. dev. of 7 runs, 10000000 loops each)</code></pre><p>125ns，我们的猜想成功了，类型转换的确拖累了<code>C++</code>运行的速度，我们再看看原生的速度如何</p><pre><code>In [4]: def nothing():   ...:     pass   ...:    ...: In [5]: %timeit nothing()85.1 ns ± 0.262 ns per loop (mean ± std. dev. of 7 runs, 10000000 loops each)</code></pre><p>竟然还是比<code>C++</code>快，虽然没有上面那么夸张，但是快了25%，我们再来分析原因，首先现在没有类型转换所以理论上那只能是代码运行问题，我们知道<code>Python</code>优化里面提过一句，少用<code>.</code>，因为<code>Python</code>要搜寻很多东西才能获得到对象的属性、方法等，所以我们这边使用了<code>python_example.nothing</code>来调用<code>nothing</code>函数，假如我们去掉<code>.</code>速度会不会提高呢</p><p>怎么去掉呢，用局部变量</p><pre><code>In [6]: pn = python_example.nothingIn [7]: %timeit pn()90 ns ± 0.761 ns per loop (mean ± std. dev. of 7 runs, 10000000 loops each)</code></pre><p>从上面可以看到的确，<code>.</code>“害人不浅”，我们的速度又快了一大截，基本上同原生没有太多差距了，一开始我以为是概率问题，运行了多次但是结果都是一样，原生就是比<code>C++</code>快了5ns，可能是<a href="https://github.com/pybind/pybind11" target="_blank" rel="noopener">pyblind11</a>“偷偷”的在哪个地方偷跑了一条语句吧，或者有可能是<code>C++</code>比<code>C</code>（<code>Python</code>是<code>C</code>写的）稍微慢了一点</p><p>一开始我以为<code>C++</code>一定会比<code>Python</code>快，但是我们从上面测试可以看出来，在“起跑”阶段，<code>C++</code>甚至比<code>Python</code>要慢，我们使用<code>C++</code>主要是为了加速大段<code>Python</code>代码，只要在这场“长征”中<code>C++</code>能够胜出，那么我们的努力就没白费，那好我们继续测试，看看在长征过程中<code>C++</code>表现如何</p><p>首先我们把<code>add</code>函数魔改一下，我们让他进行100次运算</p><pre><code>int add(int i, int j) {    int s = 0, x = 0;    for(;x&lt;100;x++){        s = s + i + j;    }    return s;}</code></pre><p>我们再把模块给安装一下<code>pip install .</code>，重新打开新的<code>ipython</code></p><pre><code>In [1]: import python_exampleIn [2]: python_example.add(1,2)Out[2]: 300In [3]: padd = python_example.addIn [4]: %timeit padd(1,1)282 ns ± 3.9 ns per loop (mean ± std. dev. of 7 runs, 1000000 loops each)In [5]: %timeit python_example.add(1,1)316 ns ± 3.78 ns per loop (mean ± std. dev. of 7 runs, 1000000 loops each)</code></pre><p>我们这次重要见到了<code>C++</code>的威力，我们进行100次运算，相比于上面一次运算，我们只增加了<code>4ns</code>的平均时间，我们来看看原生<code>Python</code>的表现如何</p><pre><code>In [6]: def add(a, b):   ...:     s = 0   ...:     for i in range(100):   ...:         s += a + b   ...:     return s   ...:    ...: In [7]: add(1, 2)Out[7]: 300In [8]: %timeit add(1, 1)4.74 µs ± 40.5 ns per loop (mean ± std. dev. of 7 runs, 100000 loops each)</code></pre><p><code>C++</code>完爆<code>Python</code>，<code>4.74us = 4750ns</code>，<code>Python</code>用时是<code>C++</code>的10倍多，只还只是100次运算，假如我们上万次运算，那结果更加夸张，<code>C++</code>在长征的过程中胜利了，但是我们也不能说<code>Python</code>是慢毕竟<code>us</code>的单位其实非常小，<code>1us=1000ms=1000000s</code>，在1s内可以执行上面函数几十万次，只能说<code>C++</code>速度太可怕了</p><h3 id="调用总结"><a href="#调用总结" class="headerlink" title="调用总结"></a>调用总结</h3><p>我们从上面可以看到，虽然<code>Python</code>调用<code>C++</code>在类型转换上会有速度损失，但是在进入到函数提内运行过程中的速度是不影响的，假如我们的运算量够大，完全可以弥补那一点点性能影响，所以要想重复利用<code>C++</code>的速度，尽量少调用<code>C++</code>，把计算结果竟然一次性返回，而不是我们多次进行交互，这样就能最大化利用<code>C++</code></p><h2 id="在C-线程中测试GIL"><a href="#在C-线程中测试GIL" class="headerlink" title="在C++线程中测试GIL"></a>在<code>C++</code>线程中测试GIL</h2><p>接下来我们来考虑这么一个问题，前面我们测试了<code>C++</code>的线程能使用多核，我们假如在让<code>Python</code>在调用<code>C++</code>的代码中中使用线程，那么我们的<code>C++</code>的线程能不能使用多核呢进而解除GIL的作用</p><p>我们把<code>nothing</code>函数改成多线程</p><pre><code>#include &lt;thread&gt;#define NUM_THREADS 50using namespace std;void f(){    while(1){};}void nothing(){    std::thread threads[NUM_THREADS];    for(int i = 0; i &lt; NUM_THREADS; ++i)    {        threads[i] = std::thread(f);    }    for (int i = 0; i &lt; NUM_THREADS; ++i) {        threads[i].join();    }}</code></pre><p>然后我们再重新编译一下<code>pip install .</code>，我们来跑一下我们这个多线程的<code>nothing</code>函数</p><pre><code>In [1]: import python_exampleIn [2]: python_example.nothing()</code></pre><p>我们在<code>htop</code>里面可以看到在单线程的<code>Python</code>程序中，成功的将所有核心都利用上了，也就是是说假如我们在<code>C++</code>扩展中使用线程的话，是不会被<code>GIL</code>影响的</p><p>说实话当我第一次运行的时候我直觉是还是会被<code>GIL</code>影响，结果最后跑出来的结果大吃我一惊，现在我们分析为什么不会被受影响，因为<code>GIL</code>锁的是<code>Python</code>解释器，当我们的代码进入到<code>C++</code>中的时候，我们已经不在<code>Python</code>解释器中了，这样即使我在<code>C++</code>中声明线程，那也是<code>C++</code>的线程，所以就不会造成无法使用多核的情况</p><p>这里我们学到一点，如果我们想摆脱<code>GIL</code>可以把线程放到<code>C++</code>中，这样线程的不再依赖<code>Python</code>解释器，前面我们知道其实<code>Python</code>底层是用<code>C</code>写的，所以基本上所以的语法都是基于<code>C</code>代码实现加上语法糖来完成的，<code>Python</code>线程也就是<code>C</code>线程，我们能不能模拟一下<code>Python</code>来构建这个<code>GIL</code></p><p>首先我们知道<code>GIL</code>是一把锁，所以我们第一件事就是查看这把锁，在这里我们通过<code>Python</code>的<code>C</code>头文件来引入一个函数<code>PyGILState_Check</code>这个函数会返回一个<code>1</code>和<code>0</code>值，假如是<code>1</code>那么意思该线程拿着<code>GIL</code>锁，反之。</p><p>所以我们先在头部加上<code>#include &quot;Python.h&quot;</code>，在Linux系统上要安装<code>python-dev</code>或者<code>python-devel</code>开发包才有这个头文件，接下来我们在<code>nothing</code>函数加上这个检测状态</p><pre><code>cout &lt;&lt; &quot;GIL is &quot; &lt;&lt;  ((PyGILState_Check() == 1) ? &quot;hold&quot; : &quot;not hold&quot;)&lt;&lt;endl;</code></pre><p>提一句为了使用<code>cout</code>，我们得在头部加上<code>C++</code>输出库<code>#include &lt;iostream&gt;</code></p><p>先在我们重新安装一下并运行<code>nothing</code>函数，程序会输出<code>GIL is hold</code>，为什么会出现这个情况呢，因为<code>Python</code>默认会锁住<code>GIL</code>当运行<code>C++</code>或者<code>C</code>代码的时候，但是为什么我们虽然锁住了<code>GIL</code>但是我们还是能够使用<code>C++</code>的线程来运行多核呢，其实很简单因为我们的线程没有像<code>Python</code>一样每次运行的时候去获取这个<code>GIL</code>锁，为了证明这一点，我们来做个实验</p><p>首先我们得在<code>nothing</code>函数里面释放<code>GIL</code>，然后让线程去获取<code>GIL</code>（如果<code>nothing</code>主函数不释放<code>GIL</code>，会造成死锁，线程无法运行，一直获取不了<code>GIL</code>锁），我们可以用<code>Python</code>的<code>C</code>头文件的函数来释放<code>GIL</code>锁，但是<a href="https://github.com/pybind/pybind11" target="_blank" rel="noopener">pybind11</a>提供了一个更加方便的函数让我们来释放<code>GIL</code>锁，我们把<code>nothing</code>函数定义修改一下，在后面添加一条语句<code>py::call_guard&lt;py::gil_scoped_release&gt;()</code></p><pre><code>//    m.def(&quot;nothing&quot;, &amp;nothing);    m.def(&quot;nothing&quot;, &amp;nothing, py::call_guard&lt;py::gil_scoped_release&gt;());</code></pre><p>然后我们在重新编译安装运行一下代码，我们的结果就会是<code>GIL is not hold</code>，我们通过简单的一条语句就释放<code>GIL</code>锁，接下来我们来测试在线程中获取<code>GIL</code>锁来模拟<code>Python</code>的情况</p><p>要想获取<code>GIL</code>锁，<a href="https://github.com/pybind/pybind11" target="_blank" rel="noopener">pybind11</a>也提供了一个非常简单的方法来实现这个：<code>py::gil_scoped_acquire acquire;</code></p><p>我们接下来把<code>f</code>函数改成下面的</p><pre><code>void f(){    cout &lt;&lt; &quot;entner F: GIL is &quot; &lt;&lt;  ((PyGILState_Check() == 1) ? &quot;hold&quot; : &quot;not hold&quot;)&lt;&lt;endl;    py::gil_scoped_acquire acquire;    cout &lt;&lt; &quot;GIL is &quot; &lt;&lt;  ((PyGILState_Check() == 1) ? &quot;hold&quot; : &quot;not hold&quot;) &lt;&lt; &quot; now is runing &quot;&lt;&lt;endl;    while(1) {    };}</code></pre><p>我们在获取<code>GIL</code>前后，添加了一些输出，方便我们调试，接下来我们再运行我们的代码，我们发现程序输出50个进入<code>entner F: GIL is not hold</code>（在我的电脑上，因为线程同时运行，获取<code>GIL</code>锁需要时间，所以在我电脑上每次运行<code>f</code>函数时锁都打开着），但是只有一行<code>GIL is hold now is runing</code>，因为当一个线程获取到<code>GIL</code>后，其他线程就没法获取到了，而且看<code>htop</code>我们也能发现只有一个核到了<code>100</code>，在我们强行模拟下<code>C++</code>也没能使用多核</p><p>其实从这里我们可以看出来，<code>GIL</code>问题其实就是一个死锁的问题，线程获取后不释放锁，导致所有线程相互竞争，用一个谚语来说就是：一个和尚挑水喝、两个和尚抬水喝、三个和尚没水喝。</p><p>那么我们怎么来解决这个问题呢，很简单就是在你不需要的锁的时候去释放它，接下来我们来模拟一下怎么释放这个锁达到多线程“和平共处”，首先我们引入<code>C++</code>时间库来使用<code>sleep</code>函数(<code>#include &lt;unistd.h&gt;</code>)，接下来我们引入<code>Python</code>的<code>C</code>头文件中的宏来释放<code>GIL</code>，我们把<code>f</code>函数改成下面的形式</p><pre><code>void f(){    cout &lt;&lt; &quot;entner F: GIL is &quot; &lt;&lt;  ((PyGILState_Check() == 1) ? &quot;hold&quot; : &quot;not hold&quot;)&lt;&lt;endl;    py::gil_scoped_acquire acquire;    cout &lt;&lt; &quot;GIL is &quot; &lt;&lt;  ((PyGILState_Check() == 1) ? &quot;hold&quot; : &quot;not hold&quot;) &lt;&lt; &quot; now is runing &quot;&lt;&lt;endl;    Py_BEGIN_ALLOW_THREADS    while(1){    };    Py_END_ALLOW_THREADS}</code></pre><p>我们使用<code>Py_BEGIN_ALLOW_THREADS</code>和<code>Py_END_ALLOW_THREADS</code>这一对宏来释放<code>GIL</code>，这样我们重新编译运行<code>nothing</code>函数我们就能看到50个<code>enter</code>和50个<code>runing</code>，而且在<code>htop</code>中我们也能发现<code>C++</code>的线程再次使用所有的核心了（利用率达到不了100%，不知道是因为宏的“副作用”还是其他原因，但是每个核还是能够到70%作用），这种在一个函数中获取和释放<code>GIL</code>锁还是不推荐的，最好在函数一开始的时候释放<code>GIL</code>，在函数结束的时候获取<code>GIL</code>返回到<code>Python</code>解释器中（假如你需要与<code>Python</code>进行交互的话），毕竟获取一次锁的成本还是挺大的，而且一不小心就会造成死锁</p><h2 id="在Python线程中测试GIL"><a href="#在Python线程中测试GIL" class="headerlink" title="在Python线程中测试GIL"></a>在<code>Python</code>线程中测试GIL</h2><p>接下来我们来看看一个已经存在的问题，就是如何解决掉使用<code>Python</code>线程时遇到的<code>GIL</code>问题，其实我们在上面的<code>C++</code>线程已经模拟出来了，解决这个问题的关键就是释放<code>GIL</code>锁，我们先测试一下在<code>GIL</code>锁下，线程调用<code>C++</code>代码的速度</p><p>我们首先添加一个新死循环函数</p><pre><code>void run_dead(){    while(1){};}</code></pre><p>然后在后面加上<a href="https://github.com/pybind/pybind11" target="_blank" rel="noopener">pybind11</a>的定义</p><pre><code>m.def(&quot;run_dead&quot;, &amp;run_dead);</code></pre><p>接着我们运行下面函数</p><pre><code>from concurrent.futures import ThreadPoolExecutorimport python_examplepool = ThreadPoolExecutor()for i in range(100):    pool.submit(python_example.run_dead)</code></pre><p>在这个函数里面我们声明了一个线程池，并且向池蕾加入了100函数，接着我们在<code>htop</code>里面查看CPU利用率，我们可以看到只有1个CPU能够跑满100%，其实从前面的实验我们就能猜到这个结果，解决方案其实前面也给了，有两种方法，第一种就是使用<code>Python</code>的C的头文件函数宏<br>：<code>Py_BEGIN_ALLOW_THREADS</code>和<code>Py_END_ALLOW_THREADS</code>，第二种就是在函数声明的地方使用<a href="https://github.com/pybind/pybind11" target="_blank" rel="noopener">pybind11</a>提供的<code>py::call_guard&lt;py::gil_scoped_release&gt;()</code>来释放<code>GIL</code>，两种方法都可以，但是第二种更加简单一点，在这里我就不测试释放<code>GIL</code>之后的性能了，前面已经做过了</p><h2 id="GIL总结"><a href="#GIL总结" class="headerlink" title="GIL总结"></a><code>GIL</code>总结</h2><p>通过前面我们的测试，<code>GIL</code>这个东西其实只是一把锁，我们经常能听到很多人抨击<code>Python</code>关于<code>GIL</code>问题，这就给人一种错觉<code>Python</code>这种语言在设计上有弊端，在前面测试我们也发现了就算是<code>C++</code>或者<code>C</code>假如不正确的使用锁其实也会有这个<code>GIL</code>问题，<code>GIL</code>的问题的并不是“编程语言”的锅，主要是我们自己的代码造成的死锁，所以面对<code>GIL</code>的时候，不需要困惑，它就是一把“锁”，把它打开，而不是碰到它就跑，你会发现它也就是一把“锁”而已。</p>]]></content>
      
      
      <categories>
          
          <category> Python </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>如何让你的Python更快</title>
      <link href="2018/09/12/python/%E5%A6%82%E4%BD%95%E8%AE%A9%E4%BD%A0%E7%9A%84Python%E5%83%8FC%E4%B8%80%E6%A0%B7%E5%BF%AB/"/>
      <url>2018/09/12/python/%E5%A6%82%E4%BD%95%E8%AE%A9%E4%BD%A0%E7%9A%84Python%E5%83%8FC%E4%B8%80%E6%A0%B7%E5%BF%AB/</url>
      
        <content type="html"><![CDATA[<h2 id="引言"><a href="#引言" class="headerlink" title="引言"></a>引言</h2><blockquote><p>其实一开始没有想到写关于<code>Python</code>的加速，一开始只想好好了解一下<code>C++</code>这门语言，没想到最后研究来研究去，基本上把所以加速框架都试验了一下，这篇博客就谈谈我对<code>Python</code>加速的看法</p></blockquote><p> 首先我先谈谈<code>C++</code>，虽然我上大学之前就自学过<code>C</code>，但是对于这个<code>C</code>的升级版还是没有过多的了解，花了几天时间学习，发现<code>C++</code>这门语言还是不错的，至少在兼容性上，它能兼容<code>C</code>还有以前的版本。</p><p>然而作为一个用惯了了脚本语言的人来说，<code>C++</code>最麻烦的就是他的第三方库管理，当然对于类<code>Unix</code>系统有自己带的包管理器（如ubuntu上的<code>apt</code>，CentOS上的<code>yum</code>）可以来安装第三方库（就是我们平常为了安装一些软件，比如要先<code>apt install xxx-dev</code>那些库），由于这些都绑定了平台的，所以你经常能看到有些软件自己编译会列出各个平台下依赖的包，然而对于一些比较新的库（比如<code>googletest</code>，就得去<code>Github</code>上掏下来自己编译安装了。</p><p>吐槽完了<code>C++</code>的缺点之后，我们不得不说<code>C++</code>的优点了，虽然比较难装（相比于脚本语言）但是那个速度真是贼快，用腾讯开源的协程库，单台机器就能开启千万协程而且内存不超过2个G，想我大<code>Python</code>开个一万都很嘚瑟了。<code>C++</code>在性能上真的的碾压的。就是因为<code>C++</code>性能上要求到极致，所以它才会有那么多的前面安装的缺点，因为<code>C++</code>是面对硬件的，对于不同的硬件，<code>C++</code>想做到最快，那么通用的代码就不可能的，通用就代表损失性能。然而让我全用<code>C++</code>写代码是不可能的，脚本语言用的多爽呀。所以了解完了<code>C++</code>的强大之后，我就越发的想了解怎么结合两者的方式来提升｀Python｀速度，最后把所有加速手段都测试了一遍，所以就有了这篇博文。</p><p>PS: 之所以花这么多时间介绍<code>C++</code>是因为<code>LLVM</code>就是使用<code>C++</code>写的，而<code>numba</code>依赖<code>LLVM</code>来动态编译出比<code>C</code>更快的机器码，这个也就<code>Python</code>最后能比<code>C</code>还快的主要原因</p><h2 id="（Python）-（C-）难在哪里"><a href="#（Python）-（C-）难在哪里" class="headerlink" title="（Python）+（C++）难在哪里"></a>（Python）+（C++）难在哪里</h2><p>大家都知道<code>Python</code>有很多实现，我们这里说的<code>Python</code>是<code>CPython</code>也是最常见的实现，它是由<code>C语言</code>编译出来的，我们的目标就是把两种语言给混合起来，<code>C+C++</code>。</p><p>我们看看其他语言，比如<code>Java</code>其实也可以混合<code>C++</code>代码，它是采用<code>JNI</code>的方式来进行交互的，如果你了解这种方式，你会发现也非常麻烦，得先写<code>Java</code>的类，然后再生成<code>C/C++</code>头文件。然后你再写<code>C/C++</code>代码，其实我很讨厌这种方式，我希望能把<code>C/C++</code>和你的语言这两种分离开来，我们能简单通过某种方式桥接一下让两个项目能够连贯起来。</p><p>我们现在来看看<code>Python</code>是如何调用<code>C++</code>的代码。在这之前我先提一下<code>Python</code>与<code>C</code>的关系。</p><p>其实<code>Python</code>和<code>C</code>一直非常友好，相比于其他语言，<code>Python</code>在支持上一直尽最大努力，因为<code>Python</code>开发者也知道<code>Python</code>非常慢（相比于C，C++，而且还有GIL的存在无法使用多线程密集CPU计算），所以<code>Python</code>开发者直接在内库上提供支持：<code>ctypes</code>，一个专为调用<code>C</code>代码的库。你只有编写少量代码就能让<code>Python</code>运行你的<code>C</code>代码。理论上你碰到性能问题直接写<code>C</code>就行了，但是我们为什么还要让<code>Python</code>运行<code>C++</code>来加速呢</p><p>四个字：比<code>C</code>更好，<code>C++</code>由于在性能上与<code>C</code>不相上下，而且比C要高级的多（面对对象等），编写速度与维护上比<code>C</code>更加好，而且要知道现在最流行的Java编辑器都是<code>C++</code>写的，还有很多高性能数据库以及机器学习库都是<code>C++</code>写的，虽然在<code>Python</code>中写<code>C</code>更加简单，但是我们还是希望能够用面对对象的方式来编写代码，毕竟我们主要使用的高级语言也是面对对象的</p><p>也正是因为<code>C++</code>提供了一些<code>C</code>没有的面对对象，以及高级特性，这就让我们融合<code>C</code>和<code>C++</code>带来了一些困难。</p><h2 id="Python为什么能够调用C-代码"><a href="#Python为什么能够调用C-代码" class="headerlink" title="Python为什么能够调用C++代码"></a>Python为什么能够调用C++代码</h2><p>我们从调用顺序来看，我们其实想用<code>C</code>代码（<code>Python</code>本质其实是<code>C</code>代码）调用<code>C++</code>，<code>C++</code>比<code>C</code>要高级，出生的也更晚，所以<code>C</code>其实是不知道<code>C++</code>这门语言的，所以<code>C</code>能调用<code>C++</code>，其实是<code>C++</code>对<code>C</code>的一种兼容，这种兼容是<code>C++</code>提供的</p><p><code>C++</code>作为一门偏底层语言，它最终的目的是生成二进制码，<code>C</code>最终也生成二进制码，这个二进制码能直接在CPU里面运行，大家都知道一个代码复用的概念，在二进制层次上，就有这个<code>链接库</code>概率，反正无论谁是最终调用主体，被调用方只需要提供一个规定好的<code>函数库</code>，那么就能实现跨语言的一种交互。</p><p>但是这个交互存在一个问题，<code>C++</code>比<code>C</code>有着更加特性，比如说类，<code>C</code>没有这个概念，假如<code>C++</code>在动态库里面想让<code>C</code>能够调用一个<code>类</code>方法，<code>C</code>根本不知道怎么用，一个类要使用必须牵扯到类初始化，类析构等等。所以<code>C++</code>提供一个关键字<code>extern &quot;C&quot;</code></p><p>这个关键字就是告诉<code>C++</code>编译器把这个块域里面的东西编译成<code>C</code>可以接受的，当然有个前提条件里面代码声明必须是<code>C</code>式的，也就是只能使用<code>C</code>关键字来声明函数结构体什么的，但是在函数内部你可以调用<code>C++</code>代码，声明一个类什么的，最后返回结果。</p><p>用一句话来总结这个关键词的作用就是：告诉编辑器和用户，里面的函数东西，不管中间过程，只需要在“开头”（函数声明），结尾（结果返回）是<code>C</code>模式的，那么这个函数就能在<code>C</code>里面用</p><p>最后我们总结一下<code>Python</code>能够调用<code>C++</code>的代码的原因：只要<code>C++</code>能够”写”成<code>C</code>代码，我们就能调用。这时候你可能有疑惑了，如果把<code>C++</code>写成<code>C</code>那么我们还不如直接写<code>C</code>代码，何必如此复杂的研究这么久了。但是你有没有想过为什么<code>Python</code>是用<code>C</code>写的，最后却能拥有<code>C++</code>、<code>Java</code>这些语言的一样的类特征这个概念。</p><p>这里我们必须要了解一个名词“语法糖”，在我们看来我们能在<code>Python</code>、<code>Java</code>、<code>Ｃ++</code>中使用一些面对对象的特性，比如类、继承、接口。其实这些都只是一些语法糖而已，在这些实现的底层，比如说<code>Python</code>它就是用<code>C</code>的函数来帮助我们构建这些语法糖，我们看到的一个对象的系统函数，其实它是<code>Python</code>帮助我们把一连串函数绑定在一个<code>module</code>上面，虽然表面上我们新建了一个对象，调用了一个对象函数，其实在<code>C</code>层我们就是调用了一连串的函数来完成一个对象的分配</p><p>我们可以在官方文档中找到这部分<a href="https://docs.python.org/3.8/extending/extending.html#the-module-s-method-table-and-initialization-function" target="_blank" rel="noopener">介绍</a>，官方文档告诉我们只要将列表的函数赋给一个模块（<code>module</code>）我们就让你的<code>C/C++</code>代码给<code>Python</code>一个模块可以使用，从官方文档我们就可以很清楚看到<code>语法糖</code></p><p><code>Python</code>的文档非常丰富，理论上我们能够根据文档完成复杂的<code>C++</code>代码与<code>Python</code>交互，但是我们从文档上可以看到，这个过程是非常繁琐的，相比于调用<code>C</code>的简单，为了实现调用<code>C++</code>的类和数据类型，我们得写很多中间代码进行转换，差不多就重新写了一遍<code>C++</code>的实现</p><p>当然作为以简单为美的<code>Pythoner</code>早就发现这个问题，也就这个问题开发了<code>ctypes</code>、<code>cffi</code>、<code>numba</code>等框架帮助，就连在<code>C++</code>大名鼎鼎的<code>boost</code>库中也提供了<code>boost/python</code>来帮助<code>Python</code>更加简单的调用<code>C++</code>，接下来我就根据我对下面这些库来谈一谈我的看法</p><h2 id="框架简析"><a href="#框架简析" class="headerlink" title="框架简析"></a>框架简析</h2><p>单纯的介绍这些库的功能太枯燥了，我就按照我对这些的库的理解将他们编成历史故事（真实出现的原因可能不是这样的）</p><p>话说在<code>Python</code>作者设计<code>Python</code>之后，它发现<code>Python</code>实在是有点慢，为了能加速它就把<code>Python</code>的<code>C</code>API告诉社区的人让他们自己编写<code>C</code>代码然后让<code>Python</code>去调用它</p><p>但是这个API实在是太繁琐了，要写太多附件的<code>C</code>代码了，有些人就发现这个问题，他们设计了一种脚本程序，你只要把你想调用的<code>C</code>函数包在<code>%{</code>里面就能帮你生成<code>Python</code>API的C代码，这样减少了不少代码量，这个框架叫做<code>Swig</code>。</p><p>大家在使用<code>Swig</code>的时候发现一个问题，这个<code>Swig</code>要生成的一个很大的<code>C</code>函数，<code>C++</code>开发者发现了这个问题，他们跟<code>Python</code>开发者说你们是不是瞧不起<code>C++</code>，这个函数这么不优雅，竟然想跟我们代码混起来，想用<code>C++</code>我们帮你，你要生成什么函数告诉我，我帮你生成你引用一下我这个库就行，这样大名鼎鼎的<code>boost::python</code>就开发出来了</p><p>你开心的用起来<code>boost::python</code>来包装一下代码，这样写完<code>C++</code>代码再引入<code>boost::python</code>把<code>Python</code>需要的函数定义一下，编译，OK，但是<code>Windows</code>用户不开心了，这个<code>boost::python</code>是在<code>boost</code>项目下的一个子项目，为了在<code>Windows</code>安装，还得下几百兆的软件包，要是碰到网络不好得下一天。这个时候<code>Python</code>大牛出来了，啥，这么麻烦，我来开发一个包，把<code>boost::python</code>从<code>boost</code>的掏出来，你只需要<code>pip</code>一下就行</p><p>经过几个”小时”开发，<a href="https://github.com/pybind/pybind11" target="_blank" rel="noopener">pybind11</a>开发出来了，还是原来的配方还是原来的味道，管他<code>Windows</code>还是<code>Unix</code>，直接<code>pip</code>一下就能使用<code>boost::python</code> 一样的语法来用了</p><p>就这样安安稳稳的过了一段时间，大家很开心用<code>Python</code>包轻轻松松解决生成<code>Python</code>C API代码的功能。但是随着大家用的越来越多，大家发现怎么我用<a href="https://github.com/pybind/pybind11" target="_blank" rel="noopener">pybind11</a>调用<code>C++</code>跑的有点慢，<code>Python</code>大牛开始研究，重要他们发现由于<a href="https://github.com/pybind/pybind11" target="_blank" rel="noopener">pybind11</a>由于秉承<code>Python</code>的简单至上，很多东西它都做了”通用性“，比如它帮你自动把<code>C++</code>的<code>Vector</code>的类型转成<code>Python</code>的<code>list</code>，这样程序在编译时候不会报错，但是由于这种类型转换太多了，严重的拖累了<code>C++</code>的速度，所以<a href="https://github.com/pybind/pybind11" target="_blank" rel="noopener">pybind11</a>虽然用的很开心，但是速度却比原生的<code>Python</code> C API要慢</p><p>这个时候精通编译原理、<code>Python</code>、<code>C++</code>的大牛出现了，它发现解决这个问题的办法很简单，创造一门中间语言，这么语言可以详细的定义怎么从<code>C++</code>到<code>Python</code>的中间过程，在<a href="https://github.com/pybind/pybind11" target="_blank" rel="noopener">pybind11</a> 中这个完全是一个黑箱子，只有把这个黑箱子拿出来，这样我们就知道你想怎么调用<code>C++</code>，这样就能设计更加优秀的<code>Python</code>C API的代码。最后<code>Cython</code>出现了，它的出现让那些苛求性能的人闭上了嘴，它自动出来的生成<code>Python</code>C API代码近乎人工编写，在这样强的性能加持下，它的速度近乎原生</p><p>至此在生成代码<code>Python</code>C API的中间代码的三方库尘埃落定，没有人想到有更好的办法来优化这一个方向。但是苛刻的人无处不在，他们攻击不了它的性能，只能攻击它的生成方式</p><p>为了使用<code>Cython</code>必须编译它，要么借用<code>setuptools</code>来简单这个步骤，要么自己手动编译，一些开发者叫嚣着，都说<code>Python</code>是个动态语言，怎么还要编译呀，麻烦死了，这个时候一些开发者就站出来了，他们觉得这是个挑战，他们想解决掉它，于是<code>cffi</code>被开发出来了，你不需要用专门的文件存贮<code>C/C++</code>代码，你可以像调用函数一样把<code>C/C++</code>函数原文作为参数传进去，实现动态加载，但是这种动态性还是付出了代价，速度有了一定影响，虽然还是比<code>Python</code>快，但是远远比不上<code>Cython</code>，有得必有失</p><p>这个时候精通汇编的大佬出现了，他们觉得动态加载这个地方还可以加强，他们觉得不需要我们在<code>Python</code>里面写<code>C</code>或者<code>C++</code>，你写一个<code>Python</code>函数，用一个装饰器包装一下，他们直接从底层出发，反正<code>Python</code>最终会编译成机器码，把<code>Python</code>函数的机器码加上类型（Python函数的参数可以是“鸭子”类型，不是强类型），省掉<code>Python</code>冗余的类型推断，直接从机器码层次上进行优化，最后编译成二进制接口给<code>Python</code>调用（背后使用了LLVM进行编译，这里就不详细介绍了），最终它的运行速度小胜<code>Cython</code>，并且比<code>C</code>还略胜一筹，这个就非常恐怖了，因为<code>C</code>基本上是除了汇编以外的速度标杆，所以懂汇编的大佬不要惹，太恐怖了，这个库的名字叫做<a href="https://numba.pydata.org/" target="_blank" rel="noopener">numba</a>，现在这个库已经开发6年多了，由于涉及到从<code>Python</code>源代码到了机器码实在太复杂了，所以仍然在开发中（主要适应各种硬件以及平台），目前处于<code>0.40.0</code>版本，基本上在主流平台使用是没有问题的。</p><p>对于各个库速度的测试可以看看这篇<a href="https://jakevdp.github.io/blog/2013/06/15/numba-vs-cython-take-2/" target="_blank" rel="noopener">博客</a>，可以看到<code>numba</code>完胜<code>C</code>和<code>Cython</code></p><p>PS: 在这里我没有提<code>ctypes</code>因为它是原生的，而且它对<code>C++</code>支持并不很好</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>在速度方面<code>numba</code>加持的<code>Python</code>无疑是No.1，但是它也有几个缺点，一个就是目前还处在开发阶段(目前是0.40版本，还没有1.0版本，而且issue有500个open状态，我在试验的时候也发现存在一些在issue的bug），第二个就是它目前支持能在函数内部运行的库只有<code>numpy</code>（当然这个也是它的设计的一个初衷，就是加速numpy与<code>Python</code>的混合代码）</p><p>当然它的优点完全可以盖过它的缺点，优点有很多，首先第一个它的速度，在<code>LLVM</code>加持下比<code>C</code>更快简直让人震惊，第二个是它调试和维护非常方便，都是由<code>Python</code>编写的，去掉装饰器就是<code>Python</code>代码，直接在IDE里面调试不知道多爽，上线的时候加上注释器跑的飞快（还能丢掉<code>GIL</code>）。目前<code>numba</code>还处于开发过程中，现阶段仍然有很多<code>bug</code>（500个Open的<code>issue</code>），不过正是由于大家都对他非常期望，所以它的<code>issue</code>才那么多，也希望<code>numba</code>能够越来越好，让<code>Python</code>真的起飞。</p>]]></content>
      
      
      <categories>
          
          <category> Python </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>千万级数据处理小结</title>
      <link href="2018/09/06/bigdata/%E5%8D%83%E4%B8%87%E7%BA%A7%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86%E5%B0%8F%E7%BB%93/"/>
      <url>2018/09/06/bigdata/%E5%8D%83%E4%B8%87%E7%BA%A7%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86%E5%B0%8F%E7%BB%93/</url>
      
        <content type="html"><![CDATA[<h2 id="引言"><a href="#引言" class="headerlink" title="引言"></a>引言</h2><blockquote><p>这两个星期的工作主要是对千万文本数据的处理，由于我以前没有接触过类似的数据量，所以我就把我在处理这千万数据的过程中遇到的问题以及解决的方法总结一下</p></blockquote><h2 id="明确目标"><a href="#明确目标" class="headerlink" title="明确目标"></a>明确目标</h2><p>完成任务之前我们必须要明确自己的目标，首先谈一下数据，数据是两张表，一张是文章列表，一张是文章内容，每篇文章都牵涉到一些人，我们的目标就是给定一些搜索条件然后把最可能相关文章给找出来</p><p>这个任务有点像实现一个搜索引擎，我们通过输入关键词把相关的网页寻找出来，简单点来实现就是直接使用<code>SQL</code>的<code>Like</code>查询，但是这里存在两个很大的问题</p><ul><li>搜索精度不准，假如我们搜<code>张华</code>可能有关<code>张华硕</code>的人也会出来</li><li>搜索耗时太长，在千万级文档中全文搜索速度非常慢</li></ul><p>我们希望我们能精确的实现查询，而且我们希望我们的查询能够实现毫秒级的速度。所以我们就尝试使用<code>ElasticSearch</code>来当我们“数据库”，并且放弃系统默认的分词，自己”手动分词“，来实现精准快速查询</p><p>所以我们的目标很简单，将数据从<code>MySQL</code>“塞”到<code>ElasticSearch</code>中，然后想办法再”取”出来</p><h2 id="第一个拦路虎“MySQL”"><a href="#第一个拦路虎“MySQL”" class="headerlink" title="第一个拦路虎“MySQL”"></a>第一个拦路虎“MySQL”</h2><p>我碰到的第一个拦路虎是数据库的响应速度，为了将数据完整的从数据库里面取出来（新数据还在产生），我按照id从小到大的顺序一小块一小块的从<code>MySQL</code>中获取出来</p><p>一开始程序运行的挺Happy，速度一直很稳定，但是我发现跑了十万之后速度突然慢下来，一开始我以为解析有问题，我开始打断点，调试，找了半天原来是数据库返回数据太慢了。</p><p>我们来分析一下这个<code>SQL</code>为什么这么慢</p><pre><code>select * from a order by id limit 10 offset 100000 </code></pre><p>我们虽然限制返回了10个但是后面有个条件我们必须要后面10万个，为了拿到这10个，<code>MySQL</code>必须要扫描10万个数据先，虽然我们是在主键上扫描会快一点，但是十万毕竟很大，即使一次主键扫描花0.01ms，乘以十万也是很大的</p><p>基本上每次解决数据库速度的时候，我们第一考虑点就是索引，那么我们这里就多聊两句：索引为什么快？</p><p>数据库其实就是一堆数据的集合，它提供工具我们快速获取我们想要，用图书馆来打比方，数据库就是图书馆，他们把所以的图书分好类，你想买什么书，按照分类去寻找就行，这样假设你图书馆有一千万本书，你要找《安徒生童话》，你只要按照这个索引（童话书&gt;丹麦&gt;安徒生)就能找到，假如你想找一本《无类》的书你不知道他的分类，你就得把整个图书馆逛一遍才能找到你要的书了，这就是没有索引的下场。</p><p>在程序的世界里也一样，你想快速找到一个记录，如果不用索引，那么就得遍历了，运气好一下子就能找到，运气不好一辈子也找不到。在    <code>MySQL</code>中，索引的背后就是B+树，也就是将数据查找最坏结果降到了一个<code>log2N</code>级。如果你想了解“树”为什么这么快，可以看看我前面写的的<a href="/2017/12/27/浅谈树这种数据结构/">博客</a></p><p>怎么来说明这个索引的作业呢，假设你有一千万数据，你最坏的情况下要进行24次查找，24:1000000 达到惊人的41万倍差距，而且当数据越大这个差距越大，从这里我们就知道索引的威力了。</p><p>我们回到前面，为了使用索引，那么我们只能在<code>id（主键）</code>上做手脚了，我第一个想到的是按照<code>id</code>分块，但是我仔细看了看数据库，<code>id</code>不是全部连续的（可能是因为删除过数据），假如我用<code>id</code>固定的区间来的话，获取到的数据可能部分有部分没有，虽然能够实现但是不够优雅，我还得增加处理空数据的代码。</p><p>这时候我想到了，我们第一次获取的<code>id</code>如果能在后面继续使用，而且更新的话那么我们就能使用上索引了，所以我们只有把每一块数据的最后一个<code>id</code>记住，然后去这个<code>id</code>获取下一批数据，这样就能实现又用索引又不用改太多代码。</p><p>那么我们的<code>SQL</code>就改成下面的语句</p><pre><code>select * from a where id &gt; 1111 order by id limit 10</code></pre><p>通过简单的测试原来需要几分钟才能“掏”出来的数据在几毫秒就取出来了，上万倍的差距。至此我们第一个拦路虎就解决了</p><p>PS：在后面看<code>ElasticSearch</code>文档的时候发现他们也提供了一个<code>scroll</code>（全库获取）的超级翻页功能，在他们的参数里面也要提供一个<code>scroll_id</code>，感觉原理应该也是和这个差不多。通过使用索引<code>id</code>来加速“翻页”</p><h2 id="ElasticSearch存贮和排序"><a href="#ElasticSearch存贮和排序" class="headerlink" title="ElasticSearch存贮和排序"></a><code>ElasticSearch</code>存贮和排序</h2><p>接下来我就介绍，我怎么优化存储和编写定制动态<code>DSL</code>来实现我们想要的功能。</p><p>在我介绍之前，我先简单的谈一下我对<code>ElasticSearch</code>的理解</p><h3 id="ElasticSearch简介"><a href="#ElasticSearch简介" class="headerlink" title="ElasticSearch简介"></a>ElasticSearch简介</h3><p>在我没有真正使用<code>ElasticSearch</code>之前，我就在知乎上听过它的大名，<code>ElasticSearch</code>真正让我震惊的是当我把上千万数据导入到它里面去，它能在毫秒级别给你响应，而我在<code>MySQL</code>调用<code>SQL</code>进行查询得花几十分钟</p><p>我们可以把<code>ElasticSearch</code>类比成一个数据库，相比于<code>MySQL</code>它在查询性能上做到了苛刻的，我一开始想好好介绍它是怎么做到的，但是我发现有人已经总结的非常好了，可以看看这份<a href="https://zhuanlan.zhihu.com/p/33671444" target="_blank" rel="noopener">资料</a>，它之所以能做到这么快的原因就是这个：索引+内存+缓存</p><p><code>ElasticSearch</code>使用倒叙索引让查询时间复杂度降到logN级，使用内存让物理查询速度达到极限，加上一些过滤缓存让其在复杂查询还是简单查询都能保持在一个很平稳的速度</p><p><code>ElasticSearch</code>相比与<code>MySQL</code>还有一个特点，就是对大文本搜索的支持，<code>ElasticSearch</code>对文本默认自动进行分词，并且通过一些高级分类算法（TF/IDF，5.0后使用更加先进的BM25算法），对匹配的文本进行打分，依次返回得分高低列表，而<code>MySQL</code>在大文本检索只有一个全文索引支持，从实现上来看就是一个加了索引的<code>Like</code>查询，所以<code>ElasticSearch</code>在设计的特定算法加持下被称为“搜索引擎”</p><p>但是<code>ElasticSearch</code>同现在商业的搜索引擎，比如Google、百度、Bing这些又有些不同，<code>ElasticSearch</code>传入的是纯文本，所以它只能使用一些<code>TF/IDF</code>算法来计算给定关键词与文本的相关项，但是现在商业引擎输入的是网页，所以现在商业引擎比如Google就使用<code>Google Page Rank</code>算法来再次计算文档相关性。当然现代商业引擎不单仅仅使用<code>Google Page Rank</code>算法，他还会考虑更加因素进去（比如百度的竞价排行，Google的恶意影响网页排行检测），但是从本质上来说，无论是<code>ElasticSearch</code>和现代商业引擎都在做同一件事，给匹配项打分，这就是他们与<code>MySQL</code>的全文检索的不同（<code>MySQL</code>没有后面打分排行的概率，他只有<code>order by</code>的这个概率）</p><h3 id="设计思路"><a href="#设计思路" class="headerlink" title="设计思路"></a>设计思路</h3><p>一开始我准备直接使用<code>ElasticSearch</code>的搜索引擎来对<code>文章</code>中的涉及到的人进行检索排序，但是我们来考虑这样一件事，假如文章中存在这么一句话：“刘二能吃两碗饭”（涉及到的人是刘二能）。假如我们使用“刘二”去检索，这篇文章中的”刘二能“也能检索到，而我们的目的就是尽可能返回最可能的结果，对于那些不可能的结果一律不返回</p><p>所以我们就不能让<code>ElasticSearch</code>自动帮我们对文本进行分词，但是我们想利用<code>打分</code>这个机制帮我们完成最可能在最前面返回</p><p>所以我们把每篇文章里面的人物解析出来的属性（姓名，出生年月，民族等）设定为<code>keyword</code>类型，这样<code>ElasticSearch</code>就不会对这个字段进行分词，查询的时候也必须全匹配才能命中，由于一篇文章可能设计到多个人，所以我把它用一个<code>list</code>存到一个<code>document</code>里面</p><p>但是这个又引起了另外一个问题，对与一个<code>document</code>里面的<code>list</code>，<code>ElasticSearch</code>会把它进行转换</p><p>我们用<a href="https://www.elastic.co/guide/en/elasticsearch/reference/current/nested.html" target="_blank" rel="noopener">官方文档的例子</a>解释，我们存入了下面这个<code>document</code></p><pre><code>{  &quot;group&quot; : &quot;fans&quot;,  &quot;user&quot; : [     {      &quot;first&quot; : &quot;张&quot;,      &quot;last&quot; :  &quot;华&quot;    },    {      &quot;first&quot; : &quot;李&quot;,      &quot;last&quot; :  &quot;四&quot;    }  ]}</code></pre><p><code>ElasticSearch</code>会把它转换成</p><pre><code>{  &quot;group&quot; :        &quot;fans&quot;,  &quot;user.first&quot; : [ &quot;张&quot;, &quot;李&quot; ],  &quot;user.last&quot; :  [ &quot;华&quot;, &quot;四&quot; ]}</code></pre><p>这样你查询这个人<code>张四</code>，我们发现上面这个文档也返回了（选了<code>user.first</code>列表的第一个值，<code>user.last</code>的第二个值，这个结果明显是错误的，我们怎么才能避免<code>ElasticSearch</code>的“自作聪明”呢，答案很简单我们把<code>user</code>声明为<code>nested</code>对象，这样<code>ElasticSearch</code>就不会把它拆开了而是把它当做两个文档（有些人可能会说这个会不要影响它的速率，恰恰相反，<code>ElasticSearch</code>会经常使用类似技术来加速，详情可以看上面的<a href="https://zhuanlan.zhihu.com/p/33671444" target="_blank" rel="noopener">博文</a>)</p><p>现在我们解决了重重困难终于要到排序的阶段了，然而我们没有使用<code>string</code>类型（支持TF/IDF算法）而使用了<code>keyword</code>类型，导致我们没有办法使用<code>ElasticSearch</code>提供的高级排序算法，所以我们得自己手动进行提分，怎么来提分呢，很简单使用<a href="https://www.elastic.co/guide/en/elasticsearch/reference/current/mapping-boost.html" target="_blank" rel="noopener">boost</a></p><p>前面我们提到了我们的文档可能解析出来对象多个属性（姓名、年龄、性别、居住地），但是有些文档也可能没有这些信息，我们查询的时候是有一个信息列表（这个人姓名、年龄、性别等等），所以我们使用<code>boost</code>对命中的信息越多的进行提分，所以我们最终就能完成命中越多信息的排在越前面，当然命中信息少的也会被筛选出来只不过位置稍微靠后</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>通过这次直面千万数据，让我学习到了不少，虽然一开始目的只想简单搜索出来最匹配的数据，但是在实际过程中，通过不断对产生结果提出问题，最终实现了一个比较满意的产品，整个产品在不断的优化过程中逐渐成型并且稳定，我觉得对我帮助最大就是撰写设计文档，并且在产品成型的过程中把结果反馈上去，最后慢慢迭代一个最好的版本</p>]]></content>
      
      
      <categories>
          
          <category> 大数据 </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>毕业两月反思</title>
      <link href="2018/08/25/summary/%E6%AF%95%E4%B8%9A%E4%B8%A4%E6%9C%88%E5%8F%8D%E6%80%9D/"/>
      <url>2018/08/25/summary/%E6%AF%95%E4%B8%9A%E4%B8%A4%E6%9C%88%E5%8F%8D%E6%80%9D/</url>
      
        <content type="html"><![CDATA[<blockquote><p>毕业近两个月了，然而这两个月对自己的状态不太满意故写下这篇博客反思</p></blockquote><h2 id="引言"><a href="#引言" class="headerlink" title="引言"></a>引言</h2><p>大约是去年十一月在室友的带领下走上了健身这条路，说起来也搞笑，当初室友邀请我去健身，然而我每天享受免费操场跑步，反而对于要花钱的健身嗤之以鼻，虽然我知道那些肌肉硕大的大块头都是从健身房出来的，但是我一直没有想过去练出他们一样的肌肉，我运动本身的目的就不是为了炫耀什么的，主要是想要一个健康的身体，然而南昌十一月妖风阵阵，虽然还没到冬天，晚上妖风能让你吹到感觉到冬天的气息，然而中午却非常热，我尝试过早上跑，但是我个人体质太容易出汗了，跑完就需要洗澡，然而早上没有热水（尝试过冷水，差点让我冻僵）</p><p>正好这个时候室友每天去健身，在我的印象里健身房好像有跑步机，于是我一狠心就花了钱办了一张健身卡，刚开始我是奔着跑步机去的，但是我们班长拉住了我开始教我健身动作，我也是在室友和班长的带领下走上了健身这条“不归路”，可怜的跑步机我就没有用过几次，本来一开始是奔着跑步机去的呀。</p><h2 id="为何健体"><a href="#为何健体" class="headerlink" title="为何健体"></a>为何健体</h2><p>到现在自己已经在三个不同的健身房办了卡了，扯远了，健身的确是一件神奇的事，我一直以为运动会让你瘦下来，没想到健身却让我“胖”起来，当我健身到六个月时，我们班长和室友惊奇的告诉我，我的胳膊整整“胖”了一圈，他们还记得我刚来健身房的时候胳膊细的同木棍一样，然而健身对于我来说却并没有很多不同，我只是找到一个地方可以挥洒我的汗水，找到一个地方可以突破自己的极限</p><h2 id="健体而不健心"><a href="#健体而不健心" class="headerlink" title="健体而不健心"></a>健体而不健心</h2><p>健身真的是一件很有成就感的东西，它的成就感不是在于别人夸你的肌肉有多大，而是你能清楚的找到自己的极限，我原来是一个脑洞很大的人，每天大脑天马行空，像一个中二少年一样总想着自己是“超级赛亚人”，能像绿巨人一样发怒把汽车给掀开，一拳把树打倒。当你在健身房，你拿起一个5KG的哑铃，然而你倾尽全身力气却没法做一个标准的动作，你这个时候才知道自己不是”超人“。健身让你更加了解你自己，慢慢的你就接受了这样的自己，健身的成就感也就在于你能慢慢的感受自己的成长。</p><p>也可能是过于关注自己的身体的进步吧，而自己一直忽略了自己心理的进步。自从踏入社会，开始工作。对于工作我一直是打着自己十二分的精力去做，然而对于工作结束后的那段时间自己一直没能够充分利用。主要的原因就是自己的不够自律。一方面也有自己身体的原因，自己健身都是在下班后，健身完之后自己总是筋疲力尽，本来需要充分利用的时间全部被自己的羸弱的意志给消磨掉了，虽然自己一开始能够控制自己，但是一不小心大脑发出疲惫的信号，慢慢的自己就变成“葛优躺”，大把时间就在不经意的打开手机玩上两把，我看一下微信，再看一下QQ，大块时间被手机打碎成一小块一小块，搞得一晚上下来该做的没做，不该做的做了一大堆。</p><p>在这里我必须要深刻的反省，即使是因为身体疲惫的原因，最主要的还是自己不够自律的原因，我其实有很多想法，有很多想做的事情，但是总是在不自律的自己把时间给浪费掉了，毕业之后书自己也很少看了，记得以前在图书馆捧起一本书津津有味的看上一晚上的酸爽。然而现在的自己，即使是看书也要打开电脑，左手拿着手机，右手捧着书，眼镜盯着电脑看一会，书再看一会。书走马观花，看了也是白看。原来经常更新的博客也停下来了。</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>自己也不下什么军令状了，我自己不是一个圣人，我会松懈，我会娱乐，但是我需要自己记住不要因为自己身体的虚弱而导致自己内心的失防，过去自己一直在因为自己过分迁就“健体”而忽视”健心“，自己要牢记只有内心强大才能强大。只要未来的自己不要因为过去的蹉跎的后悔就够了。</p>]]></content>
      
      
      
        <tags>
            
            <tag> summary </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>从关系角度来看XPath</title>
      <link href="2018/08/09/python/%E4%BB%8E%E5%85%B3%E7%B3%BB%E8%A7%92%E5%BA%A6%E6%9D%A5%E7%9C%8BXPath/"/>
      <url>2018/08/09/python/%E4%BB%8E%E5%85%B3%E7%B3%BB%E8%A7%92%E5%BA%A6%E6%9D%A5%E7%9C%8BXPath/</url>
      
        <content type="html"><![CDATA[<blockquote><p>这段时间没有写博客，一个原因是由于刚毕业没了学校的学习动力反而下降，另外一个方面由于花了很长时间研究编译原理，然而自己却对它没有太多理解，所以也就没有整理自己的知识，现在慢慢稳定下来，会继续像以前一样更新博客</p></blockquote><h2 id="引言"><a href="#引言" class="headerlink" title="引言"></a>引言</h2><p>为什么要介绍<code>XPath</code>呢，我一直以为我对<code>XPath</code>还是比较了解的，但是随着我对<code>XPath</code>的了解越来越深，我就对它的越来越敬佩，</p><p>简单来说，我以前认为<code>XPath</code>对结构性文档只能是一把“枪”，指哪打哪，没想到它是一个“巡航导弹”，自动追踪目标。</p><p>接下来我们就慢慢从<code>XPath</code>的基础来谈谈其威力</p><h2 id="什么是XPath"><a href="#什么是XPath" class="headerlink" title="什么是XPath"></a>什么是<code>XPath</code></h2><p>首先我们要知道<code>XPath</code>是一种语言，你可以理解它是正则、也可以理解它是<code>SQL</code>，他们的目的都是从数据中找到我们想要的东西。相比于<code>SQL</code>从数据库中获取数据，<code>XPath</code>是从一个<code>XML</code>文件中获取数据。</p><p>好的，我们知道<code>XPath</code>要操作的对象，什么是<code>XML</code>，它是一种结构式文档，我们也可以把它看做一种树结构。</p><pre><code>&lt;root&gt;    &lt;son&gt; I&apos; m son &lt;/son&gt;&lt;/root&gt;</code></pre><p>如上面就是一个简单的<code>XML</code>文档，首先从一个父节点点出发，到最后的一个父节点结束，中间可以有很多子节点，也可以有孙节点，但对于每个节点来说，其父亲只能有一个。</p><p>这种文档的出现是由于我们编码程序中<code>树结构</code>出现而出现的一种数据。相比于正则直接操作文本，<code>XPath</code>要面对的是是一堆有规律的文本，虽然我们也能使用正则来操作<code>XML</code>文档，但是正则无法捕获这种<code>XML</code>的关系，而这个也是<code>XPath</code>最有力的地方。</p><p>在这种关系中，我们最常使用也是最核心的就是父子关系，这个关系简单的通过一个<code>/</code>就能体现，比如现在我们把上面的<code>XML</code>复杂化给他添加一个儿子</p><pre><code>&lt;root&gt;        &lt;son id=&quot;1&quot;&gt; I&apos; m son1 &lt;/son&gt;        &lt;son id=&quot;2&quot;&gt; I&apos;m son2&lt;/son&gt;    &lt;/root&gt;</code></pre><p>为了获取第二个儿子我们简单的使用这个<code>XPath</code>语句就能获取到</p><pre><code>/root/son[@id=&apos;1&apos;]</code></pre><p>PS: 当然在<code>XPath</code>中我们可以使用<code>//</code>来代表一个泛指，通过<code>//son[@id=&#39;1&#39;]</code>我们可以把儿子找出来而不关心他的父亲，甚至更进一步，我不关心它是谁，只要它的<code>id</code>为<code>1</code>就行，我们用<code>node()</code>函数来替代一个节点，这样只要<code>//node()[@id=&#39;1&#39;]</code>就能拿到<code>id</code>为1的节点了</p><p>我们来看看这个<code>XPath</code>，我们定义了一种关系<code>root</code>和<code>son</code>的父子关系，<code>XPath</code>的威力就是能用很简单的语句来定义一个节点的关系，在这句中，<code>root</code>和<code>son</code>都是节点，我们使用<code>/</code>来约定节点父子关系，使用<code>[]</code>来定义节点与自己内部节点或者属性的直接的关系（<code>@</code>是获取属性）</p><p>要掌握<code>XPath</code>必须要明白，<code>XPath</code>重要的就是“面”和“点”的关系，“面”代表节点，“点”代表属性，对于面来说，它可以包含很多个点，对于点来说，它有可以看做由很多个更小的面组成（微观上）</p><p>就以上面的例子，对于<code>root</code>和<code>son</code>这两个节点，其中<code>root</code>是父节点，我们可以用很多个属性来定义它，比如<code>root[count(son, 2)]</code>（意思是选择有两个<code>son</code>的<code>root</code>），其中对于父节点关系的中<code>son</code>节点来说（有点绕），他又可以用属性来约束比如<code>root[count(son[@id], 2)]</code>(意思是选择有两个<code>son</code>的<code>root</code>,并且每个<code>son</code>都有<code>id</code>这个属性）。从这里我们可以看其实节点和属性是可以相互嵌套的。</p><p>从上面这个小例子我们可以看到，<code>XPath</code>的威力就是它可以用来非常详细的约束节点与其他节点或属性的关系，这种关系可以是绝对的，也可以是相对的，一切取决你的取舍，绝对代表严格，相对代表宽松。</p><p>PS：当然我们这里的<code>属性</code>是一种宽泛的理解，在<code>XPath</code>中节点还包括<code>text</code>值，我们可以把它看做节点的一种<code>text</code>属性。</p><h2 id="XPath的其他关系"><a href="#XPath的其他关系" class="headerlink" title="XPath的其他关系"></a><code>XPath</code>的其他关系</h2><p>前面我们介绍了<code>XPath</code>中最重要的一种关系：父子关系。这个也是我们使用<code>XPath</code>使用的最主要的一种关系，现在基本上网络上的教程都是基于这种关系的，我们这篇博客主要不详细介绍这种关系，你可以在<a href="https://www.w3schools.com/xml/xpath_syntax.asp" target="_blank" rel="noopener">w3cshool</a>上了解更多内容。</p><p>我们先用问题来引入其他的关系，我们再把上面的简单<code>XML</code>进行修改</p><pre><code>&lt;root&gt;    &lt;son id=&quot;1&quot;&gt; I&apos; m son1 &lt;/son&gt;    &lt;target&gt; son1 target&lt;/target&gt;    &lt;son id=&quot;2&quot;&gt; I&apos;m son2&lt;/son&gt;    &lt;target&gt; son2 target &lt;/target&gt;&lt;/root&gt;</code></pre><p> 我们引入两个目标，现在我们想拿到<code>son</code>（id为1）的旁边<code>target</code>，假如我们使用父子关系，使用<code>/root/target[1]</code>(<code>XPath</code>索引从1开始）也可以获取到，但是这里引入了一个约束，必须是<code>root</code>下第一个<code>target</code>节点，假如这个<code>XML</code>它是随机的，<code>son</code>和<code>target</code>是一个集合，但是他们的位置不定，这个时候我们不能仅仅依赖父子关系来确定节点位置。</p><p> 这里我们引入<code>兄弟</code>（sibling）这个概率，<code>son</code>和<code>target</code>是一队兄弟，我们能通过知道<code>son</code>的位置从而定位到<code>target</code>的位置，那个这个<code>XPath</code>该怎么写呢，首先我们要确定<code>son</code>的位置</p><pre><code>/root/son[@id=&apos;1&apos;]</code></pre><p>接下来我们通过定位的<code>son</code>来拿到它后面的兄弟（也有前面的兄弟语法）</p><pre><code>/root/son[@id=&apos;1&apos;]/following-sibling::target[1]</code></pre><p>在这里<code>/following-sibling</code>代表它要找到接下来的兄弟，后面<code>::target[1]</code>是进一步限定我是要拿到兄弟里面第一个<code>target</code>，我们可以通过这个<a href="https://www.freeformatter.com/xpath-tester.html#ad-output" target="_blank" rel="noopener">网站</a>在线测试一下我们的<code>XPath</code></p><p>当然我们可以通过第二个<code>son</code>来找到它前面的兄弟，对应语法是下面的</p><pre><code>/root/son[@id=&apos;2&apos;]/preceding-sibling::target[1]</code></pre><p>在前面我们可以看到这个<code>following-sibling</code>和<code>preceding-sibling</code>他们都是一种寻找兄弟关系的，其实假如我们把<code>-sibling</code>去掉，他们能更加宽泛。</p><p>我们把<code>son</code>和<code>target</code>包起来，这个在现实中可能更常见</p><pre><code>&lt;root&gt;&lt;group&gt;    &lt;son id=&quot;1&quot;&gt; I&apos; m son1 &lt;/son&gt;    &lt;target&gt; son1 target&lt;/target&gt;&lt;/group&gt;&lt;group&gt;    &lt;son id=&quot;2&quot;&gt; I&apos;m son2&lt;/son&gt;    &lt;target&gt; son2 target &lt;/target&gt;&lt;/group&gt;&lt;/root&gt;</code></pre><p>假如我们还使用上面的语句，我们会发现，我们没法找到语句，这个时候你把兄弟这个约束去掉</p><pre><code>/root/group/son[@id=&apos;2&apos;]/preceding::target[1]</code></pre><p>你会惊奇的发现<code>XPath</code>准确的找到我们的目标，这个令人震惊的是它能实现一种“翻山越岭”的查找。</p><p>假如你使用正则或者普通的父子关系，你必须先找到它的<code>group</code>然后再使用<code>for</code>循环来遍历所以的<code>group</code>找到<code>son</code>….</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>我们使用简单一个前后关系就能轻松实现上百行代码，当我以前不了解这个<code>XPath</code>的关系约束前，为了寻找这个约束写过几十行代码才能定位，而现在简简单单一行就搞定，我们不得不佩服前人的智慧，我只想说一句“真香～～～”。</p>]]></content>
      
      
      <categories>
          
          <category> Python </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>Java的char类型到底几个字节</title>
      <link href="2018/07/01/java/Java%E7%9A%84Char%E7%B1%BB%E5%9E%8B%E5%88%B0%E5%BA%95%E5%87%A0%E4%B8%AA%E5%AD%97%E8%8A%82/"/>
      <url>2018/07/01/java/Java%E7%9A%84Char%E7%B1%BB%E5%9E%8B%E5%88%B0%E5%BA%95%E5%87%A0%E4%B8%AA%E5%AD%97%E8%8A%82/</url>
      
        <content type="html"><![CDATA[<h2 id="引言"><a href="#引言" class="headerlink" title="引言"></a>引言</h2><p>之所以有这个疑问，是上次阅读Java基础书时碰到讲解<code>char</code>类型没有看明白，并且在代码验证过程中错误的理解了代码的意思，导致我对这么个简单问题产生疑惑并且“恶意揣测”Java内部的黑魔法，这里就把我如何走上歪路，并且最终找到“正确”的道路的故事讲出来</p><h2 id="问题的产生"><a href="#问题的产生" class="headerlink" title="问题的产生"></a>问题的产生</h2><p>我们知道<code>Java</code>是采用<code>Unicode</code>进行内部编码，但是使用<code>UTF-16</code>作为外部编码。</p><p>怎么来理解这个东西呢。首先你要知道<code>Unicode</code>是在我们熟悉的<code>GB 18030</code>、<code>BIG-5</code>、<code>ISO8859-1</code>之后出现的，它的出现就是为了统一全世界的编码，因为前面这些编码都太片面了，只包含自己国家或者少数几个国家的字符。</p><p><code>Unicode</code>的目的就是包括全世界的编码，并且给未来可能出现的编码留下位置，你可以理解为它是一张大“表”，一般我们使用16进制来表达它，并且在前面加上<code>U+</code>。例如<code>U+0041</code>代表字母<code>A</code>，但是这里有个历史问题</p><p>一开始我们知道<code>Unicode</code>为了包含全世界的字符从<code>ASCII</code>的一个字节扩展到两个字节，就能包含65536个字符了，但是随着字符包含越来越多，我们逐渐需要更多字符了，最后扩展到<code>U+0000 -&gt; U+10FFFF</code>去了，为了表示这些我们必须使用三个字符，假设我们不考虑内存成本，每个字符都使用四个字符来表示（不使用三个是为了内存对齐），那么问题就解决了，大家都用<code>Unicode</code>来表示，这样我传给你一串字符你就能秒懂了。</p><p>但是学过信息论就知道，单字符越长信息熵也就是信息量就少，其实在日常通信中我们并不是每个字符都会用到，为了提高效率，我们可以使用霍夫曼、香农编码技术对信息重新编码，这个就是<code>UTF-8</code>、<code>UTF-16</code>等现代编码的理论基础。</p><p>这就好比特种部队手势，我们把作战命令（Unicode）需要的指令放到手势（如UTF-8）里面，这样几个手势就能表达复杂的作战计划（假如用嘴巴说的话）。</p><p>接下来我们就从JAVA和Python来看，编码与其关系</p><h2 id="表面兄弟：JAVA"><a href="#表面兄弟：JAVA" class="headerlink" title="表面兄弟：JAVA"></a>表面兄弟：JAVA</h2><p><code>Unicode</code>对于JAVA来说，只能算是表面兄弟，虽然内部支持<code>Unicode</code>编码，但是其本质还是基于<code>UTF-16</code>编码，为什么要这么说呢。</p><p>我们来回顾一下，我们知道<code>Unicode</code>的范围是<code>U+0000-U+10ffff</code>，这意味着我们没法用两个字节来表示，但是在<code>Java</code>里面<code>char</code>类型字节为2字节，而对于字符串类<code>String</code>来说，其组成就是一个<code>char</code>字组，对于小于<code>U+10000</code>的<code>Unicode</code>码来说，<code>String</code>对象最小组成单位就是<code>char</code>，但是对于大于<code>U+10000</code>的<code>Unicode</code>码来说却是<code>char</code>数组，我们用代码来展示一下两者之间的关系。</p><pre><code>char[] chars = Character.toChars(0x1f121);String s = new String(chars);</code></pre><p>而且我们将<code>s</code>输出的话，会发现它是一个字符，但是它的<code>length</code>却为二，而且我们将<code>s</code>每个字符转换成二进制你会发现他们的值依次为<code>0xd83c</code>和<code>0xdd21</code>，他们存贮的值全部以<code>UTF-16</code>的格式存贮，具体编码详细我就不细说了，下面<a href="https://zh.wikipedia.org/wiki/UTF-16" target="_blank" rel="noopener">资料</a>介绍的很详细（需要翻墙）。在<code>Unicode</code>里面占一个字符的值，却以两个基本类型存贮，当然为了维持这种“表面兄弟”的关系，<code>Java</code>也使用了“码点”来支持一下兄弟，只要使用<code>codePointAt</code>代替<code>charAt</code>，用<code>codePointCount</code>代替<code>length</code>，我们也能处理超过<code>U+10000</code>的<code>Unicode</code>编码（对于不超过<code>U+10000</code>的字符那就是“真兄弟”）</p><p>当我不知道一个<code>char</code>只能放两个字节的时候，我强行使用<code>char c = (char)0x1f121</code>来“存”一个超过<code>U+10000</code>的<code>Unicode</code>码，结果被<code>Java</code>无情的溢出掉，只取到了部分值，但是我却误以为<code>Java</code>有黑魔法能用两个字节存贮了三个字节才能存下的值，乃至我闹了个笑话。</p><p>总结一下<code>Java</code>是一个非常严谨的语言，规定死的东西就不会变，表面上看<code>Java</code>能够支持<code>Unicode</code>编码，但是实际上他只是编译器支持，比如你写一个<code>🄡</code>（0x1f122）的值来赋给<code>String</code>如下面：</p><pre><code>String ns = &quot;🄡&quot;</code></pre><p>表面上看，<code>Java</code>完全支持<code>Unicode</code>码，但在实际的上面他内部还是用<code>UTF-16</code>进行编码，只是在编译的时候帮我们将<code>0x1f122</code>转换成为两个<br><code>0xd83c</code>和<code>0xdd21</code>存贮在<code>char</code>字符组里面。</p><p>其实这个表面兄弟是相对的，从<code>Python3`</code>Unicode`支持来比较一下就能发现不同。</p><h2 id="亲兄弟：Python"><a href="#亲兄弟：Python" class="headerlink" title="亲兄弟：Python"></a>亲兄弟：Python</h2><p><code>Python3</code>对<code>Unicode</code>是非常友好的，它在明面上完全按照<code>Unicode</code>的编码表使用来存贮<code>Unicode</code>码，对应它的<code>Unicode</code>字符串，最小单元都是<code>Unicode</code>码，多说无意，上代码。</p><pre><code>  c = chr(0x1f122)print(len(c))  # = 1print(type(c)) # str</code></pre><p>我们可以看到我们得到的最小的码元是字符串<code>str</code>类型，无论这个<code>Unicode</code>码是否大于<code>U+10000</code>，<code>Python</code>都把它视为一个基本单位，这样避免了你对其进行一些误操作，插句话来讲讲怎么得到这个大小呢，我们使用<code>sys.getsizeof</code>方法就能计算出来</p><pre><code>sys.getsizeof(chr(0x1f122))  # 80sys.getsizeof(chr(0x1f122) * 2) # 84</code></pre><p>由于<code>Python</code>使用一些字段来标注类型，所以直接使用<code>sys.getsizeof</code>得不得一个<code>Unicode</code>码需要的字节，所以我们计算两个的差，很清楚的就能得到一个<code>Unicode</code>码使用四个字节，你可以依次乘下去，而且你发现一个有趣的现象，对于小于<code>U+007F</code>的<code>Unicode</code>码，其大小为一字节，而对于<code>U+0080-U+07FF</code>其大小为两字节。具体可以看参考<a href="https://en.wikipedia.org/wiki/UTF-8" target="_blank" rel="noopener">资料</a>，<code>Python</code>内部是使用<code>UTF-8</code>来存贮<code>Unicode</code>码的，但是<code>Python</code>将这一切都隐藏起来，你从表面上看好像一个<code>Unicode</code>就是一个最小单元，对于其底层我们不得而知，我们可以从侧面来验证一下</p><pre><code> timeit.timeit(&quot;&apos;中国人&apos;.encode(&apos;gbk&apos;)&quot;)&gt;&gt; 0.6366317819999949timeit.timeit(&quot;&apos;中国人&apos;.encode(&apos;utf-8&apos;)&quot;)&gt;&gt; 0.2109854949999317</code></pre><p>我们可以看到将<code>Unicode</code>编译成其他编码方式，其中<code>utf-8</code>速度是最快的，因为基本上是复制一下就行了，而其他的差距到了三倍</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>通过前面我们知道，<code>Python</code>之所以 <code>Unicode</code>如此“亲兄弟”是因为做了一层封装得来的，相比<code>Java</code>将<code>Unicode</code>码（使用<code>UTF-16</code>作为底层编码）暴露给出来，<code>Java</code>在底层上却是非常“坦诚”，你想直接使用<code>Unicode</code>码值也可以，<code>Java</code>编译器会帮你把<code>Unicode</code>码值转换成<code>UTF-16</code>，你也可以从<code>UTF-16</code>码生成<code>String</code>字符串，这样底层在实现查找的时候也是使用统一的编码进行。但是也正是由于这么“底层”，代码看起来总不是那么“亲”，相比于<code>Python</code>的“一视同仁”，我们也可以理解这就是这两种语言的各自特点所在。</p><p>总的来说如果你想直接接触代码底层，推荐使用<code>Java</code>，假如你只想研究其本质，推荐使用<code>Python</code>来进行自然语言处理，他的封装能让你不需要了解其内部组成。</p><h2 id="引用"><a href="#引用" class="headerlink" title="引用"></a>引用</h2><p><a href="https://zh.wikipedia.org/wiki/UTF-16" target="_blank" rel="noopener">https://zh.wikipedia.org/wiki/UTF-16</a></p><p><a href="https://en.wikipedia.org/wiki/UTF-8" target="_blank" rel="noopener">https://en.wikipedia.org/wiki/UTF-8</a></p>]]></content>
      
      
      <categories>
          
          <category> Java </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>从例子里解Spring IOC</title>
      <link href="2018/05/28/java/springboot/%E4%BB%8E%E4%BE%8B%E5%AD%90%E9%87%8C%E8%A7%A3SpringIOC/"/>
      <url>2018/05/28/java/springboot/%E4%BB%8E%E4%BE%8B%E5%AD%90%E9%87%8C%E8%A7%A3SpringIOC/</url>
      
        <content type="html"><![CDATA[<h2 id="Spring的IOC理解"><a href="#Spring的IOC理解" class="headerlink" title="Spring的IOC理解"></a>Spring的IOC理解</h2><h2 id="什么是IOC"><a href="#什么是IOC" class="headerlink" title="什么是IOC"></a>什么是IOC</h2><p>在这里我们不谈Spring的基础知识,我们知道谈到Spring就会谈到IOC,这个IOC是什么呢,中文名叫控制反转，这个东西是伴随着一些编程思想出现,其实同Java的本身也有关</p><p>就好比我熟悉的Python就是一个鸭子语言,你可以随便把一个值丢掉函数里面去,只要他满足一些特性就能正常运行,但是Java是一种强类型语言,你函数给什么参数,必须传什么参数</p><p>这里就不讨论两张语言的设计优劣呢,Java这种特性也做了一些妥协,我们肯定得为语言的扩展性做点事,谁也不知道未来会发生什么,Java里面使用多态来实现这种扩展,只要他是函数参数的家族成员,他就能上去运行</p><p>这个多态是实现IOC的基础,但是造成他出现的原因是因为设计模式里面的单一职责原则,这个要求我们类功能要单一,我们这里给一个例子来说明这个问题</p><pre><code>class Car {    void run() {        System.out.println(&quot;Car running...&quot;);    }}</code></pre><p>首先我们有一个<code>Car</code>的类,一开始我们只让他有<code>run</code>这个属性,很好,接下来我们想知道是谁驾驶这辆车,于是我们便给这个类加一个字段<code>driver</code></p><pre><code>public class Car {    String driver;    public Car(String driver) {        this.driver = driver;    }    void run() {        System.out.println(&quot;Driver :&quot; + driver);        System.out.println(&quot;Car running...&quot;);    }}</code></pre><p>很好我们知道驾驶这辆车的人,接着我们又想知道这个驾驶人的驾龄,如果我们继续给<code>Car</code>加入字段,这样我们就违背了<code>单一职责原则</code>,<code>Car</code>类不但承担了车的功能还承担了人的功能</p><p>于是我们就把驾驶人隔离出来</p><pre><code>class Driver{    String name;    String age;    public Driver(String name, String age) {        this.name = name;        this.age = age;    }}class Car {    Driver driver;    public Car(Driver driver) {        this.driver = driver;    }    void run() {        System.out.println(&quot;Driver age:&quot; + driver.age + &quot;name: &quot; + driver.name);        System.out.println(&quot;Car running...&quot;);    }}</code></pre><p>我们重新将类分成两个类来实现了这个问题,但是这个时候又来了一个问题,我们有一个飞行员的也想驾驶这辆车,但是这辆车只能司机来驾驶,但是飞行员和司机开车的动作步骤是一样的,为了复用<code>run</code>这个函数,你开始揪起了你的头发.</p><p>你想呀想突然想到,Java的多态,假如我们声明一个<code>IDriver</code>的接口,让飞行员和司机都继承这个类这样我们只要给车一个<code>IDriver</code>对象就能复用<code>run</code>函数</p><pre><code>//IDriver.javapublic interface IDriver{    String getName();    void setName(String name);    int getAge();    void setAge(int age);}// Driver.javapublic class Driver implements IDriver{    String name;    int age;    @Override    public String getName() {        return this.name;    }    @Override    public void setName(String name) {        this.name = name;    }    @Override    public int getAge() {        return this.age;    }    @Override    public void setAge(int age) {        this.age = age;    }}// Aviator.javapublic class Aviator implements IDriver{    String name;    int age;    @Override    public String getName() {        return null;    }    @Override    public void setName(String name) {    }    @Override    public int getAge() {        return 0;    }    @Override    public void setAge(int age) {    }}//Car.javapublic class Car {    private IDriver driver;    public void setDriver(IDriver driver) {        this.driver = driver;    }    void run() {        System.out.println(&quot;Driver age: &quot; + driver.getAge() + &quot; name: &quot; + driver.getName());        System.out.println(&quot;Car running...&quot;);    }}</code></pre><p>我们重构代码把<code>Driver</code>抽象为接口,然后让司机和飞行员都继承它,这样不管我们再添加什么其他的人就能适配这辆车.这个就是依赖倒置(DI)的思想</p><p>网上大部分教程就停留到这里了,这里我们继续探索下去,看看Spring是如何让这个DI更加简单的</p><p>首先我们反思一下,我们使用接口参数让我们的代码符合了设计模式,但是也带来了一些繁琐,我们来用代码”开”这辆车</p><pre><code>IDriver driver = new Driver();driver.setName(&quot;allen&quot;);driver.setAge(18);Car car = new Car();car.setDriver(driver);car.run();</code></pre><p><strong>PS：当然可以把赋值放到构造器中减少代码，但是由于<code>Bean</code>依赖方法接口来赋值，所以为了后面讲解<code>Bean</code>这里就不采用构造器来减少代码</strong></p><p>代码有2行变成了6行，而且我们发现这个代码现在带来两个问题：</p><ol><li>每次运行都得创建一个实现<code>IDriver</code>的对象</li><li>每次我们想换人开车的时候都得修改源代码</li></ol><p>而且这些工作都很繁琐，作为一个偷懒的程序员，我可不想给每个用户都重新写一套代码，我们的想法很简单，我们希望这个<code>Car</code>能够开箱即用，其实前面我们已经实现了控制反转了，现在就是要解决控制反转带来的“负面影响”</p><p>而且我们发现了一个问题，假如我们把上面函数放到一个代码里面，每次我们“开车”都得创建一个司机，然而我们还是相信“老司机”的手艺，所以我们也希望是否能够”记住“司机，只让一个老司机开车</p><p>接下来就是隆重介绍<code>Spring</code>的<code>Bean</code>的用法了，前面我们知道我们需要某种机制来去除”IOC“的弊端，我们把每个<code>Car</code>当做一个对象，其实我们需要一个配置文件来记录<code>IDriver</code>这些依赖对象，对象的其实在Java里面表现就是一棵树，所以通俗来讲我们需一个”树结构“数据来存贮依赖关系</p><p>我们程序在运行的时候解析这个树结构，然后依次给对象注入你想给他实例话的对象（比如你把”IDriver“设置为飞行员），这样的话，我们把依赖关系成功放到了配置文件中</p><p>这样带来两个好处：</p><ol><li>想给不同用户使用软件时候，源代码不需要改变，只要给他们不同的配置文件就行</li><li>我们可以保存依赖实现”老司机“的复用</li></ol><p>所以现在我们理理思路，我们需要的有两个东西</p><ol><li>配置文件</li><li>一个加载配置文件并保存依赖的对象</li></ol><p>在<code>Spring</code>的<code>Bean</code>中这两个分别对应<code>xml文件</code>和实现<code>ResourceLoader</code>接口对象（有多种实现）</p><p>为了更好的理解<code>Bean</code>，接下来我们就从代码出发来测试这个<code>Bean</code></p><h2 id="最简单的实现"><a href="#最简单的实现" class="headerlink" title="最简单的实现"></a>最简单的实现</h2><p>首先我们新建一个<code>Spring</code>项目，无论你是用<code>IntelliJ</code>还是<code>Eslipse</code>都没关系，只能你能引用到<code>Spring</code>库就行，我们复用前面的代码，看看使用<code>Spring</code> <code>Bean</code>来如何解决掉IOC的”副作用“</p><p>我们把前面的类分别放到同一路径不同的文件夹中，接下来我们先创建一个<code>xml</code>文件，什么名字不重要，我们这里先命名为<code>driver.xml</code></p><pre><code>&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;beans xmlns=&quot;http://www.springframework.org/schema/beans&quot;       xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot;       xsi:schemaLocation=&quot;http://www.springframework.org/schema/beans    http://www.springframework.org/schema/beans/spring-beans.xsd&quot;&gt;    &lt;bean id=&quot;car&quot; class=&quot;Car&quot;&gt;        &lt;property name=&quot;driver&quot;&gt;            &lt;bean class=&quot;Driver&quot;&gt;                &lt;property name=&quot;name&quot; value=&quot;Allen&quot;&gt;&lt;/property&gt;                &lt;property name=&quot;age&quot; value=&quot;18&quot;&gt;&lt;/property&gt;            &lt;/bean&gt;        &lt;/property&gt;    &lt;/bean&gt;&lt;/beans&gt;</code></pre><p>写入这些东西，接下来我们看看是否能够通过这个<code>xml</code>文件来直接得到一个配置好司机<code>Allen</code>的车</p><p>随便新建一个类在上面的路径中,我们这里就新建一个<code>Main</code>吧</p><pre><code>import org.springframework.context.ApplicationContext;import org.springframework.context.support.ClassPathXmlApplicationContext;public class Main {    public static void main(String[] args) {        ApplicationContext context = new ClassPathXmlApplicationContext(&quot;driver.xml&quot;);        Car car  = context.getBean(&quot;car&quot;, Car.class);        car.run();    }}</code></pre><p>输出为</p><pre><code>Driver age:18 name: AllenCar running...</code></pre><p>我们成功通过一个<code>xml</code>配置文件和一个<code>ApplicationContext</code>对象实现了一个即开即走的车，而且假如我们想换个司机，我们可以修改配置文件把<code>class</code>换成“飞行员”，而且我们可以发现我们得到的司机都是一样的，验证方法很简单，我就不写代码了，假如我们想换司机怎么办，简单在<code>bean</code>里面加上<code>scope=“prototype”</code>就行（默认值为<code>singleton</code>）</p><p>接下来我们又有一个疑问，假如我们有一辆特别宝马车，我们希望只有某一种加上员能能开（假设只有飞行员），也就是是说，我们其实即不想放弃IOC，但是又不想将这个配置写到<code>Bean</code>里面去，有办法能够解决吗？</p><p>当然有，<code>Spring</code>2.5就支持注解来写<code>Bean</code>配置，对于一些固定的类，我们可以把依赖关系用代码写到类中，这样一方面能够保证<code>IOC</code>，一方面又能实现<code>Bean xml</code>文件瘦身</p><p>由于<code>Spring</code>默认不会去扫描注解，所以有三种方式，第一种是在<code>xml</code>里面用加上一个</p><pre><code>&lt;context:component-scan base-package=&quot;....&quot;&gt;&lt;/context:component-scan&gt;</code></pre><p>第二种是使用<code>AnnotationConfigApplicationContext</code>来对象来进行扫描，第三种就是<code>SpringApplication</code>来运行<code>Spring</code>程序自动扫描</p><p>这三种方式假如你最后要做一个<code>web</code>程序的话，第三种是非常方便的，这里我们就不谈怎么使用注解来代替<code>xml</code>文件了，本质上是一样的，其实在我没有理解<code>Bean</code>的强大之前，我比较推崇使用注解来写<code>Bean</code>，但是随着对<code>Bean</code>的探索，我发现<code>xml</code>文件才是最佳选择，他将程序依赖与代码分离开来，假如我们还想用程序依赖写在代码里面，那就违背了<code>Bean</code>的设计初衷</p><p>如果你想了解怎么使用注解可以阅读这篇<a href="https://blog.csdn.net/sinat_34596644/article/details/53080026" target="_blank" rel="noopener">博客</a></p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>至此，我们从问题的出现到问题的解决探索了IOC背后的故事，但是你可能会有一个疑问，为什么<code>Spring</code>里面会有<code>IOC</code>问题。</p><p>其实这个也跟<code>Web</code>的发展有关，我们知道从Web的发展，一开始是没有前端的，只有后端，慢慢的后端分离出来前端，Web端页面也被分离出视图层和数据层，随着逐渐分离，也就出现我们前面举到的例子，类越来越多，比如视图层依赖数据层，数据层依赖控制层…..</p><p>这种层层依赖的问题延生出来的IOC的提出，也就慢慢的促进了<code>Bean</code>这个库的开发，也正是因为<code>Bean</code>我们才能享受静态强类型语言的低耦合的酸爽。</p>]]></content>
      
      
      <categories>
          
          <category> Java </category>
          
      </categories>
      
      
        <tags>
            
            <tag> SpringBoot </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>从MNIST了解卷积神经网络</title>
      <link href="2018/04/24/ai/%E4%BB%8EMNIST%E4%BA%86%E8%A7%A3%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"/>
      <url>2018/04/24/ai/%E4%BB%8EMNIST%E4%BA%86%E8%A7%A3%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/</url>
      
        <content type="html"><![CDATA[<h2 id="引言"><a href="#引言" class="headerlink" title="引言"></a>引言</h2><blockquote><p>本文是学习Tensorflow<a href="http://www.tensorfly.cn/tfdoc/tutorials/mnist_pros.html" target="_blank" rel="noopener">官方文档</a>的过程中的一点感悟，本文假设你对矩阵运算有一定的了解，具体可以看看下面<a href="http://www.ruanyifeng.com/blog/2015/09/matrix-multiplication.html" target="_blank" rel="noopener">资料</a></p></blockquote><h2 id="加载数据"><a href="#加载数据" class="headerlink" title="加载数据"></a>加载数据</h2><blockquote><p>首先我们得先把数据下载下来，Tensorflow给我们提供了一个函数来进行下载，这个<a href="https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/learn/python/learn/datasets/mnist.py" target="_blank" rel="noopener">函数</a>为<a href="https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/learn/python/learn/datasets/mnist.py" target="_blank" rel="noopener">read_data_sets</a></p></blockquote><p>这个函数<code>read_data_sets</code>函数很简单，查看在目录下面有没有文件没有就去下载，有就解析加载，一方面方便我们获取数据，一方面方便我们直接开箱即食，但是由于这个默认下载地址是需要翻墙，所以我这里提供一个不需要翻墙的<a href="http://yann.lecun.com/exdb/mnist/" target="_blank" rel="noopener">地址</a>，你只需要加载下面的函数</p><pre><code>from tensorflow.examples.tutorials.mnist import input_datamnist = input_data.read_data_sets(&quot;input/&quot;, one_hot=True, source_url=&quot;http://yann.lecun.com/exdb/mnist/&quot;)</code></pre><p>等几分钟，数据就会下载到当前目录的<code>input</code>文件夹中，这样你下次运行就能直接本地文件夹中加载图片数据了</p><h2 id="观察数据"><a href="#观察数据" class="headerlink" title="观察数据"></a>观察数据</h2><p>首先我们看看下载了什么数据，打开<code>input</code>文件夹，我们可以看到，Tensorflow给我下载好了四个文件，分为两组，一组训练集一组测试集，每组里面2个文件，一个是手写图片文件，一个标签文件（每张手写的图片代表的数字）</p><p>加载图片数据对于新手来说挺麻烦的，为了让我们专注于模型而不是编程，Tensorflow直接帮我们做好了加载数据的事情，我们上面得到的<code>mnist</code>变量里面就存贮了我们这个项目所需要的数据，我们来看看这个<code>mnist</code>有什么</p><p>我们最关心的就是<code>mnist</code>里面训练数据，这里推荐使用<code>notebook</code>来操作这个数据集，我们首先<code>mnist</code>的训练数据是什么</p><p><img src="/images/mnist-nums.png" alt="mnist数据来源网络"></p><p><code>mnist</code>数据就是上面这些图片，我们把图片把每个像素的二值化，然后把他们放到一个数组中，每张图片对应一个数组</p><p>mnist训练数据存贮在这两个变量中</p><pre><code>mnist.train.labelsmnist.train.images</code></pre><p>其中<code>mnist.train.images</code>是一个<code>(55000, 784)</code>的二维数组，其中<code>mnist.train.labels.shape</code>是一个<code>(55000, 10)</code>的二维数组，现在摆在我们面前的其实很简单，通过<code>55000</code>个图片像素值来训练我们模型，以便能让模型能给一张图片像素值来预测图片代表的数字</p><p>这些数字在人看来非常容易辨认，但是怎么能让电脑也能辨别他呢，这就要用到卷积神经网络的力量，通过卷积神经网络，电脑的准确率能到99%，这就非常恐怖了，我们人有时候也会看走眼呢。</p><p>在谈卷积之前我们先谈谈我们以前的做法，这样通过对比就能知道卷积到底做了什么优化</p><h2 id="传统做法"><a href="#传统做法" class="headerlink" title="传统做法"></a>传统做法</h2><p>其实从传统的角度来看，其实图像识别也就是通过图片的特征值来判断图片代表的含义，但是图片这个东西又很特殊，相比于其他机器学习问题，他的特征值很多，这里我们使用28X28的图片就有784个特征，如果我们图片尺寸再大，这个特征值会变得非常巨大，而且我们知道机器学习需要大量数据才能大展身手，然而每个图片如此巨大，训练巨大的数据集电脑也吃不消</p><p>所以我们必须要将数据进行降维，机器学习里面有很多降维的方法，比如PCA，LDA这些，但是这些方法都有一个问题他们必须把一个图片看做一个整体输入，也就是前面的将28X28转换成一个784的数组，这个数组我们知道，他丧失了一个非常重要的东西维度，我们仔细观察上面的图片</p><p><img src="/images/mnist-nums.png" alt="mnist数据来源网络"></p><p>每个图片其实我们关注的都是数字的二维分布，我们通过闭合的圆的个数来区分8和0，我们通过中间的空白部分来区分0和1，所以我们希望能使用一种新的方法来确定图片特征，一方面能够保存图片的空间信息，一方面能最终数据一维的结果（图片代表的数字），这个就是卷积的引入了，卷积从二维的角度来提取图片的特征，相比于传统的一维提取，它能最大程度保留图片的信息，并且进行深度降维</p><h2 id="从项目了解卷积"><a href="#从项目了解卷积" class="headerlink" title="从项目了解卷积"></a>从项目了解卷积</h2><blockquote><p>一开始学习深度学习卷积神经网络，看了很多资料，但是总是感觉并没有很深的理解，至到接触这个项目，从代码的层次上再去理解卷积才给我恍然大悟的感觉</p></blockquote><p>首先先谈一谈Tensorflow这个库的基础知识，由于Python速度有点慢，所以Tensorflow的后端全部由C++写的，你可以这样理解Tensorflow，Python相当于一个客户端，你可以使用一个session（回话）与服务器（C++）进行交互，这样的话，我们在客户端可以享受Python的方便快捷，也可以享受C++运行的高效性，但是这个也带来一个麻烦，原来Python是一个所见即所得的，现在运行一些东西必须使用session来通知服务器来运行，我们很多中间过程就没法知道，只能通过返回的结果来进行推断了。在官方教程并没有讲太多中间过程，只是一笔带过，所以为了更好的理解卷积神经网络，我们将会以一种很难看的方法运行Tensorflow，但是我们能从这个过程中对卷积的理解更加深刻</p><p><strong>所以接下来我们基本上每个操作都会让后端运行并且分析返回结果，为了方便叙述，我们假设你在运行<code>session.run</code>之前都会运行这个<code>session.run(tf.global_variables_initializer())</code>来初始化所以的变量</strong></p><p>PS：之所以要运行这个，因为我们使用session与<code>C++</code>进行交互，如果我们“不声明”变量，c++会报错的</p><p>下面我们就从这个项目一行一行讲起</p><h3 id="准备数据"><a href="#准备数据" class="headerlink" title="准备数据"></a>准备数据</h3><p>前面我们知道，卷积就是要从二维空间中来提取我们想要的特征，首先我们把数据还原成二维的</p><pre><code>x_image = tf.reshape(x, [-1,28,28,1])</code></pre><p>x是上面我们输入的数据，来我们来检测一下，首先我们声明一个<code>session</code></p><pre><code>session = tf.Session()</code></pre><p>再从数据集中掏出50张图</p><pre><code>data = mnist.train.next_batch(50)[0]</code></pre><p>接下来我们看看这个<code>x_image</code>变成了什么</p><pre><code>session.run(tf.global_variables_initializer())x_image_data = session.run(x_image, feed_dict={x: data})</code></pre><p>我们输入两者的<code>shape</code></p><pre><code>data.shape, x_image_data.shape(50, 784) (50, 28, 28, 1)</code></pre><p>我们很清楚的看到，我们成功将一维的数组图像（784）变成了二维的数组图像（28X28），其实我们生成了三维（28 X 28 X 1），但是由于我们只有有些图片还会有多个色道（RGB），所以我们为了兼容，声明成28 X 28 X 1</p><p>好的，现在我们成功将一维图片还原成二维的，接下来就是将他们卷起来的时候了</p><h3 id="第一层"><a href="#第一层" class="headerlink" title="第一层"></a>第一层</h3><p>如果你学过一些信号处理你会发现，深度学习使用的卷积其实并不是原始意义上的卷积，他没有“旋转180”的操作，但是他的形式其实是类似的。这个“积”的操作主要是通过矩阵运算来实现的，为了更好的理解卷这个操作，我从网上找了前辈们辛苦做的动图</p><p><img src="/images/Tensorflow-mult.gif" alt="卷积操作-来源网络"></p><p>PS： 这个图与我们数据有点不同，我们每张只有一个色道，这个有三个色道，这张图有两个卷积核，但是我们这个第一层会使用32个，但是其实原理都一样，如果你实在理解不过来，你可以先值看最上面那一排</p><p>我们回到这种图，最左边就是图像输入，中间是卷积核，最后右边是输出，我们可以从图中可以很清楚的看到卷积的与我们平常操作不同，首先输入上我们是二维数据，通过二维的卷积核进行矩阵运算，最后我们输出二维结果，这就是卷积的强大之处，不但保留了原来的二维信息而且能够使用高效的矩阵运算来加速提取特征</p><p>现在我们回到代码</p><p>首先是要声明卷积核，我们可以使用简单的方法，将卷积核全部声明为全0矩阵，但是这个有可能造成0梯度，所以我们加入一点噪音，我们看看加入噪音的卷积核是什么值</p><pre><code>initial = tf.truncated_normal([5, 5, 1, 32], stddev=0.1)W_conv1 = tf.Variable(initial)session.run(tf.global_variables_initializer())W_conv1_value = session.run(W_conv1)W_conv1_value.mean(), W_conv1_value.std()(0.001365028, 0.08744488)</code></pre><p>我们使用<code>tf.truncated_normal</code>函数声明了32个5X5X1的随机卷积核，看起来随机性还挺不错哦</p><p>PS：前面（5，5，1）代表输入长、宽、色道，后面代表输出输出数量当然我说它是32个它不一点为32个矩阵，应该是（色道X输出数量）个卷积核，但是我们这里只有一个色道，所以只有32个，我们可以通过<code>W_conv1_value.shape</code>查看真实的维度（当前的维度为（5， 5， 1， 32））</p><p>这个卷积核就对应上面图中间的小矩阵，他的长宽都为5，图中长宽都为3，当然我们可以把这个长宽修改，使用5是我们的经验值，通过这个大小的卷积核能够在模型表现能力更好。</p><p>接下来我们就进行最重要的卷积操作了，由上面图可知，要进行卷积必须要有三维的数据与对应的卷积核进行相卷，其实我们在图中还可以看到一个重要的东西，卷积的步长也就是每个框移动的位置（图中的步长为2）</p><p>还有一个较隐秘的知识，你有没有注意到图中的数据原来是7X7的数据，通过卷积核转换之后就变成了3X3了，影响卷积后图像尺寸不但有步长还有框子的大小，假如你的框是7，那图中只剩下一个值了，所以我们避免尺寸减少，我们使用周围填充0来使最边缘的位置卷积也成为到框子的中心，一方面避免边缘数据流失，一方面也能突出边缘数据（周边全为0）</p><p>Tensorflow为我们封装好了上面所以的方法，我们只要通过传参过去就能改变部长，改变填充方式，好了现在就开始来正式“卷”了</p><pre><code>session.run(tf.global_variables_initializer())v = session.run(tf.nn.conv2d(x_image, W_conv1, strides=[1, 1, 1, 1], padding=&apos;SAME&apos;), feed_dict={x: data})</code></pre><p>现在我们来看看卷完后<code>v</code>的<code>shape</code></p><pre><code>   v.shape(50, 28, 28, 32)</code></pre><p>50代表50个数据，（28、28）代表图片维度，这个32就是卷积核数，50和32这两个应该是固定的，不难理解，我们现在来看看为什么通过卷积核的“卷”，图片还是保持28X28的，这个也是在知乎上涉及到的一个<a href="https://www.zhihu.com/question/46889310" target="_blank" rel="noopener">问题</a>，现在我们从实验上来解决一下</p><p>首先我们看<code>tf.nn.conv2d</code>函数，他接受四个参数，第一个图片、第二个卷积核、第三个步长，第四个卷积方式</p><p>首先问题是觉得，卷完之后应该是变成24 X 24，这个理解是没错的，我们将pading的值改成VALID再次运行</p><pre><code>session.run(tf.global_variables_initializer())v = session.run(tf.nn.conv2d(x_image, W_conv1, strides=[1, 1, 1, 1], padding=&apos;VALID&apos;), feed_dict={x: data})v.shape(50, 24, 24, 32)</code></pre><p>我们得到了24 X 24的图片，这个SAME和VALID有什么区别呢，这个区别就是填充0没有填充0的原因，SAME在图像周边填0这样就能得到28 X 28</p><p>我们也发现，这个还有一个参数strides，这个就是前面填的步长，步长的长宽就是中间两位设置的（最边上两位跟输入有关，第一个是输入图片数量，最后一个是图片的色道），我们在这里使用使用1步长，我们来试试2步长试试</p><pre><code>v = session.run(tf.nn.conv2d(x_image, W_conv1, strides=[1, 2, 2, 1], padding=&apos;SAME&apos;), feed_dict={x: data})v.shape(50, 14, 14, 32)</code></pre><p>果然输出的图像变成28的1/2了</p><p>接下来我们就要把卷积的值丢到神经元函数里面去了，为了符合实际，我们加入一个偏置量<code>b_conv1</code></p><pre><code>def bias_variable(shape):  initial = tf.constant(0.1, shape=shape)  return tf.Variable(initial)b_conv1 = bias_variable([32])</code></pre><p>这里我们使用0.1来初始化偏置量，接下来就是丢到神经元函数，这里我们使用numpy 的array的传播性，将b_conv1传递给所有的28X28的维度</p><pre><code>h_conv1 = tf.nn.relu(tf.nn.conv2d(x_image, W_conv1, strides=[1, 1, 1, 1], padding=&apos;SAME&apos;) + b_conv1)v = session.run(h_conv1,  feed_dict={x: data})v.shape(50, 28, 28, 32)</code></pre><p>我们可以看到卷积完后从神经元函数生成的数据是（50X28X28X32）的，最后维度由1变成32，所有我们得使用点方法来缩减数据维度，这里我们使用卷积池的方法</p><p><img src="/images/tensorflow_pool.png" alt="卷积池"></p><p>由上面可以看到，其实很简单就是把最大的挑出来</p><pre><code>h_pool1 = tf.nn.max_pool(h_conv1, ksize=[1, 2, 2, 1],                    strides=[1, 2, 2, 1], padding=&apos;SAME&apos;)</code></pre><p>这里的参数很简单我就不介绍，这样“瘦身”之后，数据的维度由(50, 28, 28, 32)变成(50, 14, 14, 32)，减少4倍</p><p>到这里我们的第一层卷积就结束了，接下来就是第二层卷积，为什么要多卷一次呢，因为前一层学到的还是太少了，要加强学习，这层和第一层没什么差别，所以我们就跳过这层</p><p>直接贴代码（函数就不复制了，文档里面有）</p><pre><code>W_conv2 = weight_variable([5, 5, 32, 64])b_conv2 = bias_variable([64])h_conv2 = tf.nn.relu(conv2d(h_pool1, W_conv2) + b_conv2)h_pool2 = max_pool_2x2(h_conv2)</code></pre><h3 id="全连接层"><a href="#全连接层" class="headerlink" title="全连接层"></a>全连接层</h3><p>当我们完成两层卷积之后，我们的数据变成了（50，7,7，64）的四维数组了，我们知道我们传统的机器学习其实最后都是采用二维数组来当做训练数据（X代表特征，Y代表样本），所以全连接层就是把卷积给“反卷”过来，这样后面你方便对接传统机器学习，而且最后我们需要的数据也是输出的也是二维的（对一堆数据统一进行预测，所以这里称二维），但是这里要注意全连接层不是输出层，所以我们可以随意设置输出的维度，最后输出层对接再进行一次全连接层类似操作就能输出我们想输出的维度，这里我们看看全连接层权值变量</p><pre><code>W_fc1 = weight_variable([7 * 7 * 64, 1024])b_fc1 = bias_variable([1024])</code></pre><p>这里我们声明全连接层的权值变量<code>W_fc1</code>和偏置量<code>b_fc1</code>，我们可以看看<code>W_fc1</code>的<code>shape</code>是多少</p><pre><code>session.run(tf.global_variables_initializer())session.run(W_fc1).shape(3136, 1024)</code></pre><p>我们可以看到其实就是一个二维数组维度为（3136,1024），第一个维度跟输入有关，第二个维度影响输出维度，前面我们使用<code>tf.nn.conv2d</code>卷积操作来转换图片，在全连接层我们要使用矩阵运算来转换我们的维度</p><p>矩阵运算非常有趣，我们在前面其实也提到过一点，就是降维的实现PCA就是使用矩阵运算来进行降维，我们把数据分为X（特征），Y（数量），经过一次矩阵运算我们可以实现数量不变，而特征改变，这个就非常强大了，我们可以随便修改矩阵参数来动态修改我们特征数量</p><p>但是矩阵运算也有一定局限性，就是两个运算的矩阵必须是前者长与后者的宽想同，这个跟矩阵运算特性有关，具体可以看看矩阵运算相关<a href="http://www.ruanyifeng.com/blog/2015/09/matrix-multiplication.html" target="_blank" rel="noopener">资料</a></p><p>所以为了进行矩阵运算我们第一件事就是改变输入的<code>shape</code>，让它由四维变成二维，以便能够与我们权值矩阵<code>W_fc1</code>进行运算</p><pre><code>h_pool2_flat = tf.reshape(h_pool2, [-1, 7*7*64])</code></pre><p>我们简单的使用<code>tf.reshape</code>就能把第二层卷积后的输出变量转换成（50，7<em>7</em>64）的维度，这样我们就能直接与权值矩阵<code>W_fc1</code>进行运算</p><pre><code>h_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat, W_fc1) + b_fc1)</code></pre><p>我们这里直接将运算后的值放到激活函数里面去完成全连接层的功能</p><h3 id="输出层"><a href="#输出层" class="headerlink" title="输出层"></a>输出层</h3><p>其实输出层同全连接层很类似，我们就是把前面的变量转换成我们想输出的维度，在进行这个输出层之前，我们得先搞一层<code>Dropout</code>层，这个能有效的避免神经网络的过拟合问题，具体可以看看这篇<a href="http://jmlr.org/papers/volume15/srivastava14a.old/srivastava14a.pdf" target="_blank" rel="noopener">论文</a></p><pre><code>keep_prob = tf.placeholder(&quot;float&quot;)h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob)</code></pre><p>因为同全连接层原理类似，输出层我就不就不详细介绍了</p><pre><code>W_fc2 = weight_variable([1024, 10])b_fc2 = bias_variable([10])y_conv=tf.nn.softmax(tf.matmul(h_fc1_drop, W_fc2) + b_fc2)</code></pre><p>我们可以看看最后我们输出是什么</p><pre><code>session.run(tf.global_variables_initializer())session.run(y_conv, feed_dict={x:data, keep_prob:0.5}).shape(50, 10)</code></pre><p>ok，我们最后得到一个二维数组，50个预测结果（输出采用OneHot方法）</p><h3 id="反向传播"><a href="#反向传播" class="headerlink" title="反向传播"></a>反向传播</h3><p>在前面我们得到了在初始话随机权值下得到输出结果，但是这个结果肯定是错误的，我们必须通过修改每层的权值来修正模型，使模型越来越聪明，所以第一步，我们必须“自我反省”，了解自己与真实结果差距多少</p><pre><code>y_ = tf.placeholder(&quot;float&quot;, [None, 10])cross_entropy = -tf.reduce_sum(y_*tf.log(y_conv))</code></pre><p>我们引入<code>y_</code>作为实际值（我们模型预测值为<code>y</code>），我们这里使用交叉熵来评判预测准确性，但是单单知道“自己错了”没有什么卵用，我们必须要“改正”，这里我们使用<code>AdamOptimizer</code>优化算法来反向传播我们误差，让模型好好“反省改正”</p><pre><code>train_step = tf.train.AdamOptimizer(1e-4).minimize(cross_entropy)</code></pre><p>到这里基本上差不多了，我们已经形成了一个闭环，预测-&gt;评估-&gt;改正-&gt;预测-&gt;……，只有让它不断的训练下去直到我们能接受他的误差我们的模型就训练好了</p><pre><code>correct_prediction = tf.equal(tf.argmax(y_conv,1), tf.argmax(y_,1))accuracy = tf.reduce_mean(tf.cast(correct_prediction, &quot;float&quot;))session.run(tf.initialize_all_variables())for i in range(18000):  batch = mnist.train.next_batch(50)  if i%100 == 0:    train_accuracy = accuracy.eval(feed_dict={        x:batch[0], y_: batch[1], keep_prob: 1.0}, session=session)    print(&quot;step %d, training accuracy %g&quot;%(i, train_accuracy))    if abs(train_accuracy - 1) &lt; 0.01:        break  train_step.run(feed_dict={x: batch[0], y_: batch[1], keep_prob: 0.5}, session=session)</code></pre><p>由于我们使用<code>OneHot</code>方法来输出预测变量，所以我们要使用<code>tf.argmax</code>来得到我们想要的真实数字，经过20000轮训练我们正确率可以达到99%，至此卷积神经网络发挥他的威力。</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>卷积神经网络是深度学习的一个很重要的组成部分，了解卷积必须要知道为什么要用卷积，用了有什么好处。总而言之，卷积并不是一个很新奇的东西，很早在信号处理中就有应用，但是在图像处理上由于他能保留图像维度信息从而在深度学习领域大放异彩，这也可以看做“是金子总会发光吧”</p><h2 id="引用"><a href="#引用" class="headerlink" title="引用"></a>引用</h2><p><a href="http://www.tensorfly.cn/tfdoc/tutorials/mnist_pros.html" target="_blank" rel="noopener">http://www.tensorfly.cn/tfdoc/tutorials/mnist_pros.html</a><br><a href="http://www.ruanyifeng.com/blog/2015/09/matrix-multiplication.html" target="_blank" rel="noopener">矩阵运算</a><br><a href="https://blog.csdn.net/v_july_v/article/details/51812459" target="_blank" rel="noopener">通俗理解卷积神经网络</a><br><a href="http://jmlr.org/papers/volume15/srivastava14a.old/srivastava14a.pdf" target="_blank" rel="noopener">Dropout</a></p>]]></content>
      
      
      <categories>
          
          <category> 人工智能 </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>Ubuntu16.04安装Tensorflow的CPU优化</title>
      <link href="2018/04/19/software/Ubuntu14%E5%AE%89%E8%A3%85Tensorflow%E7%9A%84CPU%E4%BC%98%E5%8C%96/"/>
      <url>2018/04/19/software/Ubuntu14%E5%AE%89%E8%A3%85Tensorflow%E7%9A%84CPU%E4%BC%98%E5%8C%96/</url>
      
        <content type="html"><![CDATA[<blockquote><p>由于我的笔记本是农卡，没法安装CUDA加速，而且我的显卡只有2G显存，安装OpenCL费力不讨好，而且由于我有一个Google云的300美元的体验，所以可以在Google云上使用TPU来进行加速，所以我就干脆不安装显卡加速，但是Tensorflow提供了指令集优化，由于默认使用pip安装没有提供这个功能，所以只能手动编译安装</p></blockquote><p>假如你是用<code>pip</code>安装的Tensorflow你可以会得到下面警告</p><pre><code>the tensorflow library wasn&apos;t compiled to use sse4.1 instructions</code></pre><h2 id="安装步骤"><a href="#安装步骤" class="headerlink" title="安装步骤"></a>安装步骤</h2><p><del>1. 首先你得先看看你CPU支持什么指令集</del></p><p> <del>cat /proc/cpuinfo|grep flags</del></p><p><del>执行这个指令就能看到你所支持的指令集</del></p><p>最新的<code>tensorflow</code>1.12不需要这步，可以根据你的CPU进行推测</p><p>~~2. 然后安装<code>bazel</code></p><p><del>由于直接从源代码安装比较简单，所以直接从<a href="https://github.com/bazelbuild/bazel/releases" target="_blank" rel="noopener">github</a>上面下载<code>0.18.1</code>相对应的版本就行（最好不要使用最新版，最新版没法编译）</del></p><p><del>3. 安装完之后下载<code>tensorflow</code>源码</del></p><pre><code>mkdir github &amp;&amp; cd githubgit clone --recurse-submodules https://github.com/tensorflow/tensorflowcd tensorflow./configure  </code></pre><p><del>接下来一路选择<code>N</code>就行</del></p><p><del>4. 生成<code>whl</code>文件</del></p><p><del>bazel build -c opt –copt=-msse3 –copt=-msse4.1 –copt=-msse4.2 –copt=-mavx –copt=-mavx2 –copt=-mfma //tensorflow/tools/pip_package:build_pip_package</del></p><p><del>在源码处开始编译，注意<code>copt</code>命令主要是添加指令集支持，这里你要看看上面的指令集（去掉m就是你的指令集，如-msse3指令集为sse3）你的CPU是否支持（一般都支持我的I5 4200U都支持），如果不支持删掉那个就行</del></p><p><del>这里你安装的时间比较长，要看你的CPU了</del></p><p><del>最新版不需要这么复杂了，直接</del></p><pre><code>bazel build --config=opt //tensorflow/tools/pip_package:build_pip_package</code></pre><p><del>当然如果你想开启<code>MKL</code>也可以在<code>opt</code>加上改成<code>--config mkl</code></del></p><p>最新版使用<code>bazel</code>编译由于依赖网络下载，而我大天朝的网络一直报check-sum错误，官方说是我们网络的问题，我在我香港服务器尝试了一下，相同的环境下使用<code>bazel</code>是能安装的，但是由于安装到后面<code>bazel</code>需要太多内存了，所以直接爆照，而我本地内存大然而尝试了N多方法（尝试使用<code>proxychain</code>由于安装时需要连接本地<code>bazel</code>服务器，而设置过滤本地网络又没有用，也尝试使用全局代理也绕不过去），最终没有办法只能安装一个别人已经编译好的CPU的加速的。现在放出地址</p><p><a href="https://github.com/mind/wheels/releases" target="_blank" rel="noopener">github tensorflow 编译好的版本下载</a></p><p>你找一个你操作系统对应以及和你Python版本对应的下载下来用<code>pip</code>安装一下就可以了。</p><ol start="5"><li>验证</li></ol><p>退出安装目录运行<code>python</code></p><p>执行下面两句</p><pre><code>    import tensorflow as tf;sess = tf.Session(config=tf.ConfigProto(log_device_placement=True))bazel --host_jvm_args=&quot;-Dhttp.proxyHost=localhost -Dhttp.proxyPort=8123 -Dhttps.proxyHost=localhost -Dhttps.proxyPort=8123&quot;    build --config=mkl //tensorflow/tools/pip_package:build_pip_package</code></pre><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p><del>如果没有报上面的不支持指令集的warning，那么你的CPU指令集优化版就安装好了，当然这个加速效果因CPU而异，对于Xeon SP系列（100核心以上）已经能加速到50倍，同GPU差距也就2倍了（原来可是100倍），但是对于我的笔记本来说，加速效果可能就在30%左右（核心少），所以当前加速性价比最高的还是GPU加速，骚年还是买个好一点的GPU吧，没事还可以吃吃鸡。</del></p><p>经过我测试在<code>I7 9700K</code>下相同的<code>mnist</code>数据下运行，不使用CPU指令优化比使用CPU指令优化下速度还要更快，所以在高版本下的Tensorflow CPU版其实优不优化都无所谓，推荐还是使用GPU加速</p>]]></content>
      
      
      <categories>
          
          <category> 软件 </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>Python正则表达式</title>
      <link href="2018/03/17/python/%E6%AD%A3%E5%88%99%E8%A1%A8%E8%BE%BE%E5%BC%8F/"/>
      <url>2018/03/17/python/%E6%AD%A3%E5%88%99%E8%A1%A8%E8%BE%BE%E5%BC%8F/</url>
      
        <content type="html"><![CDATA[<hr><h2 id="引言"><a href="#引言" class="headerlink" title="引言"></a>引言</h2><blockquote><p>这篇博客其实写于2016年，最近在重新学了一下正则表达之后，觉得有必要重新整理一下正则的用法</p></blockquote><blockquote><p>Python对正则匹配的库是<code>re</code>，re是基于Perl所用的正则表达式,并有<br>一定的改进.</p></blockquote><p>正则本质就是搜索所需的文本,正则里面有三种搜索方式</p><ol><li>第一种是知道文本内容直接使用普通字符搜索出来,比如要从<code>abcdefg</code>中搜到<code>cd</code></li><li>第二种就是模糊查询,比如我想从英文中找一个数字,一般借助特殊符号(<code>.+*?</code>)或者转义符号(<code>\w\d</code>等)</li><li>第三种就是结合前两种,比如我记得一个单词的前两个字母想把那个单词搜出来.</li></ol><p>这里不介绍正则基本知识,你想知道可以<a href="http://www.regexlab.com/zh/regref.htm" target="_blank" rel="noopener">点这里</a></p><p>ps: 由于在python里面也是用反斜杠做转义字符,所以比如<code>\\</code>和<code>\b</code>这两个特殊字符必须用<code>\\\\</code>和<code>\\b</code>来代替.但是python提供了一个元字符支持re模块,只要字符前面加上r比如<code>r&#39; regex &#39;</code>就能不关闭python的转义.</p><h5 id="正则里面我觉得很重要的一个概念就是组概念-当我们的文本比较复杂的时候将其分成多个小组是利于我们正则的后期维护和改进"><a href="#正则里面我觉得很重要的一个概念就是组概念-当我们的文本比较复杂的时候将其分成多个小组是利于我们正则的后期维护和改进" class="headerlink" title="正则里面我觉得很重要的一个概念就是组概念,当我们的文本比较复杂的时候将其分成多个小组是利于我们正则的后期维护和改进"></a>正则里面我觉得很重要的一个概念就是组概念,当我们的文本比较复杂的时候将其分成多个小组是利于我们正则的后期维护和改进</h5><blockquote><p>正则里面使用一个括号来表示组比如<code>(a)(b)</code>就分成了两个组</p></blockquote><p>re函数里面<code>search</code>和<code>finall</code>都支持组查询,而且<code>findall</code>方法假如里面有组分布会只显示组成员.</p><p>re库支持搜索选项,这几个选项对于正则有时候非常有用</p><pre><code>DOTALL [简S]-------------允许点字符匹配换行符IGNORECASE [简I] --------忽悠大小写LOCALE  [简L]  ----------支持本地化字符MULTILINE [简M] ---------多行,每行都支持锚点UNICODE [简U]  ----------支持Unicode,\w也可以是Unicode了VERBOSE  [简X]  --------------神器,会无视代码中的注释空格和换行</code></pre><p>我们也可以在正则的组里面使用这些搜索选项,只要用上面的简称的小写比如<code>(?is)</code>就可以在组里面使用这些规则.</p><hr><p>正则里面还有一些比较有趣的函数,同string里面的translate函数,sub函数可以替换找到的变量<br>    bold = re.compile(r’*{2}(.<em>?)\</em>{2}’)<br>    bold.sub(r’<b>\1</b>‘, ‘this <strong>foo</strong> and <strong>ok</strong>‘)</p><blockquote><p><code>\1</code>代表第一组变量也就是foo和ok<br>输出为<code>&#39;this &lt;b&gt;foo&lt;/b&gt; and &lt;b&gt;ok&lt;/b&gt;&#39;</code>我们使用成功用加粗了foo和ok,同<code>translate</code>不同这个方法不需要知道要替换的是什么.</p></blockquote><hr><h3 id="正则的断言"><a href="#正则的断言" class="headerlink" title="正则的断言"></a>正则的断言</h3><blockquote><p>我们可以使用一些特殊的符号来执行一些程序判断选择,比如说判断是否特殊字符,如果有<br>的话就不匹配,这就是断言</p></blockquote><p>断言有两种一种是前向,一种是后向</p><blockquote><p>前向是指判断语句在前面,这种就相当于一个if语句,而后向是匹配后判断,由于已经匹配好了文字所以<br>匹配的字符必须是固定长度的(不能使用*.?).</p></blockquote><p>前向就是在判断后面匹配的表达式必须与规定相同,比如一个邮箱地址我们要匹配可以用&lt;&gt;包起来的,但是不匹配只要一个的我们就可以在前面加上这个<code>^(?=(&lt;.*&gt;$)|([^&lt;].*[^&gt;]$))</code>通过使用<code>?=</code>来断言后面必须是用&lt;&gt;包起来或者没有&lt;&gt;,我们使用前向断言可以通过正则直接过滤掉不符合的(当然你可以用多个简单正则来做但是效率没有这个高),还有否定前向就是通过<code>?!</code>来声明.<br>相对应后向断言就是很简单了,直接在匹配后面使用一个<code>?&lt;=</code>(肯定后向)或<code>?&lt;!</code>(否定后向),不过要注意这个是判断前面匹配是否满足的.</p><blockquote><p>断言只是限定我们想选的文本的范围,他并不会被选择.<br>断言的一个有趣的应用就是选择字符间的空格,我们知道python其实假设每个字符间都一个空格(这就是我们有时候会选出一些空字符出来的原因),这个空格不是我们自己打上去的.</p></blockquote><h4 id="举个例子"><a href="#举个例子" class="headerlink" title="举个例子"></a>举个例子</h4><h3 id="两个字符串a1和a-1-第一个我们称为A-第二个我们称他为B-假如我们想把数字和字母分出来-对于B来说-很简单因为数字和字母之间有一个空格-我们可以直接使用字符自带的split就行-但是对于A来说-就不那么简单了"><a href="#两个字符串a1和a-1-第一个我们称为A-第二个我们称他为B-假如我们想把数字和字母分出来-对于B来说-很简单因为数字和字母之间有一个空格-我们可以直接使用字符自带的split就行-但是对于A来说-就不那么简单了" class="headerlink" title="两个字符串a1和a 1,第一个我们称为A,第二个我们称他为B,假如我们想把数字和字母分出来,对于B来说,很简单因为数字和字母之间有一个空格,我们可以直接使用字符自带的split就行,但是对于A来说,就不那么简单了."></a>两个字符串<code>a1</code>和<code>a 1</code>,第一个我们称为A,第二个我们称他为B,假如我们想把数字和字母分出来,对于B来说,很简单因为数字和字母之间有一个空格,我们可以直接使用字符自带的<code>split</code>就行,但是对于A来说,就不那么简单了.</h3><h4 id="字母a和数字1中间没有字符-我们必须把字母和数字之间的”空格”给选择出来-这时候就可以用到断言了"><a href="#字母a和数字1中间没有字符-我们必须把字母和数字之间的”空格”给选择出来-这时候就可以用到断言了" class="headerlink" title="字母a和数字1中间没有字符,我们必须把字母和数字之间的”空格”给选择出来,这时候就可以用到断言了."></a>字母a和数字1中间没有字符,我们必须把字母和数字之间的”空格”给选择出来,这时候就可以用到断言了.</h4><pre><code>r = re.compile(r&apos;(?&lt;=[a-z])(?=\d)&apos;) </code></pre><p>这个<code>r</code>就可以字母和数字直接的隐形空格给选择出来了</p><h3 id="遗憾的是由于python的正则并不把隐形的空格当做字符-所以我们不能简单的使用正则的re-split方法-选择字符分割-直接将字符串分解开"><a href="#遗憾的是由于python的正则并不把隐形的空格当做字符-所以我们不能简单的使用正则的re-split方法-选择字符分割-直接将字符串分解开" class="headerlink" title="遗憾的是由于python的正则并不把隐形的空格当做字符,所以我们不能简单的使用正则的re.split方法(选择字符分割)直接将字符串分解开."></a>遗憾的是由于python的正则并不把隐形的空格当做字符,所以我们不能简单的使用正则的<code>re.split</code>方法(选择字符分割)直接将字符串分解开.</h3><h4 id="我们就得写几步"><a href="#我们就得写几步" class="headerlink" title="我们就得写几步"></a>我们就得写几步</h4><h3 id="第一先把空格换成-或其他"><a href="#第一先把空格换成-或其他" class="headerlink" title="第一先把空格换成 $$$(或其他)"></a>第一先把空格换成 $$$(或其他)</h3><pre><code>&gt;&gt;&gt; s = r.sub(&apos;$$$&apos;, &apos;a1&apos;)&gt;&gt;&gt; print(s)&apos;a$$$1&apos;</code></pre><h3 id="然后在分割"><a href="#然后在分割" class="headerlink" title="然后在分割"></a>然后在分割</h3><pre><code>&gt;&gt;&gt; s.split(&apos;$$$&apos;)&gt;&gt;&gt; [&apos;a&apos;, &apos;1&apos;]</code></pre><p>成功分割好了,当然这个只能处理字母在前数字在后的”隐形空格”,只要加一个<code>&quot;|&quot;</code>在把前向改成后向,后向改成前向就可以选择任意字母和数字直接的”隐形空格”了.</p><hr><h3 id="正则的变量"><a href="#正则的变量" class="headerlink" title="正则的变量"></a>正则的变量</h3><blockquote><p>我们可以使用?P<name>来声明一个组(用括号,当然其实我们每使用一个括号re自动帮我们将组取一个<br>名,依次从1-n</name></p></blockquote><p>有时候我们可以要求上面的匹配组,下面也要相应匹配组,我们就可以通过两种方法来引用这个变量,假如你没有使用<code>&lt;?P&lt;name&gt;</code>来声明组你只能通过<code>\n</code>来引用,n是这个变量的序号,第二种是通过<code>(?P=name)</code><br>来引用这个变量,name为你自己定义的组的名字</p><p>re还提供了一种机制来让你修正你的正则,简单来说就是能判断一个组存不存在来约束匹配,语法为</p><pre><code>(?(id)yes-expression|no-expression)</code></pre><p>id为组的编号或者name.</p><h2 id="正则的起源"><a href="#正则的起源" class="headerlink" title="正则的起源"></a>正则的起源</h2><p>正则这个东西其实很简单，我们</p>]]></content>
      
      
      <categories>
          
          <category> Python </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>用户空间隔离</title>
      <link href="2018/02/20/algorithm/mit6_828/%E7%94%A8%E6%88%B7%E7%A9%BA%E9%97%B4%E9%9A%94%E7%A6%BB/"/>
      <url>2018/02/20/algorithm/mit6_828/%E7%94%A8%E6%88%B7%E7%A9%BA%E9%97%B4%E9%9A%94%E7%A6%BB/</url>
      
        <content type="html"><![CDATA[<blockquote><p>通过前面的学习我们知道，在前两个实验中最主要的程序就是<code>kern/init.c</code>文件中<code>i386_init</code>函数，但是我们看到最后却是一个<code>while</code>循环结束。</p></blockquote><pre><code>while (1)    monitor(NULL);</code></pre><p>这个<code>monitor</code>就是简单读取用户输入然后通过字符串调用给定的几个函数。我们可以把这个函数看做<code>i386_init</code>的子函数，也就是程序一直在<code>i386_init</code>中运行，也就是一直以内核形式运行。实验三就开始完成一个正常的操作系统的用户模式的建立，以及两者直接转换。</p><h2 id="引言"><a href="#引言" class="headerlink" title="引言"></a>引言</h2><p>什么是用户呢，我们把内核当做一个大程序，用户就是小程序，两者基本上没有差别，但是为了让众多用户和内核平稳运行，我们必须要区别对待用户和内核</p><p>首先我们要相信内核，内核是我们精心设计的，相反用户我们不能相信它，任何人都可以犯错误，我们知道计算机程序其实非常精巧，当他们完整的时候，他们可以完成你无法想象的工作，但他们有时候少掉一行指令时，他们就会瞬间崩溃，所以我们必须要保证就算用户怎么折腾都不会把内核搞垮</p><p>所以我们来考虑一下怎么来限制用户不影响整个系统，第一用户不能动内核在内存上的程序，甚至连查看的资格都没有，这就是为什么前面我们已经绞尽脑汁把内核放到高地址上确保其“高高在上”；第二个用户不能在CPU一直跑，假如它有权限一直在CPU上跑，那么内核就废了，没有CPU可以用相当于“断了胳膊”；第三用户必须要有一些内核的权限，比如说申请一些内存啥的。</p><p>接下来我们就从这几个问题出发来探索一下xv6为什么要这样设计用户空间。</p><h2 id="用户是个程序？"><a href="#用户是个程序？" class="headerlink" title="用户是个程序？"></a>用户是个程序？</h2><p>在完成用户设计之前，首先我们先做个实验，来验证一下用户是个程序。首先我们在<code>kern/init.c</code>文件中<code>i386_init</code> 的头部加上<code>#include &lt;inc/elf.h&gt;</code>然后在<code>#if defined(TEST)</code>代码前加入几行代码</p><pre><code>extern uint8_t _binary_obj_user_hello_start[];struct Elf* header = ((struct Elf *) _binary_obj_user_hello_start);assert(header-&gt;e_magic == ELF_MAGIC);((void (*)(void)) (header-&gt;e_entry))();</code></pre><p>这段代码你如果仔细观察过前面操作系统载入的源码（<code>boot/main.c</code>中)你会发现基本上差不多。首先我们声明了<code>_binary_obj_user_hello_start</code>外部变量，这个变量是通过<code>ld</code>编译器将内核中用户程序<code>hello.c</code>的起始地址定义来的，由于我们现在没有文件系统，内核就把用户程序一股脑链接到自己身上，在以后有了文件系统就不需要了。但是它给了我们一个便利，我们现在可以直接在内存上运行它</p><p>当然虽然它的确是个程序（assert那不会出错），但是它还是跑不起来，因为它的虚拟地址不在内核内存上，必须要像前面映射内存物理空间才能运行，但是他的确是一个程序，从这里我们得到一个结论，用户其实也就是一段程序，接下来我们就看看如何用软件与硬件的结合的设计解决上面的问题。</p><h2 id="权限"><a href="#权限" class="headerlink" title="权限"></a>权限</h2><p>首先我们来看看权限问题，<code>x86</code>系统硬件给我们提供了一个分段式权限功能，在开启内存分页后，<code>CS</code>寄存器16位里面拿出两位来当做权限管理，分为四级（<code>0b00、0b01、0b10、0b11</code>），最小的权限最大，当然xv6只用了两级，如下图所示</p><p><img src="/images/systemring.png" alt="系统环来源网络"></p><p>PS：当然<code>CS</code>寄存器拿出来三位来当权限管理，但是xv6将第三位用户和内核都设置为1，所以我们也不介绍这一位</p><p>我们在<code>CS</code>寄存器的这两位称为CPL，用来区分权限。在<a href="/2018/01/31/引导和操作系统的交互/">引导和操作系统的交互</a>这篇博客我们知道，在<code>x86</code>系统中，这个<code>CS</code>寄存器是标志段选择子，在<code>GDT</code>表中每个表项也有两位用来标志权限，我们称他们为<code>DPL</code>，他们的意义也就是<code>CS</code>寄存器选择这个段时他们最低拥有的权限。</p><p>当然在<code>GDT</code>表中的段选择子也有一个表示权限的称作<code>RPL</code>，这三者的关系在这<a href="https://stackoverflow.com/questions/36617718/difference-between-dpl-and-rpl-in-x86" target="_blank" rel="noopener">里面</a>介绍的很详细，<code>RPL</code>是历史遗留问题但是操作系统基本上没有使用这个功能，所以这里我们也不解释，感兴趣可以看一下<a href="https://stackoverflow.com/questions/36617718/difference-between-dpl-and-rpl-in-x86" target="_blank" rel="noopener">前面</a>的介绍。</p><p>在<code>kern/env.c</code>的<code>结构体gdt</code>我们声明了一个<code>GDT</code>表，并且声明了一个系统段和用户段。</p><p>这两个段就是我们权限的基础我们接下来通过实验来验证权限位对操作系统的保护</p><p>我们首先在<code>user/badsegment.c</code>中加入一句汇编</p><pre><code>asm volatile(&quot;ljmp %0,$1f\n 1:\n&quot; :: &quot;i&quot; (0x08)); // 将CS设置为0x08</code></pre><p>我们尝试将<code>CS</code>设置为<code>0x08</code>（内核段）结果直接引发一个<code>General Protection</code>的异常，但是当我们尝试执行这句</p><pre><code>asm volatile(&quot;ljmp %0,$1f\n 1:\n&quot; :: &quot;i&quot; (0x18));  // 将CS设置为0x18</code></pre><p>我们发现程序没有引发异常（这段的含义是跳转到用户段，由于已经在用户段，所以没有触发异常），这说明<code>x86</code>通过<code>CS</code>寄存器很好的实现了内核和用户的保护（用户不能直接跳到内核去执行代码）</p><p>在<code>user/faultwritekernl.c</code>和<code>user/faultreadkernl</code>中我们发现尝试写入或者读取内核也失败了，这是通过前面章节的内存分页权限实现的，这里就不介绍了。</p><p>现在我们知道，通过我们不懈努力内核和用户直接存在一个鸿沟，但是这时候摆在我们面前很大的问题，内核拥有一些非常重要的权限比如申请内存，如果用户没有这个权限那么功能非常受限，所以我们就要提到一个内核和用户交互的手段：Trap。</p><h2 id="Trap"><a href="#Trap" class="headerlink" title="Trap"></a>Trap</h2><p>其实这个Trap包括内部中断、软中断、异常。但是xv6统一将他们叫做陷阱（Trap），这个Trap主要完成从用户到内核的一种跳转，我们就不介绍内部中断和异常，因为这些都是系统完成，我们来介绍了一下软中断，这个我们人为可以操作，而且我们可以把其他中断、异常当做系统去帮我们调用这个指令</p><pre><code>int $n</code></pre><p>很简单x86共定义了256个中断向量，存在物理内存<code>0x000</code>-<code>0x3ff</code>之间，每个存贮一个<code>cs</code>值一个<code>eip</code>的值，这个n只要是在256直接就行，当调用这个的时候我们直接调到那里读取<code>cs</code>和<code>eip</code>的值。</p><p>在正式介绍<code>Trap</code>之前我们要简单的介绍一下两个知识点</p><h3 id="IRET"><a href="#IRET" class="headerlink" title="IRET"></a>IRET</h3><p>这是<code>iret</code>是一个机器码，我们前面介绍了无论是操作系统还是用户没有办法直接跳段（会引发<code>General Protection fault</code>），而且我们知道前面只是测试了跳代码段，我们知道两个不同的程序，不仅仅是代码段不同，堆栈段也要不同，才能保证各个程序的独立。所以系统为了解决这个问题，直接提供一个<code>iret</code>指令，这个全名应该叫<code>中断返回</code>，它的功能其实很简单，就是实现上面的跳代码段和堆栈段还有恢复<code>EFLAGS</code>寄存器值，这个主要应用在从系统跳转到用户上面。</p><p>感兴趣的可以看看这个详细<a href="https://stackoverflow.com/questions/6892421/switching-to-user-mode-using-iret" target="_blank" rel="noopener">资料</a>，接下来我们看看在<code>xv6</code>里面如何使用<code>iret</code>来实现跳段</p><p>在<code>kern/env.c</code>的<code>env_pop_tf</code>函数实现了切换程序段的功能</p><pre><code>asm volatile(        &quot;\tmovl %0,%%esp\n&quot;        &quot;\tpopal\n&quot;        &quot;\tpopl %%es\n&quot;        &quot;\tpopl %%ds\n&quot;        &quot;\taddl $0x8,%%esp\n&quot;         &quot;\tiret&quot;        : : &quot;g&quot; (tf) : &quot;memory&quot;);</code></pre><p>首先第一个就是把tf指针指向的值赋给<code>esp</code>寄存器，要想知道这个操作的意思就必须知道，一个结构体其实在程序里面就是一块连续的内存值，所以这里就是把<code>esp</code>的值指向<code>tf</code>代表的<code>Trapframe</code>结构体的开始位置，所以我们得看看这个<code>Trapframe</code>组成是什么</p><pre><code>struct Trapframe {    struct PushRegs tf_regs;    uint16_t tf_es;    uint16_t tf_padding1;    uint16_t tf_ds;    uint16_t tf_padding2;    uint32_t tf_trapno;    /* below here defined by x86 hardware */    uint32_t tf_err;    uintptr_t tf_eip;    uint16_t tf_cs;    uint16_t tf_padding3;    uint32_t tf_eflags;    /* below here only when crossing rings, such as from user to kernel */    uintptr_t tf_esp;    uint16_t tf_ss;    uint16_t tf_padding4;} __attribute__((packed));</code></pre><p>第一个是<code>PushRegs</code>结构体，这个对应我们第一个操作<code>popal</code>，将<code>PushRegs</code>所有值全部从堆栈中取出，我们发现当到<code>iret</code>操作时，正好对应取出代码段还有堆栈段和<code>EFLAGS</code>寄存器，所以其实<code>iret</code>的操作非常简单，只是将三个取出指令和成了一个。</p><h3 id="TSS"><a href="#TSS" class="headerlink" title="TSS"></a>TSS</h3><p>前面我们捋了捋从系统跳转到用户，因为我们给用户的都是刚新建的值，所以这个并没有什么问题，但是你要想一想，当用户想跳转到系统的时候，这个时候它也必须要切换堆栈，因为用户可以还没有准备好（有时候可以是因为堆栈内存不够，如果把寄存器值强行插值到用户堆栈会引发二次异常）。但是假如使用操作系统的堆栈，在我们切换之前系统必须要知道内核堆栈在哪</p><p>所以x86提供了一个<code>tr</code>寄存器给我们使用，这个寄存器的位数为16位，专门存贮一个选择子（类似前面段选择子），这个选择子<code>base</code>地址必须指向一块空余空间。前面知道我们要想从用户跳操作系统，必须知道操作系统的堆栈（中断向量表提供了代码段cs：ip），所以x86干脆规定了这个空间名字为<code>TSS</code>，并声明了一大串寄存器的值备用，当然这些是硬件规定的，软件用不用无所谓，但是硬件会在跳转的时候，读取里面固定位置的值，也就是我们需要的堆栈段<code>ss：esp</code>，在<code>inc/mmu.h</code>的<code>Taskstate</code>结构体中声明了这段内存规定的字段</p><p>所以我们在<code>kern/trap.c</code>中声明了（ss：esp）</p><pre><code>   ts.ts_esp0 = KSTACKTOP;ts.ts_ss0 = GD_KD;</code></pre><p>这样我们在跳转的时候就能获取到内核的堆栈段，从而进入内核模式，当然硬件设定<code>TSS</code>的作用一开始为了考虑任务切换，它也提供了相应的指令，但是需要200左右时钟周期来完成，所以我们<code>xv6</code>嫌弃其速度太慢，只是使用了其保存内核堆栈地址的作用，所以在<code>xv6</code>中<code>TSS</code>只是一个“数据库”而已。</p><hr><p>介绍完前面的基础知识之后，接下来的就是这个<code>Trap</code>，前面我们知道在得到<code>int</code>这个指令时候这个时候就开始一个<code>Trap</code>开始。</p><p>这个时候硬件接管程序控制，你也可以认为跳转到硬件代码去了。在给的<a href="https://pdos.csail.mit.edu/6.828/2016/xv6/book-rev9.pdf" target="_blank" rel="noopener">资料</a>第三章的<code>X86 Protection</code>里面介绍了这个过程，我们将前面几个步骤很我们前面给的资料联系起来</p><blockquote><p>后面推寄存器值就不解释了，同<code>Trapframe</code>的反向结构相同，之所以会这样，是因为堆栈地址是向下生长，而程序地址是向上生长的</p></blockquote><p>硬件这部分操作为</p><ul><li>获取中断向量表第n个的地址（n为<code>int</code>后面整数）</li></ul><p>然后开始权限检查，这个也是为什么程序不能随意调用系统资源的原因</p><ul><li>检测DPL与CPL的大小关系</li></ul><p>这个<code>DPL</code>不是前面<code>GDT</code>里面的，这个是存贮在中断向量表中<code>CS</code>里面的值，这个设置可以让我们给每个中断向量设置一个权限，看它允许是用户调用还是系统调用，如果只允许系统调用，那么用户调用直接引发<code>General Protection Fault</code>（13号中断），在我们的xv6中，我们只允许用户调用<code>Debug</code>和<code>System Call</code>中断，其他只能由系统调用</p><ul><li>判断保留<code>esp</code>和<code>ss</code>，检测目标的段与当前段</li></ul><p>当我们在操作系统中出现中断的时候，如果硬件还傻乎乎的将操作系统的堆栈归位那么就进入死循环了，所以这个目的就是避免同属一个段的时候不会切换堆栈</p><ul><li>当跳段的时候，从<code>TSS</code>中取到<code>esp</code>和<code>ss</code>切换堆栈段</li></ul><p>这个就是前面弹到的<code>TSS</code>的功能，接下来我们所以推出去的值全部存贮在要跳的段的堆栈上面了。</p><p>推完<code>Trap</code>程序的寄存器值，保留好“犯罪现场”后，就可以进入内核进行处理这个异常了。</p><p>至此我们介绍完了严格有序的处理从用户到内核的跳转，当我们处理完后就可以通过<code>iret</code>回到用户程序，这样就完成了用户拥有内核权限的设计。</p><hr><h2 id="用户时间控制"><a href="#用户时间控制" class="headerlink" title="用户时间控制"></a>用户时间控制</h2><p>接下来的用户使用CPU时间设置也是通过这个这个<code>Trap</code>的中断实现，你可以理解为硬件在每个CPU上在每隔个<code>10ms</code>的时候就触发一个中断。这样到内核的时候，内核发现是时间中断，就保留用户的寄存器值到用户自己空间（<code>Env</code>里面有一个<code>env_tf</code>变量可以存贮这些信息），然后调用其他或者这个程序，这样的话，我们就实现了一个时间控制。</p><p>PS：这里我们提一下这个<code>Env</code>结构体，内核初始化了1024个这个结构体，这说明我们可以同时拥有1024个用户程序运行，这个结构体就是用户环境，也就是我们在<code>Unix</code>系统上输入<code>ps</code>中的<code>pid</code>（每个<code>pid</code>代表一个程序）</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>至今我们介绍了通过一个简简单单的<code>Trap</code>操作就实现了复杂的权限管理和空间隔离，虽然操作系统看起来非常复杂，但其实都是由简单干练的概念搭建起来的，因为<code>Unix</code>的哲学就是<code>简单至上</code>。</p><h2 id="引用"><a href="#引用" class="headerlink" title="引用"></a>引用</h2><p><a href="https://stackoverflow.com/questions/6892421/switching-to-user-mode-using-iret" target="_blank" rel="noopener">https://stackoverflow.com/questions/6892421/switching-to-user-mode-using-iret</a></p><p><a href="https://stackoverflow.com/questions/36617718/difference-between-dpl-and-rpl-in-x86" target="_blank" rel="noopener">https://stackoverflow.com/questions/36617718/difference-between-dpl-and-rpl-in-x86</a></p><p><a href="https://en.wikipedia.org/wiki/Task_state_segment" target="_blank" rel="noopener">https://en.wikipedia.org/wiki/Task_state_segment</a></p>]]></content>
      
      
      <categories>
          
          <category> 算法 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> MOOC </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>KERNBASE对操作系统的影响</title>
      <link href="2018/02/19/algorithm/mit6_828/KERNBASE%E5%AF%B9%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F%E7%9A%84%E5%BD%B1%E5%93%8D/"/>
      <url>2018/02/19/algorithm/mit6_828/KERNBASE%E5%AF%B9%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F%E7%9A%84%E5%BD%B1%E5%93%8D/</url>
      
        <content type="html"><![CDATA[<h2 id="概括"><a href="#概括" class="headerlink" title="概括"></a>概括</h2><blockquote><p>这个问题主要在这本<a href="https://pdos.csail.mit.edu/6.828/2016/xv6/book-rev9.pdf" target="_blank" rel="noopener">xv6-ref</a>的第一章的练习题2中提出来</p></blockquote><p>问题如下</p><pre><code>KERNBASE limits the amount of memory a single process can use, which might be irritating on a machine with a full 4 GB of RAM. Would raising KERNBASE allow a process to use more memory?</code></pre><p>根据前面的知识可知这个问题翻译过来就是：在32位处理器下，当<code>KERNBASE</code>为<code>0x80000000</code>时用户最多可以用2GB内存，我们是否能够增加<code>KERNBASE</code>让用户使用超过2GB内存呢？</p><p>ps： 下面<code>KERNBASE</code>简写为<code>KE</code></p><h2 id="引言"><a href="#引言" class="headerlink" title="引言"></a>引言</h2><p>要想解答这个问题，我们必须要知道xv6是如何分配内存的，以及如何区分用户和内核，以及<code>KE</code>的作用。</p><p>xv6使用内存分页技术，如果你对这个不了解可以看这篇<a href="/2018/01/31/内存分页/">引导中的静态表</a>和这篇<a href="/2018/02/内存分页设计/">内存分页设计细节</a>，了解完这个我们谈谈用户和内核的内存分页应用。</p><p>注意我们所以计算的前提是在32位处理器的条件下</p><h2 id="用户和内核应用内存分页"><a href="#用户和内核应用内存分页" class="headerlink" title="用户和内核应用内存分页"></a>用户和内核应用内存分页</h2><p>其实用户和内核都是程序，对于用户来说内核只有一个，对于内核来说，用户可以有无数个。为了隔离内核和用户我们必须在内存上就进行隔离，所以单一内核和每个用户都拥有一个内存目录。</p><h3 id="内存分页的意义"><a href="#内存分页的意义" class="headerlink" title="内存分页的意义"></a>内存分页的意义</h3><p> 其实我们可以把内存的目录对应的所有内存表看做一张大的内存映射表，我们使用<code>1024 × 1024</code>个表单项代表<code>4GB</code>内存地址，每个代表<code>4K</code>连续的内存空间，当然大部分程序不会使用全部表单（一般就是内存目录一个，一到三个内存表）</p><p> 当然好处有很多，比如任何程序都可以使用相同的虚拟地址但是在实际运行的时候不会相互影响，这个好处就是通过映射来实现的。</p><ul><li>映射就是虚拟地址到物理地址的一种转换，这是一种<code>多对一</code>的映射，也就是一个虚拟地址只能对应一个物理地址，然而物理地址可以对应多个虚拟地址</li></ul><p>这个转换带来了一个问题，给你一个虚拟地址，你可以查表然后得到物理地址，但是如果给你一个物理地址，你除了遍历没有很好的办法获取虚拟地址（而且有可能你会得到多个虚拟地址）。</p><p>这个问题非常重要，因为它影响了我们后面内核与用户切换的时候地址的搜索，接下来我们看看内核初始化后内存是一种什么样的存在。</p><h2 id="内核内存利用"><a href="#内核内存利用" class="headerlink" title="内核内存利用"></a>内核内存利用</h2><p>看xv6源码我们知道，内核这个程序将程序虚拟地址起点定在<code>KE</code>上，我们知道一个程序由三部分组成指令、数据、堆栈。堆栈是向下生长，而数据、指令向上生长。而xv6把堆栈固定为32KB，所以内存只能向上生长，所以理论上内核最大占用空间只有<code>2^32(4G) - KE</code></p><ul><li>当<code>KE</code>为<code>0xf0000000</code>时，只有256M</li><li>当<code>KE</code>为<code>0x80000000</code>，有2G。</li></ul><p>由于用户内存空间不能超过<code>KE</code>（实际系统中为<code>UTOP</code>，值为<code>KE</code>减去一些为系统功能预留的空间，这里我们忽略他们），所以用户所能得到的内存空间最大为<code>KE</code>，也就是</p><ul><li>当<code>KE</code>为<code>0xf0000000</code>时，为 3840M（4G-256M）</li><li>当<code>KE</code>为<code>0x80000000</code>，有2G。</li></ul><p>通过上面简单的理论值我们推断出答案是：</p><ul><li>增大<code>KE</code>可以增大用户内存</li></ul><p>这个结果看起来是正确的，但是我们知道作为一个操作系统不能固定用户的内存，我们接下来就看看在不同的内存下的实际应用以及<code>xv6</code>自身设计问题，由此来探索这个问题的本质</p><p>我们就分两种情况来考虑 </p><h3 id="物理内存小于（4G-KERNBASE）"><a href="#物理内存小于（4G-KERNBASE）" class="headerlink" title="物理内存小于（4G-KERNBASE）"></a>物理内存小于（4G-KERNBASE）</h3><p>这种情况在很早的时候经常出现，我们计算机的内存可能就几M，这个时候内核假如真的占了<code>4G-KE</code>那连内核都跑不起来何况用户程序。</p><p>我们接下来进入源码来看<code>xv6</code>将实际占用的空间体现在内存表中，我们看到<code>kern/kernld.ld</code>配置文件，其中最主要的一行</p><pre><code>PROVIDE(end = .);</code></pre><p>这行位置出现在最后一行，它的作用是提供了一个变量<code>end</code>，我们在C中就可以用这个变量代表机器码的最后一行的虚拟地址，有了这个地址之后，我们就获取了一个重要信息内核程序总长度，假如没有这个变量，我们只能人工设定一个操作系统的占用空间，假如定大了就是浪费，定小了程序就可能崩溃（内存溢出）。分配的细节可以看这篇<a href="/2018/02/内存分页设计/">文章</a>，通过这个变量，我们把内核占用的物理空间给腾出来了。接下来的剩余的物理空间就留给用户了，所以用户能用到的内存最大为<code>实际内存 - 操作系统占用的内存</code>。</p><p>而且我们能够知道操作系统还将内核的内存映射直接固定了，将<code>KE：4G</code>的地址映射到<code>0:4G-KE</code>，公式为<code>虚拟内存=物理内存+KERNBASE</code>，这个映射存在于<code>kern/pmap.c</code>的<code>mem_init</code>函数中，这个带来了一个便利，就是程序变得很简单，假如我们得到一个物理内存地址，我们就能知道虚拟内存地址，但是这个也引来一个巨大的问题，就是当内存大于<code>4G - KE</code>的时候。</p><p>PS：关于知道物理内存能知道虚拟地址在源码的好处主要体现在用户内存空间创建的时候，这个时候处于内核态，当我们得到一个空的物理空间页，我们只要简单的使用上面公式就能获取到虚拟地址，然后内核就能在直接通过指针修改程序。假如没有这个映射，我们就后面动态建立，但是这里涉及到一个非常棘手的<code>成本</code>的问题，我们其实只需要一个值映射，然而我们却要浪费4K的连续内存空间，而且给程序带来了很大的复杂性。</p><p>我们接下来考虑系统内存大于那段映射最大值（4G-KE）时候会造成什么情况</p><h2 id="物理内存大于-（4G-KERNBASE）"><a href="#物理内存大于-（4G-KERNBASE）" class="headerlink" title="物理内存大于 （4G-KERNBASE）"></a>物理内存大于 （4G-KERNBASE）</h2><p>在现在大家内存愈来愈大，这种情况比较常见，由于xv6在程序设计中是动态创建用户空间，当用户比较少占用空间比较小的时候，程序不会用到物理地址比较大的空间，但是当物理内存超过这个阀值（4G-E），在这里我们考虑<code>KE=0xf0000000</code>,也就是物理内存地址大于256M的时候，由于内核没有映射到这么高的地址，所以我们用上面那个物理地址转虚拟地址公式就失效了。</p><p>由于xv6只是一个教学系统，所以为了简化系统，降低程序复杂性所以使用了最简单的直接映射的方法来处理内存，也正是由于这个原因提高<code>KE</code>并不能增加用户最大内存，当然解决的方法也有，我想了两种。</p><ul><li>在内核空间中分配用户内存目录空间，这种最为简单，但是会浪费一定的内存空间（必须在内核一开始运行就占用空间）</li><li>在内核空间中映射一段固定的地址来存放用户内存目录地址，这个程序设计比较复杂，但是只需要初始化一个内存表单就行</li></ul><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>通过上面的分析，我们得到一个结论：</p><p>由于xv6设计的局限性，增大<code>KERNBASE</code>并不能增大用户最大可用空间，反而会减少。但是如果修改xv6设计可以实现增大<code>KERNBASE</code>增大用户最大可用空间。</p><h2 id="引用"><a href="#引用" class="headerlink" title="引用"></a>引用</h2>]]></content>
      
      
      <categories>
          
          <category> 算法 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> MOOC </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>内存分页设计</title>
      <link href="2018/02/05/algorithm/mit6_828/%E5%86%85%E5%AD%98%E5%88%86%E9%A1%B5%E8%AE%BE%E8%AE%A1/"/>
      <url>2018/02/05/algorithm/mit6_828/%E5%86%85%E5%AD%98%E5%88%86%E9%A1%B5%E8%AE%BE%E8%AE%A1/</url>
      
        <content type="html"><![CDATA[<h2 id="引言"><a href="#引言" class="headerlink" title="引言"></a>引言</h2><p>前面已经通过lab1的这篇<a href="/2018/01/03/内存分页设计/">博文</a>了解了内存分页的实现细节，接下来就谈谈如何具体实现内存分页</p><h2 id="物理载体"><a href="#物理载体" class="headerlink" title="物理载体"></a>物理载体</h2><p>通过了解<a href="/2018/02/19/KERNBASE对操作系统的影响/">KERNBASE对操作系统的影响</a>这篇博文我们知道，其实内存分页就是完成对物理存在的一种分割和隔离，所以我们在完成内存分页系统设计之前必须要构建一个载体，完成对物理存在的一种表示。</p><p>在xv6中声明一个动态数组来代表物理内存，每一个值代表一块4k的内存页。我们主要通过<code>offset - base</code>得到偏移倍数，每个偏移量为<code>4K</code>，也就是通过<code>(offset - base )  &lt;&lt; 12</code>得到物理地址，我们看一下这个数组成员：PageInfo结构体</p><pre><code>struct PageInfo {    struct PageInfo *pp_link;    uint16_t pp_ref;};</code></pre><p>主要存在两个值：一个为下一个可用的地址指针，一个为引用次数。</p><p>引用次数比较好理解，但是这个<code>pp_link</code>有什么用呢。其实你可用把这个结构体看做成一个由链表组成的堆栈，我们只需要保留栈顶值（page_free_list)，由于它保存下一个值地址，这样通过不断的push、pop，就能维持一个可用物理内存栈。</p><h2 id="二级指针的妙用"><a href="#二级指针的妙用" class="headerlink" title="二级指针的妙用"></a>二级指针的妙用</h2><p>由于前面的博客原理已经介绍的很详细了，我就不再累赘了，在这里我提一下源码中二级指针的妙用，虽然它只有短短几行，但是运行的结果却是让人大开眼界，体会到指针的神奇威力。</p><p>这段代码出现在<code>kern/pmap.c</code>的<code>check_page_free_list</code>函数中</p><pre><code>if (only_low_memory) {    // Move pages with lower addresses first in the free    // list, since entry_pgdir does not map all pages.    struct PageInfo *pp1, *pp2;    struct PageInfo **tp[2] = { &amp;pp1, &amp;pp2 };    for (pp = page_free_list; pp; pp = pp-&gt;pp_link) {        int pagetype = PDX(page2pa(pp)) &gt;= pdx_limit;        *tp[pagetype] = pp;        tp[pagetype] = &amp;pp-&gt;pp_link;    }    *tp[1] = 0;    *tp[0] = pp2;    page_free_list = pp1;}</code></pre><p>主要的作用是将“栈底”的元素移到“栈顶”，首先它使用了两个一级指针（pp1、pp2），还有两个二级指针分别指向（pp1、pp2）</p><p> 首先<code>int pagetype = PDX(page2pa(pp)) &gt;= pdx_limit;</code>判断物理地址是否为大于4M还是小于4M，我们把物理内存页分成两组</p><ul><li>小于4M A组</li><li>大于4M B组</li></ul><p>对于小于4M的组，分两种情况</p><ol><li>第一个小于4M的内存页（page1）</li></ol><blockquote><ol><li>*tp[0] (也就是pp1） 存贮了pp的值，也就是pp1 = page1</li><li>tp[0] 存贮了pp -&gt; link 的地址（这个没有什么用）</li></ol></blockquote><ol start="2"><li>A组第二个以及以后的内存页（page2）</li></ol><blockquote><ol><li>上一个地址的值等于pp（没什么用）</li><li>tp[0] 存贮了下一个空闲地址的值</li></ol></blockquote><p>对于B组来说也是一样的，最重要的是<code>for</code>循环结束后实现的交换</p><pre><code>*tp[1] = 0; </code></pre><p>B组最后一个的<code>pp_link</code>地址地址设置为NULL，也就相当于把他放到栈底</p><pre><code>*tp[0] = pp2;</code></pre><p>A组最后一个变成<code>pp_link</code>地址设置pp2，也就是B组的第一个接到了A组的最后面去了</p><pre><code>page_free_list = pp1;</code></pre><p>栈顶变成A组第一个，通过这样的“乾坤大挪移”术就将A组部分移到B组前面去了，也就实现了先使用低地址的物理内存的作用</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>理解内存分页必须要了解背后的原理，了解了原理看具体实现的时候才能事半功倍。</p><h2 id="引用"><a href="#引用" class="headerlink" title="引用"></a>引用</h2>]]></content>
      
      
      <categories>
          
          <category> 算法 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> MOOC </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>内存分页</title>
      <link href="2018/01/31/algorithm/mit6_828/%E5%86%85%E5%AD%98%E5%88%86%E9%A1%B5/"/>
      <url>2018/01/31/algorithm/mit6_828/%E5%86%85%E5%AD%98%E5%88%86%E9%A1%B5/</url>
      
        <content type="html"><![CDATA[<h2 id="引言"><a href="#引言" class="headerlink" title="引言"></a>引言</h2><p>本来自己查了很多资料，想自己写出来，结果下笔的时候发现别人已经把我想写的部分全部写出来了，而且比我想的还要具体，所以我就不写了，把链接放出了，顺便我补充一些</p><p><a href="http://leenjewel.github.io/blog/2015/11/11/%5B%28xue-xi-xv6%29%5D-nei-he-gai-lan/" target="_blank" rel="noopener">http://leenjewel.github.io/blog/2015/11/11/%5B%28xue-xi-xv6%29%5D-nei-he-gai-lan/</a></p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>由于内存分页在后面的lab2中也会设计，而且给出的资料写的非常详细，所以我就不多提了。这里提一下为了更好的理解什么内存分页的段选择子与CS的关系，我写了一篇<a href="/2018/01/30/从CS寄存器看段的前世今生/">关于CS寄存器的发展史</a></p><p>最后我提一下<code>kern/entrypgdir.c</code>手写的内存分页表</p><p>第一个<code>entry_pgdir</code>是页目录，第二个<code>entry_pgtable</code>页表，而且注意<code>entry_pgtable</code>是代表0-4M物理内存可写可读的真实内存地址</p><p>前面知道了目录和页表的作用，在现实系统中，一个目录下有1024个页表，但是这只有一个页表，而且它代表从0-4M连续的内存空间</p><p>PS：后面12位是权限位，只看前三位，可以知道是从（000-3ff）的高20位物理地址，由于我们声明了<code>__attribute__((__aligned__(PGSIZE)))</code>（地址4K对齐，其实意思是它真实物理地址后12位一定全部为0），所以直接取<code>(uintptr_t)entry_pgtable</code>地址就是页表地址，虽然后面加上权限位<code>PTE_P + PTE_W</code>，但是在寻找真实地址时都会屏蔽（只取前20位如），当初想了很久感觉取不到页表的真实地址，还以为有什么神奇的编译设定，后面查了很久资料才明白，所以这里提个醒后面我们会在lab2中正式接触内存分页技术，这里只是相当于设定了一个“常数”，后面会将器扩展变成“函数”。</p><h2 id="引用"><a href="#引用" class="headerlink" title="引用"></a>引用</h2>]]></content>
      
      
      <categories>
          
          <category> 算法 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> MOOC </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>引导和操作系统的交互</title>
      <link href="2018/01/31/algorithm/mit6_828/%E5%BC%95%E5%AF%BC%E5%92%8C%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F%E7%9A%84%E4%BA%A4%E4%BA%92/"/>
      <url>2018/01/31/algorithm/mit6_828/%E5%BC%95%E5%AF%BC%E5%92%8C%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F%E7%9A%84%E4%BA%A4%E4%BA%92/</url>
      
        <content type="html"><![CDATA[<h2 id="引言"><a href="#引言" class="headerlink" title="引言"></a>引言</h2><p>本来自己查了很多资料，想自己写出来，结果下笔的时候发现别人已经把我想写的部分全部写出来了，而且比我想的还要具体，所以我就不写了，把链接放出了，顺便我补充一些</p><p><a href="http://leenjewel.github.io/blog/2014/07/29/%5B%28xue-xi-xv6%29%5D-cong-shi-mo-shi-dao-bao-hu-mo-shi/" target="_blank" rel="noopener">http://leenjewel.github.io/blog/2014/07/29/%5B%28xue-xi-xv6%29%5D-cong-shi-mo-shi-dao-bao-hu-mo-shi/</a></p><p><a href="http://leenjewel.github.io/blog/2015/05/26/%5B%28xue-xi-xv6%29%5D-jia-zai-bing-yun-xing-nei-he/" target="_blank" rel="noopener">http://leenjewel.github.io/blog/2015/05/26/%5B%28xue-xi-xv6%29%5D-jia-zai-bing-yun-xing-nei-he/</a></p><h2 id="ELF文件"><a href="#ELF文件" class="headerlink" title="ELF文件"></a>ELF文件</h2><p>首先你要知道什么是ELF，可以看一下这篇<a href="https://my.oschina.net/u/864891/blog/87965" target="_blank" rel="noopener">博客</a>，简单来说就是编译完C后的机器码，前面加了一些数据表记录程序的分布情况</p><p>为了帮助我们逆向分析这些代码有什么，我们必须要借助两个工具</p><ul><li>objdump</li><li>readelf</li></ul><p>我们接下来就用实际例子解读mit6.828里面的引导和核心</p><h2 id="引导"><a href="#引导" class="headerlink" title="引导"></a>引导</h2><p>首先我们查看一下引导文件，在<code>boot/</code>目录下有两个文件</p><ul><li>boot.S</li><li>main.c</li></ul><p>两者是通过gcc编译器将汇编和C编译成为一个<code>elf</code>文件，具体在<code>Makefile</code>文件中（<code>boot/Makefrag</code>)，我将它简单翻译一下</p><pre><code>gcc -N -e start -Ttext 0x7C00 -o boot.out boot.o main.o</code></pre><p><code>boot.o</code>和<code>main.o</code>就是<code>boot.S</code>和<code>main.c</code>编译后的文件，<code>-e start</code>意思程序从<code>boot.S</code>的<code>start</code>中开始运行（这样就能从汇编开始执行），<code>-Ttext</code>中<code>text</code>代表代码段，也就是说直接指定代码入口地址为<code>0x7C00</code>，我们可以用<code>readelf</code>验证一下</p><pre><code>mit-6.828-2014 ➤ readelf -h obj/boot/boot.out                         git:lab1*ELF Header:  Magic:   7f 45 4c 46 01 01 01 00 00 00 00 00 00 00 00 00   Class:                             ELF32  Data:                              2&apos;s complement, little endian  Version:                           1 (current)  OS/ABI:                            UNIX - System V  ABI Version:                       0  Type:                              EXEC (Executable file)  Machine:                           Intel 80386  Version:                           0x1  Entry point address:               0x7c00  Start of program headers:          52 (bytes into file)  Start of section headers:          4868 (bytes into file)  Flags:                             0x0  Size of this header:               52 (bytes)  Size of program headers:           32 (bytes)  Number of program headers:         2  Size of section headers:           40 (bytes)  Number of section headers:         9  Section header string table index: 6</code></pre><p>我们读取了一下生成的<code>elf</code>文件头部，我们可以看到<code>Entry point address</code>这个字段，就是我们设定的<code>0x7c00</code>，要了解这个地址的含义我们必须知道虚拟地址和物理地址的区别，可以看一下这篇<a href="http://leenjewel.github.io/blog/2014/07/29/%5B%28xue-xi-xv6%29%5D-cong-shi-mo-shi-dao-bao-hu-mo-shi/" target="_blank" rel="noopener">博客</a>，接着我们看一下反汇编的汇编代码<code>obj/boot/boot.asm</code>中，我们知道现在<code>0x7c00</code>代表程序把自己当做在内存上的真实内存上面，但是不一定会真的存在这块上，所以我们称它为虚拟的</p><p>首先我们了解一个系统知识，因为电脑启动后会按照启动盘顺序，把每个盘第一个扇形区512B取出来，如果最后两个字节为<code>0xAA55</code>的话就把它放到内存上面的<code>0x7c00</code>上去，我们看一下我们生成的<code>obj/boot/boot</code></p><p>使用<code>hexdump obj/boot/boo</code>可以看到（我把最后一行复制出来）</p><pre><code>00001f0 0000 0000 0000 0000 0000 0000 0000 aa55</code></pre><p>最后连个字节为<code>0xaa55</code>，且文件大小刚刚好512B，这时候你可以有一个疑惑了，我们知道<code>gcc</code>我们生成的<code>boot.out</code>的elf文件，但是这个文件还有头部存贮数据，我们如果把整个文件放到磁盘上，当程序在内存<code>0x7c00</code>处执行时，那么文件头部碰到的就是elf头了，所以<br>为了把机器码提取出来并生成合适文件（512B尾部为<code>0xaa55</code>），程序干了两件事</p><ul><li>用objcopy将elf文件中执行代码提取出来（相当于去掉elf头部）</li><li>用脚本修改尾部两字节（在<code>boot/sign.pl</code>用了perl程序来将生成512B且尾部为<code>0xaa55</code>的<code>boot</code>文件）</li></ul><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>后面将核心加载到内存，上面的给出资料写的很详细，我就不多说，只不过由于当前<code>2014</code>的版本同资料有点不同，这里我提一下</p><p>当前版本是将内核头加载在<code>0x10000</code>上，然后在把内核代码加载到<code>0x100000</code>上（前面4个0，后面5个0，我当初看错了，百思不得其解），并将内核地址映射到<code>f0100000</code>上。</p><p>由于资料给的操作系统与2016的操作系统实现细节有点不同，其中最主要的就是一个很重要的<code>KERNBASE</code>常量值由<code>0x80000000</code>变成<code>0xf0000000</code>，这个变量牵扯到了在给出的<code>book-rev8.pdf</code>资料中第一章的最后一个问题，我就把我对这个问题的思考放到这篇<a href="/2018/02/19/KERNBASE对操作系统的影响/">文章</a>中。</p><h2 id="引用"><a href="#引用" class="headerlink" title="引用"></a>引用</h2><p><a href="http://leenjewel.github.io/blog/2014/07/29/%5B%28xue-xi-xv6%29%5D-cong-shi-mo-shi-dao-bao-hu-mo-shi/" target="_blank" rel="noopener">http://leenjewel.github.io/blog/2014/07/29/%5B%28xue-xi-xv6%29%5D-cong-shi-mo-shi-dao-bao-hu-mo-shi/</a></p><p><a href="https://my.oschina.net/u/864891/blog/87965" target="_blank" rel="noopener">https://my.oschina.net/u/864891/blog/87965</a></p><p><a href="http://bochs.sourceforge.net/techspec/PORTS.LST" target="_blank" rel="noopener">端口信息</a></p>]]></content>
      
      
      <categories>
          
          <category> 算法 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> MOOC </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>从CS寄存器看段的前世今生</title>
      <link href="2018/01/30/algorithm/mit6_828/%E4%BB%8ECS%E5%AF%84%E5%AD%98%E5%99%A8%E7%9C%8B%E6%AE%B5%E7%9A%84%E5%89%8D%E4%B8%96%E4%BB%8A%E7%94%9F/"/>
      <url>2018/01/30/algorithm/mit6_828/%E4%BB%8ECS%E5%AF%84%E5%AD%98%E5%99%A8%E7%9C%8B%E6%AE%B5%E7%9A%84%E5%89%8D%E4%B8%96%E4%BB%8A%E7%94%9F/</url>
      
        <content type="html"><![CDATA[<h2 id="引言"><a href="#引言" class="headerlink" title="引言"></a>引言</h2><p>Intel作为作为微处理器的航头老大，一直引导CPU的进步发展，也正是因为Intel是一个有着历史包袱的企业，所以站在现代CPU看起来，有一些非常奇葩的设计遗留下来，这些设计一开始是为了兼容，慢慢的将这种兼容又发展成新的功能，把“包袱”转换成“亮点”，段设计就是其中的一个很重要的代表，要想搞懂这个设计在不同的CPU的如何保持兼容和强化，我们必须要慢慢的把CPU的历史给捋顺。</p><h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>我们要介绍第一个CPU是8086，虽然Intel前面也生产了4004、8008、8080微处理器，但是8086基本上后面所以现代化CPU始祖，Intel后面生产的CPU都对8086兼容，所以可以说段这个“祸端”就是从8086种下来的</p><p>8086作为第一款的16位处理器，由于技术的问题，出现了一个严重问题寄存器只有16位，而地址总线却有20位，要想只通过一个寄存器表达地址的值显示无法实现，所以8086采用了一个段寄存器，segment × 16+offset（CS:IP），采用这个方式就能实现20位地址索引</p><h2 id="段设计对程序影响"><a href="#段设计对程序影响" class="headerlink" title="段设计对程序影响"></a>段设计对程序影响</h2><p>前面简单的介绍了段设计的原因，现在我们来看看这个设计的对16位程序的影响。</p><h3 id="兼容性"><a href="#兼容性" class="headerlink" title="兼容性"></a>兼容性</h3><p>由于段的存在，我们设计程序的时候可以更加自由，我们可以假设我们是在任何低16位置的内存上，只有到时切换程序时候切换段就可以</p><h3 id="安全性"><a href="#安全性" class="headerlink" title="安全性"></a>安全性</h3><p>我们可以用相同的低地址存代码和数据，只需要切换段就行，而且我们也不需要考虑数据要多少空间，代码要多少空间，以便设定跳转地址</p><blockquote><p>可以说段设计是16位程序的一个很优雅的做法，但是到了３２位的时候程序不行了</p></blockquote><h2 id="32位处理器的改变"><a href="#32位处理器的改变" class="headerlink" title="32位处理器的改变"></a>32位处理器的改变</h2><p>我们知道32位处理器的诞生带来了两件事,第一个就是寄存器有32位了,说明最大内存地址可以到4G了(<code>0x000000-0xffffff</code>),而且总线地址也有32位了。这个时候就很尴尬了，原来的段设计到这里就成鸡肋了。但是为了兼容，我们还是给他一个功能，段选择子，当然现代Unix操作系统没有用这个东西，只是在初始化的时候意思意思一下，具体可以看一下这篇<a href="http://leenjewel.github.io/blog/2014/07/29/%5B%28xue-xi-xv6%29%5D-cong-shi-mo-shi-dao-bao-hu-mo-shi/" target="_blank" rel="noopener">博客</a>的<code>GDB</code>表</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>最近学操作系统感觉有很多疑惑，了解很多知识，总想好好总结一下细节，结果发现总有大神们早就写出来了，而且非常详细，我就不班门弄釜了，把下面的lab好好思考，提炼出自己的知识。</p>]]></content>
      
      
      <categories>
          
          <category> 算法 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> MOOC </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>什么是操作系统</title>
      <link href="2018/01/29/algorithm/mit6_828/%E4%BB%80%E4%B9%88%E6%98%AF%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/"/>
      <url>2018/01/29/algorithm/mit6_828/%E4%BB%80%E4%B9%88%E6%98%AF%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/</url>
      
        <content type="html"><![CDATA[<h2 id="引言"><a href="#引言" class="headerlink" title="引言"></a>引言</h2><p>本文是基于mit6.828 的lab1对操作系统的思考，网上有不少关于lab1的博客，大部分都是介绍如何完成lab1的问题，介绍的比较详细的有这个<a href="http://www.cnblogs.com/fatsheep9146/category/769143.html" target="_blank" rel="noopener">博客</a>，在这里我就不从问题出发，建议大家看完上面的博客在看我这篇博文，我这篇博文就是从把我遇到的疑惑提炼出知识点，然后再把这些知识点串起来</p><h2 id="实验目的"><a href="#实验目的" class="headerlink" title="实验目的"></a>实验目的</h2><p>作为操作系统的第一个实验，在完成lab1我们主要目的就是要了解什么是操作系统，后面的lab都是在这个基础上搭建完善好系统的各种功能，就好像你画一幅画，第一步你先把人物的轮廓外形画出来，接下来你在把人的鼻子耳朵眼睛等等慢慢画出来，所以第一个实验非常重要，如果不把骨架搭好，你的操作系统再怎么豪华强大也是空中楼阁，所以我们在这个lab上一定要花上很多时间，万事开头难，只要攻克了这个，操作系统也就离你不远了</p><h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>谈操作系统之前必须得谈谈什么是计算机，因为我们的操作系统是运行在计算机上面的，操作系统与计算机关系密切，我们可以用一个很贴切的比方，假如你想了解手套为什么长成那么奇怪，那么你得先举起你的双手看看。</p><p>闲话不多说，进入正题，计算机最核心部分就是CPU，记得电脑刚发明出来的时候，编程只能通过穿孔卡片，当然计算机发明这么久，CPU的核心功能基本上没有变，还是只能通过“穿孔卡片”，只不过我们编程语音将我们的使用的语音如C，Python等变成“穿孔卡片”，这里有个非常重要的概念：<strong>指令</strong>。也就是每个CPU的是每次只能执行一条指令。当然第二个重要的就是内存了，前面说了CPU是必须要通过”穿孔卡片“来编程的，但是随着程序愈来愈大，我们必须要使用一个东西来存贮“卡片”，所以内存孕育而生了，其实很多感叹电脑这个词实在是太贴切了，人脑可以处理问题，电脑也可以处理问题，人脑有短期记忆长期记录，电脑有寄存器（短期记忆）内存（长期记忆）。</p><p>简单的谈了谈计算机，我们提出两个重要的前提</p><ul><li>计算机通过顺序读取一条一条指令来工作</li><li>计算机从内存获取指令</li></ul><p>所以操作系统的就是在内存上面运行自己并帮助其他程序的一个大程序，总的来说它就是内存上面的一段程序，当然它的功能就是好好在它一亩三分地上面“分封诸侯”，维护自己的“王位”。</p><p>由于一开始它只是待在软盘或者磁盘等存贮设备上的一段程序，接下来我们就看操作系统如何“上位”内存的“宫斗大戏”。</p><h2 id="上位篇"><a href="#上位篇" class="headerlink" title="上位篇"></a>上位篇</h2><p>在操作系统“上位”之前，其实计算机还进行了一个小操作系统（BIOS）运行，具体的流程可以看这篇<a href="http://www.ruanyifeng.com/blog/2013/02/booting.html" title="操作系统启动" target="_blank" rel="noopener">文章</a>，如果把电脑看做人的话，BIOS就是我们的脊髓，它控制我们的身体四肢（硬件），但是却无法灵活的操作他们（比如非条件反射的膝跳反射），但是它却不可少，少了它就成植物人了。BIOS的作用总结起来就是感知硬件，调控硬件（比如CPU电压，内存频率）。</p><p>BIOS与操作系统最密切的一个地方就是设置启动盘，大家可能大多时候都是听文件文件比较多，但是文件这个概念其实是操作系统的，在没有操作系统之前是没有文件这个概念的，只有盘这个概念，比如一块硬盘、一张软盘、一个U盘，所以BIOS的一个重要工作就是选定一个存储硬盘，然后读取上面的程序运行。</p><p>所以我们从这时候可以得到一个结论，安装操作系统，只需要按照BIOS的规定把程序放到某一个存贮设备（磁盘、U盘、软盘）专门位置上（一般是头部开始），开机后然后就静等BIOS把程序加载到内存，就完成了“上位”过程。</p><p>我们已经知道操作系统的外部条件，接下来分析一个操作系统到底该有啥</p><h2 id="操作系统结构"><a href="#操作系统结构" class="headerlink" title="操作系统结构"></a>操作系统结构</h2><p>首先我们要回顾一下操作系统的发家史，刚开始是没有操作系统的这个概念的，随着在电脑上要运行的越来越多，每个程序都要写大量底层代码来操作硬件这样无疑每个程序越来越来臃肿，所以我们需要让操作系统管理来管理硬件，这样不但能够提供程序兼容性（一次编译、到处运行），而且能高效利用硬件（操作系统专供）。所以操作系统也就是程序的分工的产物</p><p>接下来我们看看操作系统的发展历史，大家都知道汇编语言是除了机器语言最接近底层的一种语言，第一个Unix系统一开始也就是使用汇编编写的。但是汇编语言不易于维护，所以1973年汤普逊和里奇用C重构了Unix，但是部分与硬件接触太大的地方还是必须使用汇编（in、out端口，操纵指定寄存器等），所以现代操作系统还是用C与汇编混合编写的（C占大部分），当然最后都会编译成机器码，所以我们从机器码角度来看，最后的结果都是相同的，不同的是我们使用不同的工具来生成机器码</p><p>前面我们介绍了启动盘，其实BIOS给我们只是从启动盘上面取了一小段数据（第一个扇形区）放到内存上面（512B），然后执行那小段上面的机器码，所以接下来就有一个问题，随着我们的操作系统功能越来越完善，取出来的代码肯定不是全部代码，代码只有放到内存上面才能跑，所以前人就把代码分成两部分，一部分称作引导，另一部分才是核心代码，引导作用就是把核心代码放到内存上面去</p><p>所以操作系统被分成两部分</p><ul><li>引导</li><li>核心</li></ul><p>这里比较有意思的地方就是引导和核心如何交换程序控制权，由于要讲解必须要牵扯代码，所以我把这部分分离出来，放到这篇<a href="/2018/01/31/引导和操作系统的交互/">博客</a>上面，要了解细节可以看这篇博客。</p><p>现在我们假设你已经知道引导将核心代码加载在了内存上面，现在我们就从这里开始分析，堆栈作为程序的基础，首先我们要提一下堆栈</p><h3 id="堆栈"><a href="#堆栈" class="headerlink" title="堆栈"></a>堆栈</h3><p>什么是堆栈呢？看下面这张图</p><p><img src="/images/stack.png" alt="堆栈"></p><p>堆顶为大地址，我们使用的时候只要不断把栈顶往下推，就能将线性内存变成一个数据结构。当然有个问题，我们必须要知道什么地方是栈顶，现在我们想想操作系统必须要放到内存的某一个地方，假如放到低位置，那么我必须要记住操作系统的最高位置，如果不这样做当堆栈顶到操作系统的存贮地址，那么操作系统就被破坏了。所以我们只能把操作系统放到高的内存地址，这样我们既能安全的使用堆栈，而且保证了操作系统的安全</p><p>所以我们接下来就谈谈由于这个设定引发的一系列问题</p><h2 id="操作系统的高地址"><a href="#操作系统的高地址" class="headerlink" title="操作系统的高地址"></a>操作系统的高地址</h2><p>在lab1中，操作系统的起始代码被设定为<code>0xF0100000</code>的高位</p><p>从这个地址我们可以分析出来什么<code>0xF0100000</code>，因为我们知道32位操作系统最高只能有4G内存，也就是2的32次方内存，这个地址代表的是系统最后面的255M内存的空间，也就是操作系统给自己留了255M剩余内存，给前面保留了（4G-255M）空间</p><p>操作系统这个设计是非常好的，不仅给自己留下扩展空间，也给其他代码带来便利，就是从0-(4G-255M）这部分内存随便用，怎么改都不会出问题</p><p>从现在角度来看，个人电脑随便都是2G、4G内存，但是在70\80年代，几M内存都很很大的存在，如果我内存没有4G这么大，那么这个操作系统就无法使用了，而且你留下256M给操作系统，有的时候我整个电脑内存都没有这么大，留下那么多，就是浪费</p><p>所以最后他们想出来一个办法，就是现实和理想之间放一个转换器（MMU），也就是动态内存管理。操作系统也不管你机主有多大内存了，我假设你有4G，当你用一块，无论是什么位置，我从空余的地方给你取出来，假如你没有4G，我就把一部分不常用内存值的放到磁盘上面，这样通过这样这样的操作的实现内存的高效利用。</p><p>操作系统对内存这种骚操作就相当于在更高的维度上建立了一种抽象，不论底层硬件怎么变（内存大小不同），我高层都不需要变，只需要底层把根据实际情况依次映射，这样无论你换多大内存，我都不需要重装系统。</p><p>这种映射不通过代码很难将清楚，所以我这里也不提太多，如果你想知道怎么映射，硬件如何配合，你可以看我这篇<a href="/2018/01/31/内存分页/">博客</a>，详细介绍了内存分页的骚操作。</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>至处到这里我们的操作系统之旅就结束了，我们成功在引导帮助下上了内存，也使用了提高操作系统的地址保证了堆栈的正常使用，接下来就是操作系统的各种附加功能，我们也会在接下来的lab中继续讲解。</p><p>lab1不止介绍了这两个地方，但是在lab1中我觉得最重要的地方就是要知道操作系统是什么，它从哪里来，它要到哪里去，操作系统毕竟牵涉到硬件，很多硬件知识我们不一定能搞懂，比如说要切换哪几个位到保护模式，怎么输出端口才能读取扇形区，但是我们必须要搞懂他这样做的原因，这也是我在这篇里面谈了很多操作系统历史的原因，希望在理解操作系统上面能给大家带来一点启发。</p><p>在最后的话，我谈一谈我对入门这门课的感受吧，我算编程基础还算比较扎实，但是在一开始学习的过程中一脸懵逼，C语言的各种骚操作，汇编与C疯狂混合，看惯了脚本语言的我，对这种改一下就编译不过的“硬骨头”异常难受，一开始根本看不下去，有的时候看几行代码，查资料好几个小时，而且资料特别难找，你不知道这种骚操作有啥意思，在这里还得感谢学习过这门课的学长学姐们，在网上无私的把自己的感受分享出来，慢慢的根据他们的资料和自己的调试还有自己找资料，慢慢的把这个代码一行一行搞懂，搞懂之后特别感慨，其实也不是很难，难的是要把你知道的所以知识点串起来，然后你就能搞懂为什么要这么做，所以我推荐大家还是多看代码，多去思考为什么要这么做，必须要自己的思考在里面，如果单纯的知识为了完成实验其实很简单，但是难就难在把这门课琢磨透，搞懂他们为什么要这样考你，毕竟大家最后的目的都是自己做出自己的操作系统，只有了解的透彻，才能“破而后立”搞出自己的创新，而不是复制他们的壳子，所以加油把每一行代码都搞懂，多思考，不要骄傲，毕竟这只是教学类的操作系统，要想自己写出来还得不懈努力。</p><p>接下来还有6个lab需要完成，终于迈出的第一步，希望在年前能够完成所有的实验吧！！！加油！！！</p><h2 id="引用"><a href="#引用" class="headerlink" title="引用"></a>引用</h2><p><a href="https://buweilv.github.io/2017/06/10/jos-bootstrap-3/" target="_blank" rel="noopener">“C语言虚拟内存表”</a></p>]]></content>
      
      
      <categories>
          
          <category> 算法 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> MOOC </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>mit6.828课程总结</title>
      <link href="2018/01/09/algorithm/mit6_828/mit6.828%E8%AF%BE%E7%A8%8B%E6%80%BB%E7%BB%93/"/>
      <url>2018/01/09/algorithm/mit6_828/mit6.828%E8%AF%BE%E7%A8%8B%E6%80%BB%E7%BB%93/</url>
      
        <content type="html"><![CDATA[<h2 id="引言"><a href="#引言" class="headerlink" title="引言"></a>引言</h2><blockquote><p>一开始想直接做一个操作系统，但是万事开头难，学习操作系统需要太多基础知识了，所以就按照网上推荐先学习mit6.828的课程，先把xvf6操作系统搞懂，然后在来实现自己的操作系统，下面就是学习这个课程的体会，按照各个lab的顺序，介绍自己的心得体会</p></blockquote><p>课程的地址是： <a href="https://pdos.csail.mit.edu/6.828/2016/schedule.html" target="_blank" rel="noopener">mit6.828</a><br>PS：由于<a href="https://pdos.csail.mit.edu/6.828/2016/schedule.html" target="_blank" rel="noopener">mit6.828</a>课程仓库需要翻墙，所以我把clone下来放到我的github仓库，我的<a href="https://github.com/mrzhangboss/mit6.828-2016" target="_blank" rel="noopener">仓库</a>里，大家可以clone下来（我会逐渐完成所以的实验）</p><h2 id="目录"><a href="#目录" class="headerlink" title="目录"></a>目录</h2><ul><li><a href="/2018/01/29/什么是操作系统/">什么是操作系统</a> lab1</li><li><a href="/2018/02/05/内存分页设计/">内存分页设计</a> lab2</li><li><a href="/2018/02/20/用户空间隔离/">用户空间隔离</a> lab3</li><li><a href="/2018/02/">并行多核设计</a> lab4</li><li><a href="/2018/02/">文件系统设计</a> lab5</li><li><a href="/2018/02/">网络设计</a> lab6</li></ul><h2 id="课程总结"><a href="#课程总结" class="headerlink" title="课程总结"></a>课程总结</h2><p>学到一半才发现这是一门研究生课程的学习，的确难度非常大，而且每一个实验都可以拿出来大做研究，但是课程给的资料非常详细，基本上每个硬件细节都给出了链接，但是对于最大的困难还是英语资料实在太多，有点“吸收”不过来。不过一路学习下来，感觉还是收获很大，基本上每个实验都环环相扣，每个实现细节都需要反复思考，为什么要这样做，还能怎么做，最终实现的xvf6还有些许跟不上时代的脚步，但是基本上框架已经有了，就是按照这个骨架完善更多细节，所以这个课程还远远没有结束，期待接下来对这个操作系统的进一步改进！！！</p>]]></content>
      
      
      <categories>
          
          <category> 算法 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> MOOC </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>新年展望</title>
      <link href="2018/01/06/summary/2018%E6%96%B0%E5%B9%B4%E5%B1%95%E6%9C%9B/"/>
      <url>2018/01/06/summary/2018%E6%96%B0%E5%B9%B4%E5%B1%95%E6%9C%9B/</url>
      
        <content type="html"><![CDATA[<h2 id="引言"><a href="#引言" class="headerlink" title="引言"></a>引言</h2><blockquote><p>最近逛知乎的时候看到一篇<a href="https://www.zhihu.com/question/36149122/answer/66366304" target="_blank" rel="noopener">知乎回答</a>很有趣，也给了很深的感触，自己对于操作系统、数据库、HTTP用的都非常熟练，但是也从来没有去想怎么怎么实现或者背后深层次的原理</p></blockquote><p>以前刚开始学习的时候自己也看过很多关于操作系统、数据库的书，但是就像走马观花一样，很多书就是过了一遍脑子，然后就没有了，在2017年下半年自己也开始尝试把自己以前只是了解只是会用的东西拿出来咀嚼咀嚼，并在这个过程中写出一些自己觉得还是有点干货的博文。而且我也比较享受把复杂的问题简单话的过程。</p><p>操作系统、计算机、数据库、编程原理在我看来都是非常复杂庞大的东西，在2018年，希望能够将上面这些复杂的东西搞的简单一点，当然在这个同时我也会将我将问题简单话的过程中分享出来。</p><p>当然这些东西都太过宽泛，比如操作系统。很多人刚开始学习编程了解了操作系统概念，还有听到很多牛人都说自己写了一个操作系统都会在心里暗暗的立下一个Flag，自己也要做一个操作系统，但是很多人的热情慢慢的被无数的复杂概念慢慢磨掉，最后能坚持到做出一个完整的系统没有几个。作为一个普通人，我们没法一口吃下一个很大的“饼”，当然你如果叫爱因斯坦给你造一个原子弹，它也只能拍拍手跟你说我只能证明它能造出来。所以有效的解决问题的方法是将问题分解，或许最终我们最终造不出操作系统，但是我们对操作系统也能更深一步了解。</p><h2 id="操作系统"><a href="#操作系统" class="headerlink" title="操作系统"></a>操作系统</h2><p>首先谈谈操作系统，其实我们对操作系统的理解大致都停留在开关机。我们要分解的问题就是我们对这个“黑盒子”的疑问</p><ul><li>操作系统如何管理硬件，比如说鼠标移动点击、键盘输入</li><li>操作系统如何让多用户使用</li><li>操作系统如何在多用户的情况下区分每个用户</li><li>操作系统如何分配内存，如何限制用户的内存</li><li>操作系统如何管理文件，如何避免文件被同时写入</li><li>…..</li></ul><p>当然我这里只是列举了一些直观的问题，在解答这些问题的时候，我们会逐渐衍生出各种问题，最终我希望所有的问题都能被简单话</p><h2 id="数据库"><a href="#数据库" class="headerlink" title="数据库"></a>数据库</h2><p>数据库我算使用的比较多了，但是对数据库背后的机制也是懵懵懂懂，只知道他能保证这些功能，比如事务的原子性、一致性、分离性、持久性。</p><p>我觉得了解数据库就是要了解它是如何实现这些特性的，当然这个问题也很宽泛，我们可以尝试分解成</p><ul><li>数据库如何将输入复杂的查询分解成为数据库的操作</li><li>数据库如何避免死锁</li><li>数据对于插入删除异常如何解决</li><li>…..</li></ul><p>其实在我看来数据库可以分解成两大块，一个是编程语音解析（SQL），一个是高效的存贮（Database）</p><p>所以了解数据库也可以尝试了解编译原理，了解如何实现解析SQL，所以接下来我就不提编译原理了</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>其实想想自己的这种把复杂问题简单话就是华罗庚先生曾说的一句话：“把书由薄变厚，再把书由厚变薄”，首先我们得把这些看起来很简单的问题复杂话，最后慢慢的把这个复杂的问题简单话，这样你才能真正的把这个问题给搞懂。</p><p>对于我来说，从小或许受应试教育的影响，自己只学会了复杂问题给解决，但是不懂得解决后再把复杂问题简单话，所以看起来我懂很多复杂问题，但是其实换一个场景，换一个问题就一知半解了。所以自己慢慢的开始改变自己的方法，不满足于解决问题，而着重于简化问题，希望自己的一点经验也能给后来人一点启发。</p>]]></content>
      
      
      <categories>
          
          <category> 随想 </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>红黑树实现原理</title>
      <link href="2018/01/05/algorithm/%E7%BA%A2%E9%BB%91%E6%A0%91%E5%AE%9E%E7%8E%B0%E5%8E%9F%E7%90%86/"/>
      <url>2018/01/05/algorithm/%E7%BA%A2%E9%BB%91%E6%A0%91%E5%AE%9E%E7%8E%B0%E5%8E%9F%E7%90%86/</url>
      
        <content type="html"><![CDATA[<blockquote><p>本篇博客主要基于这篇<a href="/">博客</a>的扩展，建议阅读前先阅读这篇博文，这篇博文详细介绍红黑树的实现原理，完整代码在<a href="https://github.com/mrzhangboss/trees" target="_blank" rel="noopener">github</a>的<code>rbtree.go</code>文件中</p></blockquote><p><a href="/">浅谈”树”这种数据结构</a><br><a href="https://github.com/mrzhangboss/trees" title="github" target="_blank" rel="noopener">Github</a><br><a href="http://www.cs.usfca.edu/~galles/visualization/RedBlack.html" target="_blank" rel="noopener">可视化页面</a></p><h2 id="引言"><a href="#引言" class="headerlink" title="引言"></a>引言</h2>]]></content>
      
      
      <categories>
          
          <category> 算法 </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>二叉搜索树实现原理</title>
      <link href="2018/01/04/algorithm/%E4%BA%8C%E5%8F%89%E6%90%9C%E7%B4%A2%E6%A0%91%E5%AE%9E%E7%8E%B0%E5%8E%9F%E7%90%86/"/>
      <url>2018/01/04/algorithm/%E4%BA%8C%E5%8F%89%E6%90%9C%E7%B4%A2%E6%A0%91%E5%AE%9E%E7%8E%B0%E5%8E%9F%E7%90%86/</url>
      
        <content type="html"><![CDATA[<blockquote><p>本篇博客主要基于这篇<a href>博客</a>的扩展，建议阅读前先阅读这篇<a href="/2017/12/27/浅谈树这种数据结构/">博文</a>，这篇博文详细介绍二叉搜索树的实现原理，完整代码在<a href="https://github.com/mrzhangboss/trees" target="_blank" rel="noopener">github</a>的<code>binary.go</code>文件中</p></blockquote><p><a href="/2017/12/27/浅谈树这种数据结构/">浅谈”树”这种数据结构</a><br><a href="https://github.com/mrzhangboss/trees" title="github" target="_blank" rel="noopener">Github</a><br><a href="http://www.cs.usfca.edu/~galles/visualization/BST.html" target="_blank" rel="noopener">可视化页面</a></p><h2 id="引言"><a href="#引言" class="headerlink" title="引言"></a>引言</h2><p>之所以使用Go来实现，个人还是比较喜欢Go的，作为一个基础数据结构，Go用来实现这个速度比Java、C++都快，而且相比Java也能节省内存，而且我也不喜欢换使用冒号。</p><h2 id="二叉搜索树的基本功能"><a href="#二叉搜索树的基本功能" class="headerlink" title="二叉搜索树的基本功能"></a>二叉搜索树的基本功能</h2><p>作为一个树结构，主要必须实现三个功能，<strong>查增删</strong>（当然有改，不过改是删和增的结合所以就不算在里面），当然二叉树还能帮我实现一些额外功能，比如寻找最大值最小值等等，实现一颗二叉搜索树必须要保证在“增”和“删”的时候保持树结构不变，也就是保证还是一颗二叉搜索树</p><p>什么二叉树是二叉搜索树呢，二叉搜索树只需要保证一点</p><ul><li>任何一个节点，它的左子树（不为空）所有值小于这个节点值，它的右子树（不为空）所有值大于这个节点值</li></ul><p>所有我们在实现增删的时候必须要牢记这点，而且我们也可以知道在你开始对这颗二叉树进行增删的时候，它一定已经满足了上面这个条件</p><h3 id="查"><a href="#查" class="headerlink" title="查"></a>查</h3><p><img src="/images/bst-1.png" alt="标准的二叉树"></p><p>如何在上面这颗标准的二叉树里面查询到我们想要的值呢？我们知道一颗二叉树只需要知道根节点（上面的10）就能通过遍历得到所以的节点的值，所以我们在实现中声明一个二叉搜索树的时候只保存一个<code>root</code>节点，通过这个节点，我们就可以实现所以的增删查改功能了</p><p>为了找到每个值a，我们依次要在每个节点上对值进行比较，因为每个节点的值代表已经将这堆数据分成两个部分，一堆比这个值大在右边，一堆比这个值小在左边，这样只要最终能找到一个那个值的最小区间的对应值（找到）或者找到一个为空的叶节点（代表没有找到），</p><p>通过最简单的查找，我们可以反推出一个理论，假如我们需要插入一个值，我们只要先尝试去寻找到那个值，假如找到了，因为已经存贮了就不插入，假如没找到就会找到一个为空的叶子节点，那样只要在父节点将这个为空的叶子节点替换成这个值为a的节点，我们就完成了插入</p><p>所以查找和插入是相对的，知道怎么查找就知道怎么插入，接下来我们详细介绍如何删除，这个非常关键，因为它是后面要介绍的高级树实现高效的删除的关键</p><h3 id="删除"><a href="#删除" class="headerlink" title="删除"></a>删除</h3><p>我们还是以上面那种图举例子，删除我们最先想到的是删除12,17那些节点，直接删除掉对二叉树没有什么影响，但是如果我们要删除那些带有节点的呢？我们先简化问题，先看一个只有两个节点的树</p><p><img src="/images/bst-2.png" alt="双节点树"></p><p>我们要想删除10，有两种可能</p><p><img src="/images/bst-3.png" alt="左移"></p><p><img src="/images/bst-4.png" alt="右移"></p><p>一种是将左边值移过来，一种是将右边值移过来，这两种都不会破坏二叉树的平衡，现在我们假设我们倾向将左边的值移过来，我们再回到上面那棵标准的二叉树，如果我们要删除掉10节点，我们要把那个值移过来，我们直接在<a href="http://www.cs.usfca.edu/~galles/visualization/BST.html" target="_blank" rel="noopener">可视化界面</a>进行这个操作</p><p><img src="/images/bst-5.png" alt="删除10后的标准二叉树"></p><p>我们发现，我们只需要将7和放到10的位置上就完成了一次删除的操作，而7与10的关系是，7是10左边的那堆值最大的，用一个通俗的话来说，<em>完成改朝换代的关键就是要找一个能镇的住手下的人</em>，由于“10”挂了，在左边只有“7”能镇住左边，所以”7”升官直接跑到“10”的位置了</p><p>根据这个原则我们将情况分成三种</p><ol><li>左右子树都为空，直接删掉</li><li>左边子树或者右边子树有一个为空，将不为空的子树提上来当做删除的节点</li><li>左右子树不为空，将左边最大的值提上来替换删除的节点</li></ol><p>对于第三种情况，为了编程方便，我们考虑一种情况，如果左子树的右节点有没有值（比如说上面的5节点），如果它右节点有值，那么左边的最大值肯定出现在，左子树的右节点上（就像上面左子树出现在5的右节点的7节点上面）。然后我们继续判断“7”节点有没有右子树，这样循环下去，我们总能找到一个节点没有右节点，这样我们就找到左子树的最大值了。</p><p>在上面这种情况下，我们将这情况分成两种</p><ul><li>如果左子树的右节点有值（循环下去找到一个节点右节点没有值）</li><li>如果左子树的右节点没值（那么第一个左子树就是最大值）</li></ul><p>具体的<a href="https://github.com/mrzhangboss/trees" title="github" target="_blank" rel="noopener">代码</a>在<code>binary.go</code>的<code>Delete</code>(寻找节点）方法和<code>delete</code>（删除节点）方法中</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>总的来说，二叉搜索树的插入是最容易理解的，但是删除的话要考虑不同四种的情况所以还是稍微需要点时间理解一下的，小小的一个二叉搜索树也花了近200行代码实现，但是相比后面的AVL树、红黑树的近千行代码来说，二叉搜索树还是比较简单的，而且后面实现的高级树结构基本上都是依赖二叉搜索树的实现（必须要按照二叉树的增删满足二叉搜索树的原则），所以我们对二叉搜索树的增删必须理解透彻。</p><p>引用：</p><p><a href="https://en.wikipedia.org/wiki/Binary_search_tree" target="_blank" rel="noopener">https://en.wikipedia.org/wiki/Binary_search_tree</a></p>]]></content>
      
      
      <categories>
          
          <category> 算法 </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>AVL树实现原理</title>
      <link href="2018/01/04/algorithm/AVL%E6%A0%91%E5%AE%9E%E7%8E%B0%E5%8E%9F%E7%90%86/"/>
      <url>2018/01/04/algorithm/AVL%E6%A0%91%E5%AE%9E%E7%8E%B0%E5%8E%9F%E7%90%86/</url>
      
        <content type="html"><![CDATA[<blockquote><p>本篇博客主要基于这篇<a href="/2017/12/27/浅谈树这种数据结构/">博客</a>的扩展，建议阅读前先阅读这篇博文，这篇博文详细介绍AVL树的实现原理，完整代码在<a href="https://github.com/mrzhangboss/trees" target="_blank" rel="noopener">github</a>的<code>avl.go</code>文件中</p></blockquote><p><a href="/2017/12/27/浅谈树这种数据结构/">浅谈”树”这种数据结构</a><br><a href="https://github.com/mrzhangboss/trees" title="github" target="_blank" rel="noopener">Github</a><br><a href="http://www.cs.usfca.edu/~galles/visualization/AVLtree.html" target="_blank" rel="noopener">可视化页面</a></p><h2 id="引言"><a href="#引言" class="headerlink" title="引言"></a>引言</h2><p>AVL树是在对二叉搜索树的一种优化，通过构造一棵高度平衡的二叉搜索树从而实现提高空间利用率，所以在了解如何实现之前，必须了解如何构造一棵二叉搜索树，你可以阅读我的这篇<a href="/2018/01/04/二叉搜索树实现原理/">博客</a>了解如何构建一棵二叉搜索树，虽然我是用Go来实现的，但是不必了解太多Go方面的知识，我在博客中尽量使用图片的方式来介绍实现原理</p><p><a href="/2018/01/04/二叉搜索树实现原理/">二叉搜索树实现原理</a></p><p>由于AVL树本质上也是一棵二叉搜索树，查找并不会改变树的性质，所以AVL的查找也是同二叉搜索树的查询相同，所以这里就重点介绍如何实现“增”和“删”</p><h3 id="树的字段"><a href="#树的字段" class="headerlink" title="树的字段"></a>树的字段</h3><p>在开始介绍如何实现“增”、“删”之前，我想提一下AVL树的存贮字段，在wiki上面的AVL和很多资料上面，AVL的树结构分别由下面四个组成： Val、 Left*、Right*、Parent*。</p><p>Val代表树存贮的值，Left、Right、Parent代表三个指针，分别指向左子树，右子树，父亲，而在我的实现中，我去掉了父亲指针，对于讲解来说，使用一个父指针能很好的解决从子遍历到父亲的经过，但在实际的应用中，我们完全可以使用一个列表存贮从父亲到儿子之间的值，这样的话，能实现一样的功能，而且能节省存贮空间（每个子树都少了一个父指针），但是相应的程序也变得比较复杂起来。</p><p>虽然我在讲解的过程中会直接获取节点的父节点，但是在实际的代码实现中，是通过获取从列表中获取该节点的父节点。</p><h2 id="AVL树的“增”"><a href="#AVL树的“增”" class="headerlink" title="AVL树的“增”"></a>AVL树的“增”</h2><h2 id="引用"><a href="#引用" class="headerlink" title="引用"></a>引用</h2><p><a href="https://en.wikipedia.org/wiki/AVL_tree" target="_blank" rel="noopener">https://en.wikipedia.org/wiki/AVL_tree</a></p>]]></content>
      
      
      <categories>
          
          <category> 算法 </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>浅谈&quot;树&quot;这种数据结构</title>
      <link href="2017/12/27/algorithm/%E6%B5%85%E8%B0%88%E6%A0%91%E8%BF%99%E7%A7%8D%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/"/>
      <url>2017/12/27/algorithm/%E6%B5%85%E8%B0%88%E6%A0%91%E8%BF%99%E7%A7%8D%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/</url>
      
        <content type="html"><![CDATA[<blockquote><p>一直以来我对树这种数据结构就比较头疼，随便找一个红黑树的博客，大部分都是在谈怎么旋转怎么插入怎么删除，将算法讲的头头是道，但是就算你看懂了也不懂为什么要这样做，所以我们这篇博文就从可视化的角度，慢慢的介绍这些树的来世今生。</p></blockquote><h2 id="引言"><a href="#引言" class="headerlink" title="引言"></a>引言</h2><p>首先给大家推荐一个神奇的网站，我们这篇博文很大程度上都是依托这个网站从可视化的角度分析各种树</p><p>网站的网址是<a href="http://www.cs.usfca.edu/~galles/visualization/Algorithms.html" target="_blank" rel="noopener">http://www.cs.usfca.edu/~galles/visualization/Algorithms.html</a></p><p>接下来我们从易到难分别谈谈下面这些树结构</p><ul><li>二叉搜索树</li><li>AVL树</li><li>红黑树</li><li>Splay树</li><li>Trie树</li><li>B树</li><li>B+树</li></ul><p>你可以点击下面高亮的标题尝试这种树结构的可视化操作，当然我在谈的过程中默认你已经打开可视化界面网站。</p><p>在开始介绍各种树结构之前，我们先谈谈为什么我们需要树这种结构。</p><p>假设我们一开始都有一个“柜子”存放东西，当我们东西很少的时候，我找东西的时候只要在柜子里面翻一翻就能找到，当我东西越来越多，在柜子里面找东西消耗的时间越来越多，这时候我就把柜子分类，让每一个柜子的东西都是固定的，这样我们查找时候不会在一个柜子里面寻找很久，这样无论你有多少东西进来，我查找的时间基本不会太长。</p><p>所以树这种数据结构就是为了解决当数据量越来越大的时候相比与线性查找时间基本不会太长。对树查找和线性的区别我们可以看一下这个可视化界面</p><p><a href="http://www.cs.usfca.edu/~galles/visualization/Search.html" target="_blank" rel="noopener">线性搜索与二分查找</a></p><p>通过输入查找344，我们发现线性搜索花了15步才搜索到，而二分查找只需要2步，而且随着数据越来越大，线性搜索花的步数直线上升，而二分查找只会维持用Log(N)的步伐增长。</p><p>树的结构基本上都是基于二分查找，但是在维护这个查找树的方法上各种树有各种优化方法，接下来我们就从可视化角度来依次介绍各种树的原理和优缺点。</p><h2 id="二叉搜索树"><a href="#二叉搜索树" class="headerlink" title="二叉搜索树"></a><a href="http://www.cs.usfca.edu/~galles/visualization/BST.html" target="_blank" rel="noopener">二叉搜索树</a></h2><p>这个树可以称作所以后面变种树的鼻祖，基本上所以后面的树都是基于这个树的不足之处进行改良达到其各自的目的。</p><p>我们现在尝试构造一棵二叉树，我们依次插入<code>3,2,4</code>。然后一棵标准的二叉树就出现在我们面前，在我们插入的过程中，我们可以清晰的看到，第一个插入的变成了母节点，而且插入程序实现的非常简单，首先跟母节点进行比较，如果比母节点大，从母节点右边下滑，然后依次找到最后一个为空的节点把自己插入进去。</p><p> <img src="/images/bst-7.png" alt="单调递增二叉搜索树"></p><p>我们接下来尝试一下如果输入一个连续递增的数组进去，比如<code>1， 2， 3， 4， 5</code>，我们发现这个二叉搜索树会逐渐把所以的值放到右节点上。这时候我们我们尝试搜索一下<code>5</code>，我们发送这个二叉搜索树需要5次查询才能得到最终结果，这个基本上等同与线性搜索。</p><p>这里我们就发现二叉搜索树一个致命缺点，它容易造成一种空间的浪费，虽然这个树看起来很“高”，有5层，但是利用率极低，每层只有一个树，这样不但查找的时候效率不高，而且插入的时候效率也不高（很可能需要“爬”很多层）。</p><p>二叉树搜索树实现起来非常简单，只要用一个<code>while</code>循环就能完成插入查找等功能，但是由于它结构控制太低，它对数据要求要很高，它时候依次插入那些无序数字，而对那些有序数据来说，二叉搜索树等同与一个数组。</p><p>所以我们接下来介绍下面改良树，看看其他树是如何提高的空间利用率的，由于这篇主要是简单的谈一下这些树的优缺点，如果你想了解如何实现这样可以看一下我写的这篇<a href="/2018/01/04/二叉搜索树实现原理/" title="二叉搜索树Go实现">博文</a>，详细介绍了如何使用Go语言实现一个高效的二叉搜索树，如果你想彻底了解下面高级的树的实现，强烈建议一下了解一下二叉搜索树实现的方法，因为后面所以的树实现的前提<strong>必须满足是一个二叉搜索树</strong>，这点非常重要。</p><p><a href="/2018/01/04/二叉搜索树实现原理/" title="二叉搜索树Go实现">二叉搜索树Go实现</a></p><h2 id="AVL树"><a href="#AVL树" class="headerlink" title="AVL树"></a><a href="http://www.cs.usfca.edu/~galles/visualization/AVLtree.html" target="_blank" rel="noopener">AVL树</a></h2><p>1962发明的AVL树现在还生龙活虎的活跃在Windows进程地址空间管理，虽然作为最早的平衡二叉树现在有点干不过后面的年轻的小伙，但是作为前辈，后面小伙很多地方都是借鉴前辈的，所以我们有必要隆重介绍一下这个老前辈。</p><p>首先要说明一下什么是平衡二叉树，前面我们也看到了，二叉搜索树有个弊端，它有可能子树高度不一样，比如说<code>1,2,3,4,5</code>那个母节点<code>1</code>的左子树高度为0，右字数高度为5，平衡二叉树就没有这个问题，它规定所以节点子树高度差不能超过1，这样规定的话我们的树空间利用率就能基本上达到100%了。</p><p>AVL树就是一颗高度平衡二叉树，它的要求也非常简单，就两个</p><ul><li>必须是二叉搜索树</li><li>必须是平衡二叉树</li></ul><p>成为一个二叉查找树非常容易，但是为了必须满足平衡二叉树条件，这里我们给每颗树定义一个高度，默认空为0，这样就可以计算左右子树的高度差是否在-1和1之间判断是否是平衡树。</p><p>了解了这个之后，我们回到web页面，我们输入<code>1,2,3</code>看看AVL树如何避免像二叉搜索树一样过度生长</p><p><img src="/images/avl-1.png" alt="插入前"></p><p><img src="/images/avl-2.png" alt="插入后"></p><p>我们从这两幅图可以看到，插入前AVL树高度为3，经过单旋转后，将2旋转到1的位置，然后让树又满足平衡二叉树的条件和二叉搜索树的条件。让我们思考一下旋转的意义。</p><h3 id="旋转的意义"><a href="#旋转的意义" class="headerlink" title="旋转的意义"></a>旋转的意义</h3><p><strong>第一旋转并没有改变二叉搜索树的特性，我们将二叉树看做一个一个小区间组成的区域，旋转将区间整体移动了，所以我们保持住了区间的稳定，而且我们把旋转的支点看做中心，我们发现通过一次旋转后，支点左右之间的数量发生的变化，其中一方增加一个，其中一个减少一方，通过这种方式，把高度差由<code>2</code>转成<code>0</code></strong></p><p>所以我们在插入和删除的时候造成的高度不平衡可以通过正确的旋转达到一个高度差的减少。而且我们可以很轻松的推断出在插入和删除的时候只需要通过有限的旋转就能实现树的平衡，但是我们也会发现这个平衡是非常严格的，每次插入删除的时候，删除或者增加的那条“路径”（从根节点到修改的节点）的各个节点的高度都会发生变化，所以我们需要检查和更新整条路径上高度差（也称平衡因子）是否满足平衡条件。</p><p>直接纪录每个树的高度并且约束高度差的方法能够很好的规避像上面二叉搜索树造成的空间利用率低问题，但是对于增添和删除操作频繁的数据来说，AVL树并不是一个很好的解决方法，因为每次删除和增加都必须检测路径上的高度差，虽然旋转比较高效，但是每次检测和修改高度会让这个算法花费太多时间在这个严格的条件上面。</p><p>总的来说，AVL树是比较好理解的一种平衡树算法，通过这个树的各种旋转操作，我们能很清晰的发现二叉树是一种很神奇的数据结构，一方面它可能是混乱的（分支混乱），一方面它又是整齐的，通过“扭动”它的身躯，将数据平均的分摊在根节点上面。我非常推荐你在这个<a href="http://www.cs.usfca.edu/~galles/visualization/AVLtree.html" target="_blank" rel="noopener">可视化界面</a>上通过一步一步将单独递增或者递减的数据插入AVL树中，你会发现AVL通过一次一次的“扭动”，将数据向根节点另一个方向传输过去。</p><p>这里我也不详细介绍AVL树实现的具体原理，如果你想了解更底层的实现，可以看我下面这篇<a href="/2018/01/04/AVL树实现原理/" title="AVL树的实现原理">博客</a></p><p><a href="/2018/01/04/AVL树实现原理/" title="AVL树的实现原理">AVL树的实现原理</a></p><h2 id="红黑树"><a href="#红黑树" class="headerlink" title="红黑树"></a><a href="http://www.cs.usfca.edu/~galles/visualization/RedBlack.html" target="_blank" rel="noopener">红黑树</a></h2><p>在AVL树发明10年后鲁道夫 贝尔发明了红黑树，直到现在红黑树还依旧在C++的STL、Java的HashMap中发挥着重要作用，只所以要先介绍AVL树再介绍红黑树，因为从我的理解上来看，其实红黑树和AVL树原理都是类似的，只不过红黑树修改了AVL树的缺点并且将AVL的平衡原理进行了抽象话。</p><p>从数据结构上来看，红黑树相比于AVL树来说，只是将存贮高度的值换成了存贮颜色的值，从空间的角度上看，红黑树用一个二进制bool值（1Bit）换掉一个存贮int值（4Bit），节省了3Bit的空间，从增删所需要的操做来看，AVL树旋转可能需要lnN 步操作，而红黑树增不超过2次旋转，删不超高3次旋转，在增删效率上远远优于AVL树。所以我们有一个疑问为什么红黑树能用更少的空间实现更高的效率呢？</p><p>回答这个问题之前，我们再捋捋AVL它的实现原理，前面介绍了AVL实现平衡的原理的就是使的左右子树高度差维持在[-1,1]之间，高度差这个东西，你想想高度这个东西其实它就是母节点到叶子节点的路径，所以高度差我们翻译一下就是左右子树路径差，现在我们把平衡后的AVL树条件更加严格一点，我们规定左右子树路径差为0，也就是左右子树高度一样。</p><p><img src="/images/bst-2.png" alt="满二叉树"></p><p>在上面那个严格的条件下，假如我们要新增一个到树末端，假如它加在像上面这样满二叉树上面，那么左右子树高度就立刻不相同了，但是对于一个AVL树来说，在这颗树上末端插一颗树并不会影响树的平衡，所以我们要想一个办法让插入这个值“不算高度”，这时候我们一想，如果我们把树节点分成两种颜色，一种红色一种黑色，红色不算高度，黑色才算高度，那样虽然我现在条件很苛刻“左右子树高度必须一样”，但是如果插入的是红色，那么就能很轻松的实现上面的条件。</p><p>好了我们通过上面的操作成功的将高度变成了黑高（黑色节点的高度），而且满足了一种更加严格的条件“左右子树黑高必须相同），假如说前面AVL树的高度差为[-1,1]之间都能实现高空间利用率的，那么在这种更加严格的条件下也能满足，但是现在又出现一个问题，我们看下面这棵树</p><p><img src="/images/rbt-1.png" alt="不平衡树"></p><p>当我们插入节点3的时候，由于插入一颗红色你把红色节点不算高度，所以在前面的约束条件下，插入3满足条件，但是实际上这棵树并不是一个平衡AVL树，为了避免这种情况的发生我们必须要做一个约束，我们要约束红色节点不能太多，所以我们就约束红色节点后面只能是黑色节点，这样路径上最少都会有一半都是黑色（红色黑色相同）。</p><p>我们现在得到两个约束，现在我们来说思考一下这两个条件的约束到底给我们带来了什么</p><ul><li>黑高相同</li><li>红后面必须为黑</li></ul><p>首先对于黑色节点来说，这棵树是高度平衡的（假设它只能看到黑色节点），高度差为0，但是对于红色节点来说，这棵树有可能是不平衡的（假设有N个黑节点，红色高度差可能有N-1），所以从整体上来看，红黑树牺牲了一定的平衡性换来了插入删除的高效性</p><p>这个高效性表现在哪呢？第一它不在需要更新所有的高度差，它只需要对受影响的路径上的有限节点进行染色就能实现再次的平衡，其次由于它不在需要高度的平衡，所以它能容忍树上的红色的不平衡现象，来减少旋转变化的次数。</p><p>前面我们分析了AVL树旋转的意义那么对于红黑树来说，旋转的意义是什么呢？</p><h3 id="红黑树的旋转意义"><a href="#红黑树的旋转意义" class="headerlink" title="红黑树的旋转意义"></a>红黑树的旋转意义</h3><p>前面我们知道了在AVL树中每一棵树都是独立的个体，所以每次旋转都是一次运输，将树按照节点扭动，但是对于红黑树来说，它要处理的情况是黑树高度的不同，当需要搬到的也是黑树，它的目的就是将黑树的高度平衡，对于红色的树来说，它更像一种胶水，它能保证当数据不平衡的时候（所以的树不可能全部变成黑树）还能使黑树保持一种平衡，从而使一棵树变得相对平衡。</p><p>所以红黑树的旋转的意义同AVL树是类似的，只不过红黑树在旋转的时候要更加注意，它不但要考虑我们要如何在不平衡的节点上平衡黑高，而且要考虑我们平衡了这个节点的黑高有没有对其父亲的黑高造成影响。</p><p>对于红黑树来说，由于颜色是可变的，所以着色（改变颜色）也是一种手段，而对于AVL树来说高度是固定的，只能通过旋转来改变，所以单独讲红黑树的旋转是没有意义的，或许有的时候只需要通过旋转就能实现黑高的统一，或许通过简单着色也能实现黑高的统一，但是只使用一种手段是不能高效的完成所以的条件。所以在我看来<em>着色和旋转是相辅相成的</em></p><p>也正因为我们多了一个着色的手段，所以红黑树比AVL树在某些方面更加高效，随着而来也是更加复杂难懂，但是总体来说AVL和红黑还是类似的，在这里我就不深谈如何实现红黑树，如果你想了解如何实现红黑树，可以看我这篇<a href="/2018/01/05/红黑树实现原理/">博客</a></p><p><a href="/2018/01/05/红黑树实现原理/">红黑树的实现原理</a></p><h4 id="剩余的四种树的简析会待代码实现后陆续更新"><a href="#剩余的四种树的简析会待代码实现后陆续更新" class="headerlink" title="剩余的四种树的简析会待代码实现后陆续更新"></a>剩余的四种树的简析会待代码实现后陆续更新</h4><h2 id="引用"><a href="#引用" class="headerlink" title="引用"></a>引用</h2><p><a href="https://en.wikipedia.org/wiki/Binary_search_tree" target="_blank" rel="noopener">https://en.wikipedia.org/wiki/Binary_search_tree</a></p><p><a href="https://en.wikipedia.org/wiki/AVL_tree" target="_blank" rel="noopener">https://en.wikipedia.org/wiki/AVL_tree</a></p><p><a href="https://www.cnblogs.com/vamei/archive/2013/03/21/2964092.html" target="_blank" rel="noopener">https://www.cnblogs.com/vamei/archive/2013/03/21/2964092.html</a></p>]]></content>
      
      
      <categories>
          
          <category> 算法 </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>几个有趣的动态规划</title>
      <link href="2017/12/26/algorithm/%E5%87%A0%E4%B8%AA%E6%9C%89%E8%B6%A3%E7%9A%84%E5%8A%A8%E6%80%81%E8%A7%84%E5%88%92/"/>
      <url>2017/12/26/algorithm/%E5%87%A0%E4%B8%AA%E6%9C%89%E8%B6%A3%E7%9A%84%E5%8A%A8%E6%80%81%E8%A7%84%E5%88%92/</url>
      
        <content type="html"><![CDATA[<blockquote><p>这篇博文是<a href="/2017/12/25/从问题理解动态规划/">从问题理解动态规划</a>的练习篇，通过几个动态规划的问题剖析进一步理解动态规划</p></blockquote><h2 id="找零钱练习题"><a href="#找零钱练习题" class="headerlink" title="找零钱练习题"></a>找零钱练习题</h2><ul><li>给定一个零钱数组比如[1, 2, 5],每个值代表一个面额的纸币，给定一个总数(aim)，求换钱有多少种方法（每种面额纸币不限量）</li></ul><blockquote><p>这个问题非常经典，所以我就从最先容易想到的算法出发慢慢推导出动态规划</p></blockquote><h3 id="正向暴力搜索"><a href="#正向暴力搜索" class="headerlink" title="正向暴力搜索"></a>正向暴力搜索</h3><p>面前一大堆钱，分成三堆，我们必须要从这三堆中抽取出来所以可能的方案，看看能够凑到总数。</p><p>我们第一个想到的就是正向暴力搜索，先从第一堆取出0张、1张、2张….，然后递归下去，让取出0张、1张、2张凑剩下的总数，等到取到最后一个钱堆或者总数正好相同的时候递归停止。</p><p>我们可以很轻松的写出下面的代码（Java）</p><pre><code>int violenceSearch(int[] level, int index, int aim) {        if (aim &lt;= 0) {            return aim == 0 ? 1 : 0;        }        if (index &gt;= level.length) {            return 0;        }        int sum = 0;        for (int i = 0; i &lt;= aim / level[index]; i++) {            sum = sum + violenceSearch(level, index + 1, aim - i * level[index]);        }        return sum;    }</code></pre><p>这个函数接受三个参数，第一个是钱堆的面额数组，第二个是当前是拿第几个钱堆的序号，第三个是剩余要凑的总数</p><p>这个算法的核心就是<code>sum = sum + violenceSearch(level, index + 1, aim - i * level[index]);</code>，我们分别从钱堆里面取出想0张、1张…，然后计算剩下的总数和剩下的堆数方法总数和。</p><p>这个算法也可以优化成记忆搜索，总共有多少种方法拼钱，主要与<code>index</code>和<code>aim</code>有关，我们只要用<code>map</code>记录一下这个值就可以优化成为记忆搜索。</p><h3 id="反向暴力搜索"><a href="#反向暴力搜索" class="headerlink" title="反向暴力搜索"></a>反向暴力搜索</h3><p>反向的话比较难想到，但是正向暴力搜索没有办法分解成子函数，也就无法实现动态规划，所以我们必须要反向思考</p><p>首先我们假设纸币面额为1， 2， 5， 我们要凑10块钱，我们假设已经得到了所以的次数F(x, y)(x为由多少种纸币组成，y为凑多少钱），所以我们得到这个我们想要的结果F(3, 10) （也就是由3种面额组成10块）</p><p>假设我们得到了F(3, 10)所以可能的组成结果结合如下面10种</p><ul><li>10张1块</li><li>8张1块、1张2块</li><li>6张1块、2张2块</li><li>5张1块、1张5块</li><li>4张1块、3张2块</li><li>3张1块、1张2块、1张5块</li><li>2张1块、4张2块</li><li>1张1块、2张2块、1张5块</li><li>2张5块</li><li>5张2块</li></ul><p>现在我们把这10种可能按照5块的张数分成3份</p><ul><li>0张5块</li><li><ul><li>10张1块</li></ul></li><li><ul><li>8张1块、1张2块</li></ul></li><li><ul><li>6张1块、2张2块</li></ul></li><li><ul><li>4张1块、3张2块</li></ul></li><li><ul><li>2张1块、4张2块</li></ul></li><li><ul><li>5张2块</li></ul></li><li>1张5块</li><li><ul><li>5张1块、1张5块</li></ul></li><li><ul><li>3张1块、1张2块、1张5块</li></ul></li><li><ul><li>1张1块、2张2块、1张5块</li></ul></li><li>2张5块</li><li><ul><li>2张5块</li></ul></li></ul><p>分别为0张5块（6种），1张5块（3种），2张5块（1种），也就是我们成功将F(3, 10)分解成 F(2, 10), F(2, 5), F(2, 0)</p><p>通过这种方式我们成功构造出子函数，我们很容易就能写出递归函数</p><pre><code>int lowBackVS(int[] level, int index, int aim) {    if (index == 0)        return aim % level[index] == 0 ? 1 : 0;    if (aim &lt; level[index]) return lowBackVS(level, index - 1, aim);    else {        int count = 0;        for (int i = 0; i * level[index] &lt;= aim; i++) {            count += lowBackVS(level, index - 1, aim - i * level[index]);        }        return count;    }}</code></pre><p>这里我们要看到第二个<code>if</code>，我们判断剩下的余额是否够当前的一张，如果不够，那直接就是前n-1种纸币能够组成的次数。（也就是假如你剩下4块钱要用一张5块来组成，肯定不可能，直接返回前面的n-1种货币能够凑出4块的种数）</p><p>这种时间复杂度为O(n) = n X aim X aim， 假如aim比较大还是比较耗时间的，我们看看是否能够优化一下</p><h3 id="反向暴力搜索优化"><a href="#反向暴力搜索优化" class="headerlink" title="反向暴力搜索优化"></a>反向暴力搜索优化</h3><p>我们还是回到上面那个例子，我们这次按照能够减去一张5块的进行分类，这样我们就分成了两类</p><ul><li>无法抽掉一张5块</li><li><ul><li>10张1块</li></ul></li><li><ul><li>8张1块、1张2块</li></ul></li><li><ul><li>6张1块、2张2块</li></ul></li><li><ul><li>4张1块、3张2块</li></ul></li><li><ul><li>2张1块、4张2块</li></ul></li><li><ul><li>5张2块</li></ul></li><li>能够抽掉一张5块</li><li><ul><li>5张1块、1张5块</li></ul></li><li><ul><li>3张1块、1张2块、1张5块</li></ul></li><li><ul><li>1张1块、2张2块、1张5块</li></ul></li><li><ul><li>2张5块</li></ul></li></ul><p>一张是无法抽掉一张5块（6种），能够抽到一张5块（4）种<br>我们回到函数定义，这样我们成功将函数F(n, aim)分解成两个F(n-1, aim) 和F(n, aim - level[index])</p><p>我们成功的将子函数进行化简成两项</p><pre><code>int backVS(int[] level, int index, int aim) {    if (index == 0)        return aim % level[index] == 0 ? 1 : 0;    if (aim &gt;= level[index])        return backVS(level, index - 1, aim) + backVS(level, index, aim - level[index]);    else return backVS(level, index - 1, aim);}</code></pre><p>我们可以看到我们成功将时间复杂度降到了O(n) = n X aim，至此我们写出了比较完美的反向暴力搜索方法，当然我们也能够像正向暴力搜索优化成记忆搜索</p><h3 id="动态规划"><a href="#动态规划" class="headerlink" title="动态规划"></a>动态规划</h3><p>前面我一直没有怎么提记忆搜索法，因为这个方法基本上等同与动态规划，只不过动态规划使用数组存贮，记忆搜索法用字典存贮。</p><p>前面我们知道我们能通过分解函数将问题划分成前面的子问题，所以我们只需要构建一个二维数组，x，y分别代表由几种货币组成（index），组成的总数（aim），这样就能通过慢慢“填”写动态规划表，最后求出由N种货币组成钱数（aim）总数</p><pre><code>int dynamic(int[] level, int index, int aim) {    int n = level.length;    int[][] dp = new int[n][aim + 1];    for (int i = 0; i &lt; n; i++) dp[i][0] = 1;    for (int i = 1; i &lt;= aim; i++) dp[0][i] = i % level[0] == 0 ? 1 : 0;    for (int i = 1; i &lt; n; i++) {        for (int j = 1; j &lt;= aim; j++) {            if (j &lt; level[i]) dp[i][j] = dp[i - 1][j];            else dp[i][j] = dp[i - 1][j] + dp[i][j - level[i]];        }    }    return dp[n - 1][aim];}</code></pre><p>这里要注意的是初值的初始化，默认当钱数为0的时候值为1（比如说前面的2张5块的方法F(2, 0) = 1），当只由一种钱币组成时，只要总数能够整除第一种钱币面额就为1（全部由这种货币组成）</p><h3 id="动态规划的优化"><a href="#动态规划的优化" class="headerlink" title="动态规划的优化"></a>动态规划的优化</h3><p>当然这个优化可有可无，因为我们观察上面的函数，我们发现其实N层只与前面N-1层一个有关，这样的话，我们如果覆盖掉上一层的值对后面的计算结果也没有影响。所以我们就不使用多维数组，直接使用一维数组，节省了（N-1）* (aim + 1)的空间</p><pre><code>int advanceDynamic(int[] level, int index, int aim) {    int[] f = new int[aim + 1];    f[0] = 1;    int n = level.length;    for (int i = 0; i &lt; n; ++i)        for (int j = level[i]; j &lt;= aim; ++j)            f[j] += f[j - level[i]];    return f[aim];}</code></pre><p>所以我们成功的将代码改的更短，而且更省空间</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>一开始本来想多讲几道动态规划的问题，但是写着写着发现其实大部分都是大同小异，找到子函数，构建表存贮计算过程，其中最大的挑战就是找到分解子函数，所以我就不介绍其他的问题了，我就把这些问题留在下面，你可以试试挑战一下自己</p><ul><li><a href="https://zh.wikipedia.org/wiki/%E8%83%8C%E5%8C%85%E9%97%AE%E9%A2%98" target="_blank" rel="noopener">背包问题</a></li></ul><blockquote><p>一个背包有一定的承重cap，有N件物品，每件都有自己的价值，记录在数组v中，也都有自己的重量，记录在数组w中，每件物品只能选择要装入背包还是不装入背包，要求在不超过背包承重的前提下，选出物品的总价值最大。</p></blockquote><ul><li><p>最优编辑</p><blockquote><p>对于两个字符串A和B，我们需要进行插入、删除和修改操作将A串变为B串，定义c0，c1，c2分别为三种操作的代价，请设计一个高效算法，求出将A串变为B串所需要的最少代价。</p></blockquote></li><li><p>LIS</p></li></ul><blockquote><p>求出序列的最长上升子序列的长度</p></blockquote><ul><li>LCS</li></ul><blockquote><p>给定两个字符串A和B，返回两个字符串的最长公共子序列的长度。例如，A=”1A2C3D4B56”，B=”B1D23CA45B6A”，”123456”或者”12C4B6”都是最长公共子序列。</p></blockquote><ul><li>走迷宫问题</li></ul><blockquote><p>有一个矩阵map，它每个格子有一个权值。从左上角的格子开始每次只能向右或者向下走，最后到达右下角的位置，路径上所有的数字累加起来就是路径和，返回所有的路径中最小的路径和。</p></blockquote>]]></content>
      
      
      <categories>
          
          <category> 算法 </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>从问题理解动态规划</title>
      <link href="2017/12/25/algorithm/%E4%BB%8E%E9%97%AE%E9%A2%98%E7%90%86%E8%A7%A3%E5%8A%A8%E6%80%81%E8%A7%84%E5%88%92/"/>
      <url>2017/12/25/algorithm/%E4%BB%8E%E9%97%AE%E9%A2%98%E7%90%86%E8%A7%A3%E5%8A%A8%E6%80%81%E8%A7%84%E5%88%92/</url>
      
        <content type="html"><![CDATA[<blockquote><p>网上关于动态规划的资料，大部分直接给结论，所以一开始我一头雾水，搞不懂为什么要这么做，这篇博文就从实际问题出发，简单的剖析动态规划</p></blockquote><h2 id="引言"><a href="#引言" class="headerlink" title="引言"></a>引言</h2><p>现实生活中总能找到一些问题你没法给出具体答案，比如给你一堆1块、5块、10块的零钱，要你找出多少种能够拼出100块的方法。还有就是迷宫问题这种。这种问题都有一个特征，我们没有办法立刻给出答案，而且我们对这种问题的想到的第一种解决方法就是暴力搜索，把所以的可能方案列出来然后得到答案。这种暴力搜索最终能够解决问题，但是他们在计算的时候花了很多时间在相同的计算上面。为了节省时间，所以我们使用动态规划“优化”暴力搜索</p><h2 id="为什么要使用动态规划？"><a href="#为什么要使用动态规划？" class="headerlink" title="为什么要使用动态规划？"></a>为什么要使用动态规划？</h2><h3 id="Example"><a href="#Example" class="headerlink" title="Example"></a>Example</h3><p>我们先举一个简单的例子，大家都知道走楼梯问题，这也是教科书一个经典的递归程序</p><ul><li>有N阶台阶，一个人一次上两个或者一个，问有多少种走法?</li></ul><p>我们拿到这道题，我们首先会想这样想，从第一个台阶开始，我们使用递归让这个人走一步或者两步，这样每次分解成为两种可能，最后直达走到N阶台阶，或者迈过去了，然后计算这种所以迈到N阶台阶的可能路径。</p><p>这种正向思维很容易理解，但是最终它直接得到的是所以可能的路径，但是这道题我们需要求的是N阶的走法，所以我们从正向思维必须要反过来思考，假如我们从一个台阶出发有两种可能，那么我们到达第N个的台阶来看，也有两种可能，第一种是N-1（迈了一步到达），第二种是N-2（迈了两步到达），这样我们就很清楚了，假如我们要想得到到达N阶台阶的走法总数，那么我们只需要把到达N-1和到达N-2的次数加起来就可以了</p><p>这是一个很重要的思想<strong>把一个复杂的问题，分解成为其他的子问题</strong>，这也是我们完成动态规划的设计的核心思想</p><p>从更好的理解动态规划的优点和源头，我们就从这个简单的例子使用不同的算法来解释为什么要用动态规划</p><h3 id="暴力搜索法"><a href="#暴力搜索法" class="headerlink" title="暴力搜索法"></a>暴力搜索法</h3><p>我们成功的完成了问题的分解，为了完成计算，但是我们还得计算子问题的结果，上面得到一个很重要的公式<code>F(N) = F(N-1) + F(N-2)</code></p><p>我们可以很轻松的写出代码（Python）</p><pre><code>def f(n):  return f(n-1) + f(n-2) if n &gt; 2 else n</code></pre><p>我们只用一行代码就能将这个问题解决掉，而且效果看起来还不错，我们可以试不同的n都能获取正确的结果，但是n大于30之后，当我在我的电脑上运行起来非常慢，需要几秒钟才能返回结果，而且当n越大，消耗的时间也越多。</p><p> 这是为什么呢？我们现在来思考一下这个暴力算法有什么弊端</p><ul><li>暴力搜索的弊端</li></ul><p>我们现在假设N=10，那么我们现在就把F(10)转换成为F(9)与F(8)的和，那么F(9)又分成了F(8)和F(7)，而F(8)被分成了F(7)和F(6)</p><p>从这里可以看到，F(7)在第二次分解的时候计算了两次,而每次计算的结果都是一样的，所以我们相当于重复了一遍耗时的操作，知道这个问题，我们就必须改进了，我们可以用一个东西存贮计算结果，这样就不需要重复计算了</p><h3 id="记忆搜索法"><a href="#记忆搜索法" class="headerlink" title="记忆搜索法"></a>记忆搜索法</h3><p>我们修改我们算法，加一个参数<code>map</code></p><pre><code>def map_get(map, n):    v = map.get(n-1)    if not v:      v = f(n, map)      map[n] = v    return vdef f(n, map):    if n &lt; 3:      return n    if n in map:      return map[n]    return map_get(map, n - 1) + map_get(map, n - 2)</code></pre><p>我们添加一个辅助的字典存贮我们中间计算过程，虽然让我们的代码臃肿了不少，但是让我们代码速度有了质的变化</p><p>我们尝试使用运行让N增大到100多都能迅速返回，但是当我们逐渐增大到1000的时候我们会发现<code>Python</code>直接报了超出最大堆栈限制的错误</p><ul><li>堆栈超出最大层数的原因</li></ul><p>由于我们使用了递归，递归函数是在递归的时候当前堆栈再次申请一个堆栈待，运行递归函数，为了避免一直无限调用下去耗空堆栈空间（申请也需要一点空间），Python限制了递归层数，由于为了计算超过1000的我们必须至少要递归超过1000次（从1000减一减到小于2），所以我们光荣的被当错程序错乱被误杀了。</p><h3 id="动态规划"><a href="#动态规划" class="headerlink" title="动态规划"></a>动态规划</h3><p>反观我们这个函数，使用递归，我们很容易理解，但是对于计算机来说，只是为了计算一个数而使用递归是非常不划算的，所以我们要思考这些中间值保留有什么共同点，我们从头开始看，对于第三个来说，它只需要知道第一个和第二个的值就行，而第一个第二个我们知道分别为1和2，对于第四个来说，它只需要知道第三个和第二个，如果我们先把第三个计算下来并保留下来，我们就能知道第四个。</p><p>从头开始思考我们知道，我们只需要保留前面计算的结果就能知道后面的值，我们使用一个列表保存这个中间计算过程，我们将函数改写成下面这个</p><pre><code>def f(n):    if n &lt; 3: return n    values = [1, 2]    for i in range(2, n):      values.append(values[i-2]+values[i-1])    return values[-1]</code></pre><p>接下来我们运行这个函数，我们会发现就算N为10000都能迅速返回</p><p>来看看我们动态规划的“损失”，我们使用了一个列表存贮中间过程，使用了空间“换回”了速度</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>我们使用了一个很简单的题目来介绍动态规划，由于这个问题太过于简单，你或许自己在不知道动态规划的时候都能写出来，但是这个从暴力搜索到记忆搜索最后动态规划的算法优化过程中，我们能够清楚的知道设计动态规范其实也非常简单，<strong>将大问题分解成小问题，然后思考小问题的如何细分，最后反过来思考从小问题逆向到最终的大问题</strong>，这就是动态规划。</p><p>当然这道问题并不是很经典的动态规划问题，为了让大家更好的理解动态规划，我在下面这篇的博文中介绍若干中经典的动态规划问题</p><p><a href="/2017/12/26/几个有趣的动态规划/">几个有趣的动态规划</a></p>]]></content>
      
      
      <categories>
          
          <category> 算法 </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>从子网到“互联网”</title>
      <link href="2017/12/14/software/http/%E6%BC%AB%E8%B0%88%E7%BD%91%E7%BB%9C/"/>
      <url>2017/12/14/software/http/%E6%BC%AB%E8%B0%88%E7%BD%91%E7%BB%9C/</url>
      
        <content type="html"><![CDATA[<h2 id="引言"><a href="#引言" class="headerlink" title="引言"></a>引言</h2><p>最近重温《TCP/IP协议簇》，读到子网这个部分，概念都能弄懂，但是不明白子网存在的目的，很多资料都说有两个好处，一是能够判断IP存在局域网还是远程网，另外一个将大的网段分成多个小子网。</p><p>这样就搞得我一头雾水，原来我对互联网的认识是从TCP、HTTP高层协议理解的，我原来对互联网信息传递的理解就像<a href="https://www.zhihu.com/question/20717354/answer/15968282" target="_blank" rel="noopener">这篇回答</a>，网络就像一个神奇的大网，你只需要把电话线插到“网”中就能同别人连起来。原来我对这个解释并没有什么疑问，但是我越来越深入“互联网”，我就我对“互联网”越来越疑惑。</p><p>举个例子，我们知道这个IPV4理论上总共有<code>4294967296（256*256*256*256）</code>个，按照当前的理论，过几年就要分配完了，那意味这至少有50%已经分配好了，也就是说至少有20亿根“网线”要连到一起，我们知道局域网只要交换机就能搭起来，假设我们一个交换机能插20根网线，那么要搭这个20亿“网线”，至少要用一个亿的交换机，你能想象一个亿的交换机堆在一起组成“互联网”吗？就算假如我们使用数字信号，作为一个程序员，我也很难想象原先的程序员是在有限的内存和硬盘，如何设计强大计算机有条不紊的处理20亿的并发。</p><p>所以这篇文章就要从OSI底层协议出发让我们从底部掀一掀“互联网”的老底，将一个有血有肉的“互联网”展现在我们面前。</p><h2 id="触摸“网”"><a href="#触摸“网”" class="headerlink" title="触摸“网”"></a>触摸“网”</h2><p>我们一直在说着互联网、互联网，由于无数在前人的不懈努力下，其实很多时候我们根本感受不到这张网的存在，比如打开浏览器，输入<code>www.baidu.com</code>，我们直接就能连上千里之外的服务器，其实在浏览器后面，我们发的“包”正沿着网跨越千山万水到达一个机房的服务器中。</p><p>所以为了触摸到网这个东西，我们得用一些工具，我们就在打开的百度中输入<code>traceroute</code>，根据你的操作系统安装好，然后我们打开命令行输入<code>traceroute  -I www.baidu.com</code></p><pre><code>traceroute to www.baidu.com (14.215.177.38), 30 hops max, 60 byte packets 1  192.168.1.1 (192.168.1.1)   2  182.96.180.1 (182.96.180.1)  3  68.248.177.220.broad.nc.jx.dynamic.163data.com.cn (220.177.248.68)   4  53.251.177.220.broad.nc.jx.dynamic.163data.com.cn (220.177.251.53)   5  182.98.159.73 (182.98.159.73)   6  202.97.75.117 (202.97.75.117)   7  113.108.208.194 (113.108.208.194)  8  * * * 9  14.29.121.194 (14.29.121.194)  10  * * *11  * * *12  14.215.177.38 (14.215.177.38) </code></pre><p>由于百度存在负载均衡，所以你们看到的最终IP地址可能不会同我一样（PS：我去掉了时间），虽然中间还有一些<code>***</code>的存在，但是不管怎么我们终于触摸到这个网的存在。</p><p>我第一次看到这个非常震惊，原来在我的心中，“网”上最多有两个端，一个是我们的客户端，一个是服务器端，现在突然冒充这么多个“中间人”，这些东西是什么呢？</p><p>接下来我们通过回答下面两个问题来慢慢了解互联网的构造。</p><ol><li>为什么第一个IP是局域网内的IP（内网）？</li><li>为什么中间有那么多IP端，他们的作用是什么？</li></ol><h2 id="内网与外网的区别"><a href="#内网与外网的区别" class="headerlink" title="内网与外网的区别"></a>内网与外网的区别</h2><p>解答第一个问题前，我们必须要知道什么是内网什么是外网。</p><p>从IP的角度上来看，刚开始创建以太网时，由于避免连在一起的电脑认错人，就用IP用来做每个电脑的“身份证”，一开始要连在电脑比较少，组织只需要拿个小本本记住哪台服务器对于的IP，但是随着想连在一起的电脑越来越多，这个小本本满满的一本快写满了，虽然可以在多买几本本子记住他们，但是本子越多每次要查他们的IP的时候消耗的时间越多，所以他们决定不再一个一个分IP了，于是他们把40亿IP分成A、B、C、D几类。</p><p>这样组织成功用一个小本子记录了几十亿的IP分配，这里组织指的是因特网协会（ICANN），协会自己分好IP后开始发本子，找到下面的运营商，让他们自己搭网线光钎把他们自己负责的国家区域连起来，但由于地方太多，一个本子也记不下来，所以他们又把组织发给他们的本子分发到地区运营商，这样慢慢缩小，最终每个地区的局域网的本子都不会太大，这样查起来速度快而且网络的压力也平摊下去，但是摆在我们面前有一个问题</p><blockquote><p>假设小明和小华分别住在同一条大街的街头和街尾，小明想给小华写信，小明然后写了一封信信放到邮箱，然后邮递员过来把信取走，在邮局分配再让邮递员送到小华家，本来两个人住在同一条街，邮递员只需要把信从街头送到街尾这次传递就结束了，但是由于不知道小明和小华住在同一条街，这封信绕了一个很大的圈才到小华手中。</p></blockquote><p>从这个故事告诉我们，要解决不必要的传输，我们必须要提供一种机制让“邮递员”知道这封“信”是直接“送”还是发到总部发出去，这种机制就是确定是否是内网还是和外网。</p><p>大家回头看一下IP，假如我们按照组织（ICANN）的本子来分内网还是外网，那会造成极大的浪费，比如说A类，从1.0.0.0 到126.255.255.255，共分了126个，每个分类下有1658万台电脑，可能现在最大的云服务商都没有几千万台机器，假如你就几十台电脑，你申请一个B类IP（子网可以容纳6万多台），那么子网的利用率约为为20/6000，这么大一个网段却只有几个IP有效，这是对IP的极大的浪费，所以我们需要再次切片，将一个IP段智能的切分成很多块。</p><p>有没有什么好的方法能够切分IP呢，我们知道在TCP、HTTP这些高层协议栈中并没有子网这个概念，他们只负责连接和解析，所以我们得从数据链路层里面查看，在这层有一个非常重要的概念：<strong>子网掩码</strong></p><h3 id="子网掩码"><a href="#子网掩码" class="headerlink" title="子网掩码"></a>子网掩码</h3><p>首先我们要了解一个概念：<strong>路由表</strong></p><p>这个就是我们前面提到过的“小本本”，这个路由表就记载了我们主机上面有关子网划分的重要数据，我们可以通过在<code>Linux</code>上的<code>route -n</code>命令显示电脑上的路由表</p><pre><code>Kernel IP routing tableDestination     Gateway         Genmask         Flags Metric Ref    Use Iface0.0.0.0         192.168.1.1     0.0.0.0         UG    0      0        0 wlan0192.168.1.0     0.0.0.0         255.255.255.0   U     9      0        0 wlan0</code></pre><p>在我的电脑的得到的结果是这样的，这里有两个很重要的参数，<code>Gateway（网关）</code>，<code>Genmask（掩码）</code>，网关和掩码是什么呢？网关就是我们连接上外网的关键，掩码就是区分内网外网的钥匙。</p><p>子网是什么呢？你可以看做是IP与掩码的按位与运算得到新IP，比如说我们上面第一个<code>192.168.1.1</code>与<code>0.0.0.0</code>的运算结果是为<code>0.0.0.0</code>，而且你会发现所以的IP跟<code>0.0.0.0</code>得到结果都是一样的<code>0.0.0.0</code>，这说明对于网关来说，所有的外网IP都是属于同一子网，接下来我们看看第二行，这个网关为<code>0.0.0.0</code>说明这个是局域网，当我们的IP与这个局域网掩码运算后得到的地址与这个局域网IP相同时，说明这个IP属于局域网，我们可以看到我们子网大小为256，由于我用的是路由器，所以说明这个路由器最多可以允许254(.255为广播地址）台设备连接</p><p>接下来我们看看在我的云服务器上的路由表</p><pre><code>Kernel IP routing tableDestination     Gateway         Genmask         Flags Metric Ref    Use Iface0.0.0.0         10.10.6.1     0.0.0.0         UG    0      0        0 eth010.15.6.0     0.0.0.0         255.255.192.0   U     0      0        0 eth0</code></pre><p>你可以清楚的看到在第二行我们的内网的大小约为<code>16128=256*(255-192)</code>，如果你感兴趣对子网计算，可以看看我下面引用，在这里我就不解释太多计算细节，你可以看到，通过改变内网掩码，我们可以轻轻松松的将局域网分成不同的大小。</p><p>由于IPV4数量较少，所以在局域网内每个主机并不能都分到单独的独立IP，只有网关需要一个独立IP来访问互联网，在局域网内我们一般使用本地局域网IP（组织特地保留下来不分配给运营商，只在局域网内使用）。所以我们这就能解释第一个问题，<code>traceroute</code>第一个发送的地址不是我们单独IP（路由器IP），而是发送给局域网上的网关。</p><h2 id="包的逐级转发"><a href="#包的逐级转发" class="headerlink" title="包的逐级转发"></a>包的逐级转发</h2><p>解决上面第一个问题之后，我们知道在一个局域网内我们能够通过子网掩码知道当前局域网的子网范围，接下来的包的“旅途”是什么呢，为什么在<code>traceroute</code>的路径中发现那么多IP地址呢。</p><p>回答这个问题前，我们从物理角度上先了解互联网是怎么搭建起来的。</p><blockquote><p>当第一台计算机出现的时候，我们不需要互联网，但是随着计算机原来越多，我们想把计算机都用网线连接起来，一开始电脑都放在一起，只需要找一些长长的网线把他们连接起来，但是随着全球各地的人都有个人电脑，这时候只能靠网络运营商，也就是比如电信、移动、联通这些运营商，他们埋光钎搭网线，慢慢的将网络在各地连接起来，最后将子网络连接到骨干网，最后互联网就这么连接起来了</p></blockquote><p>但是就像送信举的例子一样，如果邮递员能够直接把信从街头送到街尾，那么直接节省了很长一段在路上的花的时间。所以运营商就在各个分支网络搭建大大小小的“局域网”，就一层层代理一样，当一个包请求过来，首先先查看这个IP是否属于当前地区的局域网，如果是就查表找到地址发送过去，如果没有就发送到它的更高一级代理（网关），最终这个IP包到达机房区域的局域网的主机上。</p><p>所以我们能在<code>traceroute</code>查看到一个IP包要传递的那么多的IP地址，那些IP地址都是勤劳的网关，不过相比我们第一个网关，也就是我们在网上冲浪的IP地址，那些网关更像一个一个交通指挥员，指导着我们的发送的“信”一步一步走到指定地点。</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>互联网就像一个乐高拼成的巨人，刚开始不了解它，远远的观察它，它就像文明遗迹一样让人赞不绝口，等到你慢慢走进它，你就会发现它的组成其实也很普通，也就是一个一个乐高模块组成，但就是这种无数普通搭建我们的“万里长城”，这或许就是互联网的伟大之处。</p><h2 id="引用"><a href="#引用" class="headerlink" title="引用"></a>引用</h2><p><a href="http://www.cnblogs.com/JuneWang/p/3917697.html" target="_blank" rel="noopener">子网</a></p>]]></content>
      
      
      <categories>
          
          <category> 软件 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> HTTP </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>巧用Git钩子</title>
      <link href="2017/11/25/software/%E5%B7%A7%E7%94%A8Git%E9%92%A9%E5%AD%90/"/>
      <url>2017/11/25/software/%E5%B7%A7%E7%94%A8Git%E9%92%A9%E5%AD%90/</url>
      
        <content type="html"><![CDATA[<blockquote><p>以前听学长提过Git钩子，但是自己一直没有仔细了解过，记得我还写过一个github更新的Python包，现在想想其实用自带的钩子就能很好的完成</p></blockquote><h2 id="什么是钩子？"><a href="#什么是钩子？" class="headerlink" title="什么是钩子？"></a>什么是钩子？</h2><p>我们知道Git是迭代式开发工具，我们的开发流程都是<code>git add</code>、<code>git commit</code>，<code>git push</code>，钩子呢就是你完成每一步Git给你的“回调”，举个例子假如你想让服务器每次上传完新的代码后更新网站，如果你没有钩子，你只能自己<code>ssh</code>登录上服务器，自己更新软件，一次两次还好，多了的话你会骂娘的，所以钩子是给我偷懒的脚手架，我们可以很轻松的写一些脚步帮我们完成一些重复的步骤</p><p>介绍玩钩子的作用，我们来介绍一下钩子的分类</p><p>我们知道Git核心是<code>commit</code>和<code>push</code>两个命令，一个对应客户端，一个对应服务端，所以钩子主要分客户端和服务端，由于Git步骤分的很细，所以每个大分类下面还有很多小分类，比如<code>pre-commit</code>，<code>post-commit</code>这些。</p><p>钩子的全部放在<code>.git/hooks</code>下面，在新建一个项目仓库的时候，Git已经在这个文件夹下给我们生成了很多个<code>.sample</code>后缀的钩子，这些钩子只要把<code>.sample</code>去掉就可以运行了，我们可以在这些<code>sample</code>上面修改完成我们自己的钩子</p><h2 id="客户端钩子"><a href="#客户端钩子" class="headerlink" title="客户端钩子"></a>客户端钩子</h2><p>客户端钩子很好理解，你<code>commit</code>之后想做其他事，比如说编译一下程序啥的，这里我就不多讲，主要由下面几个钩子组成</p><ul><li>pre-commit 提交之前</li><li>post-commit 提交之后</li><li>pre-rebase 变基之前</li><li>post-rewrite 替换提交记录之后</li><li>pre-push 推之前</li></ul><p>详细的可以看官网链接<a href="https://git-scm.com/book/zh/v2/%E8%87%AA%E5%AE%9A%E4%B9%89-Git-Git-%E9%92%A9%E5%AD%90#_git_hooks" target="_blank" rel="noopener">钩子</a></p><p>客户端钩子我觉得一般没有太多作用，因为我在提交之前就会运行脚步进行开发调试什么的，我把介绍重点放在服务端钩子</p><h2 id="服务端钩子"><a href="#服务端钩子" class="headerlink" title="服务端钩子"></a>服务端钩子</h2><p>服务端钩子就是你<code>push</code>之后的事情服务器要运行的脚步，有用推的步骤只有一个，所以钩子只有四个</p><ul><li>pre-receive 接受之前</li><li>update 更新之前</li><li>post-update 更新之后</li><li>post-receive 接受之后</li></ul><p>服务器接收到客户端请求时，<code>pre-receive</code>先进行调用，如果返回值为非0就会拒绝推送，所以我们写钩子的时候一定要记住最后要返回0才能正常接收更新，<code>update</code>主要处理多分支推送，有的时候你一次更新，推三四个分支到服务器，<code>pre-receive</code>只会调用一次，<code>update</code>会对每个的分支调用一次，后面两个都很容易理解</p><p>一般我们就是要在服务端更新代码之后运行脚步，所以我们要修改的就是<code>post-update</code>或者<code>post-receive</code></p><p>bash脚步大家都会写，但是大家可能会很陌生什么是Git服务端，接下来我们就来介绍一下Git服务端是什么</p><h2 id="Git-服务端"><a href="#Git-服务端" class="headerlink" title="Git 服务端"></a>Git 服务端</h2><p>大家一般使用Git都是使用的客户端，但是Git这个工具的确很强，它不但可以当做客户端，也可以当做服务端，为了让大家更好的理解Git服务端，我们先来拿本地文件做”服务器“</p><p>首先我们先新建一个文件夹为<code>server</code>，在新建一个文件夹为<code>local</code>，假设文件夹都在<code>/root</code>文件夹下</p><p>我们执行下面的命令生成服务器</p><pre><code>cd /root/servergit init --bare</code></pre><p>只需要在<code>init</code>后面添加一个<code>--bare</code>选项告诉Git，Git就会帮我们生成一个空的“服务端”，我们可以查看一下文件，我们发现Git 给我们生成下面几个文件夹，其中就有我们的hooks</p><pre><code>branches  config  description  HEAD  hooks  info  objects  refs</code></pre><p>但是服务端和客户端生成的位置不一样，客户端是给我们生成一个<code>.git</code>文件夹，里面放了这些文件夹，然而服务端直接将这些文件夹放在主目录了</p><p>行我们已经生成了服务端的，接下来我们生成客户端的钩子</p><pre><code>cd /root/localgit init</code></pre><p>很简单，同我们往常操作一样，我们这时候添加一个<code>README.md</code> 然后<code>commit</code>一下准备开始往服务端推代码了</p><p>在 linux 下直接执行下面命令就行</p><p>   echo “local update” &gt;&gt; README.md<br>   git add README.md<br>   git commit -m “Add ReadME”</p><p>接下来我们就要向”服务器“提交代码了，我们先添加本地文件作为远程服务器</p><pre><code>git remote add origin file:////root/server</code></pre><p>然后直接推代码</p><pre><code>git push origin master</code></pre><p>这样我们就向我们文件提交了代码，这时候我们回到我们”服务器“</p><pre><code>cd /root/serverlsbranches  config  description  HEAD  hooks  info  objects  refs</code></pre><p>我们惊奇的发现服务器并没有我们新建的<code>README.md</code>文件，原来Git服务端并不像SVN一样只保留一份代码大家共同修改，<strong>Git服务端只是记录文件变化和分支变化</strong></p><p>这里插一句我为什么会去了解Git钩子，由于一开始实现服务器自动更新我的FastProxyScan项目代码，但是我又不想使用<code>Github</code>钩子（push后发送http请求），太麻烦了，后来我一想干脆直接推到我的服务器上，但是推到服务器上的代码只是记录了分支和提交信息，不包含源文件，所以我只好在在服务器上部署这个项目，并添加一个服务器钩子，当服务器更新完成后，再用钩子把服务器上的项目代码更新</p><h2 id="如何写服务器钩子"><a href="#如何写服务器钩子" class="headerlink" title="如何写服务器钩子"></a>如何写服务器钩子</h2><p>通过上面对本地文件新建仓库，我们知道Git“服务端”新建很简单，我们一般接触比较多的是Github服务端，但是Git非常强大，他可以支持多种协议来连接“服务端”，比如说我们上面用到的本地文件（<code>file</code>协议），假如你用<code>ssh</code>连接远程服务器，你也可以使用类似<code>git remote add origin ssh://username@ip/file/path</code>添加ssh远程仓库</p><p>git 支持的协议有ssh、http、https、file、git等协议，你只要确保你能连接上远程服务器就行，接下来我们谈谈如何写服务器钩子</p><p>在使用<code>git init --bare</code>新建了一个Git服务端之后，在服务端文件下面有一个<code>hooks</code>文件夹，我们要做的就是把脚本放到<code>hooks</code>文件夹里面（当然你要确保它有执行权限），如果你更擅长写<code>Python</code>，<code>Ruby</code>那些脚步也可以，不过要确保前缀后后缀正确。</p><p>这里要提到很重要的一点，由于在执行钩子的时候，环境变量<code>GIT_DIR</code>被设置为服务端当前目录，如果你像我一样想更新在另外一个文件夹下面的项目代码，你必须使用<code>uset GIT_DIR</code>清除变量名，否则只会更新服务端，而不会更新你的项目代码</p><p>这里我提供一个模板</p><p>文件名为 <code>post-update</code>或者<code>post-receive</code></p><pre><code>#!/bin/shcd /project/path/ || exitunset GIT_DIRgit pull origin masterexec git-update-server-info</code></pre><p>你只需修改项目文件路径和仓库名即可</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>通过这个Git钩子了解了Git服务端，也让自己更加了解Git这个软件，以前一直懵懵懂懂，只会向Github提交文件，一直以为Git只是一个版本记录工具而且，现在看来神器之名不是浪得虚名，通过一个小小的钩子，摇身一变成部署神器。</p>]]></content>
      
      
      <categories>
          
          <category> 软件 </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>FastProxyScan项目介绍</title>
      <link href="2017/11/23/opensource/FastProxyScan-Introduction/"/>
      <url>2017/11/23/opensource/FastProxyScan-Introduction/</url>
      
        <content type="html"><![CDATA[<blockquote><p>为了给我的站点增加人气，我把这个项目的介绍放到我的博客，如果你觉得这个项目还不错的话，请不要吝啬你的star</p></blockquote><p><a href="https://github.com/mrzhangboss/FastProxyScan" target="_blank" rel="noopener">github传送门</a><br><a href="http://115.159.146.115" target="_blank" rel="noopener">Demo传送门</a></p><h2 id="引子"><a href="#引子" class="headerlink" title="引子"></a>引子</h2><p>一开始自己只想做一个代理池，于是搜了搜Github发现类似的项目，大多数都是爬取网上的一些代理商的免费代理，这部分代理大多都是没有用的，可用性非常低，于是我自己就干脆做一个“代理商”，自己扫描主机把可用的代理扫描出来。</p><p>但是现在网络主机实在太多了，至少几百万台，所以这个项目的核心就是快速扫描，在最短的时间内检测更多的代理，目前项目的速度最好只能完成1000代理每小时的速度（日扫描两万代理），希望能继续优化代码，加快速度，如果你对这个项目感兴趣可以Fork下来，欢迎各位的Pull Request </p><h2 id="项目依赖"><a href="#项目依赖" class="headerlink" title="项目依赖"></a>项目依赖</h2><p>项目基于Python3.5+开发</p><p>软件依赖</p><ul><li>nmap</li></ul><h2 id="项目结构"><a href="#项目结构" class="headerlink" title="项目结构"></a>项目结构</h2><p>项目主要由三个部分组成</p><ul><li>主机扫描</li><li>端口检测</li><li>代理检查</li></ul><p>项目结构为</p><ul><li>proxy_pool</li><li><ul><li>scanner</li></ul></li><li><ul><li>display</li></ul></li><li><ul><li>database</li></ul></li></ul><p>现在依次介绍在搜索速度上的优化</p><ul><li>主机搜索</li></ul><p>全球的IP都是有ISP统一分配的，ISP主要由下面几大洲分配，我们中国处于亚太区，所以我们的IP由亚太互联网络信息中心（APNIC）分配IP，目前中国分配的IPV4总数为3亿左右，这个数量还是比较大</p><p>我们可以从 <a href="http://ftp.apnic.net/apnic/stats/apnic/delegated-apnic-latest" target="_blank" rel="noopener">http://ftp.apnic.net/apnic/stats/apnic/delegated-apnic-latest</a>下载最新分配的IP地址</p><p>但是代理服务器只存在特定的服务器上，所以现在版本还没有发布V1.0主要是因为搜索的效果不是很好（搜索的主机代理转换率太低），目前还在想其他方法，等有更好的解决方法就会发布V1.0</p><ul><li>端口检测</li></ul><p>使用<code>nmap</code>的“TCP SYN scan”最大化加快端口检测速度（需要root权限）</p><ul><li>代理检测</li></ul><p>使用<code>nmap</code>先验端口与<code>Python</code>异步最大化代理检测速率</p><h2 id="安装教程"><a href="#安装教程" class="headerlink" title="安装教程"></a>安装教程</h2><p>项目采用Django做后台管理，所以只需要一点Django基础知识就能在这个项目上做二次开发，如果你只想获取最新的可用代理，可以通过<a href="http://115.159.146.115" target="_blank" rel="noopener">http://115.159.146.115</a> 调用API接口获取最新可用代理（我的站点带宽有限，所以只开放最新100个代理，并且只是20分钟更新一次）</p><p>环境安装</p><ul><li>nmap 安装</li><li>python3.5+ 安装</li></ul><p>运行：</p><pre><code>git clone https://github.com/mrzhangboss/FastProxyScan.gitcd FastProxyScanpython3.5 install -r requirement.txt</code></pre><ul><li>主机检测</li></ul><pre><code>cd pool/proxy_poolsudo python3.5 manage.py scan --vps</code></pre><ul><li><p>端口检测</p><p>  sudo python3.5 manage.py scan  –proxy -m 100</p></li></ul><p>m是并行参数，值越大速度越快</p><ul><li><p>代理检测</p><p>  sudo python3.5 manage.py check –start -c <a href="http://115.159.146.115/ip" target="_blank" rel="noopener">http://115.159.146.115/ip</a></p></li></ul><p>c是检测ip头网址，可以使用我提供的 <a href="http://115.159.146.115/ip" target="_blank" rel="noopener">http://115.159.146.115/ip</a> 返回请求头，可以参考我的上一篇博文 <a href="/2017/11/20/%E4%BB%A3%E7%90%86%E7%9A%84%E5%89%8D%E4%B8%96%E4%BB%8A%E7%94%9F/">代理的前世今生</a></p><p>在我搭建的DEMO站点上，我使用supervisor让这三个程序循环运行，你可以使用crontab定时调度也可以像我一样。</p><p>数据库当前采用的Sqlite3，但是数据库模型全部使用ORM开发，你可以很方便的修改<code>settings.py</code>来放入其他数据库，如果你懂一点Django的话</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>如果你像了解更多开发这个项目背后的知识的话，可以看看我上一篇博文 <a href="/2017/11/20/%E4%BB%A3%E7%90%86%E7%9A%84%E5%89%8D%E4%B8%96%E4%BB%8A%E7%94%9F/">代理的前世今生</a></p>]]></content>
      
      
      <categories>
          
          <category> 开源 </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>代理的前世今生</title>
      <link href="2017/11/20/software/http/%E4%BB%A3%E7%90%86%E7%9A%84%E5%89%8D%E4%B8%96%E4%BB%8A%E7%94%9F/"/>
      <url>2017/11/20/software/http/%E4%BB%A3%E7%90%86%E7%9A%84%E5%89%8D%E4%B8%96%E4%BB%8A%E7%94%9F/</url>
      
        <content type="html"><![CDATA[<h2 id="引言"><a href="#引言" class="headerlink" title="引言"></a>引言</h2><blockquote><p>自己对代理认识不深，也只是会使用而已，由于最近想做一个代理池，于是查了很多资料，发现代理这个东西还是非常有趣的</p></blockquote><h2 id="代理是什么？"><a href="#代理是什么？" class="headerlink" title="代理是什么？"></a>代理是什么？</h2><p>从编程上来看，<code>requests</code>只需要在请求里面加上<code>proxies</code>参数例如<code>requests.get(&#39;http://www.baidu.com&#39;, proxies={&#39;http&#39;: &#39;127.0.0.1:3128&#39;})</code>，我们就能连上代理进行访问，由于<code>requests</code>包装了太多细节，我们无法知道用了代理和没有用代理的区别</p><p>接下来我们来看一下Python3的内库是如何使用代理的</p><ul><li><p>首先要申请一个<code>ProxyHandler</code></p><p>  from urllib import request<br>  proxy = request.ProxyHandler({‘http’: ‘127.0.0.1:3128’})</p></li></ul><p>然后我们通过这个<code>proxy</code>创建一个<code>opener</code>，然后用<code>opener</code>打开页面，最后输出结果。</p><pre><code>opener = request.build_opener(proxy)resp = opener.open(&apos;http://www.baidu.com&apos;)print(resp.read())</code></pre><p>然后我们看一下如果没有使用代理的请求是什么</p><pre><code>resp = request.urlopen(&apos;http://www.baidu.com&apos;)print(resp.read())</code></pre><p>你可以很清楚的看到，如果我们不需要代理，之前打开<code>url</code>就行，也就是说我们的请求，其实全部发给代理，交给代理了</p><p>这里就稍稍谈点感想了，以前经常在书上看到人歌颂互联网的伟大，但是自己一直不明白这个伟大在哪，以前一直从表象感受互联网，有了互联网，不用打开电视就可以看影片，不用去图书馆就可以看书，感觉互联网神奇的地方就是给自己带来方便，但是没有去互联网本身的架构的伟大，通过一根根网线交换器，无数主机“连”在了一起，构成了宏大的互联网，或许你通过浏览器打开的网站主机离你几百公里，但是你不要做飞机轮船，你直接在家里就能通过层层代理传递你的请求将千里之外的“敌将首级” 探入囊中。</p><p>互联网神奇的地方就是看起来各个部分非常分散，但是他们却能通过一根一根线紧密的联系起来，只有有“距离”的时候你才能感受到他的美丽</p><p>以前刚学习网站的时候，在本地调试的时候，你在本地跑一个web，直接打开浏览器访问，这时候我就有一种错觉，web就是两端，客户端和服务器端，在本地调试可以这样理解，但是一旦拿到互联网，这就是不完整的，完整的应该是客户端-代理端-服务器端，当然我们的代理端有时候可能是网关、路由器、交换机等等。</p><p>了解这些有什么用呢，因为我自己以前一直对代理没有什么很深理解，用爬虫的时候使用代理就不会被封IP，感觉代理是很BUG的东西，但其实代理很普通，而且无处不在，我们要想真正理解代理，就必须把它拖下圣坛。</p><h2 id="代理其实很普通"><a href="#代理其实很普通" class="headerlink" title="代理其实很普通"></a>代理其实很普通</h2><p>前面我们知道代理其实很普通，但是要深入了解代理必须要先了解IP，IP是什么呢，IP就是互联网的身份证，要想在互联网上“混”，必须要有“身份”</p><p>那为什么我们要用代理呢，比如说假如你是未满18岁的小朋友，你想要买上网，你必须要借一张大人的身份证去上网，这个时候代理的作用就是帮用它的身份证帮你干事。</p><p>我觉得中国文化博大精深，其实一听代理这个词，我们就能知道代理是干什么用的。其实把代理吹得神乎其神没什么用，那些作用都是它的他自己瞎几把搞的，从我们客户端来看，<strong>代理就是服务端</strong>，了解这点非常重要，因为他能让我们把所以的事情都简化，而且从客户端来看，代理就是这样的</p><h2 id="客户端、代理、-服务器三者之间的关系"><a href="#客户端、代理、-服务器三者之间的关系" class="headerlink" title="客户端、代理、 服务器三者之间的关系"></a>客户端、代理、 服务器三者之间的关系</h2><p>前面我们已经谈了客户端和代理之间的关系，对于客户端来说，代理就是服务器端，我们啥都不管，把请求发给代理，相信它就是我们请求的服务器</p><p>对于代理来说，其实它自己最清楚，自己就是个代理，它必须要把请求转发给服务器，然后在把服务器的响应发给客户端，代理就是一个中介人，有的时候我们也可以把它看做一个双向中继，把请求传递一下，再传回来，所以在这三者之间，只有代理是个明白人，它必须清楚这次任务所以细节，所以有时候虽然说代理是安全的，但其实它也不安全，只要把代理攻克了就能了解到底是哪个家伙干的坏事，所以网上干坏事的人，一般都用很多个代理，层层代理，就算你攻克了一个，也找不到坏人</p><p>对于服务器来说，代理就是客户端，它只负责响应就行，对于代理和客户端来说都是一样的策略。但是时候很奇怪，服务器为什么知道你是个代理，原来全是代理自己的锅，我们细谈一下代理的分类</p><h3 id="代理的分类"><a href="#代理的分类" class="headerlink" title="代理的分类"></a>代理的分类</h3><p>代理也分很多种，有的时候代理也不老实，把客户信息暴露了，这个时候我们就说它是小透明（透明代理），有的时候它不告诉你客户信息，但是告诉服务器我是个代理，我们就说它是匿名代理，但是有时候它连它自己是啥都不告诉你，它伪装成它是客户端，这个时候我们称它为高匿代理，所以这些代理根据暴露信息的不同可以分为这三种</p><ul><li>透明代理</li><li>匿名代理</li><li>高匿代理</li></ul><p>当然我们最喜欢高匿代理，你可以把它当做你的分身，除了身份证不一样，两个人长得一模一样。</p><p>所以我们判断一个代理的类别，必须要检测它向服务器发的报文，所以在我项目<code>FastProxyScan</code>，我搭建了一个服务器，返回客户端向服务器请求头，主要是<code>HTTP_X_FORWARDED_FOR</code>和<code>HTTP_VIA</code>头来分别暴露客户端信息和代理端信息，所以我们只要请求头检测有没有这两个字段就可以完成检测，原理非常简单</p><p>在这里我介绍用Nginx高效返回检测信息</p><pre><code>  location ~ ^/ip {      default_type application/json;      return 200 &apos;{&quot;REMOTE_ADDR&quot;:&quot;$remote_addr&quot;,&quot;HTTP_VIA&quot;:&quot;$http_via&quot;, &quot;HTTP_X_FORWARDED_FOR&quot;: &quot;$http_x_forwarded_for&quot;}&apos;;}</code></pre><p>在Nginx配置里面加上这个端口，我们只有请求<code>/ip</code>，就能直接从Nginx返回请求头信息，速度贼快</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>当然网上还有很多对代理的分类，缓存代理，正向代理，反向代理，但是这些都是代理自己的额外功能，我们前面介绍的代理都是傻呼呼，客户端要什么，它就做什么，这些如缓存代理高级的代理就是很聪明，它的目的就是最快返回客户端需求，比如说虽然说这个傻客户端傻乎乎一个请求请求了几十遍还没记住，代理自己拿个小本子记好，你下次来，正好对上号，直接抄给你，不用再跑几千里去拿了。但是其实本质上它还是逃不出上面的分类，只不过它有的自己的不同罢了。</p>]]></content>
      
      
      <categories>
          
          <category> 软件 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> HTTP </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>漫谈排序算法</title>
      <link href="2017/11/12/algorithm/%E6%BC%AB%E8%B0%88%E6%8E%92%E5%BA%8F%E7%AE%97%E6%B3%95/"/>
      <url>2017/11/12/algorithm/%E6%BC%AB%E8%B0%88%E6%8E%92%E5%BA%8F%E7%AE%97%E6%B3%95/</url>
      
        <content type="html"><![CDATA[<h1 id="0x00-引子"><a href="#0x00-引子" class="headerlink" title="0x00 引子"></a>0x00 引子</h1><blockquote><p>排序是很多算法的基础，简简单单的排序前人就归纳出很多种算法，但是这些算法多多少少都有着相同的原理</p></blockquote><p>排序算法有很多，这里我们就简单的谈谈下面7种排序的特点</p><ul><li>冒泡排序</li><li>选择排序</li><li>插入排序</li><li>希尔排序</li><li>堆排序</li><li>归并排序</li><li>快速排序</li></ul><h1 id="0x01-Summary"><a href="#0x01-Summary" class="headerlink" title="0x01 Summary"></a>0x01 Summary</h1><p>从算法的抽象程度上来看，冒泡、选择和插入是比较好理解，我们能用我们生活中的常见事物来理解，后面四种比较抽象，而且相对于前三种平均时间复杂度O（n） = n ^ 2 的来说，后面四种的平均复杂度都比前面的小，尤其是面对大量数据排序来说，后面四种能比前三种跑的更快</p><h1 id="0x02-冒泡、选择和插入的特点"><a href="#0x02-冒泡、选择和插入的特点" class="headerlink" title="0x02 冒泡、选择和插入的特点"></a>0x02 冒泡、选择和插入的特点</h1><p>这三种算法空间复杂度都为O（n） = 1，也就是说在给定一个列表的前提下，无论列表数有多大，额外的排序所需的空间都为常量。但是这三种算法的平均时间复杂度为O（n） = n ^ 2， 也就是说在给定一个长度为n的数组，必须要经历 k × n × n 次操作才能排序，当我们的n比较小的时候，我们无法察觉这个算法与更高效的算法的差别，当n很大的时候，比如一个亿，这时候的要进行的操作就瞬间爆炸了。</p><p>这三种算法很大程度上是牺牲了运行时间换取运行空间，我们可以从桶排序上面得到相反的例子，桶排序的时间复杂度为O（n） = 1，空间复杂度为O（n） = n，也就是说在排序上面他的速度是最快的，但是它所花费的空间也是巨大的，<strong>有时候时间空间就是两个双刃剑，你如果想节省空间必须浪费时间，你如果想节省时间必须浪费空间</strong></p><p>这三种算法原理很简单，而且有一个相同的地方，就是他们每一节排序就会“删掉”一个数字，接下来就是对剩下的排序。当然我这里的删掉就是代表已经排好，然而接下来的过程中不会再涉及到这个数字</p><p>这个非常好理解，随便给我们一副牌让一个小朋友把他排出来，小朋友一般就是先找出最大的牌放到最前面，然后在剩下里面找到最大的，依次排下去，最后手里就剩一张牌了，这个牌组就排好了</p><p>这三种算法都是基于这个核心，但是具体的算法细节不同。冒泡排序就是先从头到尾依次把最大的交换到最后面；插入排序的话就是我们从第一个数字开始从后面把小的数字插入到前面去；选择的话同冒泡有点相似，不过它并不会把数字传递过去，它直接将未排序的最大值与未排序的末尾值交换。</p><p>这三种排序我们都非常好理解，但是他们有一个缺点，就是未排序前必须遍历全部数组，我们都知道现在大数据时代，对于上亿数据执行一次遍历就已经非常耗时间了，为了排一个数字要几乎就遍历一遍（排到后期遍历的越来越少），所以这三种算法在面对巨量数据的时候，花在遍历上面的时间比排序时间要更多。</p><h1 id="0x03-希尔和插入排序"><a href="#0x03-希尔和插入排序" class="headerlink" title="0x03 希尔和插入排序"></a>0x03 希尔和插入排序</h1><p>希尔排序是插入排序的更高效改进方法，说到改进我们就要谈谈插入排序的优缺点</p><ul><li>优点</li></ul><p>我们给定 <code>[ 1, 3, 2, 5, 8 ]</code> 数组，这个数组基本上已经排好序了，如果使用插入排序，我们只要在插入2的时候，将2和3交换就可以，设我们挪动的距离就为1</p><p>我们在看这个数组 <code>[ 1, 3, 5, 8, 2 ]</code>，我们可以看到这里如果使用插入排序，我们会在插入2的时候，要将2依次与8、5、3交换，这样移动的距离就为3</p><p>希尔排序改进的地方就是步长，如果它的步长选择的好，它的排序效果越好。这个步长是什么呢，插入排序的步长一直为1，也就是每次遍历的时候步子迈一步，假如步长为2，也就是迈两步。在<code>[ 1, 3, 5, 8, 2 ]</code>数组中，比如说我们数字2，它在步长为1的时候，它下一步要比较的是8，假如步长为3，那它下一步就直接与3比较了。</p><p>所以希尔排序改进就在于他能直接移动多位，在上面的例子里面，步长为3，我们能直接将数组从3的位置移动到2，如果直接使用插入排序，必须移动3次才能达到希尔排序的效果。</p><p>希尔排序原理同插入是一样的，不同在于，插入的步长希尔是可变的，这样就为一些“调皮”的数字的移动加快了速度，一步一步的移动他们太累了，直接把步子迈大，一步到位。</p>]]></content>
      
      
      <categories>
          
          <category> 算法 </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>编程小结</title>
      <link href="2017/10/28/summary/%E7%BC%96%E7%A8%8B%E5%B0%8F%E7%BB%93/"/>
      <url>2017/10/28/summary/%E7%BC%96%E7%A8%8B%E5%B0%8F%E7%BB%93/</url>
      
        <content type="html"><![CDATA[<h2 id="引言"><a href="#引言" class="headerlink" title="引言"></a>引言</h2><blockquote><p>编程开发有时候也像雕刻一件艺术品</p></blockquote><p> 以前一直有一种错觉，觉得编程开发就是会用库会用框架，这阶段的感悟只是停留在库的使用上面，然而当你持续工作在一件产品上的时候，你就把思维聚焦在产品，这时候你的感悟就会是架构的搭建，库只会变成你的工具</p><p> 所以慢慢明白编程届的前辈们一直劝我们在大学不要为了钱选择做一些外包项目，外包项目这种东西就像一次性编程产物，你写完之后就再也不会<code>code review</code>了，而对于编程来说，编程就是在写BUG，只不过对于大神来说写的少，对新手来说就是写的多</p><p> 诚然大神们也是从一个一个BUG中慢慢走过来的，然而对于我们菜鸡来说，很多时候我们并不能发现自己的BUG，所以让自己成长最快的方法就是立马纠正自己的BUG</p><p> 当然这里我们所说的BUG有的时候并不是我们经常说的系统无法运行的BUG，我们这里说的BUG可以算成缺陷，有时候是在特殊情况下才触发。比如说运行时，这时候我们称它未漏洞；重构时，我们称它为SHIT代码；测试时，我们称它为过耦合。</p><p> 这里我也不想过多谈技巧如何去发现这些BUG，以为技巧是死的人是活的，我们不需要太多技巧去避免或者去查找这些BUG，还有重要的一点在于BUG是无法避免的，我们要关注的是产品本身。有幸在实习几个月的时间一直专注于一个产品的开发，期间一直经历了大大小小的重构，随着产品的成型，自己也慢慢感悟到一些方法加速查找系统BUG和如何快速开发。</p><p> 接下来就介绍一些我自己的浅显感悟</p><h2 id="迭代开发-CODE-REVIEW"><a href="#迭代开发-CODE-REVIEW" class="headerlink" title="迭代开发 + CODE REVIEW"></a>迭代开发 + CODE REVIEW</h2><p> 如何从零开始搭建一个产品，除非你是超级大牛，几个小时就能搞定一个完整的代码的开发流程，普通人都是一步一步来迭代开发，但是这个迭代开发也有讲究，有些人喜欢从头写到尾，然后看看能不能跑起来，再疯狂DEBUG，也有些人喜欢先写局部，慢慢测试，最后把所以组件都串联起来。</p><p> 这两种方式萝卜青菜各有所爱，第一种速度最快，但是不适合团队合作和CODE REVIEW，第二种速度慢，但是灵活可靠，容错率更高，对于新手来说，选择第二种能让自己的错误不会对系统造成系统性崩塌，而且可以慢慢发现自己的BUG，从而从BUG中提高自己</p><p> 对于迭代式开发我们就不得不提一下git，作为一个版本控制工具，它在迭代开发的作用堪称神器。然而这个神器我却一直没有找到正确的打开方式，只是把它当做上传服务器的工具，最近才开始慢慢掌握一点小小的技巧。</p><p> 我原来的git的工作流程用命令概况下来就是</p><ul><li>git add -A</li><li>git commit</li><li><p>git push</p><p>这三条，然而我大部分时间都只是发挥了<code>git push</code>的功能，纯粹把它当成代码的备份，然而git的核心在于<code>git add</code>和<code>git commit</code>这两个命令上，这里要检讨一下我以前的做法，以前一般完成一个组件的功能就直接快速<code>git add -A</code>然后<code>git commit</code>，虽然我是遵循迭代开发，但是我很少去REVIEW自己这次提交的<code>commit</code></p><p>所以最主要的问题在于如何在快速迭代开发的时候慢下来，好好思考和REVIEW一下自己这次提交的代码，所以在这里不得不介绍<code>git diff</code>这个命令了，对于在每个新修改的文件来说，在你执行<code>git add</code>之前，你最好<code>git diff</code>一下这个新提交的文件，git会把你所做的修改和原始代码做一个对比。</p><p>有的时候我们并不能记得原始代码是什么，我们到底对代码做了什么改变，幸亏我们有这个神器，只需要<code>git diff</code>一下，我们所做的修改和原始版本的差异就会显示出来，REVIEW代码的过程也就是我们发现错误的过程</p><p>我们要想提高自己的就要<strong>不断的改变修正</strong>，所以正确的git的工作流程应该是这样</p></li><li><p>git diff</p></li><li>git add</li><li>git commit</li><li>git push</li></ul><p>当然每个<code>commit</code>可以由很多个<code>git diff</code> + <code>git add</code> 组成，但是我们必须要保证自己对<code>git add</code>的每一个文件都要<code>REVIEW</code>一遍，而且我们在每次<code>git add</code>之前，要思考这次的改变是否能够改进，是否必要等</p><p><strong>产品开发就像爬山，你不可能一步登山，所以我们要做的就是，在每次停下来的时候确定方向，修正自己，甚至回头</strong></p><h2 id="拥抱变化-快速开发"><a href="#拥抱变化-快速开发" class="headerlink" title="拥抱变化 + 快速开发"></a>拥抱变化 + 快速开发</h2><p>前面我们谈了在每一个<code>commit</code>的时候我们都得慎重再慎重，小心又小心，但是这种思想有时候如果把它带入开发过程中则会让你寸步难行，具体是什么呢，我来根据我自己经历来介绍</p><p>我自己是一个有一点叫做代码“洁癖”的人，由于看了不少编程理论的书，我容忍不了自己写出很SHIT的代码，面对新东西，我一般喜欢研究个透再下手，我要确保我的设计是万无一失的，所以这就造成如果我接触一个新的库或者新的功能我会花上很长时间在上面，而且由于我自己思考原来越深，我可能会把原来简单的问题搞得越来越复杂，等到我觉得开始CODING完的时候，我发现自己把一个超简单的问题搞得那么复杂，牺牲了太多时间却适得其反</p><p>造成这种原因主要是吸收太多而没有消化，我看过很多编程理论的书，技术大牛用他们的开发经验告诉我们要模块话开发，要注意设计模式，要考虑系统灵活性耦合性，这种大牛经验是很宝贵，但是这些经验就好像最高的武功秘籍，假如你没有相应的基础，贸然去练的话你会走火入魔，对于新东西新功能，我们就要想爬山者先驱一样，我们不是要找一条最锻炼自己的路去走，而是<strong>找一条最简单的路</strong>，只有爬到山顶我们才需要考虑其他问题</p><p>这种快速开发的思想还有很重要的一点就是“拥抱变化”，我们在快速开发的过程中无法避免由于快速开发造成的部分SHIT代码，当你写出这部分的时候，其实对于菜鸟来说，这部分代码才是你最宝贵的代码，因为它暴露了你的缺点，要想提高自己，必须<strong>发现自己的缺点</strong>，所以快速开发的过程中不但激发自己的潜能，而且让自己对自己的缺点有了更好的了解，了解了自己缺点，才能慢慢改进，所以快速开发第一能够节省时间，第二个就是能缓慢提高自己，当然前提是去修正它，所以快速开发你必须要把自己的代码”洁癖“和速度结合起来，抱着一颗永不满足的心去不断锤炼自己的”代码“</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>在我看来编程开发就像是打太极，<strong>一方面我们得快，以目标为驱动，快速开发；一方面我们得慢，以变化为驱动，迭代开发</strong></p><p>看似快慢是两个极端，其实两者相得益彰，快促进慢，慢促进快，两者相互促进</p><p>当然这只是我的自己小小感悟，编程开发博大精深，这些只是技巧，关键在于自己，思考并转换才是最核心的</p>]]></content>
      
      
      <categories>
          
          <category> 随想 </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>HTTP复用</title>
      <link href="2017/10/10/software/http/HTTP%E5%A4%8D%E7%94%A8/"/>
      <url>2017/10/10/software/http/HTTP%E5%A4%8D%E7%94%A8/</url>
      
        <content type="html"><![CDATA[<blockquote><p>曾经有人问过我一个问题什么是TCP复用，我当时没有回答上来，后面我又遇到一些并发性能问题的时候，我才开始慢慢明白为什么会有这个问题，以及这个问题背后的秘密</p></blockquote><p>其实当时应该他想考我的是爬虫的请求优化，准确来说是HTTP持久连接（HTTP persistent connection），并不是TCP复用，这才导致我当时查阅很多资料，并没有发现TCP复用能优化客户端，因为<a href="http://gaibianziji.blog.51cto.com/1082897/1211940" target="_blank" rel="noopener">TCP复用</a>是服务端的事，现在就让我从源头开始慢慢解读这个问题</p><h2 id="起因"><a href="#起因" class="headerlink" title="起因"></a>起因</h2><p>我们知道我们每次发的HTTP请求在底层都是一个套接字的通信，我们可以从底层开始做一个测试</p><p>我们使用个for循环，申请1024个socket</p><pre><code>import socketl = [ ]for  i in range(1024):    sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)    sock.bind((&apos;www.baidu.com&apos;),80))    print(i)    l.append(sock)</code></pre><p>这个过程有点慢，但是你会发现在申请到1000左右的时候，会直接报<code>Open too many file</code> 这个错误，但是我们并没有打开文件，为什么会报这个错误</p><p>原来在Unix系统下，我们申请的套接字也就是<code>socket</code>在底层是以文件的形式存在的，<strong>客户端通过申请一个<code>socket</code>来写入和接受服务端的请求</strong>，这是一个非常重要的概念，对我们后面解析库函数有很大的帮助。</p><p>由于系统资源有限而且打开很多文件系统响应会变慢，所以Unix系统或者Windows都对单个进程申请套接字有限制，在Unix系统下我们可以通过<code>ulimit -n</code>查看这个值，在笔者的<code>Ubuntu</code>上这个值为1024，基本没有修改过都是这个值，我们可以通过我们可以在命令行执行<code>ulimit -HSn 4096</code>临时增加至4096,</p><p>所以我们一般来说单台机器单个进程最多只能并发1024个请求，在不修改配置的情况下这个值是固定的，所以我们提高并发数只有两种方法</p><ol><li>修改系统配置</li><li>使用多进程</li></ol><p>在写这篇文章之前，我一直以为HTTP复用能在作用在并发上提高爬虫性能，但是其实并不是，它能提高性能但是却不是在并发上提高，接下来我们仔细介绍HTTP复用是怎么提高爬虫性能的</p><h2 id="HTTP复用"><a href="#HTTP复用" class="headerlink" title="HTTP复用"></a>HTTP复用</h2><p>说道HTTP复用，我们不得不介绍一下HTTP和TCP协议，我们都知道Internet是由OSI七层协议构成的，但是OSI只是规定了框架，具体协议我们是通过TCP/IP来实现的</p><p>我们先来说说这个TCP，我们都说互联网能够发展到现在这么稳定可靠多亏了这个TCP可靠协议，但是这个可靠是要付出代价的，建立一次连接的过程要经过三次握手，断开的过程也得四次分手，而且这个连接的过程完全不涉及我们要请求的内容，我们知道爬虫一般请求一个站点只有通过一两次请求就行，如果每次请求都得握三次手，还得分四次手，这样的代价也太大了</p><p>所以HTTP的复用优化的方向就是减少TCP的连接，谈到如何减少TCP连接，我们就得说说HTTP长连接（HTTP persistent connection）</p><h3 id="HTTP长连接"><a href="#HTTP长连接" class="headerlink" title="HTTP长连接"></a>HTTP长连接</h3><p><strong>在HTTP1.1规定了默认都是长连接，TCP不断开，并且在请求头添加一个<code>Connection</code>的header，如果是值为<code>keep-alive</code>则保留TCP连接，假如为<code>Close</code>请求完成之后就会关闭，在HTTP1.0的下默认为关闭状态</strong></p><p>怎么来理解这个长连接呢，我们都听说过<strong>HTTP是无状态的</strong>这句话，从HTTP协议上来看，<strong>服务器客户端就是一个“Request”，“Response”组成，无论多复杂的页面都是由一个个“Request”组成</strong></p><p>为了更好的理解上面的话，我们回到那个套接字，我们把HTTP请求比作打电话，对于每个电话，我们只需要先拨号，然后滴滴滴三下后确定我们同对面连上了（服务器“协商”好），然后我们把我们要说的话通过话筒传给对方，等我们说完之后，由于信号差，对面听完还要想怎么回，然后我们安安静静的在听筒那等，等他想好说什么，在慢慢的说给我们听。</p><p>在HTTP1.0的时代，我们每次拨完一次好，说完一句话，听完对面的回应后，我们就会挂断电话，如果我们还想说就得再重复这个过程，在HTTP1.1下我们增加了长连接这个概念，就是如果你想这个电话里多聊几句，那么就在最后加上“你等下不要挂了，我还要说”（在header加上“Connection: Keep-alive”），那么对方就不会挂断电话，等它说完之后也想你一样在听筒那而等着，这样我们就省掉了一次拨号的时间</p><p>我们现在了解为什么HTTP复用能够节省爬虫的性能了，接下来我们就从编程语言对HTTP复用的实现上了解如何实现HTTP复用</p><h2 id="存贮单元—ConnectionPool"><a href="#存贮单元—ConnectionPool" class="headerlink" title="存贮单元—ConnectionPool"></a>存贮单元—ConnectionPool</h2><p>在介绍ConnectionPool之前我们先简单介绍一下HTTP复用的具体表现</p><h3 id="TCP与URL的关系"><a href="#TCP与URL的关系" class="headerlink" title="TCP与URL的关系"></a>TCP与URL的关系</h3><p>我们知道HTTP复用的是TCP的连接，而TCP连接由四个部分组成</p><ol><li>本地ip</li><li>本地port</li><li>服务器ip</li><li>服务器port</li></ol><p>简单来说就是两个二元组（local_ip, local_port), (server_ip, server_port)</p><p>但是我们发一次的请求是一般是通过URL，也就是类似“<a href="http://www.baidu.com”，这样的url来请求的，这个同我们TCP有什么关系呢？" target="_blank" rel="noopener">http://www.baidu.com”，这样的url来请求的，这个同我们TCP有什么关系呢？</a></p><p>首先介绍一下“http”代表通信协议，这里使用的是HTTP协议，“://”后面的就是请求的域名，域名后面如果有冒号就是我们请求的端口号这里没有，根据HTTP协议这里默认是80端口（HTTPS是443），域名后面的就是请求路径，这里也没有就默认问“/”，也就是我们通过这个“url”就知道我们这次请求的具体位置了，现在我们找到了端口，但是请求的IP在哪呢？</p><p>这里就要介绍一下DNS了，我们为了让我们的站点更好记，我们使用域名代替ip地址，通过在DNS服务上注册我们域名，以及绑定我们域名对应的IP地址，我们就能让计算机通过域名来转换成IP地址，这里就不详细介绍了</p><p>所以呢我们现在了解了，一个TCP连接只是涉及到URL的域名和端口号，我们请求站点的时候主要是通过不同的路径来获取内容，所以我们可以很清楚的知道，只要我们URL的域名和端口一样，那么我们所以的URL都能共用这个TCP接口</p><h3 id="ConnectionPool的实现"><a href="#ConnectionPool的实现" class="headerlink" title="ConnectionPool的实现"></a>ConnectionPool的实现</h3><p>简单来说为了实现HTTP复用，我们只需要保存TCP连接就行了，但是通过前面我们知道，我们保留的TCP连接必须和你要请求的url要域名端口一样，有时候一个站点的服务可能由多个域名多个端口组成，所以原本我们只要用一个变量保留上一次请求的TCP连接，为了程序更加健壮，我们需要一个TCP连接池，存贮不同的TCP连接。</p><p>每次新的URL来的时候我们就是先从TCP连接池中查看有没有相同的域名和端口，如果有就用它发请求，如果没有就新建一个TCP连接，这就是TCP连接的基本原理，当然还要一点编程的时候要注意，我们从池子里面取出一个用完必须放回，否则池子用完了又得新建，那就完全丢掉了复用这个概念了</p><h2 id="HTTP复用在Requests的具体表现"><a href="#HTTP复用在Requests的具体表现" class="headerlink" title="HTTP复用在Requests的具体表现"></a>HTTP复用在Requests的具体表现</h2><p>前面介绍了一大堆概念，但是从头到尾如果让我们自己来做一个实在太难了，幸好我们有<code>Requests</code>这个库，它的<code>Session</code>对象在<a href="http://requests-docs-cn.readthedocs.io/zh_CN/latest/user/advanced.html" target="_blank" rel="noopener">文档</a>介绍了它就维护了一个TCP连接池并且能够复用TCP连接</p><p>接下来我们就从代码入手来更好的理解这个进程池的高级用法，我们为了更好看到每一次请求底层的操作，我们这里自己先自己搭建一个本地服务器，我们使用<code>Flask</code>来搭建一个本地服务器<br>新建一个<code>web.py</code>文件，在运行</p><pre><code>from flask import Flask, requestfrom werkzeug.serving import WSGIRequestHandlerapp = Flask(__name__)WSGIRequestHandler.protocol_version = &quot;HTTP/1.1&quot;@app.route(&apos;/&apos;)def hello_world():    return &apos;%s %s&apos; % (request.remote_addr, request.environ.get(&apos;REMOTE_PORT&apos;))if __name__ == &apos;__main__&apos;:    app.run(host=&apos;0.0.0.0&apos;, port=8000)</code></pre><p>这里我们在8000端口开了一个服务器并且设置为HTTP/1.1协议，我们返回用户请求的ip和端口</p><p>接下来我们开一个Python解释器来看看这个进程池的用法</p><pre><code>&gt;&gt;&gt; import requests, logging&gt;&gt;&gt; logging.basicConfig(level=logging.DEBUG)&gt;&gt;&gt; session = requests.Session()&gt;&gt;&gt; session.get(&apos;https://baidu.com&apos;)DEBUG:requests.packages.urllib3.connectionpool:Starting new HTTP connection (1): www.baidu.com            DEBUG:requests.packages.urllib3.connectionpool:http://www.baidu.com:80 &quot;GET / HTTP/1.1&quot; 200 None&lt;Response [200]&gt;</code></pre><p>看我们可以从打印的logging日志看到我们在进程池中新建了一个TCP连接，我们在试着再请求一次</p><pre><code>&gt;&gt;&gt; session.get(&apos;https://www.baidu.com&apos;)    DEBUG:requests.packages.urllib3.connectionpool:https://www.baidu.com:443 &quot;GET / HTTP/1.1&quot; 200 None&lt;Response [200]&gt;</code></pre><p>看我们的HTTP复用实现了，在同一个TCP连接中我们请求了两次</p><h2 id="深入requests的ConnectionPool"><a href="#深入requests的ConnectionPool" class="headerlink" title="深入requests的ConnectionPool"></a>深入<code>requests</code>的ConnectionPool</h2><p>在上面我们验证了<code>requests</code>的<code>Session</code>对象的确实现连接池，但是似乎<code>requests</code>并没有给我们接口来操作这个值，通过分析代码和资料，我们发现在<code>Session</code>初始化的时候，绑定了一个    <code>HTTPAdapter</code>对象，这个对象就是<code>requests</code>封装了<code>urllib3.connectionpool.ConnectionPool</code>来实现TCP池</p><p>我们查看这个<code>HTTPAdapter</code>文档发现它的用法是这个</p><pre><code>&gt;&gt;&gt; import requests&gt;&gt;&gt; s = requests.Session()&gt;&gt;&gt; a = requests.adapters.HTTPAdapter(max_retries=3)&gt;&gt;&gt; s.mount(&apos;http://&apos;, a)</code></pre><p>我们可以通过创建将一个TCP池绑定到一个<code>session</code>对象上，我们可以看一下这个创建一个<code>HTTPAdapter</code>的参数</p><pre><code>HTTPAdapter(self, pool_connections=10, pool_maxsize=10, max_retries=0, pool_block=False)</code></pre><p>我们主要看这两个参数<code>pool_connections</code>和<code>pool_maxsize</code>，通过一番测试（比较长就不演示了，可以参考引用来进行实验），我们发现这个<code>pool_connections</code>主要控制TCP池的种类数，我们知道在进程池中我们可以有很多相同的TCP连接（主要是并发新建的），这些连接有些是连接相同的域名和端口，这个<code>pool_connections</code>就是控制有多少种类的站点（域名和端口）同时能够存在池中，那么这个<code>pool_maxsize</code>代表的就是池中不管种类有多少总共的TCP连接数</p><p>假如你只写单线程程序那么你只要考虑<code>pool_connections</code>这个参数，因为单线程你发出一个请求只会占用一个TCP连接，在你每次开始请求时，池中不同站点的连接只有一种，所以你可以把<code>pool_connections</code>当做池的大小，但是假如你写多线程程序，每个时间点需要的TCP连接同你多线程的个数有关，由于requests不会限制当池中无可用连接时新建TCP连接，所以你一个站点的TCP连接可能有多个，这时我们就要用<code>pool_maxsize</code>来限制池子的容纳量，为了避免无限制存贮TCP连接，TCP连接池会把超过总数的连接按照时间顺序踢出去，让池中保持不大于限制总数的TCP连接。</p><p>当然这里有个非常重要的知识点，<strong>requests的TCP池并不会限制新建TCP连接</strong>,它只是限制存贮量和种类，这个知识点非常重要，这对后面我们理解<code>aiohttp</code>异步请求时候为什么要限制并发数有非常大的帮助（它只限制TCP连接总数）</p><h2 id="TCP连接池的作用"><a href="#TCP连接池的作用" class="headerlink" title="TCP连接池的作用"></a>TCP连接池的作用</h2><p>经过上面的探索，我们知道TCP连接池一方面能够实现HTTP复用达到减少TCP连接时耗的作用，另一方面我们通过复用TCP连接可以节省套接字，避免经常碰到”Too many file“的错误，顺便提一下，由于TCP连接具有冷启动的特点，在刚连接上TCP时，速度会非常慢，只有系统发现负载不多才会恢复正常速度，所以这就是我们有时候用浏览器打开一个新页面要加载很久的原因。</p><p>前面一直在介绍HTTP复用的理论基础，最后我们实战演练一下在异步框架<code>aiohttp</code>使用HTTP复用</p><h2 id="异步框架下HTTP复用"><a href="#异步框架下HTTP复用" class="headerlink" title="异步框架下HTTP复用"></a>异步框架下HTTP复用</h2><p>在这里我们使用<code>Python</code>的<code>aiohttp</code>异步请求框架（在这里我们要求<code>Python</code>的版本必须大于等于3.5），<code>aiohttp</code>也提供了<code>TCP</code>连接池的功能，要想共享TCP连接池，我们先新建一个<code>Session</code>对象</p><pre><code>connector = aiohttp.TCPConnector(limit=50)session = aiohttp.ClientSession(connector=connector)</code></pre><p>我们直接创建了一个最大容量为50的TCP池，并把它绑定到session对象上，接下来先试试跑个200个请求（要先在按照前面的代码搭建本地服务器）</p><pre><code>async def fetch(url, session, semaphore):    async with semaphore:        async with session.get(url) as response:            print(await response.read())loop = asyncio.get_event_loop()</code></pre><p>接下来我们就可以直接使用aiohttp框架</p><pre><code>semaphore = asyncio.Semaphore(20)tasks = [fetch(url, session, semaphore) for x in range(nums)]begin = time.time()try:    loop = asyncio.get_event_loop()    loop.run_until_complete(asyncio.wait(tasks))except:    passfinally:    end = time.time()    loop.close()    session.close()    print(&apos;cost&apos;, end - begin, &apos;speed&apos;, nums / (end - begin), &apos;req/s&apos;)</code></pre><p>在我的电脑测试测试下<code>421.73 req/s</code>,基本上达到异步的效率（可以调节limit至100左右达到最大）</p><p>在这里解释一下为什么要使用<code>semaphore</code>（asyncio锁），由于当前版本（aiohttp==2.2.5）下<code>aiohttp</code>的HTTP连接池无法在没有锁的情况下复用TCP连接（具体可以看一下我提的这个<a href="https://github.com/aio-libs/aiohttp/issues/2323" target="_blank" rel="noopener">issue</a>,这里由于牵扯到太多异步框架的知识，我就详细不介绍异步库，如果想了解更多的话就看我上一片博文<a href="/2017/10/09/Python%E5%BC%82%E6%AD%A5%E7%9A%84%E7%90%86%E8%A7%A3/">Python异步的理解</a></p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>在如何提高请求效率和速度上，HTTP复用算是从协议出发上的一种优化，他主要利用方向是在单个站点多次请求上面，假如每个站点都只是一个请求的话，那他就无用武之地，不过现在站点不可能一次请求就完成交互，所以了解这个HTTP复用如何是非常有帮助的。</p><p>引用：</p><p><a href="https://laike9m.com/blog/requests-secret-pool_connections-and-pool_maxsize,89/" target="_blank" rel="noopener">Requests’ secret: pool_connections and pool_maxsize</a></p><p><a href="https://pawelmhm.github.io/asyncio/python/aiohttp/2016/04/22/asyncio-aiohttp.html" target="_blank" rel="noopener">Making 1 million requests with python-aiohttp</a></p>]]></content>
      
      
      <categories>
          
          <category> 软件 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> HTTP </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Python异步的理解</title>
      <link href="2017/10/09/python/Python%E5%BC%82%E6%AD%A5%E7%9A%84%E7%90%86%E8%A7%A3/"/>
      <url>2017/10/09/python/Python%E5%BC%82%E6%AD%A5%E7%9A%84%E7%90%86%E8%A7%A3/</url>
      
        <content type="html"><![CDATA[<h2 id="起因"><a href="#起因" class="headerlink" title="起因"></a>起因</h2><blockquote><p>异步的出现主要是单线程的io等待，由于任务大部分是io处于等待，假如让一个线程工作，所有任务按照流水线形式执行，假如一个请求需要1秒，五个请求需要五秒，那么如果能让他们同时运行的话，那么速度就能增加五倍</p></blockquote><p>如何让五个任务同时进行有两种方法</p><ol><li>多线程</li><li>异步</li></ol><p>调试过过多线程的人都知道，线程就是从头到复制主线程一遍，开多个线程不仅成本高，而且调试成本高，异步就不一样呢，你可以把它当做一个单线程来进行编程，而且比多线程更加高效</p><h2 id="Python异步的多种实现"><a href="#Python异步的多种实现" class="headerlink" title="Python异步的多种实现"></a>Python异步的多种实现</h2><p>Python实现异步的框架有很多，但是核心思想大概是基于下面两种方式</p><ul><li>twister</li><li>gevent</li></ul><p>twister思想是将异步操作封装起来，通过回调的方式来操作，我们看<code>scrapy</code>里面中间请求的实现就是<code>twister</code>方式</p><pre><code>scrapy.Request(url=&apos;xxx&apos;, callback=func)</code></pre><p>通过传递封装的<code>request</code>，当框架帮我们请求完后，会通过callback进行回调，如果你的请求很简单那还好，只需要回调一次就可以，假如你的请求较复杂，那么你就会进入<code>回调地狱（callback hell）</code></p><p>而且你还要写处理各种回调产生的异常，你可以看看<code>scrapy</code>中间件的实现就知道<code>scrapy</code>的异常处理有多繁琐了。但是中间间的存在的确让我们代码模块话更加容易，这里暂且不谈。</p><p><code>twister</code>这种回调比较反人类，它必须依赖背后的核心进行调度，离开了背后核心的支持，这个根本跑不起来，而且由于它依赖回调来进行后续步骤处理，所以我们的代码必须被切分为不同的部分，假如我们不知道背后的核心如何回调函数或者约束，我们根本不知道这两个函数是有关联的</p><p>这种编程方式比较有利于模块话开发，但是对于我们熟悉顺序编程来看，这种回调方式显然是一场噩梦，相比于<code>twister</code>这种回调方式，<code>gevent</code>采用的是绿色协程的方式进行回调。</p><p>PEP-380定义了<code>yield from</code>的语句，Python3.3开始使用，为了区别协程和生成器，Python3.5开始使用<code>await</code>代替<code>yield from</code>，这样协程就有了一个专门的方法来声明（<code>await</code>和<code>async</code>），后者用来标记异步函数</p><p>协程之所以能够在异步中大方光彩，其中很大一部分就是协程天生就是异步的，理解协程我们可以从一个简单的生成器与普通函数来对比</p><pre><code>a = (x for x in range(10))b = [x for x in range(10))</code></pre><p>我们来看这样一个生成器a，一般我们来用这个生成器必须加    <code>for</code>循环才能得到里面的值，假如我们尝试使用<code>a.send(None)</code>，我们会发现，我们依次从返回值得到了b里面的序列</p><p>就是这么一个send与接受的功能让我们实现了一种”绿色“回调，就是协程这个性质让他写异步变得更加顺理成章了，而且相比<code>twister</code>回调，协程的回调更为彻底，它把”自己”包装起来全部回调回去了。</p><h2 id="了解异步基础"><a href="#了解异步基础" class="headerlink" title="了解异步基础"></a>了解异步基础</h2><p>前面简单的聊了协程的性质，现在谈谈异步存在的基础，异步的存在最关键的在于等待，为了了解这个等待意思和后面解读<code>asycio</code>库，我们先使用<code>selectors</code> （Python3对<code>select</code>的封装）来做个演示</p><pre><code>import selectorssel = selectors.DefaultSelector()</code></pre><p>声明一个<code>select</code>对象sel，现在我们要调用这个核心函数</p><pre><code>sel.select(10)</code></pre><p>这个10是代表<code>timeout</code>的时长，也就是最长等待时间，10秒之后我们发现，这个结果返回了一个空列表，这是显而易见的，我们并没有指明让它等待什么</p><p><code>selectors</code>这个库的功能非常好理解，类比寄信，你如果想等别人回信，假如你没有寄出去你自己的信，你一直在邮箱那等，除了等到你不想等，否则你是收不到你的回信的，所以这个库的核心在于，“寄信”（register）和等信（select），然后自己选择处理信件</p><pre><code>import selectorsimport socketsel = selectors.DefaultSelector()def accept(sock, mask):    conn, addr = sock.accept()     print(&apos;accepted&apos;, conn, &apos;from&apos;, addr)sock = socket.socket()sock.bind((&apos;localhost&apos;, 8000))sock.listen(100)sock.setblocking(False)sel.register(sock, selectors.EVENT_READ, accept)while True:    events = sel.select()    for key, mask in events:        callback = key.data        callback(key.fileobj, mask)</code></pre><p>这个程序最关键的地方在于<code>sel.register</code>、<code>sel.select</code>和<code>callback</code>那里，前者是注册函数，后面是等待，最后就是回调</p><p>上面就是<code>twister</code>式最简单的回调，你可以看到，为了得到连接<code>sock</code>的连接，我们必须把处理注册到等待中去，但是这只是得到<code>sock</code>连接，为了成功建立一个<code>TCP</code>连接，我们还得进行三次握手，还得处理每次回调时的错误</p><p>而且你可以看到回调函数与核心驱动<code>select.select()</code>耦合度非常高，我们必须完全了解系统如何回调，处理一件事被回调分割成一段一段</p><p>接下来我们来看看基于<code>gevent</code>的<code>asyncio</code>实现</p><pre><code>async def wget(host):    connect = asyncio.open_connection(host, 80)    reader, writer = await connect    header = &apos;GET / HTTP/1.0\r\nHost: %s\r\n\r\n&apos; % host    writer.write(header.encode(&apos;utf-8&apos;))    await writer.drain()    while True:        line = await reader.readline()        if line == b&apos;\r\n&apos;:            break    writer.close()</code></pre><p>我们成功的用一个函数描绘了建立一次连接并且进行通信的过程，假如你懂一点<code>asyncio</code>，你就会发现它与<code>twister</code>回调的不同，使用<code>await</code>关键字把函数挂起，然后等待回调，根据回调接着进行下面的操作，我们成功的用同步的语句把异步写出来，而且是使用Python的原生实现，所以当<code>asyncio</code>出来的时候Guido（Python之父）是多么自豪，你可以看下面引用 <a href="https://www.youtube.com/watch?v=1coLC-MUCJc" target="_blank" rel="noopener">Tulip: Async I/O for Python 3</a>演讲的视频</p><h2 id="浅析Python异步实现"><a href="#浅析Python异步实现" class="headerlink" title="浅析Python异步实现"></a>浅析Python异步实现</h2><p>前面我们知道了异步的基础就是等待，那么Guido是如何在协程的帮助下将异步实现出来的呢，接下来我们就简单的谈一下这个实现基础</p><p>我们先将上面<code>twister</code>改成<code>gevent</code>方式的</p><pre><code>sel = selectors.DefaultSelector()@asyncio.coroutinedef get_connection(sock):    sel.register(sock, selectors.EVENT_READ)    yield Trueasync def create_connection():    sock = socket.socket()    sock.bind((&apos;localhost&apos;, 8000))    sock.listen(100)    sock.setblocking(False)    await get_connection(sock)    conn, addr = sock.accept()    print(&apos;accepted&apos;, conn, &apos;from&apos;, addr)event = create_connection()event.send(None)events = sel.select(100)for key, mask in events:    try:        event.send(None)    except StopIteration:        pass</code></pre><p>我们稍稍修改一下上面的<code>twister</code>函数，我们创建一个<code>get_connection</code>函数把<code>sock</code>绑定到我们的<code>sel</code>上面，然后回调一个<code>True</code>，当然这个回调没有处理异常什么的，然后我们将得到的协程向其发送一个<code>None</code>让它启动，这时候你在在另外一个<code>ipython</code>客户端执行</p><pre><code>import socketsocket.socket()..connect((&apos;localhost&apos;, 8000))</code></pre><p>然后你就会发现在主线程里面打印出来客户端的连接信息</p><p>通过这个小例子我们知道，实现异步要解决的问题就是一个公用注册器（能够注册所以的io等待），一个容器（能够存贮所以的协程），一个核心能够一直执行等待回调和处理回调（多个协程）</p><h2 id="深入asyncio了解Python异步"><a href="#深入asyncio了解Python异步" class="headerlink" title="深入asyncio了解Python异步"></a>深入asyncio了解Python异步</h2><p>通过上面我们简单的知道了，如何通过协程与<code>select</code>合作完成异步操作，然而我们上面写的只是最最最基本的实现，接下来我们来深入<code>asyncio</code>源码了解如何让异步变得更加简单</p><h3 id="引用"><a href="#引用" class="headerlink" title="引用"></a>引用</h3><p><a href="http://blog.csdn.net/screaming/article/details/51377870" target="_blank" rel="noopener">Python异步并发框架</a></p><p><a href="http://python.jobbole.com/87988/" target="_blank" rel="noopener">Python 中的异步编程：Asyncio</a><br><a href="https://www.youtube.com/watch?v=1coLC-MUCJc" target="_blank" rel="noopener">Tulip: Async I/O for Python 3</a><br><a href="https://xidui.github.io/2015/10/29/%E6%B7%B1%E5%85%A5%E7%90%86%E8%A7%A3python3-4-Asyncio%E5%BA%93%E4%B8%8ENode-js%E7%9A%84%E5%BC%82%E6%AD%A5IO%E6%9C%BA%E5%88%B6/" target="_blank" rel="noopener">【译】深入理解python3.4中Asyncio库与Node.js的异步IO机制</a></p>]]></content>
      
      
      <categories>
          
          <category> Python </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>PostgreSQL的自增键</title>
      <link href="2017/10/08/software/PostgreSQL%E7%9A%84%E8%87%AA%E5%A2%9E%E9%94%AE/"/>
      <url>2017/10/08/software/PostgreSQL%E7%9A%84%E8%87%AA%E5%A2%9E%E9%94%AE/</url>
      
        <content type="html"><![CDATA[<blockquote><p>平常在Django项目中大量使用自增这个键，平常都是使用ORM，很少去了解这个东西在数据库中具体使用，最近遇到要备份和复原数据的事情，趁着这次好好探索一下这个自增键的使用</p></blockquote><p><code>Django</code>里面大部分都是将其作为<code>Int</code>自增主键来使用，第一个不需要维护一个唯一值，第二个使用<code>Int</code>作为主键的话，搜索和外键关联速度比较快。</p><p>我们这次从原生<code>SQL</code>出发，探索一下这个自增主键在数据库中的具体使用</p><h3 id="新建数据库"><a href="#新建数据库" class="headerlink" title="新建数据库"></a>新建数据库</h3><p>我们先新建一个数据库</p><pre><code>create table inc(id serial not null,name text);</code></pre><p>在<code>PG</code>里面简单的使用<code>serial</code>关键字就会生成一个自增键，默认会在数据库新建一个索引表，例如上面就会新建一个<code>inc_id_seq</code>的索引表，这个字段类型为<code>int</code>，如果数据库很大，我们可以使用<code>BIGSERIAL</code>键申请一个<code>bigint</code>类型的字段</p><p>我们可以看一下这个索引表里面有什么</p><pre><code>         Sequence &quot;public.inc_id_seq&quot;    Column     |  Type   |        Value        ---------------+---------+--------------------- sequence_name | name    | inc_id_seq last_value    | bigint  | 1 start_value   | bigint  | 1 increment_by  | bigint  | 1 max_value     | bigint  | 9223372036854775807 min_value     | bigint  | 1 cache_value   | bigint  | 1 log_cnt       | bigint  | 30 is_cycled     | boolean | f is_called     | boolean | t</code></pre><p>我们可以这个索引表其实就是维护了一个参数，通过字段我们可以知道，这是一个自增为1的键，下一个值为2，目前没有插入一个值</p><h3 id="增删查减"><a href="#增删查减" class="headerlink" title="增删查减"></a>增删查减</h3><p>我们通过一些基本操作来看看这个自增键的作用</p><ul><li>首先是插入</li></ul><pre><code>insert into inc (name) values (&apos;1&apos;),(&apos;2&apos;), (&apos;3&apos;);</code></pre><p>我们插入三个值，我们再查看索引表，发现<code>last_value</code>变成了3</p><p>这个是没有指定<code>id</code>的值插入，我们试试显式声明插入</p><pre><code>insert into inc  values (1, &apos;1&apos;),(2, &apos;2&apos;), (3, &apos;3&apos;), (4, &apos;4&apos;);</code></pre><p>我们惊奇的发现，在我们显式声明自增键的值的时候，索引表并没有变化，<code>last_value</code>还是3,<strong>这说明只有在不声明自增键，让数据库自己新建的时候，索引表才会更新</strong></p><p><strong>我们可以把自增键看做一个默认值，当没有给自增键赋值的时候，这个自增键会从这个键的索引表中得到下一次自增的值</strong></p><p>所以我们再尝试使用不声明自增键值的方法插入一个新值</p><pre><code>insert into inc (name) values (&apos;4&apos;)</code></pre><p>我们发现索引表中<code>last_value</code>变成了4</p><h3 id="主键自增"><a href="#主键自增" class="headerlink" title="主键自增"></a>主键自增</h3><p>由于我们在<code>Django</code>里面使用自增，一般都是将其声明为主键，设为唯一值，所以如果我们将声明表的结构变成</p><pre><code>    create table inc(id serial not null PRIMARY KEY,name text    );</code></pre><p>上面的情况就不可能发生了，因为我们把自增键声明为主键，不过有意思的事就是如果你像上面一样指定了一个自增主键值为4，然后不指定再插入4，你会发现第一次会报主键不允许重复的错误，第二次则会成功插入，而且索引表的<code>last_value</code>变成了5</p><p>看来并不是每次成功的时候才会更新<code>last_value</code>值，只要让系统自己去申请自增值就会更新索引表，我尝试了对表的增删查改，发现只有<code>insert</code>并且申请自增值的时候才会更新索引表，而且这个索引表之后增加，不会减少，所以有时候你删掉最大的值，自增键默认又从最后一次开始更新</p><h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p>在对单个数据表备份还原的时候，由于简单的使用了<code>COPY</code>命令进行备份还原，通过上面的探索我们发现如果涉及到自增主键的导入导出，在新表导入旧数据是不会出错的，但是由于我们没有考虑自增键的影响（我们导入自增键是显示赋值），在后面插入数据的时候有可能会报主键重复的错误</p><p>为了避免以后插入入数据出现这样的错误，我们有两种措施</p><ol><li>使用<code>COPY</code>命令导入导出时候不获取自增键值</li><li>在<code>COPY</code>导入新表后自己更新索引表</li></ol><p>第一种的话<code>SQL</code>比较繁琐（必须写出表所有字段值），推荐使用第二种</p><p>我们可以简单的使用</p><pre><code>SELECT MAX(id) FROM your_table;</code></pre><p>先获取自增键最大值，然后更新索引值(999为上面你获取的最大值）</p><pre><code>SELECT setval(&apos;your_table_id_seq&apos;, 999, false);</code></pre><p>当然我们可以将这条语句合正一句话</p><pre><code>SELECT setval(&apos;your_table_id_seq&apos;, COALESCE((SELECT MAX(id)+1 FROM your_table), 1), false);</code></pre><p>这样我们就可以开心的完成单表导入导出了</p><p>ps：<br>在使用<code>COPY</code>命令时必须是superuser才能从文件中读取和导入数据，最简单的方法是用superuser账号加权使用<code>alter user xxx superuser</code>，待倒完数据后再降权<code>alter user xxx nosuperuser</code></p>]]></content>
      
      
      <categories>
          
          <category> 软件 </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>大学的错觉</title>
      <link href="2017/10/07/summary/my_college_mistakes/"/>
      <url>2017/10/07/summary/my_college_mistakes/</url>
      
        <content type="html"><![CDATA[<blockquote><p>其实这篇文章很早就像写了，但是自己也一直没有明白自己想写什么，直到最近自己慢慢才有一点思路</p></blockquote><p>   这篇文章并不想高谈阔论，只是自己的一些碎碎念，把自己对人生的一些看法的小总结。你可以把它当做一篇小说来看，我也想把它当做小说来写。</p><h2 id="引子"><a href="#引子" class="headerlink" title="引子"></a>引子</h2><p>中秋回家，同自己表妹聊天，她突然问了一句,大学哥哥没有谈一个女朋友，接着说道没有恋爱的大学是不完整的，我楞了一愣，一本正经的对她说道，大学其实就是培养自己一个完整人格的过程，在这个过程中我们学习并且养成自己一个完整独立认知。</p><p>回去之后我仔细想想，我好像并没有回答表妹的问题，但是我自己也陷入了深思，大学这几年到底对我干了什么。</p><h2 id="错觉"><a href="#错觉" class="headerlink" title="错觉"></a>错觉</h2><p>先不说大学对我做了什么，我仔细想了想我自己对自己定位。</p><h3 id="你觉得你很努力"><a href="#你觉得你很努力" class="headerlink" title="你觉得你很努力?"></a>你觉得你很努力?</h3><p>我一直觉得自己在大学还是很努力的，我没有沉迷游戏超过两天，我没有放弃学习新知识，在大学图书馆借了几百本书，可能比全班人加起来都多，大一到大三我经常去图书馆读书，尤其是大三，有时候会在图书馆读一天书</p><p><strong>但是上面的上面全部我自己的错觉，我虽然不沉迷游戏但是经常会被游戏分心，我虽然读过很多书但是我一直是读那些别人认为是必读的书，而且我读过的书大部分都没有转换成为我真正的源泉</strong></p><p>我就像一个饥渴的行者，在大河面前用手拼命的往口里塞水，我的确看了很多的书，但是这么多书就像流水一样，全部都流走了;这些知识对于我来说只是解渴之物，当我非常饥渴的时候，我会拼命的想得到它，但是当我满足的时候，这些东西就像泥土一样对我一文不值</p><p>所以这就可以解释为什么我每次借书的时候都是兴高采烈，但是当借回来时候往往翻了几页，然后就束之高阁，然后循环往复</p><h3 id="你觉得你懂很多"><a href="#你觉得你懂很多" class="headerlink" title="你觉得你懂很多?"></a>你觉得你懂很多?</h3><p>没有出去之前，我在大学社团里面干过不少项目，所以我有时候觉得自己技术很牛逼，我是大神级别的人。我懂很多别人不知道的知识，我用过很多框架，我知道怎么搭集群，我知道什么是机器学习，什么是分布式，什么是代码规范。</p><p>然而出去之后才发现这都是错觉，你做过很多项目，你经历过百万规模的并发吗，你了解很多框架，随便挑一个框架出来，你能说出它的优点和不足吗，你看过源代码吗，你知道如何保证上千集群的容错率，你知道什么是大规模机器学习吗</p><p>挑出任何一个你会发现自己一直处在皮毛阶段，有时候你会用你还年轻但是学习能力强来掩饰你的不足，但是这只是你的错觉吧，不懂就是不懂。</p><h3 id="你觉得你不需要明确方向"><a href="#你觉得你不需要明确方向" class="headerlink" title="你觉得你不需要明确方向?"></a>你觉得你不需要明确方向?</h3><p>有段时间我一直很困惑自己未来发展方向，我搞过UI，搞过前端、后端，搞过机器学习、数据分析，搞过分布式、爬虫。编程语言更是多，C、C#、Python、Java、C++，node，JS。我对我自己的定位一直很模糊，我不知道我未来到底想干什么，我很羡慕那些从小就明确目标的人。</p><p>我一直为此苦恼，我也看过很多人的书籍、博客，我也看到过很多人写的相同的文章，在很长一段时间我都认为它是正确的，它告诉我你不需要明确你的职业规划，它给了很多有名的人例子，奥巴马、马云、马化腾、李开复他们在大学都不知道自己要干什么。在很长一段时间我都觉得大学就是应该多学东西，把东西学杂。</p><p>但是我仔细想想，这个也是<strong>错觉</strong>。</p><p>我从大二就开始有转行的念头，当时我是web后端开发，我当时觉得有没有方向无所谓，只要你多学就行，就这样陆陆续续混杂看了一年多书，直到七八个月前，我才开始反思。</p><p>我开始明确我的目标，把它当做我要干一辈子的方向去搞，我开始扣书，像一个干涸的大地一样汲取天空飘下的雨滴，我这时候发现知识是那么的宝贵，自己是多么的“native”；从一窍不通到入门到小小成就只花了短短几个月的时间，完成了一年多都完成不了的入门。</p><p>当然我最终没有选择这个方向，但是这个过程我从来没有后悔过，而且在这个学习过程中，它帮助我更加了解我自己，而且节省了我选择的时间。</p><h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p>好像我的大学一直全部都是由错误组成的，但是这些错误真的对我来说毫无作用吗。其实未必，当我学完第一门编程语言C的时候，所以的“错误”都在默默的发挥的作用，我用静态语言的辩证思想学Python，我用动态语言的思想反过来学习Java，好像全部的“错误”全部融合成为一个圈，</p><p><strong>我们好像一直在害怕自己出错，其实慢慢的发现那些没有错误的人生不是完整的。</strong></p><p>小时候很羡慕那些一直走在正确的道路上的人，也有时候会幻想成为他们一样的人。没错那样的人生固然完美，但是我更喜欢一直跌跌撞撞的我，或许我经常走在错误的道路上，但是我享受了沿途的风景，不论最终结果如何，人生的意义还是沿途的风景吧!</p>]]></content>
      
      
      <categories>
          
          <category> 随想 </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>唯品会预测和携程房屋预测总结</title>
      <link href="2017/08/09/ai/competition/vip_ctrip_competitions_summary/"/>
      <url>2017/08/09/ai/competition/vip_ctrip_competitions_summary/</url>
      
        <content type="html"><![CDATA[<blockquote><p>最近打了两个比赛，一直忙着工作和打比赛，没有时间总结，今天抽空好好总结一番</p></blockquote><p>先说一下比赛结果吧，队名全为<code>OfferGo</code><a href="https://www.datafountain.cn/competitions/260/details" target="_blank" rel="noopener">唯品会购买预测</a>第五名，<a href="https://www.kesci.com/apps/home/#!/competition/597172316a1cd9104c2df248/leaderboard/0" target="_blank" rel="noopener">携程房屋预测复赛</a>第六名,两个比赛打的都不算太好,只能算勉强及格,虽然离大神的距离还有十万八千米，不过总算可以称的上入了门，现在来总结一下我入门的经验吧。</p><h2 id="观察数据"><a href="#观察数据" class="headerlink" title="观察数据"></a>观察数据</h2><blockquote><p>我参加过很多群,发现很多新手缺乏观察数据的能力,他们每次进入一个群总是嚷嚷这让大神发<code>baseline</code></p></blockquote><p>这一点对于新手来说很不利的，比赛考的就是你对数据的掌握能力,你对数据把握的越好，你的比赛成绩就越好，要真正掌握数据就要从观察数据入手</p><hr><p>在我看来观察数据主要从四个方面来，我总结为<code>望闻问切</code></p><h3 id="望"><a href="#望" class="headerlink" title="望"></a>望</h3><p>观察数据缺失值，缺失值对数据影响很大，有时候我们能够从缺失值里面了解很多信息,而且对于缺失值，后期我们对不同的缺失值要采取不同的手段，比如补全、统计占比、丢弃等等。</p><p>对于缺失值我一般从两个方面来观察</p><ul><li>全局观察</li><li><ul><li>一般采用<code>datafram.info(null_counts=True, verbose=True)</code>方法来观察全局数据缺失情况</li></ul></li><li>局部观察</li><li><ul><li>一般采用<code>series.isnull().count()</code>和<code>series.loc[series.notnull()]</code>观察单一列表缺失情况</li></ul></li></ul><p>这个阶段我们主要从大的方向远远的<code>望</code>一下数据，主要建立对数据的全局观。</p><h3 id="闻"><a href="#闻" class="headerlink" title="闻"></a>闻</h3><blockquote><p>对于数据来说，一般分为三种，一种为数值型数据（整数、浮点数、时间等），一种为字符型，最后一种为图像型，三种类型数据处理难度依次增强</p></blockquote><p>对于大多数比赛都是设计前两种数据，第三种只有牵扯到图像处理才能遇到。对于前两种数据，我们在闻的阶段，主要是探查数据分布情况，了解数据分布情况，我们才能对症下药。</p><p>了解数据分布情况有两种方法</p><ol><li>图像观察</li><li>数学统计观察</li></ol><p>图像观察主要使用<code>Pandas</code>的<code>Matplotlib</code>绘图接口，或者使用<code>seaborn</code>（一个友好的封装了<code>Matplotlib</code>的包），一般我们可以从直方图、饼图、频率图方向来观察数据</p><p>数学统计我们主要采用<code>Pandas</code>的<code>describe</code>方法，对于数值型数据，主要从平均值（<code>mean</code>）、中位数（<code>50%</code>）、标准差（<code>std</code>）、最大（<code>max</code>）、最小（<code>min</code>）、非空总数（<code>count</code>）来探测数据，对于字符型我们主要从最频繁的值（<code>top</code>）、最频繁的值的个数（<code>freq</code>），非空总数（<code>count</code>）、不相同的值（<code>unique</code>）。</p><p>通过上面两种方法，我们能够从数据分布的角度大致勾画出数据的轮廓。</p><h3 id="问"><a href="#问" class="headerlink" title="问"></a>问</h3><blockquote><p>比赛的目的就是找到最优解，而最优解的跟相关特征紧密联系的，你的特征对结果影响越大，你就要审<code>问</code>这个特征</p></blockquote><p>举个例子，我们要预测三组数据</p><ul><li>1 1.8  2</li><li>2 3.5  4</li><li>3  5.4  16</li></ul><p>第一行为我们要训练的值，我们发现第二行的数据是第一行的1.8倍，而第三行只是2的次方，对于这两个特征来说，第二个特征就是最好的特征，我们只要建立一个映射，准确率能接近100%，而第三个特征对预测结果毫无联系，这个特征不但对结果没有作用，而且有时候会起到反作用</p><p>当然在这里我们举这么一个例子在实际中不可能遇到，我们遇到是更多数据，而数据之间的联系并不是这么简单的线性关系，但是线性关系有的时候能让我察觉到特征与预测值的关联，毕竟如果特征值是随机值那么与预测值之间的相关性是非常低的。</p><p>在<code>Python</code>里面探测线性关系最简单的方法是调用特征值和预测值的相关性系数（<code>corr</code>），我们可以简单的使用<code>df[[&#39;feature&#39;, &#39;target&#39;]].corr()</code>就可以得到线性相关系数，这个数的绝对值越接近1，相关性越大，一般来说相关性越大和越好对结果都不好，最好的特征相关性处于中间位置。</p><p>相关性低我们可以理解，为什么相关性高反而不好呢，因为数据比赛里面给我们的数据大部分都是不平衡的数据，正负样本失衡，一般相关性很高的值一般为分类同预测值相同，比如一个二分类问题，预测值为0和1，给的样本正负比为1000:1，那么如果有一个特征全为0或者其他，那么他与预测值的相关性会达到90%以上，然而这个值是毫无作用的。</p><p>所以我们通过简单的相关系数并不能很好的观察特征真正相关性，一般我们要辅助图像法和统计法。</p><p>图像法就是通过将特征值分布与预测值相关性图表画在同一个图表里，具体可以参考<a href="https://www.kaggle.com/benhamner/d/uciml/iris/python-data-visualizations" target="_blank" rel="noopener">可视化特征</a></p><p>统计法类似图标，使用统计方法观察，特征值与预测值的相关性，一般使用<code>groupby</code>方法对两个特征进行统计就可以进行简单的观察</p><p><code>问</code>只是一个简单的手段，一般我们在大量添加特征的时候，为了节省模型训练时间，在将特征放入管道之前进行一个简单的过滤删除的工作，真正重要的步骤在<code>切</code>这个方面</p><h3 id="切"><a href="#切" class="headerlink" title="切"></a>切</h3><blockquote><p><code>切</code>这个步骤放在最后是因为，这个步骤也是我们一趟循环下来的最后一步</p></blockquote><p>数据比赛中前期大家最喜欢用的模型是树模型，比如随机森林、<code>Xgboost</code>，<code>LightBoost</code>等，这些模型属于弱学习器组合模型，我们最后可以从训练结果得到每个特征在模型占的比重</p><p>对于这个比重，是非常重要的，他代表了每个特征对应在模型中占的权重，也可以理解特征与结果的相关性</p><p>对于相关性很强的不同的特征，我们可以将他们组合，有时候这种强强组合生成出来的特征会比原来母特征相关性更强，当然组合的方法有千种万种，如何验证他们有效就要从头开始对数据进行<code>望闻问切</code>了</p><h4 id="总结：-数据比赛就如同问诊，我们不断对特征进行望闻问切，对于高手来说他们能很快的从原始特征中挑选出病根，对症下药，而新手的话，一阵摸瞎，经常会碰到在比赛中期做出一个很好的结果，接下来很长一段时间都没有进步的情况。掌握科学有效的挑选特征方法需要一个“医者心”，必须学会对特征“负责”，要学会望闻问切。"><a href="#总结：-数据比赛就如同问诊，我们不断对特征进行望闻问切，对于高手来说他们能很快的从原始特征中挑选出病根，对症下药，而新手的话，一阵摸瞎，经常会碰到在比赛中期做出一个很好的结果，接下来很长一段时间都没有进步的情况。掌握科学有效的挑选特征方法需要一个“医者心”，必须学会对特征“负责”，要学会望闻问切。" class="headerlink" title="总结： 数据比赛就如同问诊，我们不断对特征进行望闻问切，对于高手来说他们能很快的从原始特征中挑选出病根，对症下药，而新手的话，一阵摸瞎，经常会碰到在比赛中期做出一个很好的结果，接下来很长一段时间都没有进步的情况。掌握科学有效的挑选特征方法需要一个“医者心”，必须学会对特征“负责”，要学会望闻问切。"></a>总结： 数据比赛就如同问诊，我们不断对特征进行<code>望闻问切</code>，对于高手来说他们能很快的从原始特征中挑选出<code>病根</code>，对症下药，而新手的话，一阵摸瞎，经常会碰到在比赛中期做出一个很好的结果，接下来很长一段时间都没有进步的情况。掌握科学有效的挑选特征方法需要一个“医者心”，必须学会对特征“负责”，要学会<code>望闻问切</code>。</h4><h2 id="并行化算法"><a href="#并行化算法" class="headerlink" title="并行化算法"></a>并行化算法</h2><blockquote><p>由于<code>Python</code>本身对多核利用不好，如何利用多核加快特征生成对于比赛来说意义重大</p></blockquote><p>就拿我来举例子，我每天下班打比赛的时间不超过8个小时，前期算法没有并行化的时候，走一遍管道要四个小时，这意味着我一天只能跑两次，而进行并行化优化以后，我跑一遍四线程全开（笔记本双核四线程）只要十分钟就能跑完，每次生成新特征只有10分钟就能拿到特征相关性数据，来验证特征的好坏。</p><p>下面我从三个方面来谈谈怎么实现并行话算法</p><h3 id="1-使用系统自带函数，拒绝for循环"><a href="#1-使用系统自带函数，拒绝for循环" class="headerlink" title="1 . 使用系统自带函数，拒绝for循环"></a>1 . 使用系统自带函数，拒绝<code>for</code>循环</h3><p>举个例子，作为新手，实现对两个个特征求平均，一般采用<code>for</code>循环将每一行两个特征值加起来然后除以2，假如有1000万行，每行加法和除法运算花0.001ms，那1000万也要10秒钟，只是进行一个最简单的求平均，你就花掉10秒钟，上百个特征你得运行几天</p><p>学过矩阵的都知道，矩阵就是一种高效的并行化结构，它将集合统一进行计算，可能一个大矩阵运算要比单一计算要慢，但是单一计算要1000万次的话，大矩阵运算只需要两次就够了，这个效率比就出来了</p><p>而<code>Python</code>由于是一门解释性语言，比其他静态语音速度要慢许多，你一方面使用<code>for</code>循环加大运算次数，一方面执行一次时间长，这相重叠加你的算法会跑的比蜗牛还慢</p><p>所以我们避免使用我们写的函数，尽量使用库系统函数，因为库系统函数底层是使用<code>C</code>或<code>C++</code>实现的，而且他们在底层进行使用矩阵话运算代替单一浮点计算，我们使用库的函数（比如<code>mean</code>，<code>groupby</code>等）一方面能底层能使用C加快速度，一方面使用矩阵运算加快速度，两个叠加你的算法跑的比飞机还快。</p><h3 id="2-使用多进程，充分发挥使用多核性能"><a href="#2-使用多进程，充分发挥使用多核性能" class="headerlink" title="2 . 使用多进程，充分发挥使用多核性能"></a>2 . 使用多进程，充分发挥使用多核性能</h3><p>由于<code>Python</code>的<code>GIL</code>锁，使得<code>Python</code>无法利用多核进行计算，所以我们只能使用多个进程来充分利用多核</p><p>实现多进程有两个要点（具体可以参考我携程比赛代码 <a href="https://www.github.com/mrzhangboss" target="_blank" rel="noopener">Github地址</a>)</p><ul><li>特征提取模块化</li><li>进程池的搭建和维护</li></ul><p>我在携程比赛中的<code>mult_run.ipynb</code>中搭建了一个进程池，通过第三方调度和监控进程内存CPU等信息，达到充分“榨干”每个核的功效</p><h3 id="3-压缩数据，让矩阵运算更快"><a href="#3-压缩数据，让矩阵运算更快" class="headerlink" title="3 . 压缩数据，让矩阵运算更快"></a>3 . 压缩数据，让矩阵运算更快</h3><p>由于在对特征进行提取过程中，<code>Python</code>会自动将低位制值转换成高位制值，比如<code>float16</code>在进行一次<code>groupby</code>之后就会转换成<code>float64</code>，由于在矩阵运算时候，高进制值会占更多内存和运行时间，所以为了加快算法运行，我们要将其压缩，一方面节省内存，一方面能够让算法运行的更快</p><p>在携程的比赛中，原始数据有一个G，我将其压缩之后只占用300M内存空间，这为我后面在一台12G内存的笔记本实现并行化算法提供了巨大帮助，当然我每次在生成新特征的时候也会进行压缩，具体可以参考我携程的<code>utils.py</code>文件</p><h3 id="总结：-这两次比赛，我从菜鸟出发慢慢的从一个程序员变成了数据挖掘机，在模块化和并行化方面，我觉得我的进步不错，但是在数据特征挖掘方面我与大神之间的差距还是巨大的，这也是我止步于前五的主要原因，接下来我要加强对数据方向的锻炼，希望能够在工作和比赛之中得到更好的进步"><a href="#总结：-这两次比赛，我从菜鸟出发慢慢的从一个程序员变成了数据挖掘机，在模块化和并行化方面，我觉得我的进步不错，但是在数据特征挖掘方面我与大神之间的差距还是巨大的，这也是我止步于前五的主要原因，接下来我要加强对数据方向的锻炼，希望能够在工作和比赛之中得到更好的进步" class="headerlink" title="总结： 这两次比赛，我从菜鸟出发慢慢的从一个程序员变成了数据挖掘机，在模块化和并行化方面，我觉得我的进步不错，但是在数据特征挖掘方面我与大神之间的差距还是巨大的，这也是我止步于前五的主要原因，接下来我要加强对数据方向的锻炼，希望能够在工作和比赛之中得到更好的进步"></a>总结： 这两次比赛，我从菜鸟出发慢慢的从一个程序员变成了数据挖掘机，在模块化和并行化方面，我觉得我的进步不错，但是在数据特征挖掘方面我与大神之间的差距还是巨大的，这也是我止步于前五的主要原因，接下来我要加强对数据方向的锻炼，希望能够在工作和比赛之中得到更好的进步</h3><h4 id="在最下面贴一下我的携程比赛代码（基于Notebook）"><a href="#在最下面贴一下我的携程比赛代码（基于Notebook）" class="headerlink" title="在最下面贴一下我的携程比赛代码（基于Notebook）"></a>在最下面贴一下我的携程比赛代码（基于Notebook）</h4><p><a href="https://www.github.com/mrzhangboss/ctrip_room_predict" target="_blank" rel="noopener">https://www.github.com/mrzhangboss/ctrip_room_predict</a></p>]]></content>
      
      
      <categories>
          
          <category> 人工智能 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 比赛 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>如何搭建自动生成上万特征的管道</title>
      <link href="2017/06/13/ai/competition/%E5%A4%A7%E8%88%AA%E6%9D%AF%E2%80%9C%E6%99%BA%E9%80%A0%E6%89%AC%E4%B8%AD%E2%80%9D%E7%94%B5%E5%8A%9BAI%E5%A4%A7%E8%B5%9B%E5%8F%82%E8%B5%9B%E7%BB%8F%E9%AA%8C/"/>
      <url>2017/06/13/ai/competition/%E5%A4%A7%E8%88%AA%E6%9D%AF%E2%80%9C%E6%99%BA%E9%80%A0%E6%89%AC%E4%B8%AD%E2%80%9D%E7%94%B5%E5%8A%9BAI%E5%A4%A7%E8%B5%9B%E5%8F%82%E8%B5%9B%E7%BB%8F%E9%AA%8C/</url>
      
        <content type="html"><![CDATA[<p>基于:大航杯“智造扬中”电力AI大赛参赛经验</p><p><img src="/images/electric_power_ai.jpg" alt="大航杯AI大赛"></p><p>赛题背景</p><blockquote><p> 主办方为大航集团提供21个月江苏省杨中市1454家企业日用电量,来估计下一个月日总用电量</p></blockquote><p>从给的数据分析,这次给的数据只有历史企业日用电量,用来估计日总用电量,是一个典型的时域分析问题</p><p>但是这同我们以往的时序问题不一样,向往常时序问题预测的是每个企业的未来每日的用电量,而这个比赛却是求全部企业的总数.</p><p>由于我报名比赛时候比较晚,比赛已经接近尾声,比赛5月18号开始,6月8号中午切换数据,13号截止,我6月8号晚上下载数据,由于我以前已经做了几个类似的比赛,但是一直没有系统的做一个,抱着锻炼的自己的态度,决定系统做一次,权当练手.</p><p>首先分析一下提交的结果,预测一个月的日总用电量,总共为31个数据,给的历史数据只有21个月的,按月的比例来看,只有21个值去训练值去预测一个值,根据往常的比赛经验来看,这种比赛适合使用规则方法来做,然而我剩下的验证机会不多了,只能用模型,但是过拟合的危险非常大,如果不能找到一个好的方法克服过拟合,复赛都进不去.</p><p>当然最后还是没有找到一个很好的办法,止步于复赛,不过这次比赛让我学到很多,主要通过这次比赛自己琢磨出来自己如何搭建基于<code>IPython Notebook</code>的管道结构,这个管道帮我自动生成上万特征.</p><h2 id="管道"><a href="#管道" class="headerlink" title="管道"></a>管道</h2><blockquote><p>什么是管道,在数据挖掘比赛中很多大神都着重讲了一定要搭建一个自动化的架构,我们暂且称他为”管道”,这个”管道”我们要能够把数据倒进去,结果倒出来.</p></blockquote><p>这个管道用专业的术语来看要有以下几个功能</p><ul><li>能够自由添加<code>Feture</code></li><li>能够自动评判得到添加的<code>Feture</code>的效果</li><li>管道能够自己选择合适的参数训练模型</li><li>能够输出结果</li></ul><p>其实简单来说,我们要做的是一个能够非常方便的扩展的脚手架,我们不可以第一次就把所以的特征全部找出来,所以我们要搭建一个能够实时添加<code>Feture</code>的框架.</p><p>其实很早以前就看过类似文章,也有很多人推荐<a href="https://github.com/ChenglongChen/Kaggle_HomeDepot" target="_blank" rel="noopener">大神开源的一个脚手架</a>,然而找到的大多是用<code>python</code>实现</p><p>我因为一开始就是使用<code>Notebook</code>进行数据挖掘,主要<code>Notebook</code>能够提供一个实时的反馈,而纯<code>python</code>,对于复杂多变的数据来说,显得非常笨重,你经常有个好想法想验证一下,又得重新跑一遍,尤其是对于我的机器配置来说,重新跑一边的时间都够我喝杯茶了.而且<code>notebook</code>有个特点,可视化特别方便,有时候从数据上看不到,可以画个图表</p><p>好了,夸了这么久,现在就来仔细讲讲脚手架如何搭建.</p><p>我们先回到赛题,第一步审题,当时我看到日平均两个字,直接把日字省略,看成平均用电量,结果白白浪费了两个验证机会…..</p><p>审完了题我们来看我们要提交的数据,换数据后要预测十月日用电总量.我们来看看给我们数据,只有一份数据,表头如下</p><pre><code>record_date,user_id,power_consumption2015/1/1,1,11352015/1/2,1,5702015/1/3,1,34182015/1/4,1,39682015/1/5,1,3986</code></pre><p>解释一下字段,<code>record_date</code>–日期,<code>user_id</code>–企业id, <code>power_consumption</code>–日用电量<br>非常简单,就这么简单单单的数据,我现在要教大家怎么从这么简单的数据上抽取6000维度的</p><p>我把代码已经推到<code>Github</code>上了(由于数据比较少,我把数据也推上去了,方便大家本地跑跑,看完如果对你有帮助的话,请不要吝啬你的star哦),我就对照我的代码解释如何搭建一个可以跑出上万维度的脚手架</p><h2 id="数据划分-split-samples-ipynb"><a href="#数据划分-split-samples-ipynb" class="headerlink" title="数据划分(split_samples.ipynb)"></a>数据划分(<code>split_samples.ipynb</code>)</h2><p>首先要搭建本地预测集,也就是线下样本(这个很重要,有时候线下的结果很大程度对应你线上的结果)</p><p>给的数据要我们从前面21个月预测下一个月的日总用电量,我们很容易就能想到,那我们用前面20个月预测第21个月来做线下测试,但是这样我们就只有30个训练样本,要来预测30个,99.999%过拟合啊,首先我们要扩大样本,我们采用滑动移窗的方法把预测的样本按照月份推移,也就是分别预测9月8月7月等等</p><p>这种方法在实现<code>Notebook</code>有几个难点,首先你划分了预测集,那么就也要划分训练集,就相当于把一份数据切分成好几份,切分完之后有个问题,你必须要隔离每个部分</p><p>举个例子,我们把训练集划分成为2份,1月到7月预测8月,2月到8月预测9月,训练1-7月数据集的时候,我们不能让这个训练集接触到2月到8月的数据,因为8月对于前一个训练集来说是未知的,<br>如果我们让第一个训练集接触倒第二个训练集我们称为信息泄露,很影响线上的结果</p><p>我们知道这个问题之后,我们就要用巧妙的方法来解决,首先我们要考虑我们代码的复杂度,以前我的解决训练集隔离的方法采用的是循环法,使用一个列表存贮所以训练集,然后使用<code>for</code>循环分别传参到函数里面,这个方法能解决隔离训练集,但是有几个问题</p><p>在单个<code>ipy</code>文件中训练所以的样本,在测试的时候跑起来太慢,而且要把数据全部加载在内存里面,这次数据量还算小,但是对于某些小内存的电脑来说,这种方法时不时就得报<code>Memory Error</code>,而且感觉调试起来特别麻烦,所以一直在寻找更好的解决方案.</p><p>这次想到了一种巧妙的方法,虽然有点取巧但是效果我很满意.</p><p>我们先看到<code>split_samples.ipynb</code>文件,首先我把数据划分为9个样本,一个预测样本.分别放入不同文件夹进行物理隔离.但是名字相同.</p><p>再其次我让<code>ipy</code>能够获取参数,这样我通过外部参数就能更换数据集,平常添加<code>Feture</code>的时候默认选取一个训练集,这样我开发的时候调试就非常方便,而且可以丢掉<code>for</code>循环,还我一个清新脱俗的<code>ipy</code>.</p><p>这里说一个小细节,因为我传参必须要外部调用这种,对于运行<code>ipy</code>我使用了<code>runipy</code>这个工具,然后我死活没有找到,如何使用<code>runipy</code>把参数传倒<code>ipy</code>里面去的方法(如果找到了请告诉我),我一拍脑袋那就转换成<code>py</code>文件传过去,通过<code>sys.argv</code>很轻松就能获取到,所以我又用<code>jupyter nbconvert</code>的工具把<code>ipy</code>转换成<code>py</code>文件</p><p>所以绕了一圈最后又回到了<code>py</code>上(手动滑稽).不过我们工作还是在<code>ipy</code>上进行,生成的<code>py</code>文件我好像没打开过….</p><h2 id="特征提取-extract-fetures-ipynb"><a href="#特征提取-extract-fetures-ipynb" class="headerlink" title="特征提取(extract_fetures.ipynb)"></a>特征提取(<code>extract_fetures.ipynb</code>)</h2><p>聊完如何划分数据集,现在我们进入如何特征提取,我们可以看到这次数据其实就三个特征:时间-企业-用电量.由于企业的信息只有一个<code>id</code>,所以我首先提取的是时序的特征,首先把时间分解为八个维度</p><ul><li>dayofweek</li><li>dayofyear</li><li>days_in_month</li><li>quarter</li><li>week</li><li>weekofyear</li><li>month</li><li>year</li></ul><p>我们可以通过<code>pandas</code>轻松提取出来</p><p>然后我们再从两个方向来看,第一个就是我们日总用电量特征,从全部企业日总用电量</p><p>第二个就是日用电量特征,从每个企业日用电量来看,这些特征我们使用简单统计又可以得到10个维度数据(mean,std,等等)</p><p>看完这些之后我们又可以从多个时间维度来看这些特征,比如30天前,90天前等等(我划分了30,60, 90,180,360五个),</p><p>这样我们就有了 <code>8 * 2 * 5 * 10</code>个特征,但是这远远达不到我们说的上万维度,</p><p>现在我们从业务逻辑上来思考,因为我们知道,其实我们中国节假日和周末,天气这些对用电量影响非常大(我们老家打雷就停电…..)</p><p>所以我们要引入外部数据集,我采用两个爬虫分别是<code>weather_crawl,holiday_crawl</code>爬取了天气和节假日的数据</p><p>我们按照前面的思路,从天气节假日的角度又可以划分出n多特征(这时候我的特征已经达到3000了)</p><p>完了这些基础特征后,我发现有些特征重要性特别大(使用<code>Randomforest</code>得到),这时候我们又要请出我们第二大神器,交叉特征,比如月和假期的特征融合,这一波操作直接让我的特征到了6000+维度(如果将窗口扩大轻轻松松上万)</p><p>在这里要介绍一个特征生成的方法,有时候我们特征少,我们会采用自己命名的,自己生成,然后这个由于规律性比较大,<br>如果我们自己手动一个一个写的话,这上万<code>Fetures</code>够你写的,所以要让他自己生成特征,我们只要建好模子就行,由于这次<br>时间仓促,基本上我没有自己手动命名<code>feture</code>,全部都由程序生成,省掉很多代码量,具体可以看看代码实现,原理很简单.</p><h1 id="训练模型-train-model-ipynb"><a href="#训练模型-train-model-ipynb" class="headerlink" title="训练模型(train_model.ipynb)"></a>训练模型(<code>train_model.ipynb</code>)</h1><p>训练模型的话,一般比赛都推荐先使用树模型,一方面速度快,第二个可以看到<code>feture</code>的重要性,这对于你挑选交叉特征非常有用,模型调参的我这里就不讲,一方面我自己也不是很懂,第二个方面也网上教程也多,我讲的不一定比他好</p><p>这里要推荐一个发现有趣的包,<code>mlxtend</code>,我用他来进行<code>stacking</code>特别方便,有意思的时,我用他融合了四个模型,最后我的训练结果竟然为1,完全拟合了……</p><p>这个包可以很简单的进行模型的<code>stacking</code>,然而这个比赛我没有把他用好(手动滑稽)</p><p>通过训练模型后我们把模型存到<code>pkl</code>文件中,然后在用他来预测数据,这样在文件夹里转一圈的原因,因为原来打过部分比赛数据量太大,训练模型后内存不足,只能先<code>del</code>,清空内存,再预测,存到文件夹后,结束进程,清空内存,这样就能省下空间来读取下一步数据.</p><h1 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h1><p>其实在这个脚手架上可以扩展很多东西,比如最后搏一搏单车变摩托的时候,我就在分割数据和训练数据之间加了一个过滤清洗数据层,在训练模型和融合特征之间加了一个降维的中间层.</p><p>建立一个好的脚手架只是能让你在增添特征,选择特征时更加轻松,其实比赛看的还是你对数据的一种掌控力,建立这个脚手架主要是为了节省更多时间给提取特征、选择特征上.特征决定你的上限.</p><p>这次比赛比较特殊,模型在这个比赛效果可能没有规则好,因为数据量太小,我<code>stacking</code>一下直接完全拟合了.可惜验证的次数还是<br>太少,除去前面两次错误的提交,我只有三次验证机会,如果次数多一点的话,选择特征降维或者模型调参一下遏制拟合结果可能会好很多吧.</p><p>但是这次比赛自己学到了如何搭一个<code>ipy</code>的管道和增加了一些特征调参、特征降维的经验。因为以前看到的搭建管道资料都是基于<code>py</code>,很少基于<code>ipy</code>的,所以把自己搭建<code>ipy</code>管道经验分享出来,也希望自己写的这篇博文能够抛砖引玉,帮助大家搭建自己的完美管道.</p><p>附上我的<a href="https://github.com/mrzhangboss/electricAI" target="_blank" rel="noopener">开源示例</a>:  <a href="https://github.com/mrzhangboss/electricAI" target="_blank" rel="noopener">https://github.com/mrzhangboss/electricAI</a><br>大家觉得有帮助就给我点个star吧</p>]]></content>
      
      
      <categories>
          
          <category> 人工智能 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 比赛 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>如何成为一名数据挖掘机</title>
      <link href="2017/05/16/ai/%E5%A6%82%E4%BD%95%E6%88%90%E4%B8%BA%E4%B8%80%E5%90%8D%E6%95%B0%E6%8D%AE%E6%8C%96%E6%8E%98%E6%9C%BA/"/>
      <url>2017/05/16/ai/%E5%A6%82%E4%BD%95%E6%88%90%E4%B8%BA%E4%B8%80%E5%90%8D%E6%95%B0%E6%8D%AE%E6%8C%96%E6%8E%98%E6%9C%BA/</url>
      
        <content type="html"><![CDATA[<p>自我介绍</p><p>前端 - &gt; 后端  -&gt; 数据挖掘机</p><p>ML DM AI  的区别</p><p>我的自学之旅</p><p>给新手的推荐</p><ul><li>机器学习课程(MOOC)</li><li>Kaggle、天池、数据城堡</li></ul><p>掌握的技能</p><ul><li>Java + Python</li><li>数据可视化</li><li>训练团队感</li></ul><p>未来的发展方向</p><ul><li>全栈数据挖掘工程师</li><li>增长黑客</li><li>ML算法工程师</li></ul>]]></content>
      
      
      <categories>
          
          <category> 人工智能 </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>爬虫分布式总结</title>
      <link href="2017/04/04/python/%E7%88%AC%E8%99%AB%E5%88%86%E5%B8%83%E5%BC%8F%E6%80%BB%E7%BB%93/"/>
      <url>2017/04/04/python/%E7%88%AC%E8%99%AB%E5%88%86%E5%B8%83%E5%BC%8F%E6%80%BB%E7%BB%93/</url>
      
        <content type="html"><![CDATA[<blockquote><p>由于最近在一家数据服务公司实习，项目需要了解分布式，所以在这里基于<code>scrapy</code>的分布式总结一下爬虫的分布式实习</p></blockquote><h1 id="分布式起因"><a href="#分布式起因" class="headerlink" title="分布式起因"></a>分布式起因</h1><blockquote><p>单机无法完成全部工作任务所以要使用集群加速完成工作任务</p></blockquote><p>分布式有点像蚁群，一只蚂蚁举不起一只卡壳虫，但是几百只就能轻松的把他运回家</p><p>但是分布式设计必须科学，否则就像下面一样，一个和尚挑水，其他和尚围观</p><p><img src="/images/timg.jpg" alt="分工不合理，来源网络"></p><h1 id="分布式设计"><a href="#分布式设计" class="headerlink" title="分布式设计"></a>分布式设计</h1><p>分布式设计原理在于<strong>分工</strong></p><p>首先我们来看看爬虫怎么进行分工，单个爬虫运行根据<code>url</code>获取响应报文，然后通过解析报文返回结果或者下一次爬取目标，如果单个爬虫我们只要在内存维持一个<code>set</code>变量记住爬取过的<code>url</code>，这就是<code>scrapy</code>默认的方法。</p><p>但是我们无数个爬虫由于不在同一个进程，无法共享变量，所以我们只要让一个“<code>variable</code>（变量）”能够被被所以爬虫共享到就完成了主要功能</p><p>现在我们来完善具体细节<br>要求：</p><ul><li>爬虫能够轻松读取所以已爬取变量</li><li>爬虫能够加入已读取变量</li><li>爬虫能够获取下一次请求具体参数</li></ul><p>原则上我们可以使用内存映射来构建这个变量，但是读取，修改都不便利，所以可以先使用<code>redis</code>作为存贮变量的地方，使用<code>redis</code>提供的<code>set</code>我们替代<code>scrapy</code>框架的<code>set</code>变量。</p><p>现在我们已经决定我们要使用什么容器来存贮变量，接下来我们要考虑存什么变量。</p><p>我们先看<code>scrapy-redis</code>存贮了什么，分析源代码可知，<code>scrapy-redis</code>将返回的<code>Request</code>pickle话存入数据库，并且计算这个<code>Request</code>的32位hash值存入<code>redis</code>的<code>set</code>中过滤列表。</p><p><code>scrapy-redis</code>通过修改<code>scrapy</code>的调度器（scheduler）让其当爬虫没有<code>Request</code>需要处理时在<code>redis</code>中提取<code>Request</code>，实现分布式。</p><p>我们来分析一下这种方法，爬虫在爬取的过程中从<code>master</code>端获取<code>Request</code>，并不断生成<code>Request</code>到<code>master</code>端，<code>master</code>只是一个<code>redis</code>数据库，负责对<code>url</code>去重，分发任务。</p><p>我们来比较一下直接存取<code>url</code>这种方法，这种方法好处在于，<code>slaver</code>能够从上一个<code>Request</code>中获取全部信息，假如上一个<code>Request</code>需要存取获取的表单提取地址，我们下一次爬虫发起<code>Request</code>就能从上一个<code>Request</code>中获取参数。</p><p>当然由于我们存贮的是<code>Request</code>，一个<code>Request</code> <code>pickle</code>化之后的字符串比较长，当我们的任务列表里面有很多<code>Request</code>的时候，<code>redis</code>占用的内存会非常巨大。</p><p>当然如果爬虫启动的够多，生成一个就能把任务被调度下去，那么这个任务列表就能稳定在一个可控的范围。</p><p> <strong>总结</strong></p><p>每个爬虫即负责爬取数据，又负责生成下一个任务，即无主次之分，我们可以一次性在<code>docker</code>中启动上百个实例，我们只是用<code>redis</code>充当一个存放<code>变量</code>的地方。</p><p>但是这种方法也有一个缺点，我们不能自由的添加初始<code>url</code>，要想添加新的爬取任务，必须新建一个爬虫更新初始<code>url</code>，我们如果是想搭建一个自由添加<code>url</code>的爬虫，这种实现方式不大优雅。</p><h1 id="分布式改良"><a href="#分布式改良" class="headerlink" title="分布式改良"></a>分布式改良</h1><p>我们要修改程序框架，达到随时可以添加要爬取新任务，然而不影响爬虫集群</p><p><img src="/images/distribute_crawl.png" alt="爬虫框架"></p><p>我们独立出来<code>master</code>，<code>master</code>负责生成<code>Request</code>去重以及任务调度，而<code>slaver</code>只负责从<code>master</code>获取任务爬取。</p><p>这种方法我们可以很轻松对<code>master</code>改良而不影响<code>slaver</code>，通过让<code>master</code>定时从<code>数据库</code>中获取新的任务生成到任务列表，我们可以轻松添加新的任务到<code>slaver</code>集群中去。</p><p>下一步我们就介绍如何修改<code>scrapy-redis</code>达到我们新框架需要</p><h1 id="重构scrapy-redis"><a href="#重构scrapy-redis" class="headerlink" title="重构scrapy-redis"></a>重构<code>scrapy-redis</code></h1><p>参考：<br><a href="http://blog.csdn.net/bone_ace/article/details/50989104" target="_blank" rel="noopener">基于Redis的三种分布式爬虫策略</a></p>]]></content>
      
      
      <categories>
          
          <category> Python </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>协程解决递归错误原理</title>
      <link href="2017/03/15/python/%E5%8D%8F%E7%A8%8B%E8%A7%A3%E5%86%B3%E9%80%92%E5%BD%92%E9%94%99%E8%AF%AF/"/>
      <url>2017/03/15/python/%E5%8D%8F%E7%A8%8B%E8%A7%A3%E5%86%B3%E9%80%92%E5%BD%92%E9%94%99%E8%AF%AF/</url>
      
        <content type="html"><![CDATA[<h2 id="引言"><a href="#引言" class="headerlink" title="引言"></a>引言</h2><blockquote><p>递归是高度抽象化问题的一个好东西，我们能从很多算法书里面看到这个，<br>但是递归虽然对于人来说好理解，但是计算机执行起来会损失性能，一个差的递归可能会耗光计算机的资源</p></blockquote><p>接下来我们来看一个非常经典的算法问题<code>Fibonacci数</code></p><pre><code>f(n) = n (n &lt; 2)f(n) = f(n-1) + f(n-2)   (n &gt;= 2)</code></pre><p>  我们可以很轻松的用递归解决掉它</p><pre><code>def fibonacci(n):  if n &lt; 2:      return n  else:      return fibonacci(n-1) + fibonacci(n-2)</code></pre><p> 当<code>n</code>比较小的时候很快就出结果了，但是当<code>n</code>大于100时候要很久才能出结果，如果<code>n</code>大于1000，直接报出超出迭代深度的错误（python默认迭代深度是1000）</p><p>现在我们来解决两个问题</p><ol><li>为什么<code>n</code>大于100时候就很久才能算出结果</li><li>为什么<code>n</code>大于1000就报迭代深度的错误</li></ol><p>首先我们要知道一个概念就是堆栈段，每个进程开始运行时都会初始化一个堆栈段，这在物理上就是一小块内存，初始化堆栈段的时候计算机要做一些看起来同程序毫无关系的事情，比如说将寄存器的值推入堆栈里面等等</p><p>当你在运行主程序的时候你调用一个子函数，系统又会在当前堆栈段新建一个堆栈段，你子程序运行完了后会删掉这个堆栈段回到主程序，但是递归有个问题，就是他调用子程序的时候不会立即返回又会再调用自己</p><p>没办法因为子程序还没返回，所以计算机又初始化一个堆栈段，一个<code>n</code>为10的<code>fibonacci</code>函数就会初始化掉 <code>2 ** 10 = 1024</code>个堆栈段，<code>n</code>越大值会指数型增长，虽然1000个初始化在当今计算机上发不了多少时间，但是当我们<code>n</code>大于20就要 百万次初始化了</p><p>这就是为什么<code>n</code>很大的时候要很久才能算出结果，在一些单片机上面，循环调用空函数就是延时的功能，原理也就是堆栈初始化耗时间，而且不但耗时间假如像递归这样调用上百万次初始化而不返回将会耗掉大量内存在堆栈段上。</p><h2 id="对策"><a href="#对策" class="headerlink" title="对策"></a>对策</h2><blockquote><p>要解决这两个问题，一种方法是改算法，使用非递归算法，这个网上有很多，感兴趣的可以去搜一下，第二种是使用协程解决递归问题</p></blockquote><p>如何使用协程来解决递归呢我们先改主程序，将<code>return</code>换成<code>yield</code></p><pre><code>def _fibonacci(n):  if n &lt; 2:      yield n  else:      yield ( (yield (_fibonacci(n-1)) + (yield (_fibonacci(n-2)))</code></pre><p>接下啦我们运行一下函数</p><pre><code>&gt;&gt;&gt; _fibonacci(10) &lt;generator object _fibonacci at 0x00000013A74779E8&gt;</code></pre><p>没有返回结果,返回一个生成器，那我们用<code>list</code>简单的试一下吧</p><pre><code>&gt;&gt;&gt; list(_)............TypeError: unsupported operand type(s) for +: &apos;NoneType&apos; and &apos;NoneType&apos;</code></pre><h4 id="生成器小知识"><a href="#生成器小知识" class="headerlink" title="生成器小知识"></a>生成器小知识</h4><blockquote><p>这里补充几点生成器的知识，懂得可以跳过</p></blockquote><p>生成器大家都用过，无论是<code>Python2</code>或<code>Python3</code>都不陌生,最简单的生成器是这种</p><pre><code>&gt;&gt;&gt; items = ( x for x in range(10))</code></pre><p>我们一般搭配<code>for</code>来使用</p><pre><code>&gt;&gt;&gt; for i in items:...         print(i)...</code></pre><p>我们也可以用协程来实现这个生成器</p><pre><code>def iter_func(n):    for i in range(n):        yield n</code></pre><p>像上面一样使用<code>for</code>就能实现一样的功能，在这个例子里面<code>yield</code>好像变成了一个<code>return</code>的作用，在<code>for</code>语句中，随着每次请求都会<code>return</code>一个数过来</p><p>在这个里面<code>yield</code>好像就是这么个功能，但是<code>yield</code>的作用远远不止于此</p><p>我们现在来改一下这个函数</p><pre><code>def iter_func(n):    for i in range(n):        r = yield i        print(&apos;result&apos;, r)</code></pre><p>我们用<code>list</code>来运行一下这个函数</p><pre><code>&gt;&gt;&gt; list(iter_fun(2))0result None1result None</code></pre><p><code>r</code>返回了一个<code>None</code>，我们尝试自己实现一下<code>for</code>循环,有两种方式</p><ul><li>next(generator)</li><li>generator.send(msg)</li></ul><p>先尝试用<code>next</code></p><pre><code>&gt;&gt;&gt; it = iter_fun(2)&gt;&gt;&gt; next(it)0&gt;&gt;&gt; next(it)result None1</code></pre><p>我们介绍一下<code>next</code>函数, <code>next</code>接受两个参数，第一个是生成器，第二个是返回的默认值,<code>next</code>函数在这里相当于下面这个函数</p><pre><code>def next(iterator, default=None):    try:        iterator.send(None)    except StopIteration:        if default:            return default        else:            raise StopIteration()</code></pre><p><strong>为什么第二个执行了<code>print</code>函数而第一个没有执行?</strong></p><h4 id="生成器工作原理"><a href="#生成器工作原理" class="headerlink" title="生成器工作原理"></a>生成器工作原理</h4><blockquote><p>这里我们介绍一下生成器的工作原理</p></blockquote><p>当我们使用调用一个函数的时候，一般是碰到<code>return</code>或者执行全部函数就会返回父函数</p><p>但是<strong>生成器</strong>不同，假如他执行函数碰到<code>yield</code>，他就会直接返回一个生成器。</p><p>这个生成器我们可以把它看做是邮递员，我们必须写好目的地,他才会帮我们把信寄出去。</p><p>现在我们分析一下生成器的具体流程，我们先定义一个简单的生成器</p><pre><code>def mygenerator(n):    while True:        r = yield n        n -= 1        print(&apos;result&apos;, r)</code></pre><p> 然后我们调用这个生成器</p><pre><code>&gt;&gt;&gt; i = mygenerator(10)&gt;&gt;&gt; i&lt;generator object mygenerator at 0x7f420a339d00&gt;</code></pre><p>我们得到一个生成器，我们先尝试发送一个地址给“邮递员”</p><pre><code>&gt;&gt;&gt; i.send(0)...TypeError: can&apos;t send non-None value to a just-started generator</code></pre><p>我们得到一个错误，必须传递一个<code>None</code>，我们先不管，先送一个<code>None</code>值过去</p><pre><code>&gt;&gt;&gt; i.send(None)10</code></pre><p>我们得到一个<code>10</code>，再送一个地址过去</p><pre><code>&gt;&gt;&gt; i.send(None)result None9</code></pre><p>我们现在来分析一下代码，第一次调用的时候直接返回了，第二次调用我们从<code>r = yield n</code>那行开始执行，并且运行到第二个<code>r = yield n</code>那里停止了</p><p><strong>就可以解释上面为什么要第一次传递<code>None</code>过去，因为第一次调用它会直接返回<code>yield</code>后面的值给我们，第二次调用 我们可以根据第一次生成器递给我们的值，决定我们第二次想寄的“信”，因为第一次传递过去“信”并不能被处理，所以Python强制我们传递一个None值过去</strong></p><hr><blockquote><p>我们回到上面的函数</p></blockquote><pre><code>def _fibonacci(n):    if n &lt; 2:        yield n    else:        yield ( (yield (_fibonacci(n-1)) + (yield (_fibonacci(n-2)))</code></pre><p>我们来分析一下流程，为了解决上面的问题我们先把函数简化，去掉递归</p><pre><code>def f(n):    yield (yield n) + (yield n - 1)</code></pre><p>我们先创建一个生成器<code>i</code></p><pre><code>&gt;&gt;&gt; i = f(5)&gt;&gt;&gt; i &lt;generator object f at 0x7f4a421d8f10&gt;</code></pre><p>我们先启动i</p><pre><code>&gt;&gt;&gt; i.send(None)5</code></pre><p>我们再把得到<code>5</code>传给<code>i</code></p><pre><code>&gt;&gt;&gt; i.send(5)4</code></pre><p>我们得到<code>yield n -1</code>返回的4，我们再把4传给<code>i</code>，得到最终结果</p><pre><code>&gt;&gt;&gt; i.send(5)9</code></pre><p>假如我们把后面两个<code>send</code>的值换成其他值我们会得到不同的结果，这里我们可以看到我们，要实现上面函数必须要依靠一个<code>栈</code>，保存我们返回的生成器，然后依次调用生成器返回结果，具体代码如下</p><pre><code>def fibonacci(n):    stack = [ _fibonacci(n)]    last_result = None    while stack:        last = stack[-1]        try:            if isinstance(last, types.GeneratorType):                stack.append(last.send(last_result))                last_result = None            else:                last_result = stack.pop()         except StopIteration:             stack.pop()     return result</code></pre><p>我们这里用<code>stack</code>作为我们的堆栈，用<code>last_result</code>保存上一个生成器返回的值</p><h1 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h1><p>我们使用协程解决掉了递归错误，但是这个方法并不可以给我们算法加速，虽然n为1000以上不会报递归错误，但是等待的时间还是很长很长。。。</p><p>虽然协程在这个方法里面并没有起到多大作业，协程在算法方面还是没有太多帮助，协程在计算机<code>I/O</code>还有网络请求方面有更好的效率，但是这次尝试让我们对协程如何使用有了一个清晰的了解</p><p>有兴趣的可以去了解一下协程在异步网络请求的应用</p>]]></content>
      
      
      <categories>
          
          <category> Python </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>天池大赛-口碑预测参赛感想</title>
      <link href="2017/02/17/ai/competition/%E5%A4%A9%E6%B1%A0%E5%A4%A7%E8%B5%9B-%E5%8F%A3%E7%A2%91%E9%A2%84%E6%B5%8B%E5%8F%82%E8%B5%9B%E6%84%9F%E6%83%B3/"/>
      <url>2017/02/17/ai/competition/%E5%A4%A9%E6%B1%A0%E5%A4%A7%E8%B5%9B-%E5%8F%A3%E7%A2%91%E9%A2%84%E6%B5%8B%E5%8F%82%E8%B5%9B%E6%84%9F%E6%83%B3/</url>
      
        <content type="html"><![CDATA[<blockquote><p>第一次听说这个比赛还是去年在知乎上，当时也不知道这个比赛具体是怎么回事，当时自己还是一个小白，忙着搞懂各种主流的机器学习模型算法。</p></blockquote><p>当时在我心中，模型算法是数据挖掘的最重要的组成部分，搞懂这些才能真正搞定数据挖掘。我当时对算法模型和数据的理解是：模型就是风车，数据就是流水。我要做的事就是撘一个强健的风车，让数据流过。</p><p>当我还没接触实际的工作前，我还没有没有从编程工转向挖掘工。我太注重编程本身了，而忘记我自己真正要挖掘的宝藏。</p><p>我以前在<code>Quora</code>上搜如何成为数据科学家，我发现很多有经验的数据科学家他们都把“对数据的敏感和兴趣”作为数据科学家最重要的特征，而“了解各种算法模型并能应用到数据上”才是第二重要的。我当时不是太理解，我觉得后者才是更重要的。</p><h2 id="参赛感想"><a href="#参赛感想" class="headerlink" title="参赛感想"></a>参赛感想</h2><p>这次参赛算是我学习数据挖掘第一次实际的挖掘，以前学习各种算法模型都是准备的很好的数据，只要套上算法模型就能跑的很好。所以我一开始就拼命的去找类似的大赛，看看获胜者他们用的模型是什么。</p><p>这几天我好像抱着一堆瓶子，拼命的想把巨大的石头（数据）塞进瓶口里，看起来工作量很大，流了很多汗，其实什么都没有干。今天在看一个类似的比赛选手答辩的时候的视频，突然明白自己好像走了一个死胡同。自己拼命的想这找一个合适的瓶子（模型），其实我更应该做的是把石头（数据）磨碎。</p><h4 id="模型本身不重要，他只是一个载体，更重要的是数据。"><a href="#模型本身不重要，他只是一个载体，更重要的是数据。" class="headerlink" title="模型本身不重要，他只是一个载体，更重要的是数据。"></a>模型本身不重要，他只是一个载体，更重要的是数据。</h4><p>第一次参加这样大型比赛，有点激动也有点惶恐，如何将所学的应用到实际，还有在实际中提高自己还有待自己“挖掘”。虽然这个比赛奖金“丰富”，但是我觉得在这个比赛中得到的体会乐趣比奖金更诱人。</p><hr><p>比赛还有一个月，在这里立个小目标，争取跑到到前五页，我也会尽量抽时间把自己感想写出来。<br>未完待续。</p>]]></content>
      
      
      <categories>
          
          <category> 人工智能 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 比赛 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>怎么成为数据科学家（翻译）</title>
      <link href="2016/12/23/useless/how-to-be-data-scientist/"/>
      <url>2016/12/23/useless/how-to-be-data-scientist/</url>
      
        <content type="html"><![CDATA[<blockquote><p>这是我从<code>Quora</code>上看到的一篇非常简短但详细的数据科学家的‘技能点’<br>来自eBay的一个数据科学家的回答<br>翻译来自<a href="https://www.quora.com/How-do-I-become-Data-analyst-scientist-Do-I-need-to-have-good-programming-skills/answer/Paul-Jeon-3?srid=ue3s8" target="_blank" rel="noopener">Quora回答</a></p></blockquote><p>这是面试谷歌、英特尔、脸书等大的世界五百强公司的数据科学家相关岗位常见的技术要求，在我看来主要有七点</p><ul><li>基本的编程基础</li></ul><blockquote><p>你应该了解一门统计学相关的编程语言，比如说<code>R</code>或<code>Python</code>（同时要了解<code>Numpy</code>和<code>Pandas</code>库），还要一门数据库查询语言比如<code>SQL</code></p></blockquote><ul><li>统计学</li></ul><blockquote><p>你应该要能解释零假设、P值、最大似然估计和置信空间这些短语，统计学在非常巨大的数据库里压缩数据和从挑选最重要的特征非常重要，在你得出结论和设计实验过程中也帮助巨大</p></blockquote><ul><li>机器学习</li></ul><blockquote><p>你必须能够搞懂K-近邻、随机森林和集合方法等机器学习算法，这些算法基本上都在<code>R</code>或<code>Python</code>中得到实现，这些算法能告诉你雇主你能够将计算机科学运用在实际的管理中。</p></blockquote><ul><li>数据重组</li></ul><blockquote><p>你应该要能够“清理”数据。比如数据库中”California” （加利福利亚）和“CA”是一样的，数据库里面可能出现用负值代表人口。这个总的来说就是识别坏（或者不正确）的数据然后校正（或删除)他们。</p></blockquote><ul><li>数据可视化</li></ul><blockquote><p>数据科学家不能就只是自己搞懂就行，他们需要把他们发现告诉你的产品经理，这样就能确保数据能很好的应用到程序里面去。所以，熟悉数据可视化工具比如说<code>ggplot</code>非常重要（这样你就能展示你的数据而不是仅仅谈谈而已）</p></blockquote><ul><li>软件工程</li></ul><blockquote><p>你应该了解算法和数据结构，因为这些东西在你写高效率的机器学习算法时非常重要，知道如何使用分支和使用高效的数据结构：队列、数组、列表、堆栈、树等等。</p></blockquote><ul><li>产品管理</li></ul><blockquote><p>这个绝对是有争议的，但是那些了解产品的人将会知道什么指标是最重要的。这里有很多数据可以用来做A/B测试，但是产品导向的数据科学家将会把最好的指标用来做测试。你要知道这些的意思：可用性测试、线框、保留和转换率、流量分析、客户反馈、内部日志、A/B测试。</p></blockquote>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>ansible管理nginx负载均衡</title>
      <link href="2016/10/09/software/ansible%E7%AE%A1%E7%90%86nginx%E8%B4%9F%E8%BD%BD%E5%9D%87%E8%A1%A1/"/>
      <url>2016/10/09/software/ansible%E7%AE%A1%E7%90%86nginx%E8%B4%9F%E8%BD%BD%E5%9D%87%E8%A1%A1/</url>
      
        <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><blockquote><p>因为手头自己有三个服务器，所以想折腾一下负载均衡。</p></blockquote><p>两个<code>CentOS</code>，一个<code>Ubuntu</code>,都是比较新的。</p><p>一开始准备用<code>haproxy</code>来做负载均衡服务器，因为<code>haproxy</code>相比与<code>nginx</code>对<code>cookie</code>和<code>session</code>支持比较好，但是由于两个原因还是放弃了。</p><ol><li>服务器被阿里云封掉</li></ol><blockquote><p>简单的在<code>haproxy</code>中设置后端服务器后，过一段时间就显示强制备案页面，由于我的域名没有备案。</p></blockquote><p>后来我翻看了<code>nginx</code>日志发现，<code>haproxy</code>默认在<code>request header</code>里面带了<code>X-Host</code>，被阿里云发现了，这里提供一个解决方法</p><pre><code># 删除掉你header里面的 Host# 在backend里面添加一句http-request del-header Host</code></pre><p>然而<code>nginx</code>里面默认是没有添加<code>Host</code>这个的，要你在<code>localtion</code>中添加两句，如下面</p><pre><code>    server {  listen 80;  server_name example.com;  location / {    proxy_pass       http://main;    proxy_set_header Host            www.example.com; # add Host        proxy_set_header X-Forwarded-For $remote_addr; # add X-Forwarded  }}</code></pre><ol start="2"><li>haproxy支持多开</li></ol><blockquote><p>我试了很多种选项，确定<code>pidfile</code>、改变<code>uid</code>、<code>gid</code>等等，<code>haproxy</code>似乎可以允许很多个相同进程绑定同一个端口，虽然可以通过<code>pid</code>来写一套类似<code>service</code>管理的脚本，但终归很麻烦</p></blockquote><p>我看网上有人写了这个脚本，但是<code>nginx</code>自带了，还是用<code>nginx</code>比较好，而且<code>ansible</code>与<code>service</code>的交互还不错。</p><h2 id="nginx负载均衡"><a href="#nginx负载均衡" class="headerlink" title="nginx负载均衡"></a>nginx负载均衡</h2><blockquote><p>nginx负载均衡是通过反向代理来实现的，也就是把一台服务器的压力分摊到多台上面</p></blockquote><p>要想实现这个必须要有后端服务器，假设我们有一台后端服务器<code>1.1.1.1</code>，在代理主机的<code>nginx</code>配置系统<code>location</code>里面只要添加一条<code>proxy_pass</code>就行了</p><pre><code>    server {listen 80;server_name example.com;location / {    proxy_pass http://1.1.1.1;}}</code></pre><p>  上面只是简单的实现了一个反向代理的功能，当你有一个后端服务群的时候，你就要使用负载均衡模块了，负载均衡模块在<code>nginx</code>配置特别简单，添加一个<code>upstream</code>模块，把服务器ip或者域名放到里面</p><pre><code>  upstream webservers{    server 1.1.1.1 weight=10;    server my.domain.com  weight=10;}</code></pre><p>  然后修改<code>proxy_pass</code>后的为<code>http://webservers</code>就行了</p><p>  ps： <code>nginx</code>对于后端反向代理服务器有个<code>max_fails</code>和<code>fail_timeout</code>属性，你要是设定了一个<code>max_fails</code>次数，你代理服务器拿取失败了几次就会在<code>fail_timeout</code>值之后尝试，和<code>haproxy</code>的<code>retry</code>属性差不多，但是似乎<code>haproxy</code>的<code>retry</code>不好使，我故意使用两个错误<code>ip</code>和正确<code>ip</code>，结果<code>nginx</code>能一直正确返回正确<code>ip</code>响应，而<code>haproxy</code>有时候能，有时候不行。</p><h2 id="nginx错误日志"><a href="#nginx错误日志" class="headerlink" title="nginx错误日志"></a>nginx错误日志</h2><blockquote><p>在调试<code>nginx</code>碰到一些错误，记录一下如何系统的解决方法</p></blockquote><ul><li>调用<code>service nginx start</code>失败</li></ul><p>首先看给的错误信息，假如让你看<code>systemctl status nginx.service</code>或<code>journalctl -xn</code>，输入去看</p><ol><li>格式错误（format error）</li></ol><p>一般你写的<code>nginx</code>的配置文件有问题，这时候可以用<code>nginx -t</code>检查格式，修改正确后会显示<code>success</code></p><ol start="2"><li>无法绑定地址（bind error）</li></ol><p>一般是因为有别的应用程序占用端口造成的，这时候用<code>netstat -tulpn</code>检查端口，然后选择<code>kill</code>掉占用端口的程序或者换一个端口</p><h2 id="ansible-playbook-编写"><a href="#ansible-playbook-编写" class="headerlink" title="ansible playbook 编写"></a>ansible playbook 编写</h2><blockquote><p>具体代码可以参考<a href="https://github.com/mrzhangboss/nginx-load-balance-ansible-playbook" target="_blank" rel="noopener">nginx均衡负载ansible-playbook</a><br>首先你得写一个<code>hosts</code></p></blockquote><pre><code>[ali]my ansible_ssh_host=1.1.1.1 ansible_ssh_user=root [tencent]main ansible_ssh_host=1.1.1.2 ansible_ssh_user=root[digital]google ansible_ssh_host=1.1.1.3 ansible_ssh_user=root</code></pre><p>前面<code>[  ]</code>包着的是组名，最前面的<code>my</code>和<code>main</code>和<code>google</code>是<code>别名</code>，后面就是ip和用户名了。</p><p>写完<code>hosts</code>后要写两个<code>nginx</code>配置文件一个代理服务器的配置文件和一个后端服务器配置文件，<code>playbook</code>很简单就是复制<code>nginx</code>配置文件和重启<code>nginx</code>。</p><pre><code>---- hosts: tencent  remote_user: root    tasks:  - name: copy nginx config file     template: src=~/test/lunge_proxy.conf  dest=/etc/nginx/conf.d/lungelog.conf    notify: restart nginx    handlers:  - name: restart nginx    service: name=nginx state=restarted enabled=yes</code></pre><p>解释一下<code>notify</code>，在复制完成之后就启用一个<code>handler</code>完成<code>nginx</code>的重启，当然这里也可以使用<code>reload</code>，假如在生产环境的话。</p><p>客户端和代理的<code>playbook</code>差不多就不多介绍了。</p><h4 id="引用"><a href="#引用" class="headerlink" title="引用"></a>引用</h4><p><a href="https://www.zybuluo.com/phper/note/90310" target="_blank" rel="noopener">nginx的配置、虚拟主机、负载均衡和反向代理</a></p>]]></content>
      
      
      <categories>
          
          <category> 软件 </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>关于驱动力</title>
      <link href="2016/10/01/summary/about-diving-force/"/>
      <url>2016/10/01/summary/about-diving-force/</url>
      
        <content type="html"><![CDATA[<h2 id="驱动力是什么"><a href="#驱动力是什么" class="headerlink" title="驱动力是什么"></a>驱动力是什么</h2><blockquote><p>驱动力就鞭子，小的时候我们被父母教育考的好就是棒棒糖,考的差就是鞭子，等我们走进社会,工资就是我们是驱动力。</p></blockquote><p>我们看到过那些年薪百万的程序员，也看到过一些碌碌无为的码农,每个人都想成为那群大牛，工作得心应手、万人敬仰，工资难以”望其项背”.</p><p>但是我们同大牛和码农(差点打成马蓉…..)有什么区别呢，有些人说是人家那些大牛早早就积累了十万个小时，我们同大牛只是差了十万个小时.</p><p>这从某一方面上来看是对的，从某一方面来说又是不对的，君不看那些在公司辛辛苦苦工作几十年的码农早就积累了几十万小时，但是他们依旧是码农，除了业务逻辑比新手强。</p><p>那是什么原因让几十万个小时造不了一个大神呢？</p><p>很简单，就是<strong>驱动力</strong>。</p><p>码农是以工资作为驱动力的，而大神是以兴趣为驱动力的，很多码农一开始都同大神一样被编程的乐趣而吸引，然而大神坚持下来了，而码农呢，慢慢的像小孩子玩厌了新玩具，在慢慢的走入社会被工资左右，在慢慢的就开始盼望早点下班……</p><h2 id="怎么改变"><a href="#怎么改变" class="headerlink" title="怎么改变"></a>怎么改变</h2><blockquote><p>环境对我们的影响是潜移默化的，我们处在这个环境里面可能不知不觉就慢慢改变我们自己了</p></blockquote><p>那我们如何改变自己</p><h3 id="工作环境"><a href="#工作环境" class="headerlink" title="工作环境"></a>工作环境</h3><p> 假如你是一个工作党，找一个好的开发团队对你的影响是巨大的，假如你的小组死气沉沉，最好换一个即使工资很低</p><p> 假如你是一个学生党，比如我，尽量参加学生社团，那种偏技术的部门，在同一个部门里面一起奋斗的感觉非常好。</p><h3 id="学习环境"><a href="#学习环境" class="headerlink" title="学习环境"></a>学习环境</h3><ul><li>在搜索引擎上面多走几步</li></ul><p>很多时候我们遇到问题，google一下解决了就完了，我们要多问几个自己几个问题，这个问题为什么产生，如何避免，下一次还会遇到吗，还有及时收集自己的问题多总结，你要知道圣斗士之所以那么牛是因为人家从不在跌倒的地方跌倒第二次，你要知道bug不是我们的试卷，bug是我们的成神的<code>补丁</code></p><ul><li>培养开源精神</li></ul><p>github没事都上去溜达溜达，看到好的项目可以跟进，看看人家的代码同你的有什么不同，开源不代表抄袭，任何创新都是从模仿开始，不要老想着搞个大新闻大项目，其实很多项目都是从小项目开始的</p><ul><li>多输出</li></ul><p>其实很多人不知道，写东西也是学习的一种方法，因为很多时候我们学的东西有时候学的模拟两可，写出来有助你理清脉络，而且帮助后来者少走弯路，何乐而不为呢。</p><p>有些人说我没什么想写的，你找一些外国博客翻译也是可以的，通过翻译学习，一方面锻炼自己，一方面让更多人了解国外文化。</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>成神是很难的，但是只要你在路上，不要回头就不难了。</p>]]></content>
      
      
      <categories>
          
          <category> 随想 </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>集体智慧编程之推荐系统(Programming Colletive Intelligence)</title>
      <link href="2016/09/17/ai/recommendations-Programming-Colletive-Intelligence-impression/"/>
      <url>2016/09/17/ai/recommendations-Programming-Colletive-Intelligence-impression/</url>
      
        <content type="html"><![CDATA[<blockquote><p>看过好几把关于机器学习的书,但是很多书只是停留于算法原理阶段,或者更着重介绍算法原理, &lt;&lt;集体智慧编程&gt;&gt;这本书更多的是从实践来介绍书,比如你要撘一个推荐系统你怎么做,,还有怎么来做一个垃圾邮件过滤系统等….<br>接下来介绍一下我对于做一个推荐系统的理解.</p></blockquote><h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><blockquote><p>推荐系统是一个什么东西呢<br>简单来说就是两个字—-<code>推荐</code></p></blockquote><p>推荐系统的出现是伴随评价系统出现而出现的,评价系统就是我们的打分制,比如对一部电影每个人都对他进行打分</p><table><thead><tr><th>用户</th><th>电影A评价</th><th>电影B评价</th><th>新电影C评价</th><th>新电影D评价</th></tr></thead><tbody><tr><td>张三</td><td>5.0</td><td>3.5</td><td>4.9</td><td>2.0</td></tr><tr><td>李四</td><td>2.5</td><td>5.0</td><td>2.0</td><td>3.0</td></tr><tr><td>王二</td><td>3.1</td><td>4.7</td><td></td></tr></tbody></table><p>比如现在三个人张三李四王二,王二有两部电影没有看过,但是张三和李四看过了,现在就是要把电影C或D推荐给王二,推荐系统就是从其他数据分析来确定推荐次序</p><h2 id="相似度"><a href="#相似度" class="headerlink" title="相似度"></a>相似度</h2><blockquote><p>推荐系统如何根据其他用户的数据来推荐呢</p></blockquote><p>这里就要介绍一个相似度的概念,也可以加权系数,对于张三和李四,这两个人都看过电影C和D,但是我们该听谁的呢,我们可以把两个人的评价加起来然后排序,但是假如张三和李四品味相差太多,而且张三的评价可能会把某个王二喜欢看的电影评价总值拉低</p><p>这个时候我们就要考虑一个相似度问题,就是尽量避免一些品味同王二不同的人对排名造成影响</p><p>这里我们引进相似度这个概念,我们看张三和李四对电影A、B同王二的差别，这里我们使用距离这个概念，我们使用距离公式 <code>l=（（r1^2 + r2^2+....rN^2)^1/n)</code></p><p>然后通过距离算得<code>S(相似度)=1/(1+l)</code></p><p><em>当l=0时,两者相似度为1(最大),当l很大时,相似度几乎为零</em></p><table><thead><tr><th>用户</th><th>电影A评价</th><th>电影B评价</th><th>相似度</th></tr></thead><tbody><tr><td>张三</td><td>5.0</td><td>3.5</td><td>0.23</td></tr><tr><td>李四</td><td>2.5</td><td>5.0</td><td>0.59</td></tr></tbody></table><p>通过求相似度(权重)我们就把同我们臭味相投的人的评价提高了,那些同我们不一样品味的人评价作用降低了</p><h2 id="选择"><a href="#选择" class="headerlink" title="选择"></a>选择</h2><p>基于人物的评价推荐有个缺点就是人的评价有时候会比物品多,而且变化频繁,假如使用上面的方法的话,每次每一个用户评价过后,就要重新算一次相似度了,假如用户不多或者电影不多还好,一旦数据偏多服务器就扛不住了</p><p>所以我们要换一个角度出发,计算物品的相似度,这样图表就变成这样了</p><table><thead><tr><th>电影</th><th>用户-张三</th><th>用户-李四</th><th>相似度</th></tr></thead><tbody><tr><td>A</td><td>5.0</td><td>2.5</td><td></td></tr><tr><td>B</td><td>3.5</td><td>5.0</td><td></td></tr></tbody></table><p>这样有个好处就是可以离线处理数据,我们可以搭建一个离线处理系统,让一个系统专门处理数据,当服务器需要的时候再拿取过去.</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>搭建一个简单推荐系统并不会你想象中那么简单,但是要搭建一个功能强大速度快的推荐系统需要的不仅仅是这么一点,还需要考虑系统的稳定和速度.</p>]]></content>
      
      
      <categories>
          
          <category> 人工智能 </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>git 工作流程</title>
      <link href="2016/09/07/software/git-working-streamline/"/>
      <url>2016/09/07/software/git-working-streamline/</url>
      
        <content type="html"><![CDATA[<blockquote><p><code>git</code>是当今流行的版本控制工具,一般我们可能只要会<code>push</code>, <code>pull</code>就可以了,<br>但是当我们同别人共同工作的时候,我们必须要了解<code>git</code>协同开发的一些重要流程.</p></blockquote><h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><blockquote><p><code>git</code>作为当今最流行的版本控制工具之一,当时开发出来就是为了管理<code>Linux</code>庞大源代码的分布式版本控制工具.<br>由于<code>Linux</code>源代码过于巨大,仅靠一个人的力量是完成不了的,那就必须把工作分配下去,然后将代码合并,所以<code>git</code>一开始设计的时候就是一种分布式的、多分支的</p></blockquote><h2 id="概念"><a href="#概念" class="headerlink" title="概念"></a>概念</h2><p>所以<code>git</code>最重要的就是<strong>分支</strong>这个性质,分支是什么呢.</p><p>要了解<strong>分支</strong>必须要了解<code>git</code>工作原理.</p><p><code>git</code>工作原理很简单就是<code>add</code>、<code>commit</code>，<code>add</code>、<code>commit</code>….,简单来说就是添加记录,添加记录,保存快照,添加记录,添加记录,保存快照</p><p><img src="/images/git-workflow-release-cycle-4maintenance.png" alt="git工作分支流程-来源网上"></p><p>  如上图,随着master分支快照的一个一个建立,软件就慢慢的迭代下去了</p><h2 id="分支工作流程"><a href="#分支工作流程" class="headerlink" title="分支工作流程"></a>分支工作流程</h2><p>  接下来我们要着重讲一下分支,我们看到<code>master</code>分支的<code>v0.1</code>版本,我们已经开发出稳定的<code>v0.1</code>版,这时候我们决定开发一个新功能.</p><p> 在这里我们分了一个<code>Develop</code>分支,我们在<code>Develop</code>分支开发新代码.</p><p> 这时候我们发现<code>v0,1</code>的一个<code>bug</code>,假如是没有使用版本控制的话,一般人会停下手中的活,然后从当前的新代码处来修复这个<code>bug</code>,当这个<code>bug</code>很简单的时候,我们不会遇到很大困难,但是当<code>bug</code>藏的很深,而且新代码隐藏了这个<code>bug</code>,或者被这个<code>bug</code>影响,这时修复工作就变得很困难.</p><p> 还好我们有<code>git</code>,我们从<code>v0.1</code>直接分一个<code>Hotfix</code>分支,这两个分支的父都是<code>v0.1</code>,我们直接从稳定版本修复,不牵涉到新代码,这样修改好后我们就能很快从<code>Develop</code>分支继续工作了</p><p> 而且这样有一个好处我们将<code>master</code>和<code>Develop</code>分支合并的时候很大可能不会产生冲突.</p><p> <em>冲突(coflic)是什么了,怎么能避免呢?</em></p><p> 从两个分支的父亲<code>v0.1</code>看起,我们每次改动一个保存文件就会产生一个<code>modify</code>(修改)的动作,我们假如分支里面都对同一个文件产生了<code>modify</code>(修改)动作,当我们合并的时候这就是一个冲突,<code>git</code>无法理解采用哪个分支的<code>modify</code>动作,这时候就要你人工来修改采用哪个分支.</p><p> 假如没有相同的文件有<code>modify</code>(修改)动作,<code>git</code>就会聪明的知道采用每个分支的最新的<code>modify</code>(修改)修改出一份所以文件的最新版.</p><p> 那我们怎么来避免这个冲突呢,这就要求我们分支要分的合理,分支只要完成特定的工作,不要越俎代庖,那有些人会说我这个分支一定要改父的耦合地方否则我的代码工作不了,这时我们要好好思考自己的分支的功能,把合并耦合的代码放在主分支里面,次分支只要完成特定功能就可以了,这样合并分支时候不但可以安全的<code>merge</code>(合并)了, 而且修改<code>bug</code>的时候也可以对症下药,直接在问题开始的地方修改.</p>]]></content>
      
      
      <categories>
          
          <category> 软件 </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>刷题笔记</title>
      <link href="2016/08/21/algorithm/%E5%88%B7%E9%A2%98%E7%AC%94%E8%AE%B0/"/>
      <url>2016/08/21/algorithm/%E5%88%B7%E9%A2%98%E7%AC%94%E8%AE%B0/</url>
      
        <content type="html"><![CDATA[<blockquote><p>在牛客网刷了16道题了,在这做个总结</p></blockquote><h2 id="概况"><a href="#概况" class="headerlink" title="概况"></a>概况</h2><p>编程题无非两种一种考算法,一种考数据结构</p><p>算法的话,考验你对事情的分析程度和脑袋的灵光,用好的算法还是又大又重的算法,用算法复杂度来看,一般能到 <code>o(n)</code>就算勉强可以,当到了<code>o(n * n)</code>你就要考虑是不是你算法有问题了.</p><p>数据结构的话,队列和链表和二叉树是比较常见的,当然有些奇怪的一般算法反倒很简单.</p><h3 id="算法"><a href="#算法" class="headerlink" title="算法"></a>算法</h3><p>谈算法的话不得不谈递归了,递归其实我感觉更想一种思想</p><p>话不多说请看题,斐波那契数列一直是递归的代表</p><pre><code>                      0                 n= 1    f(n) ={           1                   n= 2            f(n-1) + f(n-2)     n&gt;2</code></pre><p>  虽然</p>]]></content>
      
      
      <categories>
          
          <category> 算法 </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>8月 总结</title>
      <link href="2016/08/21/summary/8-conclusion/"/>
      <url>2016/08/21/summary/8-conclusion/</url>
      
        <content type="html"><![CDATA[<blockquote><p>因为这段时间待在家没什么事干,虽然完成一个小的项目,也还有很多功能需要完善,但是说实话还是有点迷茫,明年就大三了,接下来有两条路给我走,要么是考研,要么是出去工作.<br>自己算是个半个程序员吧,web开发那一套前后端撸起袖子都能搞出点名堂来,但是为什么说我是半个程序员呢,因为现在我还不知道我是爱他还是不爱他呢.</p></blockquote><p>兴趣这东西是培养来的,谁也不是天生就死心塌地的爱上某个东西.</p><p>现在学习新东西的成本太低了,这两个月来我也尝试过很多以前没有接触的东西,<code>Android</code>,<code>Deskapp</code>,用了几个框架做了几个<code>apk</code>和软件,虽然用前端栈实现的东西但是所以的语言都差不多.</p><p>当然项目基本功能都很单一,当然复杂的也就是简单的堆积而成.</p><p>完成之后反倒更迷茫了,为什么了,互联网技术太好学了,无论你的文化程度有多低,按照<code>demo</code>依葫芦画瓢搞几下也能像模像样,我们作为一个文化程度比较高的一群人,或许花半天就能比别人一天能掌握的东西.</p><p>但是人家索要的报酬却比你的四分之一还要低,你拿什么跟人家竞争</p><p>我有一些志同道合的同学虽然像我一样学了几年web这一块,但是他们决定考研,web谁都能做,不用读大学就能做,他们想多读点出来改造世界,也可以说让世界更美好吧.</p><p>其实我对考研的态度是消极的,我现在嘛并不准备投身科研这方面,在美国其实大学都是少部分人才能上,比起我们现在博士硕士满天飞,在美国其实只有真正喜欢科研的人才会去读大学.</p><p>其实我写的时候很纠结,该怎么写才不跑题……</p><p>我其实想总结的是<strong>注重基础</strong>,但是我脑海里面出现无数个论点,好像回到了高中时代写议论文了, 想把主人公塑造成一个焕然大悟顿悟人生的形象…..</p><p>以上是个小插曲跳过跳过</p><p>现在开始正式总结了,越短越好.</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>七八两个月自己还是很急躁的两个月,一方面在于选择技术栈,一方面在于选择出路.</p><p>虽然没有想通,但是在想的过程中发现自己好焦躁…….</p><p>总是想一秒钟决定自己未来N年的时,有点像人生规划一样</p><p>虽然说搞个人生规划很好,但是发现真的是白费力气,因为无论那个伟人,谁也不知道自己将来会干什么,奥巴马在大学客堂睡觉的时候也不知道自己将领导美国.</p><p>焦躁点找到了,接下来就静下来想了怎么解决了,既然无法预测未来那就好好准备未来吧.</p><p>怎么准备呢?做你不屑做的事,并坚持下去.</p><p>你们不要想歪了,不屑做的事是指那些你看起来没什么卵用的建议.</p><p>比如说每天背几个单词,每天看一点英文原著,每天刷一刷编程题</p><p>在这我要提一句,虽然很多人从高中过来有点嫌弃刷题了,但是我们是大学生不能用高中生的观点去看待问题,高中的刷题是为了分数,大学的刷题是为了醒脑</p><p>你看不起一天在<code>leetcode</code>刷几道题,那是因为你刷不动,不愿刷,以前刷题刷到你一眼就能看出答案,现在我们刷题要每刷一遍都要有新的感受,让大脑活跃起来.</p><p>而且当年把运动刷题背单词看书写博客当成<code>副本</code>规定题量字数的时候,每次完成你的任务都会很爽,一方面脑子变得活跃了,另一方面你有种马上就升级的感觉.</p><p>每天都能发现自己的进步这是你前进的动力,虽然我们无法预测未来,好好准备,当未来真的来的时候,不会慌.</p><p>私下插个话,现在<code>app</code>泛滥,你刷单词完全可以搞个扇贝,刷题搞个牛客或leetcode,健身搞个keep,虽然我没收他们的广告费,但是这些东西给我的反馈比我自己搞个小本记着好多了.不多说了与君共勉,希望大家都能沉下焦躁的心,一起慢慢”升级”.</p>]]></content>
      
      
      <categories>
          
          <category> 随想 </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>用例子学TDD</title>
      <link href="2016/08/21/algorithm/TDD/TDD-byexample-transform/"/>
      <url>2016/08/21/algorithm/TDD/TDD-byexample-transform/</url>
      
        <content type="html"><![CDATA[<blockquote><p>翻译自<strong>TDD-byexample</strong><br>作者<strong>Kent Beck, Three Rivers Institute</strong><br>有删减</p></blockquote><h2 id="表现"><a href="#表现" class="headerlink" title="表现"></a>表现</h2><h3 id="测试驱动开发核心"><a href="#测试驱动开发核心" class="headerlink" title="测试驱动开发核心:"></a>测试驱动开发核心:</h3><ol><li>除非你有失败的自动化测试千万不要写一行新代码</li><li>拒绝重复</li></ol><p>这两个的简单原则构成了<code>TDD</code>的核心,但是他能规划一个复杂的项目乃至一个团队.这里有一些<code>TDD</code>的建议.</p><ul><li>你的项目设计不能太过全面,只要有一个模型或者相应的功能,然后你让你的测试代码测试你模型,通过反馈来完善你设计.</li><li>你必须自己写测试代码,你不能依靠别人来每天帮你修改无数次测试</li><li>你的开发环境必须能监控到代码的微小的变化</li><li>你测试代码必须要非常简单,复杂的测试代码说明你的程序有问题</li></ul><p>根据核心我们总结了一种具体的测试方法:<strong>红绿重构法</strong></p><pre><code>红 --- 写一段测试代码让他无法通过,有时候可以编译都通过不了绿 --- 写尽可能少的代码让测试代码通过,通过后保存一下系统状态重构 ----删掉所以重复的代码只要让测试代码还能通过</code></pre><p>红绿重构法是<code>TDD</code>最高作战计划,他看起来很笼统,其实他具体到了每一行代码.</p><p>如果我们按照<code>TDD</code>这种开发模式,我们会有什么好处呢</p><ul><li>如果我们新功能出了问题,质检部门能很快的将新代码回归到上一个稳定代码,尽快避免损失</li><li>如果代码的测试能将”惊喜”挖的差不多的话,项目经理能更准确的知道客户在使用过程中碰到的各种奇葩问题</li><li>如果我们的测试能够让各种技术交流变得更加清晰,我们能更快的进行技术排错</li><li>如果我们项目的bug更少的话,我们的项目能变得更加灵活,我们可以很轻松的添加新功能呈现给我们顾客</li></ul><p>这些概念看起来很简单,但是我们的动机呢?为什么我要在写代码过程添加自动测试?为什么我要每次迈小小的一步在我能够做到更多的情况下?两个字:<strong>担忧</strong></p><h2 id="担忧"><a href="#担忧" class="headerlink" title="担忧"></a>担忧</h2><blockquote><p>测试驱动开发是一种管理你担忧的一种编程方式,担忧也不是说就是坏东西,自信过头也不好,但是恐惧给你一种在项目开头时,”我看不到这个项目的能够完成”,这种感觉,</p></blockquote><p>假如说痛给你一个”停下来”的信号,担忧给你一个”要认真”的提醒信号,但是与之而来的,担忧带来一些你消极的影响</p><ul><li>让你变得迟疑</li><li>让你开始抱怨</li><li>让你开始不愿交流</li><li>让你开始不想接受反馈</li></ul><p>这些没有一个对你编程有帮助,尤其是当你在编一个比较复杂的软件的时候,所以你怎么来面对这些呢</p><ul><li>抛弃迟疑,学习快速有效的编程</li><li>不要拖沓,跟别人交流思路要清晰</li><li>不要拒绝反馈,去寻找更多有帮助的反馈</li></ul><p>想象一下编程就是你提桶着水过河,当你水桶很小的时候,你轻微的震动没什么影响,但是当你的水桶很大,而且水很满的时候,你会很累,你无时无刻不在担心你的水是否会撒掉.</p><p>这个时候你需要一个运水的管道,每当你用水桶打一点水,你可以把他放进管道,确保这点水安全到达对面,继续打水.</p><p>这个测试驱动开发中的测试就是运水的管道,一旦你的测试通过,你就知道水已经送过去了,不需要担心水到不到对岸了,你就这样一步一步让所以的工作正常进行,但你测试失败的时候,专注于让他通过,这样下一个下一个,慢慢的我们接触到了编程难题,你的测试慢慢覆盖到整个项目.</p><p><code>TDD</code>给你一种控制的能力.当年在外面开车碰到下雪,你可以迅速停车去做其他琐事,当雪停了,你可以继续开车.</p><p>所以很多人说他们使用<code>TDD</code>对于项目的变化更有控制力.</p><p>接下来我会用一个例子来详细的介绍<code>TDD</code>开发的流程.</p><blockquote><p>由于原作者是用<code>java</code>来介绍的,本人用<code>Python</code>较多,所以就用自己写的一个项目<code>sample</code>来做介绍,<a href="/2016/08/18/hookman-development-blog/">详细链接</a><br>接下来翻译一下书后面关于<code>TDD</code>的一些答疑</p></blockquote><h2 id="TDD答疑"><a href="#TDD答疑" class="headerlink" title="TDD答疑"></a><code>TDD</code>答疑</h2><blockquote><p>我希望在这里提出一些或大或小问题帮助你思考如何将<code>TDD</code>引入到你的个人实践里面</p></blockquote><h3 id="你的测试步伐到底该多大"><a href="#你的测试步伐到底该多大" class="headerlink" title="你的测试步伐到底该多大?"></a>你的测试步伐到底该多大?</h3><p>这里有两个引申过来的问题</p><ul><li>每个测试覆盖范围该多大</li><li>当你重构时到底有要迈多大步伐</li></ul><p>你的测试可以覆盖到你写的每一行代码和你每下一构,你的测试也可以覆盖你上百行代码和你几个小时后的重构,但是哪个才是我们该写的测试呢.</p><p>从某方面来看,你应该要能做到其中一个测试,虽然<code>TDD</code>宗旨就是每一步都非常清晰,每一步伐都要求非常小,但是我们对软件驱动开发的经验会使这个步伐或多或少产生影响.</p><p>当你开始重构的时候,你应该准备好将一个重构分成很多小的步伐.重构很有可能会发生错误,你把坑都踩一遍然后填上,然后你接下来重构的可能性就会小很多.一旦你完成每次用多个小步伐的一个重构,你可以试试遗漏一些步骤.</p><h2 id="你需要多少反馈"><a href="#你需要多少反馈" class="headerlink" title="你需要多少反馈"></a>你需要多少反馈</h2>]]></content>
      
      
      <categories>
          
          <category> 算法 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> TDD </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>hookman develpment blog</title>
      <link href="2016/08/18/opensource/hookman-development-blog/"/>
      <url>2016/08/18/opensource/hookman-development-blog/</url>
      
        <content type="html"><![CDATA[<blockquote><p>其实写这个项目的初衷是想实践一下<code>TDD</code>开发,因为自己刚看完一本<code>&lt;&lt;Test-Driven Development with Python&gt;&gt;</code>,以前只是了解一点开发测试,看完这本书感觉这种敏捷开发方式非常适合我,自己完整写过一些小项目,但是大项目经常由于各种代码框架,代码规模搞得最后成了烂项目,而且关于<code>TTD</code>一些建议比如<code>YAGNI(You ain&#39;t gonna need it)</code>(你不需要这个)对于你对项目的规模有一定的控制.写这篇博客一方面将我开发<code>hookman</code> 的<code>TDD</code>开发经历告诉大家,一方面希望更多人了解<code>TDD</code>开发,换一种开发方式,或许能让你找回编程的乐趣.</p></blockquote><p>##引言</p><p>当然网上有些人对<code>TDD</code>测试开发嗤之以鼻,认为开发过程中不应该由测试驱动,应该先把所有的核心都先完善,最后在来完成项目.</p><p>在这里我想说一点我的看法,测试驱动是一种从<strong>核心到细节</strong>的开发方式.</p><p>用画一个人来打比方,给你一张白纸,<code>TDD</code>要求你先画一个躯干,这个<code>人</code>是<code>人</code>首先要有个<code>人样</code>,当我们想画一个奔跑的人时,<code>TDD</code>要求你得给他再画两条裸腿,然后这两条腿怎么摆才能控制平衡,这两条腿穿什么鞋才比较生动,要接下来你慢慢测试考虑.</p><p>而我们传统的开发方式,是先从局部到整体,这种开发方式适合于小项目,而且像一个流水线出来的产物,比如说<code>web开发</code>,我们知道下一个要做的是视图层,然后模型层.但是这种开发方式不适合开发比如说一个新的软件,假如我们用传统开发方式,除非<code>leader</code>的掌控力非常好,一个越来越臃肿的项目很难坚持到最后,因为我们在项目完成的时候才能让项目真正的运行起来,在完成的过程中我们很容易迷失最后将项目烂掉.</p><p>而且我比较欣赏<code>TDD</code>开发的一个主要原因就是他同linux推崇的那种<strong>简单</strong>的软件开发文化很默契,我们在让我们测试通过时候,我们感觉在:欺骗自己”.</p><p>举个例子:</p><p>比如我有一个比较两个值谁大谁小的函数</p><pre><code>def my_max(a, b):    pass</code></pre><p> 我们写一个单元测试</p><pre><code>Mytest(unittest.TestCase):       test_my_max(self):           m = my_max(1,2)           self.aseertEqual(m, 2)</code></pre><p>我们运行这个测试肯定失败,然后我们开始”欺骗自己”让测试通过,修改<code>my_max</code></p><pre><code>def my_max(a, b):    return b</code></pre><p>再运行测试,通过了,ok,”欺骗成功”,但我们用脑袋一想就知道,不行这个代码不对,但是咋办”测试通过了”,修改测试.</p><pre><code>Mytest(unittest.TestCase):        test_my_max(self):            m = my_max(1,2)            self.assertEqual(m, 2)            n = my_max(2, 1)            self.assertEqual(n, 2)</code></pre><p>这下测试又通不过了,我们想要的功能就在一步一步测试拖动中慢慢实现,我们尽量用最简单的代码通过我们的测试.</p><p>看到这里有些人会认为我很傻,明明可以用两行代码(不能用系统函数<code>max</code>)</p><pre><code>def my_max(a, b):    if a &gt; b: return a    return b</code></pre><p>轻轻松松通过测试,但是这反倒是<code>TDD</code>非常不推崇的,你一次走了太多步,你让你的代码跑到<code>Test</code>前面去了,其实很多程序员都会写测试,但是他们的测试很多都是基于代码的,当一个大功能实现的时候才开始测试,这个时候你会发现很多你的隐藏<code>bug</code>就藏在你一大堆代码里面,所以有些人为了排<code>bug</code>一行一行删代码来<code>debug</code>.</p><p>诚然很多时候我们感觉我们能毫无<code>bug</code>的完成一大段的代码,但是我们不能保证100%正确,而且或许今天能行明天就不行了,<code>TDD</code>推崇测试山羊精神,想象一只山羊行走在陡崖峭壁上,他只能走一步停一步,我们写代码也一样,我们前面是陡崖峭壁,我们不能保证我们下一步就不掉坑里,所以我们要学习山羊精神一步一步.</p><p>接下来关于<code>TDD</code>开发一些详细经历,我会每天抽出一点时候写.</p>]]></content>
      
      
      <categories>
          
          <category> 开源 </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>hookman development notebook</title>
      <link href="2016/08/18/opensource/hookman-development-notebook/"/>
      <url>2016/08/18/opensource/hookman-development-notebook/</url>
      
        <content type="html"><![CDATA[<blockquote><p><code>hookman</code>  是基于<code>github</code>上的<code>webhooks</code>开发的一个用Python写的小程序<br>,基于TDD开发.<br>如果你想向项目贡献代码,请看<a href="http://blog.zhanglun.work/2016/08/18/hookman-development-blog/" target="_blank" rel="noopener">hookman 开发blog</a></p></blockquote><h2 id="简单用法"><a href="#简单用法" class="headerlink" title="简单用法"></a>简单用法</h2><ol><li>安装 <code>pip install hookman</code></li><li>配置<a href="https://developer.github.com/webhooks/" target="_blank" rel="noopener">github的webhooks</a>,设置监听服务器ip和端口3610(默认)</li><li>使用<br>3.1 进入github项目目录<code>cd /my/github/projectdir</code><br>3.2   运行 <code>hookman --run -d</code> </li><li>关闭<br>3.1 <code>hookman --stop</code></li></ol><h1 id="版本差异"><a href="#版本差异" class="headerlink" title="版本差异"></a>版本差异</h1><h2 id="v0-1-0"><a href="#v0-1-0" class="headerlink" title="v0.1.0"></a>v0.1.0</h2><ol><li>基本实现监听端口</li><li>添加 <code>run</code>,<code>stop</code>,<code>daemon</code>,<code>pidfile</code>,<code>logfile</code>,<code>projectdir</code>选项</li></ol><h2 id="v0-1-1"><a href="#v0-1-1" class="headerlink" title="v0.1.1"></a>v0.1.1</h2><ul><li>修复了在前台shell模式下的bug</li></ul>]]></content>
      
      
      <categories>
          
          <category> 开源 </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>TDD-隔离测试</title>
      <link href="2016/08/09/algorithm/TDD/TDD-Isolution-Test/"/>
      <url>2016/08/09/algorithm/TDD/TDD-Isolution-Test/</url>
      
        <content type="html"><![CDATA[<blockquote><p> 隔离测试是相对与于整合测试来说的，现代软件架构流行分层式、模块化，而隔离测试就是相当于在每层上进行测试，整合测试就是跨越多个层进行测试</p></blockquote><h2 id="introduction"><a href="#introduction" class="headerlink" title="introduction"></a>introduction</h2><p>举个简单例子来说，在django系列中，我们把表单提交分成两个层，一个<code>form</code>层，一个式<code>model</code>层</p><p><code>form</code>层相当于接近用户交互层，而<code>model</code>层与数据库联系更大， <code>form</code>层负责获取用户数据并验证，而<code>model</code>层根据<code>form</code>层数据将数据存入数据库。</p><p>隔离测试就是隔离<code>form</code>层向<code>model</code>层提交，而整合测试就是直接测试<code>form</code>层和<code>model</code>层。</p><p>判断一个测试是整合测试还是隔离测试就是看测试的边界，整合测试相当于我们更加熟悉，我们测试时通过伪造form提交，然后通过数据库获取存入数据来得到验证，而隔离测试就比较复杂，因为我们很难在一个耦合度高代码找到怎么隔离两个层的方法。</p><p>接下来我就介绍python里一个神器：mock</p><p><em>自己用过其他语言框架中的mock，但是python里面mock里面最神奇的的是里面的<code>patch</code>，就像一个超级补丁一样。</em></p><h2 id="Example"><a href="#Example" class="headerlink" title="Example"></a>Example</h2><blockquote><p>接下来我用一个例子来介绍一下如何写隔离测试</p></blockquote><p>首先我们在 <code>lists.forms.py</code>中一个表单model</p><pre><code>class ListForm(forms.models.ModelForm):    def save(self):        pass</code></pre><p>我们在<code>lists.models.py</code>有个model</p><pre><code>class List(models.Model):    def create_now():        pass</code></pre><p>现在来分析一下这个隔离测试，我们要测试<code>ListForm</code>中<code>save</code>方法</p><p>首先<code>ListForm</code>和<code>List</code>两个类是耦合的，一个整合测试，我们只要调用<code>save</code>方法，然后查询数据库就可以完成这个测试，然而隔离测试不同，我们只能测试在调用<code>save</code>方法时，他“干”了什么。他可能调用了<code>List</code>的<code>create_now</code>方法，将得到表单数据传了过去。</p><p>意味着我们只能测试到<code>save</code>方法调用了<code>List</code>方法。</p><p>那这个隔离单元测试该怎么写？</p><p><em>接下来我们隆重介绍mock里面的最强补丁<strong>patch</strong></em></p><p>根据TDD原则，我们先新建一个单元测试<code>IsolutionFormTest</code></p><pre><code>import unittestfrom unittest.mock import patch, Mock      ##load super patch from lists.models import Listfrom lists.forms import ListFormclass IsolutionFormTest(unitest.TestCase):       @patch(&apos;lists.forms.List.create_new&apos;)       def test_save_creates_new_list(self, mock_list_create_new):            form = ListForm(data={&apos;text&apos;: &quot;example text&quot;})            form.is_valid()    # get clean data            form.save()            mock_list_create_new.assert_called_once_with(                text= &quot;example text&quot;            )   # the major test</code></pre><p>我来介绍一下这个<code>patch</code>，就像名字一样补丁，通过我们使用字符串将要替换的函数写出来，当<code>test</code>运行时，会自动将函数替换成一个<code>mock</code>对象，通过参数（上面的<code>mock_list_create_new</code>）赋给函数。</p><p>你可以这样想象，当<code>form.save()</code>调用时，在<code>save</code>函数里面，我们如果使用了<code>lists.forms.List.create_new</code>这个函数，这个函数就会被直接被补丁替换掉，你如果使用了<code>lists.forms.List.create_new(text=&quot;xxx&quot;)</code>就会变成<code>mock_list_create_new(text=&quot;xxx&quot;)</code>,当你调用了<code>mock_list_create_new(text=&quot;xxx&quot;)</code>时，<code>mock_list_create_new</code>这个<code>mock</code>对象就会记录下来。</p><p>这样我们就通过<code>mock_list_create_new</code>测试了函数是否执行了没有，因为隔离开<code>form</code>层和<code>model</code>层的就是通过两者之间的接口。我们只用测试接口是否执行了没有就可以了。</p><p>这样我们就完成了隔离单元测试，运行一下肯定失败，我们接下来就把<code>save</code>方法完善一下通过测试。</p><pre><code>from django import formform list.models import Listclass ListForm(forms.models.ModelForm):    def save(self):        List.create_new(text=self.cleaned_data[&apos;text&apos;])</code></pre><p>ok测试通过了，我们就可以歇一口气了。</p><h2 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h2><p>相对于整合测试，隔离测试运行速度更快，但是相对的隔离测试对接口要求非常严格，好的方面利用我们进行更好的代码设计，更好的分析代码的复杂程度，并且当接口变迁的时候，隔离测试能迅速发现变化而报警，然而隔离测试工作量比较大，而且没有整合测试那么好理解。在实际生产中对于复杂的接口我们尽量进行隔离测试，对于简单接口我们使用整合测试能根据减少程序的耦合性，而且能迅速发现集成问题。</p><p>ps：对<code>patch</code>感兴趣的童鞋可以自行google，<code>patch</code>好玩的地方还有很多，这里为了篇幅我只介绍了最核心的使用方法，大家可以自行探索<code>patch</code>更多好玩的东西。s</p>]]></content>
      
      
      <categories>
          
          <category> 算法 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> TDD </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>TDD测试开发理解</title>
      <link href="2016/07/26/algorithm/TDD/TDD%E6%B5%8B%E8%AF%95%E5%BC%80%E5%8F%91%E7%90%86%E8%A7%A3/"/>
      <url>2016/07/26/algorithm/TDD/TDD%E6%B5%8B%E8%AF%95%E5%BC%80%E5%8F%91%E7%90%86%E8%A7%A3/</url>
      
        <content type="html"><![CDATA[<blockquote><p>看了 &lt;&lt; Python Web 开发 测试驱动方法&gt;&gt; 以后, 感觉自己找到自己的一些项目之所以不能够很好的 维护下去的原因.总结了一下自己的理解, 记录下来.</p></blockquote><h2 id="什么是-TDD"><a href="#什么是-TDD" class="headerlink" title="什么是 TDD"></a>什么是 TDD</h2><blockquote><p>Test-Driven Development</p></blockquote><p> TDD 是一种以测试为驱动开发的方法, 自己以前也听到过这个名词, 但是平常只是稍微写一点测试,并没有让测试领导开发节奏.</p><p>简单来说TDD是先写测试再写代码, 这于我们平时开发的时候有很大不同,比如我们写一个web页面, 我们一般是先写好代码,然后在浏览器上调试(相当于测试),然而测试驱动开发就不同了,我们一开始只写测试, 然后写代码让他通过我们的测试.</p><p>TDD的主题基本上很多人都知道,但是具体步骤和诀窍很多人不明就里.下面来详细介绍怎么让<code>Test</code><br>引导你的开发.</p><h2 id="怎么进行TDD"><a href="#怎么进行TDD" class="headerlink" title="怎么进行TDD"></a>怎么进行TDD</h2><blockquote><p>首先让我来看一看<code>Test</code>是什么, Test 分两种</p></blockquote><ol><li>功能测试</li><li>单元测试</li></ol><h3 id="功能测试"><a href="#功能测试" class="headerlink" title="功能测试"></a>功能测试</h3><blockquote><p>很多人知道单元测试却很少听到功能测试, 功能测试在开发过程中经常被人忽略,但是功能测试能很好的把握开发方向</p></blockquote><p>功能测试就是从用户角度出发, 从用户的角度测试代码.</p><p>用web项目开发来打比方, 用户只能通过浏览器来浏览你的web, 所有的交互只能通过浏览器来实现,功能测试就是模拟用户进行浏览器上的操作,具体来说我们可用使用<code>selenium</code>来操控<strong>firefox</strong>或<strong>chrome</strong>,通过我们使用<code>selenium</code>提供的接口操作浏览器访问页面获取<code>html</code>, 来进行功能测试.</p><p>功能测试就是测试一个项目成品的功能, 在平常的项目开发中,这个测试往往是提到项目完成之后人工进行, 然而在<strong>TDD</strong>中这个却被提到了最前面,他成了一个风向标,所有的代码目的都是为了实现这个功能测试.</p><h3 id="单元测试"><a href="#单元测试" class="headerlink" title="单元测试"></a>单元测试</h3><blockquote><p>单元测试这个很多人都很了解, python里面有<code>unittest</code> 这个单元测试框架.就是专门为单元测试而开发的.</p></blockquote><p>单元测试历史悠久,很多人都写过单元测试,不过大部分人写单元测试都是在函数或者类完成之后写的.</p><p>在TDD中,单元测试进行最频繁的测试, 在功能测试完, 写每一个函数都提倡先写单元测试, 然后进行开发.</p><p>单元测试关注点与功能测试不同, 单元测试注重的是每一个函数执行的结果, 给定一个输入就一定要得到一个确定的输出,比功能测试他更关注底层代码,毕竟功能测试只关注用户最后得到结果,单元测试将你的代码函数形成一个单元,逐个运行,逐个测试.</p><blockquote><p>前面介绍了一大堆概念,却没有落到实处, 功能测试很简单,我们评价一个项目,能很快的写出测试方法,但是对于单元测试来说,我到底该怎么进行,这同我平时的开发有什么不同,下面就详细的介绍单元测试的几个重要的要点.</p></blockquote><hr><p>留个坑慢慢填.</p>]]></content>
      
      
      <categories>
          
          <category> 算法 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> TDD </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>js的this引发的思考</title>
      <link href="2016/07/20/useless/js%E7%9A%84this%E5%BC%95%E5%8F%91%E7%9A%84%E6%80%9D%E8%80%83/"/>
      <url>2016/07/20/useless/js%E7%9A%84this%E5%BC%95%E5%8F%91%E7%9A%84%E6%80%9D%E8%80%83/</url>
      
        <content type="html"><![CDATA[<blockquote><p>最近这几天在开发一个hmtl5的游戏, 但是对于js怎么使用面对对象来编程有点困惑,查了一些资料<br>整理如下.</p></blockquote><h2 id="js的this用法"><a href="#js的this用法" class="headerlink" title="js的this用法"></a>js的this用法</h2><ul><li><p>非对象属性函数(内部函数)</p><pre><code>var point = {  x : 0,  y : 0, moveTo : function(x, y) {     // 内部函数    var moveX = function(x) { this.x = x;//this 绑定到了哪里？    };       // 内部函数    var moveY = function(y) {         this.y = y;//this 绑定到了哪里？     };      moveX(x);    moveY(y);  } }; point.moveTo(1, 1);  point.x; //==&gt;0  point.y; //==&gt;0  x; //==&gt;1 y; //==&gt;1</code></pre></li></ul><ul><li>对象属性函数</li></ul><pre><code>var point = {         x : 0,         y : 0,         moveTo : function(x, y) {                 this.x = this.x + x;                 this.y = this.y + y;                              }          }; point.x; // ==&gt; 1 point.y; // == &gt;1  point.moveTo(1, 1)//this 绑定到当前对象，即 point 对象 point.x; // ==&gt; 1 point.y; // == &gt;1 </code></pre><p>其实this的用法还有很多,我为什么只列出上面两种是因为所有的this用法都可以归为这两类.</p><blockquote><p>非对象属性就是这个函数不存在对象的内存空间里面的函数<br>对象属性就是说这个函数存在于对象内存空间的函数</p></blockquote><p>这样说很绕, 抽象的来说, 把每个函数都看做是对象, 我们把这个对象point想象成一个上锁的柜子A, 每个变量名就是一把钥匙, 我们这个柜子A里面可用放很多钥匙,每个钥匙又对应着其他柜子,我们也可以放东西,但是这些东西只能是一些数字字符串什么的,这就对应着对象的这种继承性,</p><p>现在我们来看放在柜子A里面的钥匙,有一个钥匙可用开其他某一个的柜子B(相当于对象的属性的<code>moveTo</code>函数),当我们就在柜子B里面使用<code>this</code>时,这个<code>this</code>是什么呢.</p><p>这个this就要找一个柜子, 那为什么要找一个柜子(对象呢)</p><blockquote><p>这就是js语言设计的松散, 当在非严格模式下, <code>this</code> 会被强制转换成一个对象, 对于例子一,因为内部函数 <code>this</code> 并没有给他赋值(你可以把他看做一个我们找不到他的钥匙的柜子), 所以<code>this</code>被强制转换成了全局的柜子(全局变量)</p></blockquote><h5 id="ps-严格模式在函数或变量前加上-39-use-strict-39"><a href="#ps-严格模式在函数或变量前加上-39-use-strict-39" class="headerlink" title="ps: 严格模式在函数或变量前加上 &#39;use strict&#39;;"></a>ps: 严格模式在函数或变量前加上 <code>&#39;use strict&#39;;</code></h5><p>怎么解决这个问题了, 有两种方法, 不是没有赋值给<code>this</code>嘛, 我们可用call来给函数内部的<code>this</code>赋值,       </p><pre><code>moveX.call(this, x);   // 我们把当前函数的this赋给内部函数</code></pre><p>但是这种方法不够优美, 我们还可以直接把 <code>this</code>用个变量<code>that</code>给内部函数用,</p><pre><code>var that = this;var moveY = function(y) {         that.y = y;//this  }; </code></pre><p>这样这个<code>this</code> 就不会被强制转换成全局变量了, 当然你可以开启严格模式, 这样this的话就会变成<code>undefined</code>, 你也不会因为你的代码问题而污染全部变量.</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>js 的 <code>this</code> 是面对对象编程的一种体现, 但是js的<code>this</code>由于有点不严格,所以有时候会出现一些令人意向不到的结果,</p><p><em>引用</em></p><ul><li><p><a href="https://www.ibm.com/developerworks/cn/web/1207_wangqf_jsthis/" target="_blank" rel="noopener">深入浅出 JavaScript 中的 this</a></p></li><li><p><a href="https://developer.mozilla.org/zh-CN/docs/Web/JavaScript/Reference/Strict_mode" target="_blank" rel="noopener">严格模式</a></p></li></ul>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>关于技术栈的总结</title>
      <link href="2016/07/19/summary/%E5%85%B3%E4%BA%8E%E6%8A%80%E6%9C%AF%E6%A0%88%E7%9A%84%E6%80%BB%E7%BB%93/"/>
      <url>2016/07/19/summary/%E5%85%B3%E4%BA%8E%E6%8A%80%E6%9C%AF%E6%A0%88%E7%9A%84%E6%80%BB%E7%BB%93/</url>
      
        <content type="html"><![CDATA[<blockquote><p>这段时间重新学习了web前端的技术栈,其实一开始加入社团时就是从前端开始入门的,等到了会仿写几个页面后就跳到了后端的坑,从html、css、js（只学了一点）又马不停蹄的转到.net平台、后来在老司机的带领下我们放弃了不开放的.net平台投入了python的怀抱</p></blockquote><p> python的确很好，“内裤”很多，相对于.net 相对于封闭的生态圈， python对第三方的类库的依赖很大，<code>pip</code> 是程序的常客，看到一个好的类库就pip下来了。</p><blockquote><p>学了不少程序语言，从静态语言到动态语言，这其中的转变刚开始的确让人很苦闷。</p></blockquote><p> 自己从c系的语言过来，刚接触C的时候对于“过程编程”有很深的体会，结果并不重要，重要的是过程，有时候为了写好一个完美的函数自己苦思冥想好几天，花在一个函数上的时间比项目的时间都多。</p><blockquote><p>然而到了学习python, 自己把C系的学习习惯带到python , 在做项目的过程中,当自己写完一个函数的时候总在想怎么优化代码,怎么抽象化对象让事情简单,慢慢的走入一个误区,让我忘记了python的口号  <strong>life is short, I use python</strong> .</p></blockquote><h5 id="python是一门目的性很强的语言-先让我实现功能-其他以后再说-我以前使用的时候对过程强调的太多了-一个函数我得测试测试很多遍-确定每个参数的作用还有影响-花了太多时间-适得其反-我的代码一点都不-pythonic"><a href="#python是一门目的性很强的语言-先让我实现功能-其他以后再说-我以前使用的时候对过程强调的太多了-一个函数我得测试测试很多遍-确定每个参数的作用还有影响-花了太多时间-适得其反-我的代码一点都不-pythonic" class="headerlink" title="python是一门目的性很强的语言, 先让我实现功能,其他以后再说, 我以前使用的时候对过程强调的太多了, 一个函数我得测试测试很多遍,确定每个参数的作用还有影响, 花了太多时间, 适得其反,我的代码一点都不 pythonic"></a>python是一门目的性很强的语言, <strong>先让我实现功能,其他以后再说</strong>, 我以前使用的时候对过程强调的太多了, 一个函数我得测试测试很多遍,确定每个参数的作用还有影响, 花了太多时间, 适得其反,我的代码一点都不 <strong>pythonic</strong></h5><p>我在学习 python的过程中走了很多误区,总结起来有几点:</p><ul><li>太注重过程,不注重结果</li><li>太注重功能丰富,不注重简单</li></ul><h4 id="在我看来-pythonic-就是用最简短清晰的代码最快的完成自己的目的"><a href="#在我看来-pythonic-就是用最简短清晰的代码最快的完成自己的目的" class="headerlink" title="在我看来, pythonic 就是用最简短清晰的代码最快的完成自己的目的"></a>在我看来, <strong>pythonic</strong> 就是<strong>用最简短清晰的代码最快的完成自己的目的</strong></h4><h2 id="python和javascript"><a href="#python和javascript" class="headerlink" title="python和javascript"></a>python和javascript</h2><blockquote><p>这个都是动态的脚本语言,javascript更倾向于脚本</p></blockquote><p>两个家伙在我看来都是<strong>鸭式</strong>语言, 当然js更倾向函数式,而python更倾向于对象式.</p><h3 id="python-是目的性强-javascrip-是表现力强"><a href="#python-是目的性强-javascrip-是表现力强" class="headerlink" title="python 是目的性强, javascrip 是表现力强"></a>python 是目的性强, javascrip 是表现力强</h3><p>以前在后端的时候总觉得只要把自己那端的语言学好就行了, 任何事python都能用来解决, 无论是数据库还是服务器,就算是图像都能用python处理(使用PIL),虽然有着GIL,有这性能低下等等的问题 .<br>颇有一番”学好数理化走遍全天下都不怕”的念头,</p><p>  然而在实际项目中慢慢发现, 计算我能用几百行python代码画出一朵花来,用js几句话就在canvas上弄出来了,而且很轻松就能换成其他的东西.</p><p>  得益于node.js社区的火热, 现在javascript也能在后端大显身手了, 学习了一点node.js,感觉通过node.js对js的封装,让javascript变成了一门类python的语言, 原本js文件只能运行在浏览器里面,通过node.js将每个文件封装成模块,就像python将每个文件封装成package,通过模块与模块的合作,js也能像python那样简单的用几行代码就构成一个强壮的服务器(虽然说现在node.js有点不稳定,但是随着node社区的努力也能将node.js强壮起来)</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><blockquote><p> 通过一个星期的重温js, 自己简单的涉猎了现在很火的Angular,Ionic.和Node.js,并简单的搭建 Electron + Cordova + Ionic + Angular <a href="http://www.zhanglun.me" target="_blank" rel="noopener">原来博客</a> 的 Desktop端<br>  (linux + windows + OS)的软件版,还有移动版(由于本人没有OS操作系统,只做了Android版),还花了半天时间搭建了这个基于hexo的博客,算是完成了全平台的搭建 </p></blockquote><p> 接下来我会自己工作前的大学时光好好的培养自己解决问题的能力,希望能在工作前爱上并享受自己将要做的工作!</p>]]></content>
      
      
      <categories>
          
          <category> 随想 </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>PIL （Pillow）</title>
      <link href="2016/07/18/python/PIL/"/>
      <url>2016/07/18/python/PIL/</url>
      
        <content type="html"><![CDATA[<p>Pillow 是 PIL的对Python3支持的另外一个分支，当然他对Python2也兼容，由于PIL安装起来比较烦，而使用pip可以很轻松的安装Pillow，所以我选择Pillow使用，但是其核心还是PIL库的。</p><hr><p>Python的图形处理库如PIL一直很强大，但是要想使用好它必须对图片有一定的知识储备。<br>使用起来很简单</p><pre><code>from PIL import Image</code></pre><p>引用Image包</p><pre><code>im = Image.open(&apos;1.png&apos;)</code></pre><p>打开图片，得到一个im对象，我们接下来就可以对这个对象进行操作（前提有这个1.png图片）</p><p>我们先看一下他的一些属性</p><pre><code>&gt;&gt;&gt; print im.format, im.size, im.modePNG (83, 81) RGB</code></pre><p>第一个我们输出图片的格式，图片有很多种格式，常用的有jpg、png还有gif动图啊，PIL支持很多种格式，我们可以使用PIL轻松的将格式转换，<code>im.save(&#39;1.jpg&#39;)</code>,当然你可以选择格式假如你没选好后缀名的话，im.size就是图片大小，他返回的是一个元组第一个长度第二个是宽度，单位是像素。<br>现在就谈谈 这三个属性对应的关系吧<br> 首先我们使用一张像素图来说吧</p><p><img src="http://img1.imgtn.bdimg.com/it/u=214288124,4080808149&amp;fm=21&amp;gp=0.jpg" alt></p><p>我们存贮图片的时候是将整个图像分成很多个相同的小方块，每个小方块我们称为像素，当然一张图片分的越小，像素越多，那么图片就越接近真实图片，上面的<code>im.size</code>属性就告诉我们，这张图片分成了，长为83px，宽为81px的图片，那么一共有83*81=6723个像素点，每个像素点里面存什么呢，这就是<code>im.mode</code>属性告诉我们的，贴一下属性有什么吧</p><ol><li>1 (1-bit pixels, black and white, stored with one pixel per byte)</li><li>L (8-bit pixels, black and white)</li><li>P (8-bit pixels, mapped to any other mode using a color palette)</li><li>RGB (3x8-bit pixels, true color)</li><li>RGBA (4x8-bit pixels, true color with transparency mask)</li><li>CMYK (4x8-bit pixels, color separation)</li><li>YCbCr (3x8-bit pixels, color video format)</li><li>I (32-bit signed integer pixels)</li><li>F (32-bit floating point pixels)</li></ol><p>像素存贮就是涉及到颜色的存贮，在早期的黑白游戏机，只有黑和白两种，那么每个像素点就只有1位颜色来存贮，1位只能存贮两种颜色，八位色就能存256种颜色，像八位我们能用256个油漆桶/256色调色板来形容，像上面我们使用的RGB是由三种三原色红绿蓝混合而成，我们知道大自然所有的颜色都可以用红绿蓝三种颜色调配出来，所以RGB又被称为真彩（true color），每种颜色我们都分成256种，所以我们一共有256<em>256</em>256=16777216种颜色可以调配，像素的其他模式我们不介绍太多，有兴趣的可以自己钻研。<br>那么我们知道每个像素占多少字节，又知道共有多少个像素，那我们是不是就可以直接计算出来图片大小，来验证一下</p><p>以第一张图片为例，共有83<em>81=6723个像素点，用RGB模式，每个像素三个字节，共有6723</em>3=20667b=20kb，但是我这张图片只有11.6kb，误差太大了吧，这时候我们就要介绍一下上面那个<code>im.format</code>属性了，这张图片采用png格式，我们先尝试一下把他转成JPG格式吧</p><pre><code>im.save(&apos;1.jpg&apos;)</code></pre><p>我们再查看一下这个<code>1.jpg</code>的大小，只有2.24kb了，我们用PIL打开这张图片</p><pre><code>&gt;&gt;&gt; im2 = Image.open(&apos;1.jpg&apos;)&gt;&gt;&gt; print im2.format, im2.size, im2.mode JPEG (83, 81) RGB</code></pre><p>图片大小没有改变，但是format变成了JPEG，而且文件大小变成原来的1/5,<br>JPEG和GIF和PNG是三种图片压缩技术，他们使用压缩算法把图片压缩成很小，当我们打开图片时，解密算法把他还原出来，所以我们算出来的大小与压缩后的大小是不一样的。<br>有了这些概念我们就能更好的使用PIL提供给我们的magic方法，下次在谈我对PIL的高级应用吧。</p>]]></content>
      
      
      <categories>
          
          <category> Python </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>泛型继承的理解</title>
      <link href="2016/07/18/useless/%E6%B3%9B%E5%9E%8B%E7%BB%A7%E6%89%BF%E7%9A%84%E7%90%86%E8%A7%A3/"/>
      <url>2016/07/18/useless/%E6%B3%9B%E5%9E%8B%E7%BB%A7%E6%89%BF%E7%9A%84%E7%90%86%E8%A7%A3/</url>
      
        <content type="html"><![CDATA[<h3 id="泛型对于解决面对对象编程的算法设计可以提高其运算速度，但是对于引用类型来说还是没什么差别，因为引用类型只是指针的地址的调用，简单来说泛型还是挺好理解的，但是对于泛型、非泛型、继承和接口的融合就有些迷惑了。"><a href="#泛型对于解决面对对象编程的算法设计可以提高其运算速度，但是对于引用类型来说还是没什么差别，因为引用类型只是指针的地址的调用，简单来说泛型还是挺好理解的，但是对于泛型、非泛型、继承和接口的融合就有些迷惑了。" class="headerlink" title="泛型对于解决面对对象编程的算法设计可以提高其运算速度，但是对于引用类型来说还是没什么差别，因为引用类型只是指针的地址的调用，简单来说泛型还是挺好理解的，但是对于泛型、非泛型、继承和接口的融合就有些迷惑了。"></a>泛型对于解决面对对象编程的算法设计可以提高其运算速度，但是对于引用类型来说还是没什么差别，因为引用类型只是指针的地址的调用，简单来说泛型还是挺好理解的，但是对于泛型、非泛型、继承和接口的融合就有些迷惑了。</h3><h5 id="比如说这种接口"><a href="#比如说这种接口" class="headerlink" title="比如说这种接口"></a>比如说这种接口</h5><pre><code>public interface IEnumeratot&lt;T&gt;:IDisposable,IEnumerator,ICompare&lt;T&gt;</code></pre><h4 id="这个泛型接口继承了两个非泛型接口，和一个泛型接口。"><a href="#这个泛型接口继承了两个非泛型接口，和一个泛型接口。" class="headerlink" title="这个泛型接口继承了两个非泛型接口，和一个泛型接口。"></a>这个泛型接口继承了两个非泛型接口，和一个泛型接口。</h4><hr><h3 id="我一开始理解泛型就是一个个模型，只要我们把类型一个参数赋给他，他就能生成一个标准的类型，他缺少的只是一个参数而已，我们引用的时候感觉就像我们引用一个“全体方法”，把参数赋给类型后就可以一直调用类中的方法了，但是对于接口的继承如何理解？"><a href="#我一开始理解泛型就是一个个模型，只要我们把类型一个参数赋给他，他就能生成一个标准的类型，他缺少的只是一个参数而已，我们引用的时候感觉就像我们引用一个“全体方法”，把参数赋给类型后就可以一直调用类中的方法了，但是对于接口的继承如何理解？" class="headerlink" title="我一开始理解泛型就是一个个模型，只要我们把类型一个参数赋给他，他就能生成一个标准的类型，他缺少的只是一个参数而已，我们引用的时候感觉就像我们引用一个“全体方法”，把参数赋给类型后就可以一直调用类中的方法了，但是对于接口的继承如何理解？"></a>我一开始理解泛型就是一个个模型，只要我们把类型一个参数赋给他，他就能生成一个标准的类型，他缺少的只是一个参数而已，我们引用的时候感觉就像我们引用一个“全体方法”，把参数赋给类型后就可以一直调用类中的方法了，但是对于接口的继承如何理解？</h3><p>对于泛型类的继承，继承的类必须实现泛型的参数或者保留泛型的参数，比如下面</p><pre><code>  public class A&lt;T&gt;{    public T tt;}public class C&lt;T&gt; : A&lt;T&gt;{    public T tt;}</code></pre><p>或者是这样</p><pre><code>  public class A&lt;T&gt;{    public T tt;}public class C : A&lt;string&gt;{    C cc;}</code></pre><p>对于泛型继承非泛型类，比如下面</p><pre><code>public class A {     A aa; } public class B&lt;T&gt;:A{    B&lt;T&gt; bb;}</code></pre><p>基类是非泛型，而继承的是泛型类，我感觉这种构造就是让泛型类多了一种<strong>包容性</strong>,比如下面的链表实现的代码，让基类是非泛型，而继承是泛型，就能让链表可以连起很多种类型的数据，而本身的类型安全没有丢失。</p><pre><code>public class Node{        pretected Node next;          public Node(Node next){            this.next=next;            }}public class TypeNode&lt;T&gt;:Node{    public T data ;    public TypeNode(T data):this(data ,null){        }    public TypeNode(T data,Node next):base(next){        this.data=data;        }    }</code></pre><p>泛型的约束</p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>关于python3和python2 import区别</title>
      <link href="2016/07/18/python/%E5%85%B3%E4%BA%8Epython3%E5%92%8Cpython2import%E5%8C%BA%E5%88%AB/"/>
      <url>2016/07/18/python/%E5%85%B3%E4%BA%8Epython3%E5%92%8Cpython2import%E5%8C%BA%E5%88%AB/</url>
      
        <content type="html"><![CDATA[<blockquote><p>最近从python2转到python3,发现还是有一些不同,一些库改名字很好解决,但是这个import机制不了解原理是不好理解的.</p></blockquote><h4 id="python2是默认相对路径导入-python3是默认绝对路径导入"><a href="#python2是默认相对路径导入-python3是默认绝对路径导入" class="headerlink" title="python2是默认相对路径导入,python3是默认绝对路径导入"></a>python2是默认相对路径导入,python3是默认绝对路径导入</h4><p>首先这个包的导入机制,就是你在一个module里面引用另一个module,python运行文件有两种方式,一种是直接以主文件运行(默认以这种方式运行,同下面一种有点区别),一种是以module形式运行,就是用<code>python -m filename</code>方式调用.</p><h2 id="以module的方式运行"><a href="#以module的方式运行" class="headerlink" title="以module的方式运行"></a>以module的方式运行</h2><p>####### 什么是相对导入和绝对导入呢<br>相对导入是用一个<code>.</code>来声明的,相当于Unix上的选择当前文件夹.</p><p>假设你的文件目录为下面的</p><pre><code>main|    main.py|     __init__py|   momod+|      | __init__.py|      | pack.py|       | flask|      |    |  __init__.py|       |    |  myflask.py--------------------</code></pre><h5 id="python里面的module分三种-一种是build-in-module-内建库-一种是第三方库-还有一种是你自己写的库-如上面的flask"><a href="#python里面的module分三种-一种是build-in-module-内建库-一种是第三方库-还有一种是你自己写的库-如上面的flask" class="headerlink" title="python里面的module分三种,一种是build-in module(内建库),一种是第三方库,还有一种是你自己写的库(如上面的flask)."></a>python里面的module分三种,一种是build-in module(内建库),一种是第三方库,还有一种是你自己写的库(如上面的flask).</h5><p>在python2里面,当你import 一个module时,搜索的顺序是 内建库,自己的库,第三方库.</p><p>####### 而在python3里面顺序为 内建库,第三方库,你自己的库.</p><p>我自己感觉python3的import的机制更为清晰,因为当你import一个库时,假如你写的库和第三方库重合时,你优先导入第三方库,如果你不适用声明相对导入的话,你无法正确的导入自己的库,而使用<code>.</code>来声明库来自自己的代码让代码的结构更加清晰了.所以如果你想让你的代码兼容py2和py3,你自己的库都要采用相对导入方法来导入.<br>比如在momod 里面的pack.py假如想引用flask的myflask.py要这样写</p><pre><code>from .flask import myflask</code></pre><p>########### 假如你在py2中写了 <code>from flask import myflask</code>(并且你安装了flask库),这个是可以成功运行但是在py3中就会报错,因为他会优先导入flask库假如你没有显示声明相对导入的话.</p><hr><p>上面成立的前提是将pack.py以module方式运行,或者运行main.py在其中引入pack.py.接下来讲讲以主文件运行的不同.</p><h4 id="当你直接使用"><a href="#当你直接使用" class="headerlink" title="当你直接使用"></a>当你直接使用</h4><pre><code>python pack.py</code></pre><p>你假如在pack.py里面使用了这个</p><pre><code>from .flask import myflask</code></pre><p>引用了自己的myflaskmodule,在py2和py3中都会下面报这个错</p><pre><code>SystemError: Parent module &apos;&apos; not loaded, cannot perform relative import</code></pre><p>因为当你以主文件方式运行 pack.py ,python会吧pack.py重命名为<code>__main__</code>,所以用.相对路径也不会是当前文件.所以全部都只能用绝对引用.所以在主文件运行在python3里面有个问题,假如你自己的库与第三方库有重名.</p><h4 id="python3默认绝对路径-自己的库不会优先于第三方库被扫描-有两个解决方法-把自己的库重命名-第二个方法就是把包含主文件的文件夹加上init-py-你可以在sys-path的路径里加上-或者具体上一个上一个文件夹的路径-怪绕口的-其实你只要python能找到你的上一个文件夹-就行"><a href="#python3默认绝对路径-自己的库不会优先于第三方库被扫描-有两个解决方法-把自己的库重命名-第二个方法就是把包含主文件的文件夹加上init-py-你可以在sys-path的路径里加上-或者具体上一个上一个文件夹的路径-怪绕口的-其实你只要python能找到你的上一个文件夹-就行" class="headerlink" title="python3默认绝对路径,自己的库不会优先于第三方库被扫描.有两个解决方法,把自己的库重命名,第二个方法就是把包含主文件的文件夹加上init.py,你可以在sys.path的路径里加上..或者具体上一个上一个文件夹的路径.怪绕口的,其实你只要python能找到你的上一个文件夹,就行."></a>python3默认绝对路径,自己的库不会优先于第三方库被扫描.有两个解决方法,把自己的库重命名,第二个方法就是把包含主文件的文件夹加上<strong>init</strong>.py,你可以在sys.path的路径里加上<code>..</code>或者具体上一个上一个文件夹的路径.怪绕口的,其实你只要python能找到你的上一个文件夹,就行.</h4><p>说到这里顺便插一句对doctest和集成测试的理解.</p><h4 id="由于我平时喜欢一边写代码一遍测试功能-图方便就使用doctest直接插在方法里面-在代码后面加上"><a href="#由于我平时喜欢一边写代码一遍测试功能-图方便就使用doctest直接插在方法里面-在代码后面加上" class="headerlink" title="由于我平时喜欢一边写代码一遍测试功能,图方便就使用doctest直接插在方法里面,在代码后面加上"></a>由于我平时喜欢一边写代码一遍测试功能,图方便就使用doctest直接插在方法里面,在代码后面加上</h4><pre><code>if __name__ == &apos;__main__&apos;:        import doctest        doctest.tesmod()</code></pre><p>平时写小module的时候没有问题,在将python2转python3时候,出现问题,因为我要测试这个module时,会报上面的那个错,因为我要测试他的话必须将它作为主文件.查了资料知道,其实doctest虽然在当前页面代码测试,但是对于module的话,最好采用集成测试,一是module很多,假如一个一个运行很麻烦,二是有时候module必须多个一起测试,所以测试module时要用集成测试来取代doctest.</p>]]></content>
      
      
      <categories>
          
          <category> Python </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>python学习</title>
      <link href="2016/07/18/python/python%E5%AD%A6%E4%B9%A0/"/>
      <url>2016/07/18/python/python%E5%AD%A6%E4%B9%A0/</url>
      
        <content type="html"><![CDATA[<h4 id="由于有其他编程语言基础，所以对于python的学习并不吃力，但是整体感觉python的确与前面学习c、c———"><a href="#由于有其他编程语言基础，所以对于python的学习并不吃力，但是整体感觉python的确与前面学习c、c———" class="headerlink" title="由于有其他编程语言基础，所以对于python的学习并不吃力，但是整体感觉python的确与前面学习c、c———"></a>由于有其他编程语言基础，所以对于python的学习并不吃力，但是整体感觉python的确与前面学习c、c———</h4><h3 id="1-实时编译VS静态编译"><a href="#1-实时编译VS静态编译" class="headerlink" title="1. 实时编译VS静态编译"></a>1. 实时编译VS静态编译</h3><p>不需要输入任何前缀，直接将代码放在python解释器上面就能运行，虽然window下不支持直接点开文件就能使用，但是只要安装了python解释器就能很轻松的运行。</p><p>分量轻是他的特点吧！相比打开vs等半天然后，编译连接最后执行。python是一门很轻巧的语言，没有满屏的分号，大括号，基本类型比如int、string、float不区分直接拿来用就可以了，任何一个变量都是一个对象，对象可以千变万化，感觉python是一门很野的熊孩子什么都不在乎，比如说你什么了相同的两个变量</p><h1 id="这门强类型语言则不允许，在同级作用域内他只允许声明一次，python或许已经没有声明了，每个名字只是一个对象而已并没有他的归属。"><a href="#这门强类型语言则不允许，在同级作用域内他只允许声明一次，python或许已经没有声明了，每个名字只是一个对象而已并没有他的归属。" class="headerlink" title="这门强类型语言则不允许，在同级作用域内他只允许声明一次，python或许已经没有声明了，每个名字只是一个对象而已并没有他的归属。"></a>这门强类型语言则不允许，在同级作用域内他只允许声明一次，python或许已经没有声明了，每个名字只是一个对象而已并没有他的归属。</h1><h4 id="2-动态语言VS静态语言"><a href="#2-动态语言VS静态语言" class="headerlink" title="2.动态语言VS静态语言"></a>2.动态语言VS静态语言</h4><h1 id="给我感受是一颗静止的树的话，那么python就是一匹‘野马’，C"><a href="#给我感受是一颗静止的树的话，那么python就是一匹‘野马’，C" class="headerlink" title="给我感受是一颗静止的树的话，那么python就是一匹‘野马’，C"></a>给我感受是一颗静止的树的话，那么python就是一匹‘野马’，C</h1>]]></content>
      
      
      <categories>
          
          <category> Python </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>字符串处理</title>
      <link href="2016/07/18/python/%E5%AD%97%E7%AC%A6%E4%B8%B2%E5%A4%84%E7%90%86/"/>
      <url>2016/07/18/python/%E5%AD%97%E7%AC%A6%E4%B8%B2%E5%A4%84%E7%90%86/</url>
      
        <content type="html"><![CDATA[<hr><h1 id="string"><a href="#string" class="headerlink" title="string"></a>string</h1><blockquote><p>原来的很多函数都逐渐迁移到<code>str</code>和<code>unicode</code>对象上去了,<br>不过有两个函数没有迁移出去</p></blockquote><hr><p>第一个是<code>capwords</code></p><blockquote><p>个人觉得没什么卵用<br>就是将英文单词首字母大写</p></blockquote><p>比如</p><pre><code>string.capwords(&apos;this are some words&apos;)</code></pre><p>输出为 <code>This Are Some Words</code></p><hr><p>第二个我觉得挺有趣,他可以帮你把对应的文字换成你设定的</p><p>我们可以用它来设计出莫斯密码’=’.</p><pre><code>&quot;&quot;&quot;create a table&quot;&quot;&quot;table = string.maketrans(&apos;abc&apos;,&apos;123&apos;)print &apos;abc123&apos;.translate(table)</code></pre><p>输出为<code>123123</code>成功把abc转成了123</p><hr><p>string还有一个模板类型<code>Template</code><br>这个类型同我们转义差不多(%),不过能够实现的<br>更加自主化<br>我们可以继承这个类来修改模板类的具体实现<br>而且这个模板类有一个</p><hr><h1 id="textwrap"><a href="#textwrap" class="headerlink" title="textwrap"></a>textwrap</h1><blockquote><p>由于sublime输出一个很长的字符串很卡,这个格式化字符串的类能够帮我们<br>解决很多问题</p></blockquote><p>我一般去除整体缩进然后去填充字符串</p><pre><code>dedented_text = textwrap.dedent(&apos;...a long word...&apos;)print textwrap.fill(dedented_text, width=50)</code></pre>]]></content>
      
      
      <categories>
          
          <category> Python </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>EF搭建可扩展菜单</title>
      <link href="2016/07/18/useless/EF%E6%90%AD%E5%BB%BA%E5%8F%AF%E6%89%A9%E5%B1%95%E8%8F%9C%E5%8D%95/"/>
      <url>2016/07/18/useless/EF%E6%90%AD%E5%BB%BA%E5%8F%AF%E6%89%A9%E5%B1%95%E8%8F%9C%E5%8D%95/</url>
      
        <content type="html"><![CDATA[<blockquote></blockquote><h3 id="由于要做一个三级菜单存贮菜单和文章，由于菜单在很多地方用的到，于是想做一个可扩展性的菜单以便以后使用。"><a href="#由于要做一个三级菜单存贮菜单和文章，由于菜单在很多地方用的到，于是想做一个可扩展性的菜单以便以后使用。" class="headerlink" title="由于要做一个三级菜单存贮菜单和文章，由于菜单在很多地方用的到，于是想做一个可扩展性的菜单以便以后使用。"></a>由于要做一个三级菜单存贮菜单和文章，由于菜单在很多地方用的到，于是想做一个可扩展性的菜单以便以后使用。</h3><h4 id="由于以前从来没有做过动态的菜单，所以走了很多弯路，尤其搭配EF-Code-First更是坑了我一把，我想把我碰到坑给大家分享一下。"><a href="#由于以前从来没有做过动态的菜单，所以走了很多弯路，尤其搭配EF-Code-First更是坑了我一把，我想把我碰到坑给大家分享一下。" class="headerlink" title="由于以前从来没有做过动态的菜单，所以走了很多弯路，尤其搭配EF Code First更是坑了我一把，我想把我碰到坑给大家分享一下。"></a>由于以前从来没有做过动态的菜单，所以走了很多弯路，尤其搭配EF Code First更是坑了我一把，我想把我碰到坑给大家分享一下。</h4><h2 id="1-类库的实现"><a href="#1-类库的实现" class="headerlink" title="1.类库的实现"></a>1.类库的实现</h2><h3 id="首先我选择树这个数据结构来存贮我的菜单，我定义菜单Menus来作为一个最小单元，定义一个bool类型IsFoot来定义是否为根菜单，每个Menus有一个父级菜单Menus，有一群子菜单，下面是我定义的Menu库。"><a href="#首先我选择树这个数据结构来存贮我的菜单，我定义菜单Menus来作为一个最小单元，定义一个bool类型IsFoot来定义是否为根菜单，每个Menus有一个父级菜单Menus，有一群子菜单，下面是我定义的Menu库。" class="headerlink" title="首先我选择树这个数据结构来存贮我的菜单，我定义菜单Menus来作为一个最小单元，定义一个bool类型IsFoot来定义是否为根菜单，每个Menus有一个父级菜单Menus，有一群子菜单，下面是我定义的Menu库。"></a>首先我选择树这个数据结构来存贮我的菜单，我定义菜单Menus来作为一个最小单元，定义一个<code>bool</code>类型<strong>IsFoot</strong>来定义是否为根菜单，每个Menus有一个父级菜单Menus，有一群子菜单，下面是我定义的Menu库。</h3><pre><code>  [Description(&quot;菜单&quot;)]public class Menus{    [Key]    [Display(Name=&quot;菜单ID&quot;)]    public int MenusID { get; set; }    [Required]    [Display(Name=&quot;是否为根节点&quot;)]    public bool IsFoot { get; set; }    [Required]    [StringLength(25)]    [Display(Name=&quot;目录名字&quot;)]    public string Name { get; set; }    [Display(Name=&quot;是否删除&quot;)]    public bool IsDelete { get; set; }    [Display(Name=&quot;包含的文章&quot;)]    public virtual List&lt;Article&gt; articles { get; set; }    [Display(Name = &quot;父级菜单&quot;)]    public virtual Menus fatherMenus { get; set; }    [Display(Name = &quot;子菜单&quot;)]    public virtual List&lt;Menus&gt; sonList { get; set; }}</code></pre><h3 id="每个Menus都包含了一个文章集合-虽然有些菜单不一定有文章但是EF可以允许我们0对多，或1对多。"><a href="#每个Menus都包含了一个文章集合-虽然有些菜单不一定有文章但是EF可以允许我们0对多，或1对多。" class="headerlink" title="每个Menus都包含了一个文章集合,虽然有些菜单不一定有文章但是EF可以允许我们0对多，或1对多。"></a>每个Menus都包含了一个文章集合,虽然有些菜单不一定有文章但是EF可以允许我们0对多，或1对多。</h3><h2 id="2-生成数据库"><a href="#2-生成数据库" class="headerlink" title="2.  生成数据库"></a>2.  生成数据库</h2><hr><p>EF比较人性化的是，当我们数据库里面没有我们想要生成的表时，我们不需要多余的代码，只要当成数据库有表，像平时一样添加数据然后EF会帮我们自动在数据库里面建好表，当然你如果有相同名字的表话它报错，会提醒你数据库里面有如果想保存数据要做好数据迁移工作，数据迁移不是我们的重点，如果想了解的话，<a href="http://www.cnblogs.com/guomingfeng/archive/2013/06/15/mvc-ef-configuration-migration.html" target="_blank" rel="noopener">点击这里</a></p><h3 id="生成的数据库包含两个表，一个Menus表，一个是Article表（PS：上面没有给出Article的类型定义，想要的可以自己写），对于这个来说，我们并没有在表里面定义外键属性，只是用来一个引用属性，引用属性是一种“虚属性”，我们通过这个属性来建立起两个对象的虚拟联系，比如说父与子，这种关系是虚拟的对于两者之间的联系是通过血缘来联系的，这个血缘是存在的，相对应就是数据库里面的外键联系，外键也可以看做是表中的一个字段，它记录了一种关系。"><a href="#生成的数据库包含两个表，一个Menus表，一个是Article表（PS：上面没有给出Article的类型定义，想要的可以自己写），对于这个来说，我们并没有在表里面定义外键属性，只是用来一个引用属性，引用属性是一种“虚属性”，我们通过这个属性来建立起两个对象的虚拟联系，比如说父与子，这种关系是虚拟的对于两者之间的联系是通过血缘来联系的，这个血缘是存在的，相对应就是数据库里面的外键联系，外键也可以看做是表中的一个字段，它记录了一种关系。" class="headerlink" title="生成的数据库包含两个表，一个Menus表，一个是Article表（PS：上面没有给出Article的类型定义，想要的可以自己写），对于这个来说，我们并没有在表里面定义外键属性，只是用来一个引用属性，引用属性是一种“虚属性”，我们通过这个属性来建立起两个对象的虚拟联系，比如说父与子，这种关系是虚拟的对于两者之间的联系是通过血缘来联系的，这个血缘是存在的，相对应就是数据库里面的外键联系，外键也可以看做是表中的一个字段，它记录了一种关系。"></a>生成的数据库包含两个表，一个Menus表，一个是Article表（PS：上面没有给出Article的类型定义，想要的可以自己写），对于这个来说，我们并没有在表里面定义外键属性，只是用来一个引用属性，引用属性是一种“虚属性”，我们通过这个属性来建立起两个对象的虚拟联系，比如说父与子，这种关系是虚拟的对于两者之间的联系是通过血缘来联系的，这个血缘是存在的，相对应就是数据库里面的外键联系，外键也可以看做是表中的一个字段，它记录了一种关系。</h3><h4 id="由于EF的智能关系，当我们Code-First时，他会帮我们自动建好外键如果我们不定义的话，当我们使用EF的时候是不需要考虑外键的值的初始化，如果我们没有给他赋值EF会自动给他赋值。"><a href="#由于EF的智能关系，当我们Code-First时，他会帮我们自动建好外键如果我们不定义的话，当我们使用EF的时候是不需要考虑外键的值的初始化，如果我们没有给他赋值EF会自动给他赋值。" class="headerlink" title="由于EF的智能关系，当我们Code First时，他会帮我们自动建好外键如果我们不定义的话，当我们使用EF的时候是不需要考虑外键的值的初始化，如果我们没有给他赋值EF会自动给他赋值。"></a>由于EF的智能关系，当我们Code First时，他会帮我们自动建好外键如果我们不定义的话，当我们使用EF的时候是不需要考虑外键的值的初始化，如果我们没有给他赋值EF会自动给他赋值。</h4><hr><p>讲完了EF的建立，现在就谈谈使用Code First在项目中遇到的问题。</p><hr><p>这个问题主要出在给创建子菜单上，当我们创建子菜单时，我们用的是我们自己的类库代码进行初始化数据库，我们先得到菜单的ID然后在EF里面查询这个菜单，我们查询到这个实体，然后在菜单实体里面添加子菜单，在SaveChange()时候就报错了，EF称检测到有循环赋值的可能，让我们添加外键以避免冲突，我不记得看到的那篇博客看到有人也遇到相同的问题，如果只是普通的一对多（假如是A对1，2，3，4···），当我们给A那个新建一个5时，这个外键的位置是知道的，我们只要在5的外键位置存贮A的主键，然而当我们建立这种父级菜单时，每个菜单里面的外键可以是存贮父级的主键，也可以是子集的主键，所以EF并不能解决冲突，解决这个问题的方法有两种一种是在表中添加外键<br>如：</p><pre><code>       [ForeignKey(&quot;sonList&quot;)]public int sonListMenusID{get;set;}</code></pre><p>或者用Fluent API 在继承的方法 onModelCreate中添加</p><pre><code>modelBuilder.Entity&lt;Menus&gt;().HasRequired(p=&gt;p.sonList).WithMany(l=&gt;l.Menus).HasForeignKey(p=&gt;p.sonListMenusID)</code></pre><p>通过这种创建方式当我们创建子集菜单时我们就可以成功利用EF特性帮我们自动添加上外键，以及建立好实体关系。</p><p>关于更详细的外键知识可以点击<a href="http://www.cnblogs.com/Gyoung/archive/2013/01/22/2869782.html" target="_blank" rel="noopener">这里</a></p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>Python 线程(threading) 进程(multiprocessing)</title>
      <link href="2016/07/18/python/Python%E7%BA%BF%E7%A8%8B%E5%92%8C%E8%BF%9B%E7%A8%8B/"/>
      <url>2016/07/18/python/Python%E7%BA%BF%E7%A8%8B%E5%92%8C%E8%BF%9B%E7%A8%8B/</url>
      
        <content type="html"><![CDATA[<p>### </p><blockquote><p>最近学了两个python库，一个负责管理线程，一个负责管理进程，原来一直写的都<br>是些单线程的程序，虽然web也关于并发和多涉及到线程，但都是框架管理的，学习&gt;过后发现了解线程和进程对python的web开发也有一定帮助。下面先谈谈这对python对线程和进程的支持再谈谈对这两个库的应用。</p></blockquote><p><strong>python</strong>对线程的支持并不是非常好，所以你可以在很多文章上批评python的多线程的弊端，但是为什么<strong>python</strong>对多线程支持不好呢，为什么其他语言比如</p><h1 id="静态语言没有这个弊端呢。"><a href="#静态语言没有这个弊端呢。" class="headerlink" title="静态语言没有这个弊端呢。"></a>静态语言没有这个弊端呢。</h1><blockquote><p>首先我们要知道python是一种解释性语言，每段代码都需要解释器编译运行，解释器有很多种最主要的是<strong>CPython</strong>，其他还有<strong>IronPython</strong>和<strong>Jython</strong>，官方的是CPython解释器，我们一般说对多线程支持不好的就是说的CPython解释器（用的人最多就省略成python解释器),python解释器为什么对多线程支持不好呢，是因为GIL的存在，当然这个存在就是因为这门语言的的特性产生的。</p></blockquote><p>GIL是什么呢，下面是官方的解释</p><blockquote><p>In CPython, the global interpreter lock, or GIL, is a mutex that prevents multiple native threads from executing Python bytecodes at once. This lock is necessary mainly because CPython’s memory management is not thread-safe. (However, since the GIL exists, other features have grown to depend on the guarantees that it enforces.)</p></blockquote><p>就是GIL是python的互斥锁，简单的理解就是代码会锁住python解释器。理解代码的锁定是什么必须要先了解什么是多线程</p><blockquote><p>多线程表示一个主线程，多个子线程，主线程是程序执行时系统自动给你申请的一个线程，而子线程我们可以理解为一个代码块，我们可以充分利用硬件的支持比如说多核，让一个CPU执行主线程，其他CPU执行子线程，通过操作系统的虚拟内存技术让所有线程共享相同代码空间达到提高代码效率的作用，我们可以通俗的把一个进程比作一辆火车，车厢头为主线程，每节车厢为子线程，只要你车厢(子线程)越多，你运的货物也越多，但是也要考虑硬件的方面，</p></blockquote><p>了解完多线程是什么我们就可以解释GIL对多核CPU工作性能的影响了，在单核CPU里面，主线程在释放GIL的时候，把CPU让给子线程，子线程代码块得到GIL，然后执行，这样就能充分利用CPU，这个GIL对单核性能的发挥没有影响，能得到100%的利用，但是在多核的的时候就有问题了，假如主线程的代码一直需要解释器来执行，<br>比如说下面</p><pre><code>GIL.acquire()try:    while True:        do_something()finally:    GIL.release()</code></pre><p>主线程代码对GIL的锁定和解开只间隔很小的一个系统时间，子线程在其他CPU核心得到GIL解开后CPU的调度命令后才能被唤醒，但是当唤醒后，主线程的代码又锁了GIL，然后只能等待主线程下次调度命令，但是到了切换时间又切换回去到待调整状态，一直处于唤醒，等待的恶性循环，多核的功能完全没有发挥出来而且还比单核更加差，所以python因为GIL的存在对密集型的线程支持不佳，但是假如主线程是在执行想web这样等待用户输入，而不是每分每秒都在使用解释器执行代码，多线程的优势就能发挥出来。</p><h3 id="解决方案"><a href="#解决方案" class="headerlink" title="解决方案"></a>解决方案</h3><blockquote><p>GIL作为解释器的一个Bug一样的存在，我们也有一定的解决方法，开线程，和用Ctype绕过解释器是我们一般的解决方法，你想了解更多可以看<a href="http://zhuoqiang.me/python-thread-gil-and-ctypes.html" target="_blank" rel="noopener">这个</a><br>接下来主要解绍用multiprocessing来绕过多线程的瓶颈</p></blockquote><h3 id="线程锁和进程锁"><a href="#线程锁和进程锁" class="headerlink" title="线程锁和进程锁"></a>线程锁和进程锁</h3><h5 id="为了实现线程安全，我们也要借助锁的存在，我们先用下面的代码来验证一下多线程对于线程安全的问题。我们声明一个线程锁-threading-Lock"><a href="#为了实现线程安全，我们也要借助锁的存在，我们先用下面的代码来验证一下多线程对于线程安全的问题。我们声明一个线程锁-threading-Lock" class="headerlink" title="为了实现线程安全，我们也要借助锁的存在，我们先用下面的代码来验证一下多线程对于线程安全的问题。我们声明一个线程锁 threading.Lock(),"></a>为了实现线程安全，我们也要借助锁的存在，我们先用下面的代码来验证一下多线程对于线程安全的问题。我们声明一个线程锁 <code>threading.Lock()</code>,</h5><pre><code>class Counter(object):    def __init__(self, start=0):    self.lock = threading.Lock()    self.value = startdef increment(self):    logging.debug(&apos;Waiting for lock&apos;)    self.lock.acquire()    try:        if self.value &lt; 8:</code></pre><h1 id="模拟负载"><a href="#模拟负载" class="headerlink" title="模拟负载"></a>模拟负载</h1><pre><code>            logging.debug(&apos;Acquired lock&apos;)            self.value = self.value + 1    finally:        self.lock.release()def worker(c):    for i in range(2):        pause = random.random()        logging.debug(&apos;Sleeping %0.02f&apos;, pause)        time.sleep(pause)        c.increment()    logging.debug(&apos;Done&apos;)counter = Counter()for i in range(20):    t = threading.Thread(target=worker,args=(counter,))    t.start()main_thread = threading.currentThread()for t in threading.enumerate():    if t is not main_thread:</code></pre><h1 id="保护线程"><a href="#保护线程" class="headerlink" title="保护线程"></a>保护线程</h1><h1 id="得到value值"><a href="#得到value值" class="headerlink" title="得到value值"></a>得到value值</h1><p>我们运行之后得到<code>counter.value</code>值为8，这很好理解因为我们限制了它的大小小于8时才自增1，但是如果我们把锁去掉呢，我们把<code>self.lock.acquire()`</code>self.lock.release()<code>都注释掉，得到的结果却是一个21，而且每次运行的结果都可能不一样，由于线程在实现自增的时候有一定的时间(</code>time.sleep(2)<code>),所以当多个进程执行的时候当他们从堆栈上取到</code>counter.value<code>值都为7时，这时候他们都满足</code>counter.value<code>小于8，所以都执行了自增，在系统负载2秒之间（</code>time.sleep(2)`）有多少个线程执行就会逃过我们给他的限制，这样就造成了线程的不安全，但是我们给他加上锁之后，无论开多少个线程，最终结果都是8。在python里面我们线程锁和进程锁我们可以看做是同一种东西。</p><h4 id="ps：当同一线程相互争夺锁时，失败的会进出线程队列等待锁解开。"><a href="#ps：当同一线程相互争夺锁时，失败的会进出线程队列等待锁解开。" class="headerlink" title="ps：当同一线程相互争夺锁时，失败的会进出线程队列等待锁解开。"></a>ps：当同一线程相互争夺锁时，失败的会进出线程队列等待锁解开。</h4><h1 id="线程进程工作方式"><a href="#线程进程工作方式" class="headerlink" title="线程进程工作方式"></a>线程进程工作方式</h1><h3 id="单行"><a href="#单行" class="headerlink" title="单行"></a>单行</h3><blockquote><p>单行主要通过锁来实现，线程通过锁<code>threading.Lock()</code>对象创造锁，进程通过<code>multiprocessing.Lock()</code>对象创建进程锁，单行操作一般都是对共享数据修改的一种保护。</p></blockquote><h3 id="并行"><a href="#并行" class="headerlink" title="并行"></a>并行</h3><blockquote><p>并行操作是一般是对数据的一种共享，一般不对公共数据涉及修改，我们可以创造很多线程和进程一起并行操作，也可以限制线程和进程的并行数量，两种方式选择主要是判断代码类型是I/O密集还是线程密集型的。如何限制并行数量我们可以通过<code>threading.Semaphore（sizenum）</code>(进程为<code>multiprocessing.Semaphore(sizenum)</code>)我们可以控制对共享的线程数量。进程提供了一个进程池的类型(<code>multiprocessing.Pool</code>)，我们可以创建一个维护了一定程的进程池，但是他同时并行的数量并没有控制，只是帮我们创建了这个进程池，每个进程并不是只执行一个任务，可能执行多个方法通过一个进程.</p></blockquote><h3 id="单行混合并行"><a href="#单行混合并行" class="headerlink" title="单行混合并行"></a>单行混合并行</h3><blockquote><p>单行和并行混合我们可以通过在代码中设置锁来实现，当然python给我们提供了两种对象来实现单行和并行的控制，线程的是<code>threading.Event()</code>和<code>threading.Condition()</code>,进程的是<code>multiprocessing.Event()</code>和<code>multiprocessing.Condition()</code> 两种对象都是提供了一种命令指令，但是Event对象可以用来判断命令是否下达而做出相应的反应，而Condition对象更倾向于当命令下达后才执行并行的操作。</p></blockquote><h2 id="线程和进程通信方式"><a href="#线程和进程通信方式" class="headerlink" title="线程和进程通信方式"></a>线程和进程通信方式</h2><blockquote><p>当我们想让线程和进程共同执行一些固定的任务，我们就需要线程和进程之间能够通信，线程和进程通信我们使用队列(<code>Queue</code>),进程和线程的<code>Queue</code>有点差异，就是进程<code>Queue</code>传递的对象必须pickle化，而且为了能够使用<code>join()</code>（保护进程)<code>task_done</code>(通知任务完成),我们一般使用<code>JoinableQueue</code><br>代替<code>Queue</code>在进程中。</p></blockquote><p>Queue对象之间通过<code>put</code>和<code>get</code>通信，我们把任务put上去，<code>Queue</code>自动分配给当前的线程或进程，<br>这样就能实现对任务的流水作业话。</p><p>引用</p><p> <a href="https://zh.wikipedia.org/wiki/GIL" target="_blank" rel="noopener">12/26/2015 10:50:21 PM GIL维基资料</a></p><p><a href="http://cenalulu.github.io/python/gil-in-python/" target="_blank" rel="noopener">GIL博文</a></p>]]></content>
      
      
      <categories>
          
          <category> Python </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>EFCode First 导航属性</title>
      <link href="2016/07/18/useless/EFCodeFirst%E5%AF%BC%E8%88%AA%E5%B1%9E%E6%80%A7/"/>
      <url>2016/07/18/useless/EFCodeFirst%E5%AF%BC%E8%88%AA%E5%B1%9E%E6%80%A7/</url>
      
        <content type="html"><![CDATA[<h4 id="首先谈谈自己对EF的接触的过程吧，最先接触EF只是因为EF支持从数据库把关系扒下来，可以省掉自己写Select、Update、Insert这些SQL语句，而且修改非常方便，后来在使用的过程中发现导航属性这个关系，然后才慢慢知道数据库的索引是什么，由于自己接管的是大学生社团的数据库，大多时候创建者并不会考虑表的联系，一般创个主键就完事了（顺便吐槽一句，握草，数据库的表名和列名是什么鬼全用拼音首字母，为了兼容前面的内容我们还得花一半时间猜你们的列名，简直醉了，除了ID这个英文他们会，你们的英语是体育老师教的吗？？？）言归正传，用EF的确学到了对数据库表的的建立的理解，毕竟自己刚学数据库的时候就是把所有的字段塞到一张表里面，刚开始自己使用EF从数据库拔下来的表然后修改实体的关系的数据（感觉其实就是使用EF的EMDX的Code-First），使用这个并没有出现很多问题，后来又接触完整的Code-First，就是直接用代码生成数据库，虽然中间遇到无数的BUG但是这些BUG让我对数据库和EF的关系有了更深的理解，话不多说，直接上BUG。"><a href="#首先谈谈自己对EF的接触的过程吧，最先接触EF只是因为EF支持从数据库把关系扒下来，可以省掉自己写Select、Update、Insert这些SQL语句，而且修改非常方便，后来在使用的过程中发现导航属性这个关系，然后才慢慢知道数据库的索引是什么，由于自己接管的是大学生社团的数据库，大多时候创建者并不会考虑表的联系，一般创个主键就完事了（顺便吐槽一句，握草，数据库的表名和列名是什么鬼全用拼音首字母，为了兼容前面的内容我们还得花一半时间猜你们的列名，简直醉了，除了ID这个英文他们会，你们的英语是体育老师教的吗？？？）言归正传，用EF的确学到了对数据库表的的建立的理解，毕竟自己刚学数据库的时候就是把所有的字段塞到一张表里面，刚开始自己使用EF从数据库拔下来的表然后修改实体的关系的数据（感觉其实就是使用EF的EMDX的Code-First），使用这个并没有出现很多问题，后来又接触完整的Code-First，就是直接用代码生成数据库，虽然中间遇到无数的BUG但是这些BUG让我对数据库和EF的关系有了更深的理解，话不多说，直接上BUG。" class="headerlink" title="首先谈谈自己对EF的接触的过程吧，最先接触EF只是因为EF支持从数据库把关系扒下来，可以省掉自己写Select、Update、Insert这些SQL语句，而且修改非常方便，后来在使用的过程中发现导航属性这个关系，然后才慢慢知道数据库的索引是什么，由于自己接管的是大学生社团的数据库，大多时候创建者并不会考虑表的联系，一般创个主键就完事了（顺便吐槽一句，握草，数据库的表名和列名是什么鬼全用拼音首字母，为了兼容前面的内容我们还得花一半时间猜你们的列名，简直醉了，除了ID这个英文他们会，你们的英语是体育老师教的吗？？？）言归正传，用EF的确学到了对数据库表的的建立的理解，毕竟自己刚学数据库的时候就是把所有的字段塞到一张表里面，刚开始自己使用EF从数据库拔下来的表然后修改实体的关系的数据（感觉其实就是使用EF的EMDX的Code First），使用这个并没有出现很多问题，后来又接触完整的Code First，就是直接用代码生成数据库，虽然中间遇到无数的BUG但是这些BUG让我对数据库和EF的关系有了更深的理解，话不多说，直接上BUG。"></a>首先谈谈自己对EF的接触的过程吧，最先接触EF只是因为EF支持从数据库把关系扒下来，可以省掉自己写Select、Update、Insert这些SQL语句，而且修改非常方便，后来在使用的过程中发现导航属性这个关系，然后才慢慢知道数据库的索引是什么，由于自己接管的是大学生社团的数据库，大多时候创建者并不会考虑表的联系，一般创个主键就完事了（顺便吐槽一句，握草，数据库的表名和列名是什么鬼全用拼音首字母，为了兼容前面的内容我们还得花一半时间猜你们的列名，简直醉了，除了ID这个英文他们会，你们的英语是体育老师教的吗？？？）言归正传，用EF的确学到了对数据库表的的建立的理解，毕竟自己刚学数据库的时候就是把所有的字段塞到一张表里面，刚开始自己使用EF从数据库拔下来的表然后修改实体的关系的数据（感觉其实就是使用EF的EMDX的Code First），使用这个并没有出现很多问题，后来又接触完整的<strong>Code First</strong>，就是直接用代码生成数据库，虽然中间遇到无数的BUG但是这些BUG让我对数据库和EF的关系有了更深的理解，话不多说，直接上BUG。</h4><hr><h2 id="1-EF未能确定外键，请用注解属性或Fluent-API标记外键"><a href="#1-EF未能确定外键，请用注解属性或Fluent-API标记外键" class="headerlink" title="1.    EF未能确定外键，请用注解属性或Fluent API标记外键"></a>1.    EF未能确定外键，请用注解属性或Fluent API标记外键</h2><p>网上关于如何用代码的（<a href="http://www.cnblogs.com/Gyoung/archive/2013/01/22/2869782.html" target="_blank" rel="noopener">Fluent API或注解属性</a>）指定外键的文章有很多有很多。在这里我想谈谈对外键的理解，首先建立起一张主表</p><table class="table table-bordered table-striped table-condensed"><br>主表<br><tr><br><th><br>列名<br></th><br><th><br>类型<br></th><br></tr><br><tr><br><br><td><br>ID<br></td><br><td><br>int<br></td><br></tr><br><tr><br><td><br>Name<br></td><br><td><br>nvarchar(50)<br></td><br></tr><br></table><h4 id="首先ID是独一无二的，而Name不是（重名的有很多），当我们给ID套上主键的时候，这时候插入这张表的ID只能有一种（这是数据库的一种约束，当然你可以不选择这种约束），一个人除了姓名还有其他东西，假如这时我们还有帮他加入性别这个信息，我们可以修改上一张表添加一个字段，也可以新建一张表存贮性别这个信息（当然在实际生活中只用一张表存一个信息很少），我们新建的这张表是这样的，"><a href="#首先ID是独一无二的，而Name不是（重名的有很多），当我们给ID套上主键的时候，这时候插入这张表的ID只能有一种（这是数据库的一种约束，当然你可以不选择这种约束），一个人除了姓名还有其他东西，假如这时我们还有帮他加入性别这个信息，我们可以修改上一张表添加一个字段，也可以新建一张表存贮性别这个信息（当然在实际生活中只用一张表存一个信息很少），我们新建的这张表是这样的，" class="headerlink" title="首先ID是独一无二的，而Name不是（重名的有很多），当我们给ID套上主键的时候，这时候插入这张表的ID只能有一种（这是数据库的一种约束，当然你可以不选择这种约束），一个人除了姓名还有其他东西，假如这时我们还有帮他加入性别这个信息，我们可以修改上一张表添加一个字段，也可以新建一张表存贮性别这个信息（当然在实际生活中只用一张表存一个信息很少），我们新建的这张表是这样的，"></a>首先ID是独一无二的，而Name不是（重名的有很多），当我们给ID套上主键的时候，这时候插入这张表的ID只能有一种（这是数据库的一种约束，当然你可以不选择这种约束），一个人除了姓名还有其他东西，假如这时我们还有帮他加入性别这个信息，我们可以修改上一张表添加一个字段，也可以新建一张表存贮性别这个信息（当然在实际生活中只用一张表存一个信息很少），我们新建的这张表是这样的，</h4><table class="table table-bordered table-striped table-condensed"><br>附表<br><tr><br><th><br>列名<br></th><br><th><br>类型<br></th><br></tr><br><br><tr><br><td><br>Sex<br></td><br><td><br>bit<br></td><br></tr><br></table><p>这张表存贮了性别这个信息，但是如何将他从主表联系起来呢，我们先提取主表中的ID作为联系（我们称为外键）表改为</p><table class="table table-bordered table-striped table-condensed"><br>附表<br><tr><br><th><br>列名<br></th><br><th><br>类型<br></th><br></tr><br><tr><br><br><td><br>ID<br></td><br><td><br>int<br></td><br></tr><br><tr><br><td><br>Sex<br></td><br><td><br>bit<br></td><br></tr><br></table><p>我们把列名ID设为主键，这样我们就建立了一对一的关系，这个附表的ID必须不为空，这种关系还有一种就是将外键存贮在主表里面，就是将主表里面添加一个外键SexID，主表和附表要改成下面这种</p><table class="table table-bordered table-striped table-condensed"><br>主表<br><tr><br><th><br>列名<br></th><br><th><br>类型<br></th><br></tr><br><tr><br><br><td><br>ID<br></td><br><td><br>int<br></td><br></tr><br><tr><br><td><br>Name<br></td><br><td><br>nvarchar(50)<br></td><br></tr><br><tr><br><br><td><br>SexID<br></td><br><td><br>int<br></td><br></tr><br><tr><br></tr></table><table class="table table-bordered table-striped table-condensed"><br>附表<br><tr><br><th><br>列名<br></th><br><th><br>类型<br></th><br></tr><br><br><tr><br><td><br>Sex<br></td><br><td><br>bit<br></td><br></tr><br></table><p>现在这种结构就是外键SexID可以为空（注上面的外键不能为空），</p><h6 id="ps：说到外键不能为空我插一句，有些教科书上说外键不能为空也是对的，外键只是一个列名，当这个列名不唯一（也就是不为主键的时候）这是外键可以为空，为空的含义是不确定对应主表的值。"><a href="#ps：说到外键不能为空我插一句，有些教科书上说外键不能为空也是对的，外键只是一个列名，当这个列名不唯一（也就是不为主键的时候）这是外键可以为空，为空的含义是不确定对应主表的值。" class="headerlink" title="ps：说到外键不能为空我插一句，有些教科书上说外键不能为空也是对的，外键只是一个列名，当这个列名不唯一（也就是不为主键的时候）这是外键可以为空，为空的含义是不确定对应主表的值。"></a>ps：说到外键不能为空我插一句，有些教科书上说外键不能为空也是对的，外键只是一个列名，当这个列名不唯一（也就是不为主键的时候）这是外键可以为空，为空的含义是不确定对应主表的值。</h6><p>现在开始谈谈这种情况在EF发生的原因，你吧主表设为Person表，附表为SexInfo表，对应的代码如下</p><pre><code>public Person{    public int ID{get;set;}    public string name{get;set;}    public virtual SexInfo Sex{get;set;                                            }public SexInfo{    public int ID{get;set;    public bool Sex{get;set;    public Person person{get;set;}                            }</code></pre><p>这个时候EF无法判断哪个是主表那个是附表，就是无法将外键加在哪个表的ID上，或者像上面的表中在Person表中添加一个外键。也就是在这种情况里面有四种可能的情况</p><ol><li>在Person表里面添加一个外键（假设为Person_SexInfoID)</li><li>将Person表中的ID设为主键和外键</li><li>在SexInfo表中添加一个外键（假设为SexInfo_PersonID)</li><li>将SexInfo表中的ID设为主键和外键。</li></ol><h6 id="注假设在EF中没有给属性添加-Key-注解属性或在Fluent-API中声明一个属性为主键的话，EF会自动将有ID后缀的属性设置为主键并让他为标志字段自增，还有表中没有主键无法导入到EF中。"><a href="#注假设在EF中没有给属性添加-Key-注解属性或在Fluent-API中声明一个属性为主键的话，EF会自动将有ID后缀的属性设置为主键并让他为标志字段自增，还有表中没有主键无法导入到EF中。" class="headerlink" title="注假设在EF中没有给属性添加[Key]注解属性或在Fluent API中声明一个属性为主键的话，EF会自动将有ID后缀的属性设置为主键并让他为标志字段自增，还有表中没有主键无法导入到EF中。"></a>注假设在EF中没有给属性添加[Key]注解属性或在Fluent API中声明一个属性为主键的话，EF会自动将有ID后缀的属性设置为主键并让他为标志字段自增，还有表中没有主键无法导入到EF中。</h6><p>虽然EF有自动检测代码生成关系，但是本人还是比较推崇自己在Code First时就想好外键，这样在用模型绑定的时候就不会发生一些很可能发生的错误。在这张表里面为了节约数据库空间最好在SexInfo里面添加一个外键，现在我就来谈谈分别在两个表里面添加外键可能会遇到的BUG。</p><ol><li>在SexInfo里面添加外键PersonID</li></ol><p>类修改成为</p><pre><code>public Person{    public int ID{get;set;}    public string name{get;set;}    public virtual SexInfo Sex{get;set;                                            }public SexInfo{    public int ID{get;set;    public bool Sex{get;set;    public int PersonID{get;set;}    public Person person{get;set;}                            }</code></pre><p>然后我们可以选择在PersonID上加上<code>[ForeignKey(&quot;Person&quot;)]</code>和<code>[Requird]</code>,或者在重写的OnModelCreating方法中加入 这样一句代码 </p><pre><code>modelBuilder.Entity&lt;SexInfo&gt;().HasRequired(x =&gt; x.Person).WithRequiredPrincipal(x =&gt; x.BindingRole).HasForeignKey(x =&gt; x.MenusManageID)        </code></pre><p>其实我更推崇写Fluent API 来约束，因为将注解属性放在Model里面太乱而且容易错，比如说假如你在<strong>PersonID</strong>上面少注释了一个<code>[Required]</code> 你又会得到一个模型验证错误，这个BUG是隐藏的最深的，现在来重点提一提这个BUG</p><h4 id="BUG：模型验证错误····多重性与关系“········”中-Role“··············”中的引用约束冲突。因为-Dependent-Role-中的所有属性都不可以为-null，Principal-Role-的多重性必须为“1”。"><a href="#BUG：模型验证错误····多重性与关系“········”中-Role“··············”中的引用约束冲突。因为-Dependent-Role-中的所有属性都不可以为-null，Principal-Role-的多重性必须为“1”。" class="headerlink" title="BUG：模型验证错误····多重性与关系“········”中 Role“··············”中的引用约束冲突。因为 Dependent Role 中的所有属性都不可以为 null，Principal Role 的多重性必须为“1”。"></a>BUG：模型验证错误····多重性与关系“········”中 Role“··············”中的引用约束冲突。因为 Dependent Role 中的所有属性都不可以为 null，Principal Role 的多重性必须为“1”。</h4><h1 id="里面值类型不能为空（如果没有初始化时为0），所以EF报错，你要么给外键加上Required标记指定它必须存在，要么给一个可为空的int型，像这个示例里面外键PersonID是必须的，然后有些对应是0-1-对-1，所以这时候就疑惑了我们怎么给外键赋值，我们有一种办法命名一种类型他的值可以int也可以为空，但是EF会认识我们这种独特的外键吗？还好EF早想到了这点，有一种泛型可以为空也可以为你想要的类型，这种就是Nullable-lt-T-gt-在这个方法中我们只要将外键PersonID的类型换成-这个"><a href="#里面值类型不能为空（如果没有初始化时为0），所以EF报错，你要么给外键加上Required标记指定它必须存在，要么给一个可为空的int型，像这个示例里面外键PersonID是必须的，然后有些对应是0-1-对-1，所以这时候就疑惑了我们怎么给外键赋值，我们有一种办法命名一种类型他的值可以int也可以为空，但是EF会认识我们这种独特的外键吗？还好EF早想到了这点，有一种泛型可以为空也可以为你想要的类型，这种就是Nullable-lt-T-gt-在这个方法中我们只要将外键PersonID的类型换成-这个" class="headerlink" title="里面值类型不能为空（如果没有初始化时为0），所以EF报错，你要么给外键加上Required标记指定它必须存在，要么给一个可为空的int型，像这个示例里面外键PersonID是必须的，然后有些对应是0-1 对 1，所以这时候就疑惑了我们怎么给外键赋值，我们有一种办法命名一种类型他的值可以int也可以为空，但是EF会认识我们这种独特的外键吗？还好EF早想到了这点，有一种泛型可以为空也可以为你想要的类型，这种就是Nullable&lt;T&gt; ,在这个方法中我们只要将外键PersonID的类型换成 这个"></a>里面值类型不能为空（如果没有初始化时为0），所以EF报错，你要么给外键加上<code>Required</code>标记指定它必须存在，要么给一个可为空的int型，像这个示例里面外键PersonID是必须的，然后有些对应是0-1 对 1，所以这时候就疑惑了我们怎么给外键赋值，我们有一种办法命名一种类型他的值可以int也可以为空，但是EF会认识我们这种独特的外键吗？还好EF早想到了这点，有一种泛型可以为空也可以为你想要的类型，这种就是<code>Nullable&lt;T&gt;</code> ,在这个方法中我们只要将外键PersonID的类型换成 这个</h1><pre><code>public Nullable&lt;int&gt; PersonID{get;set;}</code></pre><h1 id="自己本身与数据库类型的对应，C"><a href="#自己本身与数据库类型的对应，C" class="headerlink" title="自己本身与数据库类型的对应，C"></a>自己本身与数据库类型的对应，C</h1><p>还有一个比较常见的BUG吧，来提一提。</p><h4 id="BUG：······-引用约束的-Dependent-Role-中所有属性的类型都必须与-Principal-Role-中相应的属性类型相同。引用约束“·····”中，实体“····”的属性“····”的类型与实体“·····”的属性“·····”的类型不匹配。"><a href="#BUG：······-引用约束的-Dependent-Role-中所有属性的类型都必须与-Principal-Role-中相应的属性类型相同。引用约束“·····”中，实体“····”的属性“····”的类型与实体“·····”的属性“·····”的类型不匹配。" class="headerlink" title="BUG：······: 引用约束的 Dependent Role 中所有属性的类型都必须与 Principal Role 中相应的属性类型相同。引用约束“·····”中，实体“····”的属性“····”的类型与实体“·····”的属性“·····”的类型不匹配。"></a>BUG：······: 引用约束的 Dependent Role 中所有属性的类型都必须与 Principal Role 中相应的属性类型相同。引用约束“·····”中，实体“····”的属性“····”的类型与实体“·····”的属性“·····”的类型不匹配。</h4><p>这个bug就是相对应主体和外键不匹配的情况，相对应的类如下</p><pre><code>    public Person{    public long ID{get;set;}    public string name{get;set;}    public virtual SexInfo Sex{get;set;                                            }public SexInfo{    public int ID{get;set;    public bool Sex{get;set;    public int PersonID{get;set;}    public Person person{get;set;}                            }</code></pre><p>Person里面的主键我改成了long型，然而外键PersonID却是int型，出现这个错误是对外键的认识还不够，外键其实就是主键的“分身”,主键是long型，外键必须也是long型，同理主键是int型外键也必须是ing型，</p><h4 id="ps：导航属性是指对象，比如说Person类实例person，而外键是指存贮在数据库里面的一个特殊的列名。"><a href="#ps：导航属性是指对象，比如说Person类实例person，而外键是指存贮在数据库里面的一个特殊的列名。" class="headerlink" title="ps：导航属性是指对象，比如说Person类实例person，而外键是指存贮在数据库里面的一个特殊的列名。"></a>ps：导航属性是指对象，比如说Person类实例person，而外键是指存贮在数据库里面的一个特殊的列名。</h4><hr><h3 id="充分认识导航属性和外键是搭建一个扎实的数据库结构的基础，在学习和应用EF的过程中也是了解数据库的结构的学习过程，EF或许在运行速度方法上比一般的SQL语句要慢，但是用EF我们可以更加方便的搭建一个好的数据体系，搭建一个好的数据体系可以让你在完成项目的时候事半功倍。"><a href="#充分认识导航属性和外键是搭建一个扎实的数据库结构的基础，在学习和应用EF的过程中也是了解数据库的结构的学习过程，EF或许在运行速度方法上比一般的SQL语句要慢，但是用EF我们可以更加方便的搭建一个好的数据体系，搭建一个好的数据体系可以让你在完成项目的时候事半功倍。" class="headerlink" title="充分认识导航属性和外键是搭建一个扎实的数据库结构的基础，在学习和应用EF的过程中也是了解数据库的结构的学习过程，EF或许在运行速度方法上比一般的SQL语句要慢，但是用EF我们可以更加方便的搭建一个好的数据体系，搭建一个好的数据体系可以让你在完成项目的时候事半功倍。"></a>充分认识导航属性和外键是搭建一个扎实的数据库结构的基础，在学习和应用EF的过程中也是了解数据库的结构的学习过程，EF或许在运行速度方法上比一般的SQL语句要慢，但是用EF我们可以更加方便的搭建一个好的数据体系，搭建一个好的数据体系可以让你在完成项目的时候事半功倍。</h3>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>GitHub Education Pack</title>
      <link href="2016/07/18/software/GitHubEducationPack/"/>
      <url>2016/07/18/software/GitHubEducationPack/</url>
      
        <content type="html"><![CDATA[<blockquote><p>GitHub推出一个对学生和教师的福利包,对于学生来说这是一个不小<br>的福利,只要通过一个edu邮箱就可以领取,但奈何国内有些无良人买卖<br>邮箱,所以GitHub对于.cn的邮箱一律拒绝,但是可以通过上传学生证的方法<br>得到验证,题主刚开始用学校邮箱试了试,失败了,抱着试一试的心态,上传了<br>学生证,没想到第二天就给我回复,并给我这个豪华大礼包,接下来我就介绍介绍<br>如何用这个包来.</p></blockquote><h3 id="有些人在网上说-上传学生证没有用-可能是那个plan-GitHub会叫你写一点你想用GitHub做什么-用的是中文写的-最好用英文写-回复的会快一点"><a href="#有些人在网上说-上传学生证没有用-可能是那个plan-GitHub会叫你写一点你想用GitHub做什么-用的是中文写的-最好用英文写-回复的会快一点" class="headerlink" title="有些人在网上说,上传学生证没有用,可能是那个plan(GitHub会叫你写一点你想用GitHub做什么)用的是中文写的,最好用英文写-_-,回复的会快一点."></a>有些人在网上说,上传学生证没有用,可能是那个plan(GitHub会叫你写一点你想用GitHub做什么)用的是中文写的,最好用英文写-_-,回复的会快一点.</h3><h2 id="Digital-Ocean-—VPS-50刀"><a href="#Digital-Ocean-—VPS-50刀" class="headerlink" title="Digital Ocean —VPS 50刀"></a>Digital Ocean —VPS 50刀</h2><blockquote><p>以前貌似是100刀,现在缩水一半了,不知道为什么.</p></blockquote><h4 id="DigitalOcean是一家以优质的VPS服务器著名-毕竟用SSD做存贮的服务器商没几家"><a href="#DigitalOcean是一家以优质的VPS服务器著名-毕竟用SSD做存贮的服务器商没几家" class="headerlink" title="DigitalOcean是一家以优质的VPS服务器著名,毕竟用SSD做存贮的服务器商没几家."></a>DigitalOcean是一家以优质的VPS服务器著名,毕竟用SSD做存贮的服务器商没几家.</h4><h6 id="这个是包小时的我们可以最便宜的5刀每月-提供20GSSD-1TB流量-我们可以用它来搭建服务器或者搭建一个shadowsocks服务器-安装shadowsocks很简单-但是怎么得到这50刀就要花点时间了"><a href="#这个是包小时的我们可以最便宜的5刀每月-提供20GSSD-1TB流量-我们可以用它来搭建服务器或者搭建一个shadowsocks服务器-安装shadowsocks很简单-但是怎么得到这50刀就要花点时间了" class="headerlink" title="这个是包小时的我们可以最便宜的5刀每月,提供20GSSD,1TB流量,我们可以用它来搭建服务器或者搭建一个shadowsocks服务器,安装shadowsocks很简单,但是怎么得到这50刀就要花点时间了."></a>这个是包小时的我们可以最便宜的5刀每月,提供20GSSD,1TB流量,我们可以用它来搭建服务器或者搭建一个shadowsocks服务器,安装shadowsocks很简单,但是怎么得到这50刀就要花点时间了.</h6><h4 id="你要是想得到这50刀必须先充值5刀-但是怎么给钱有是个问题-真是有钱也花出去啊-你可以选择绑卡-但是很复杂不一定能绑的上-最好的方式使用PayPal付这5刀-PayPal可以绑定银联卡付款-但是这个PayPal注册又是个问题-当时题主注册的时候一直提示服务器故障"><a href="#你要是想得到这50刀必须先充值5刀-但是怎么给钱有是个问题-真是有钱也花出去啊-你可以选择绑卡-但是很复杂不一定能绑的上-最好的方式使用PayPal付这5刀-PayPal可以绑定银联卡付款-但是这个PayPal注册又是个问题-当时题主注册的时候一直提示服务器故障" class="headerlink" title="你要是想得到这50刀必须先充值5刀,但是怎么给钱有是个问题,真是有钱也花出去啊.你可以选择绑卡,但是很复杂不一定能绑的上,最好的方式使用PayPal付这5刀,PayPal可以绑定银联卡付款,但是这个PayPal注册又是个问题,当时题主注册的时候一直提示服务器故障."></a>你要是想得到这50刀必须先充值5刀,但是怎么给钱有是个问题,真是有钱也花出去啊.你可以选择绑卡,但是很复杂不一定能绑的上,最好的方式使用PayPal付这5刀,PayPal可以绑定银联卡付款,但是这个PayPal注册又是个问题,当时题主注册的时候一直提示服务器故障.</h4><h3 id="当时去上网搜了搜-中国大陆是有这个情况-可以通过贝宝-PayPal在中国的分公司-来注册"><a href="#当时去上网搜了搜-中国大陆是有这个情况-可以通过贝宝-PayPal在中国的分公司-来注册" class="headerlink" title="当时去上网搜了搜,中国大陆是有这个情况,可以通过贝宝(PayPal在中国的分公司)来注册"></a>当时去上网搜了搜,中国大陆是有这个情况,可以通过贝宝(PayPal在中国的分公司)来注册</h3><h5 id="提醒一句-绑银联卡的时候最好用IE来绑定-别问我为什么谷歌浏览器不行—"><a href="#提醒一句-绑银联卡的时候最好用IE来绑定-别问我为什么谷歌浏览器不行—" class="headerlink" title="提醒一句,绑银联卡的时候最好用IE来绑定,别问我为什么谷歌浏览器不行—-"></a>提醒一句,绑银联卡的时候最好用IE来绑定,别问我为什么谷歌浏览器不行—-</h5><h4 id="选择VPS的时候推荐San-Francisco-延迟最低"><a href="#选择VPS的时候推荐San-Francisco-延迟最低" class="headerlink" title="选择VPS的时候推荐San Francisco,延迟最低."></a>选择VPS的时候推荐San Francisco,延迟最低.</h4><h4 id="搭建shadowsocks可以参考这篇博客"><a href="#搭建shadowsocks可以参考这篇博客" class="headerlink" title="搭建shadowsocks可以参考这篇博客"></a>搭建shadowsocks可以参考这篇<a href="https://segmentfault.com/a/1190000002511795" target="_blank" rel="noopener">博客</a></h4><h2 id="NameCheap"><a href="#NameCheap" class="headerlink" title="NameCheap"></a>NameCheap</h2><h3 id="ME域名一个（一年，价值8-99刀）PositiveSSL一个-（一年，价值9刀）"><a href="#ME域名一个（一年，价值8-99刀）PositiveSSL一个-（一年，价值9刀）" class="headerlink" title="ME域名一个（一年，价值8.99刀）PositiveSSL一个 （一年，价值9刀）"></a>ME域名一个（一年，价值8.99刀）PositiveSSL一个 （一年，价值9刀）</h3><blockquote><p>这个要想得到域名必须通过邮箱验证还好NameCheape承认.edu.cn邮箱</p></blockquote><h5 id="提醒一下-通过DVC验证的时候选择邮箱验证就够了-虽然不是你的邮箱但是会把资料发到你提供的邮箱"><a href="#提醒一下-通过DVC验证的时候选择邮箱验证就够了-虽然不是你的邮箱但是会把资料发到你提供的邮箱" class="headerlink" title="提醒一下,通过DVC验证的时候选择邮箱验证就够了.虽然不是你的邮箱但是会把资料发到你提供的邮箱"></a>提醒一下,通过DVC验证的时候选择邮箱验证就够了.虽然不是你的邮箱但是会把资料发到你提供的邮箱</h5><h4 id="可以参考这篇博客搭建你的https网站-本站也是采用这种方法搭建的-但是有一点不同的时-现在NameCheap直接发给我一个-crt文件和-ca-bundle文件-用于Apache-所以把-key文件和-crt文件放到服务器上配置一下就好了"><a href="#可以参考这篇博客搭建你的https网站-本站也是采用这种方法搭建的-但是有一点不同的时-现在NameCheap直接发给我一个-crt文件和-ca-bundle文件-用于Apache-所以把-key文件和-crt文件放到服务器上配置一下就好了" class="headerlink" title="可以参考这篇博客搭建你的https网站,本站也是采用这种方法搭建的.但是有一点不同的时,现在NameCheap直接发给我一个.crt文件和.ca-bundle文件(用于Apache),所以把.key文件和.crt文件放到服务器上配置一下就好了."></a>可以参考这篇<a href="http://www.freehao123.com/namecheap-ssl/" target="_blank" rel="noopener">博客</a>搭建你的https网站,本站也是采用这种方法搭建的.但是有一点不同的时,现在NameCheap直接发给我一个.crt文件和.ca-bundle文件(用于Apache),所以把.key文件和.crt文件放到服务器上配置一下就好了.</h4><h2 id="GitHub-Micro-account-7刀-month"><a href="#GitHub-Micro-account-7刀-month" class="headerlink" title="GitHub Micro account 7刀/month"></a>GitHub Micro account 7刀/month</h2><blockquote><p>这个不错我们可以有五个私有项目,一直可以用到你毕业.</p></blockquote><p>这个不错哦,妈妈再也不怕我写的stupid代码被人看到了 O(∩_∩)O哈哈~.</p>]]></content>
      
      
      <categories>
          
          <category> 软件 </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>阿里云服务器初体验</title>
      <link href="2016/07/18/software/%E9%98%BF%E9%87%8C%E4%BA%91%E6%9C%8D%E5%8A%A1%E5%99%A8%E5%88%9D%E4%BD%93%E9%AA%8C/"/>
      <url>2016/07/18/software/%E9%98%BF%E9%87%8C%E4%BA%91%E6%9C%8D%E5%8A%A1%E5%99%A8%E5%88%9D%E4%BD%93%E9%AA%8C/</url>
      
        <content type="html"><![CDATA[<h3 id="申请了一个阿里的15体验的云服务器，同自己玩的虚拟机还是有点不同的。"><a href="#申请了一个阿里的15体验的云服务器，同自己玩的虚拟机还是有点不同的。" class="headerlink" title="申请了一个阿里的15体验的云服务器，同自己玩的虚拟机还是有点不同的。"></a>申请了一个阿里的15体验的云服务器，同自己玩的虚拟机还是有点不同的。</h3><h3 id="1-用户名和密码"><a href="#1-用户名和密码" class="headerlink" title="1.用户名和密码"></a>1.用户名和密码</h3><p>找了半天没有找到那个是用户名，试了实例的id，没有用，最后终于在登录帮助名里面找到了，用户名竟然是<code>root</code>！！！！,我用的是Ubuntu系统，说好的Ubuntu不提供root权限的呢，阿里还真会改造Linux系统<br>，但是我觉得用root登录还是不安全，我觉得新建一个用户吧</p><p>在root权限下</p><pre><code>useradd -s /bin/bash -r -m yourname</code></pre><p>解释一下，本来直接<code>useradd yourname</code> 就可以新建一个<code>yourname</code>账号，但是如果用你新建的用户登录的话，你无法使用Tab键和上下左右键 ，你键入 <code>echo $SHELL</code>，会发现是<code>/bin/sh</code>，因为Ubuntu默认创建账号使用<code>/bin/sh</code>，假如你没加这个可以删掉 用这个<code>userdel yourname</code>重新来一遍，搞完这个只是创建了一个普通用户，你使用不了sudo获取最高权限，这个怎么办，很简单在<code>/etc/sudoers</code>里面找到<br>        root ALL=(ALL:ALL) ALL</p><p>模仿它加上自己的账号名就可以</p>]]></content>
      
      
      <categories>
          
          <category> 软件 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> HTTP </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>富文本编辑器CKEditor配置CKFinder</title>
      <link href="2016/07/18/software/%E5%AF%8C%E6%96%87%E6%9C%AC%E7%BC%96%E8%BE%91%E5%99%A8CKEditor%E9%85%8D%E7%BD%AECKFinder/"/>
      <url>2016/07/18/software/%E5%AF%8C%E6%96%87%E6%9C%AC%E7%BC%96%E8%BE%91%E5%99%A8CKEditor%E9%85%8D%E7%BD%AECKFinder/</url>
      
        <content type="html"><![CDATA[<blockquote><p>由于网站要实现图片和文字的混排的上传<br>所以在网上找了富文本编辑器的插件，发现<a href="http://ckeditor.com/download" target="_blank" rel="noopener">CKEditor</a>这款还不错的插件</p></blockquote><h3 id="我用的是4-5-1这个版本，的确很好用，只是引用了一个js文件就可以实现。"><a href="#我用的是4-5-1这个版本，的确很好用，只是引用了一个js文件就可以实现。" class="headerlink" title="我用的是4.5.1这个版本，的确很好用，只是引用了一个js文件就可以实现。"></a>我用的是<strong>4.5.1</strong>这个版本，的确很好用，只是引用了一个<strong>js</strong>文件就可以实现。</h3><hr><ol><li>将网上下的CKEditor包解压放在根目录下</li><li><p>在页面上引用CKEditor的核心包<strong>ckeditor.js</strong></p><pre><code>&lt;script src=&quot;ckeditor/ckeditor.js&quot; type=&quot;text/javascript&quot;&gt;&lt;/script&gt;;</code></pre></li><li><p>在页面添加一个输入框<strong>textarea</strong></p><pre><code>&lt;textarea name=&quot;individual&quot; id=&quot;individual&quot; runat=&quot;server&quot;&gt;&lt;/textarea&gt;&lt;script type=&quot;text/javascript&quot;&gt; CKEDITOR.replace(&apos;individual&apos;); &lt;/script&gt;</code></pre></li></ol><hr><h3 id="只要通过上面上面几个步骤就能实现富文本编辑器，但是点开图片上传功能，发现只有上传url的功能，并不能本地上传图片，百度了一下发现由于安全性问题CKEditor没有上传功能，只有安上CKFinder才能实现上传功能，于是我在官网下了ckfinder-aspnet-2-5-0-1-同CKEditor一样引用JS文件（只要引用ckfinder-js"><a href="#只要通过上面上面几个步骤就能实现富文本编辑器，但是点开图片上传功能，发现只有上传url的功能，并不能本地上传图片，百度了一下发现由于安全性问题CKEditor没有上传功能，只有安上CKFinder才能实现上传功能，于是我在官网下了ckfinder-aspnet-2-5-0-1-同CKEditor一样引用JS文件（只要引用ckfinder-js" class="headerlink" title="只要通过上面上面几个步骤就能实现富文本编辑器，但是点开图片上传功能，发现只有上传url的功能，并不能本地上传图片，百度了一下发现由于安全性问题CKEditor没有上传功能，只有安上CKFinder才能实现上传功能，于是我在官网下了ckfinder__aspnet_2.5.0.1,同CKEditor一样引用JS文件（只要引用ckfinder.js)"></a>只要通过上面上面几个步骤就能实现富文本编辑器，但是点开图片上传功能，发现只有上传url的功能，并不能本地上传图片，百度了一下发现由于安全性问题<strong>CKEditor</strong>没有上传功能，只有安上<a href="http://ckfinder.com/download" title="CKFinder" target="_blank" rel="noopener">CKFinder</a>才能实现上传功能，于是我在官网下了<strong>ckfinder__aspnet_2.5.0.1</strong>,同CKEditor一样引用JS文件（只要引用ckfinder.js)</h3><pre><code>&lt;script src=&quot;ckfinder/ckfinder.js&quot; type=&quot;text/javascript&quot;&gt;&lt;/script&gt;</code></pre><ul><li>由于我是MVC的网站基于.net4.5，在官方给的包里面有一个asp.net网站实例放在<strong>_source</strong>文件夹里面，里面有一个基于.net2的示例网站，一运行就报找不到 <strong>System.Web.UI.Design</strong>这个命名空间的错，所以我把它从项目中排除，把项目中bin中debug文件夹下的<strong>CKFinder.dll</strong>复制出来，引用到我自己的项目中。<h3 id="接下来要配置CKEditor来让CKFinder引用进来，在CKEditor文件夹下config-js在CKEDITOR-editorConfig-function-config-方法中添加如下代码："><a href="#接下来要配置CKEditor来让CKFinder引用进来，在CKEditor文件夹下config-js在CKEDITOR-editorConfig-function-config-方法中添加如下代码：" class="headerlink" title="接下来要配置CKEditor来让CKFinder引用进来，在CKEditor文件夹下config.js在CKEDITOR.editorConfig = function (config) {};方法中添加如下代码："></a>接下来要配置<strong>CKEditor</strong>来让CKFinder引用进来，在CKEditor文件夹下config.js在CKEDITOR.editorConfig = function (config) {};方法中添加如下代码：</h3>   config.filebrowserImageBrowseUrl = ‘ckfinder/ckfinder.html?Type=Images’;<br>   config.filebrowserFlashBrowseUrl = ‘ckfinder/ckfinder.html?Type=Flash’;<pre><code>config.filebrowserUploadUrl = &apos;ckfinder/core/connector/aspx/connector.aspx?command=QuickUpload&amp;type=Files&apos;;</code></pre>  config.filebrowserImageUploadUrl = ‘ckfinder/core/connector/aspx/connector.aspx?command=QuickUpload&amp;type=Images’;<br>  config.filebrowserFlashUploadUrl = ‘ckfinder/core/connector/aspx/connector.aspx?command=QuickUpload&amp;type=Flash’;<br>  config.filebrowserWindowWidth = ‘800’;  //“浏览服务器”弹出框的size设置<br>  config.filebrowserWindowHeight = ‘500’;</li></ul><hr><h4 id="注意在配置Url的时候要修改成相对于网站本地网站磁盘文件路径，比如说，你的网址是http-example-com-cn-你把ckfinder文件夹放在Admin下的Editor文件夹，那么所有url要改成下面类似的格式"><a href="#注意在配置Url的时候要修改成相对于网站本地网站磁盘文件路径，比如说，你的网址是http-example-com-cn-你把ckfinder文件夹放在Admin下的Editor文件夹，那么所有url要改成下面类似的格式" class="headerlink" title="注意在配置Url的时候要修改成相对于网站本地网站磁盘文件路径，比如说，你的网址是http://example.com.cn ,你把ckfinder文件夹放在Admin下的Editor文件夹，那么所有url要改成下面类似的格式"></a>注意在配置Url的时候要修改成相对于网站本地网站磁盘文件路径，比如说，你的网址是<a href="http://example.com.cn" target="_blank" rel="noopener">http://example.com.cn</a> ,你把ckfinder文件夹放在Admin下的Editor文件夹，那么所有url要改成下面类似的格式</h4><pre><code>config.filebrowserImageBrowseUrl =&apos;/Admin/Editor/ckfinder/ckfinder.html?Type=Images&apos;;</code></pre><h4 id="否则会报404错误，"><a href="#否则会报404错误，" class="headerlink" title="否则会报404错误，"></a>否则会报404错误，</h4><h4 id="最后一步是修改一个函数让所有人能看到服务器上传文件夹里面的文件，在ckfinder文件夹下面的cofig-ascx文件，找到-CheckAuthentication函数将返回值改成true"><a href="#最后一步是修改一个函数让所有人能看到服务器上传文件夹里面的文件，在ckfinder文件夹下面的cofig-ascx文件，找到-CheckAuthentication函数将返回值改成true" class="headerlink" title="最后一步是修改一个函数让所有人能看到服务器上传文件夹里面的文件，在ckfinder文件夹下面的cofig.ascx文件，找到 CheckAuthentication函数将返回值改成true"></a>最后一步是修改一个函数让所有人能看到服务器上传文件夹里面的文件，在ckfinder文件夹下面的cofig.ascx文件，找到 CheckAuthentication函数将返回值改成true</h4><p>当然如果你想修改上传文件的地址，你可以在上面方法里面找到SetConfig()方法，找到BaseUrl，修改为你想上传的地址，</p><hr><hr><h3 id="PS："><a href="#PS：" class="headerlink" title="PS："></a>PS：</h3><h4 id="我是在在VS里面进行调试的，由于VS的IIS在调试的时候不允许对磁盘文件的路由地址访问，就是CKFinder通过ckfinder-html这个html来实现上传图片的功能，但是这个在调试的时候VS无法访问这个文件，所以一直报404错误，可以修改IIS来允许IIS访问磁盘文件，步骤如下："><a href="#我是在在VS里面进行调试的，由于VS的IIS在调试的时候不允许对磁盘文件的路由地址访问，就是CKFinder通过ckfinder-html这个html来实现上传图片的功能，但是这个在调试的时候VS无法访问这个文件，所以一直报404错误，可以修改IIS来允许IIS访问磁盘文件，步骤如下：" class="headerlink" title="我是在在VS里面进行调试的，由于VS的IIS在调试的时候不允许对磁盘文件的路由地址访问，就是CKFinder通过ckfinder.html这个html来实现上传图片的功能，但是这个在调试的时候VS无法访问这个文件，所以一直报404错误，可以修改IIS来允许IIS访问磁盘文件，步骤如下："></a>我是在在VS里面进行调试的，由于VS的IIS在调试的时候不允许对磁盘文件的路由地址访问，就是CKFinder通过<strong>ckfinder.html</strong>这个html来实现上传图片的功能，但是这个在调试的时候VS无法访问这个文件，所以一直报404错误，可以修改IIS来允许IIS访问磁盘文件，步骤如下：</h4><ol><li>右键点击IIS Express，选择显示所有应用程序</li><li>找到运行网站的配置，进入applicationhost.config文件夹</li><li>ctrl+f 寻找UrlRoutingModule</li><li><p>将preCodition设置为空字符</p><pre><code>&lt;add name=&quot;UrlRoutingModule-4.0&quot; type=&quot;System.Web.Routing.UrlRoutingModule&quot; preCondition=&quot;&quot; /&gt;</code></pre></li></ol><hr><h3 id="如果你是用MVC进行表单传值的话，你必须在post方法上面添加-ValidateInput-false-属性，如果不这样的话就会报下面的错"><a href="#如果你是用MVC进行表单传值的话，你必须在post方法上面添加-ValidateInput-false-属性，如果不这样的话就会报下面的错" class="headerlink" title="如果你是用MVC进行表单传值的话，你必须在post方法上面添加 [ValidateInput(false)] 属性，如果不这样的话就会报下面的错"></a>如果你是用MVC进行表单传值的话，你必须在post方法上面添加 <strong>[ValidateInput(false)]</strong> 属性，如果不这样的话就会报下面的错</h3><pre><code>“/”应用程序中的服务器错误。从客户端(content=&quot;&lt;p&gt;sdfsdafwewo shdfh...&quot;)中检测到有潜在危险的 Request.Form 值。</code></pre>]]></content>
      
      
      <categories>
          
          <category> 软件 </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>有用的几个Shell命令</title>
      <link href="2016/07/18/software/%E6%9C%89%E7%94%A8%E7%9A%84%E5%87%A0%E4%B8%AAShell%E5%91%BD%E4%BB%A4/"/>
      <url>2016/07/18/software/%E6%9C%89%E7%94%A8%E7%9A%84%E5%87%A0%E4%B8%AAShell%E5%91%BD%E4%BB%A4/</url>
      
        <content type="html"><![CDATA[<h3 id="管线命令-pipe-："><a href="#管线命令-pipe-：" class="headerlink" title="管线命令 (pipe)："></a>管线命令 (pipe)：</h3><ol><li>撷取命令： cut, grep</li><li>排序命令： sort, wc, uniq</li><li>双向重导向： tee</li><li>字符转换命令： tr, col, join, paste, expand</li><li>分割命令： split</li><li>参数代换： xargs<h3 id="分割文档"><a href="#分割文档" class="headerlink" title="分割文档"></a>分割文档</h3></li></ol><blockquote><p>长长的一大片文档有时我们并不愿意看到全部内容,我们只想关注<br>部分内容的时候了可以考虑使用分割文档命令</p></blockquote><p><code>cut</code>是一个很好的分割文档工具</p><h3 id="vi常用命令"><a href="#vi常用命令" class="headerlink" title="vi常用命令"></a>vi常用命令</h3><ol><li>[Ctrl] + [u] 屏幕『向上』移动半页</li><li><ul><li>光标移动到非空格符的下一列</li></ul></li><li><ul><li>光标移动到非空格符的上一列</li></ul></li><li>n<space>那个 n 表示『数字』，例如 20 。按下数字后会向右移动这一行的n 个字符。例如 20&lt;spac移动 20 个字符距离。</space></li><li>0 这是数字『0 』：移动到这一行的最前面字符</li><li>$ 移动到这一行的最后面字符处(常用)</li><li>H 光标移动到这个屏幕的最上方那一行</li><li>M 光标移动到这个屏幕的中央那一行</li><li>L 光标移动到这个屏幕的最下方那一行</li><li>G 移动到这个档案的最后一行(常用)</li><li>nG n 为数字。移动到这个档案的第 n 行。例如 2档案的第 20 行(可配合 :set nu)</li><li>gg 移动到这个档案的第一行，相当于 1G 啊！ (<br>n<enter> n 为数字。光标向下移动 n 行(常用)</enter></li></ol>]]></content>
      
      
      <categories>
          
          <category> 软件 </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>在Linux下玩转Vim</title>
      <link href="2016/07/18/software/%E5%9C%A8Linux%E4%B8%8B%E7%8E%A9%E8%BD%ACVim/"/>
      <url>2016/07/18/software/%E5%9C%A8Linux%E4%B8%8B%E7%8E%A9%E8%BD%ACVim/</url>
      
        <content type="html"><![CDATA[<h4 id="学了鸟哥的书前面基础后，突然想在Linux下用gcc玩C语言，然后了解到了Vim这个神一样的编译器，接下来经过超长时间虐心的安装无数插件无数依赖包，突然有种打自己一顿的感觉，还好终于把Vim装的和VS差不多了，接下来我介绍我安装Vim的经验吧。"><a href="#学了鸟哥的书前面基础后，突然想在Linux下用gcc玩C语言，然后了解到了Vim这个神一样的编译器，接下来经过超长时间虐心的安装无数插件无数依赖包，突然有种打自己一顿的感觉，还好终于把Vim装的和VS差不多了，接下来我介绍我安装Vim的经验吧。" class="headerlink" title="学了鸟哥的书前面基础后，突然想在Linux下用gcc玩C语言，然后了解到了Vim这个神一样的编译器，接下来经过超长时间虐心的安装无数插件无数依赖包，突然有种打自己一顿的感觉，还好终于把Vim装的和VS差不多了，接下来我介绍我安装Vim的经验吧。"></a>学了鸟哥的书前面基础后，突然想在Linux下用gcc玩C语言，然后了解到了Vim这个神一样的编译器，接下来经过超长时间虐心的安装无数插件无数依赖包，突然有种打自己一顿的感觉，还好终于把Vim装的和VS差不多了，接下来我介绍我安装Vim的经验吧。</h4><h3 id="我虚拟机下的Linux原来是红旗6的，但是我改了一下yum的包源成CentOS的并且全部update一下后就神奇的变成了CentOS6，虽然他们两个是同一家公司，但是总给我一种由盗版成了正版的感觉。。"><a href="#我虚拟机下的Linux原来是红旗6的，但是我改了一下yum的包源成CentOS的并且全部update一下后就神奇的变成了CentOS6，虽然他们两个是同一家公司，但是总给我一种由盗版成了正版的感觉。。" class="headerlink" title="我虚拟机下的Linux原来是红旗6的，但是我改了一下yum的包源成CentOS的并且全部update一下后就神奇的变成了CentOS6，虽然他们两个是同一家公司，但是总给我一种由盗版成了正版的感觉。。"></a>我虚拟机下的Linux原来是红旗6的，但是我改了一下yum的包源成CentOS的并且全部update一下后就神奇的变成了CentOS6，虽然他们两个是同一家公司，但是总给我一种由盗版成了正版的感觉。。</h3><h2 id="闲话不多说，刚开始装第一个插件是Ctags"><a href="#闲话不多说，刚开始装第一个插件是Ctags" class="headerlink" title="闲话不多说，刚开始装第一个插件是Ctags"></a>闲话不多说，刚开始装第一个插件是Ctags</h2><p>刚开始装的时候我是在X-Windows里面的<a href="http://ctags.sourceforge.net" target="_blank" rel="noopener">这里</a>下载再转回shell敲</p><pre><code>$ tar -xzvf ctags-5.6.tar.gz$ cd ctags-5.6$ make</code></pre><h1 id="make-install"><a href="#make-install" class="headerlink" title="make install"></a>make install</h1><h6 id="后来我发现不用这么复杂直接在X-Window下面复制到Terminal里面就可以了，毕竟后面安装的代码都是十几行，根本扛不住。。。。"><a href="#后来我发现不用这么复杂直接在X-Window下面复制到Terminal里面就可以了，毕竟后面安装的代码都是十几行，根本扛不住。。。。" class="headerlink" title="后来我发现不用这么复杂直接在X-Window下面复制到Terminal里面就可以了，毕竟后面安装的代码都是十几行，根本扛不住。。。。"></a>后来我发现不用这么复杂直接在X-Window下面复制到Terminal里面就可以了，毕竟后面安装的代码都是十几行，根本扛不住。。。。</h6><p>装完三个感觉整个我太苦逼，简直是辛苦啊，不是说好的敲几下键盘就可以吗。<br>后来了解到linux有几个软件可以帮忙安装而且解决依赖性，比如ubuntu的apt-get，redhat的rmp，yum，但是问题来了，我的黄狗（yum：yellow dog）怎么没用啊，全都装不上，一查错误提示，原来我没用被授权，原来RedHat更新软件是要收钱的，但是CentOS不用，然后我就用了CentOS的源，当时我就想只想装个git，然后就没用然后了。终于全部更新完了，yum也可以用了，用yum果然爽多了，直接</p><pre><code>yum install git    </code></pre><p>git就装好了。如果想知道怎么改源，<a href="http://blog.chinaunix.net/uid-23683795-id-3477603.html" target="_blank" rel="noopener">点击这里</a></p><hr><p>好了装好了源接下来装软件就敲一敲代码就行，不用去网上找包下载再安装。</p><p>装好一些热门的插件后，我碰到了第一个最难装的插件YouCompleteMe(YCM)，装完之后感觉就是神器一般的存在，但是装完之前一直在感慨，我去怎么没个卵用啊，而且装llvm（安装YCM必须先安装的软件)时，我用编译安装方法因为一些库的缺失一直报错，后面采用二进制安装方法安装完llvm才成功按上YCM。<br>推荐新手采用二进制安装方法安装llvm，想知道怎么安装YCM这些<a href="http://www.linuxzen.com/vim-dai-ma-bu-quan-he-jian-cha-youcompleteme-syntastic.html" target="_blank" rel="noopener">点击这里</a></p><h2 id="安上一些实用的插件后，最后安装调试神器gdb，有三种方式使vim可以调试程序（与gdb一起工作），"><a href="#安上一些实用的插件后，最后安装调试神器gdb，有三种方式使vim可以调试程序（与gdb一起工作），" class="headerlink" title="安上一些实用的插件后，最后安装调试神器gdb，有三种方式使vim可以调试程序（与gdb一起工作），"></a>安上一些实用的插件后，最后安装调试神器gdb，有三种方式使vim可以调试程序（与gdb一起工作），</h2><p>vimgdb，pyclewn，clewn，三个我都试了，第一个要编译一下vim，我失败了，第二个要用python2.4+支持，我python2.6然并卵，每次都说不支持python2，第三个是用c写的，终于成功，安装非常简单，<br><a href="http://blog.csdn.net/linlianghui2004/article/details/7676792" target="_blank" rel="noopener">点击这里</a>了解详情.<br>装完了，我终于可以使用clewn打开gdb和gvim，然并卵，我写的程序并不能在gdb里面运行，一直说无法识别。。。。。。搜了一下原来是编译的时候的问题，用gcc编译的程序要带上<code>-g</code>参数才能在gdb里面用（我还以为我clewn没装好）<br>例如</p><pre><code>gcc -g test.c -o test</code></pre><p>加上<code>-g</code>参数就行了。终于我的vim可以用了。。。。。。</p>]]></content>
      
      
      <categories>
          
          <category> 软件 </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>大数据学习小总结</title>
      <link href="2016/07/18/bigdata/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%AD%A6%E4%B9%A0%E5%B0%8F%E6%80%BB%E7%BB%93/"/>
      <url>2016/07/18/bigdata/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%AD%A6%E4%B9%A0%E5%B0%8F%E6%80%BB%E7%BB%93/</url>
      
        <content type="html"><![CDATA[<blockquote><p>最近在学习大数据,书看了不少,但是总是觉得很迷茫,不知道怎么学下去,今天<br>突然想写点什么来<br>总结一下这些天.</p></blockquote><h3 id="起因"><a href="#起因" class="headerlink" title="起因"></a>起因</h3><p>逛博客的时候看到一篇分享<a href="http://www.36dsj.com/archives/8094" target="_blank" rel="noopener">如何修炼成大数据高手</a>感觉很有趣</p><h4 id="买了两本书"><a href="#买了两本书" class="headerlink" title="买了两本书"></a>买了两本书</h4><p><img src="http://www.36dsj.com/wp-content/uploads/2014/05/15.png" alt="大数据时代"></p><p><img src="http://www.36dsj.com/wp-content/uploads/2014/05/23.png" alt="失控"></p><h3 id="这两本书都是很多年前写的-但是最近这几年随着”大数据”被大家越炒越热-随即被大家所看重"><a href="#这两本书都是很多年前写的-但是最近这几年随着”大数据”被大家越炒越热-随即被大家所看重" class="headerlink" title="这两本书都是很多年前写的,但是最近这几年随着”大数据”被大家越炒越热,随即被大家所看重."></a>这两本书都是很多年前写的,但是最近这几年随着”大数据”被大家越炒越热,随即被大家所看重.</h3><h3 id="这两个作者很牛-在30年前大数据还只是萌芽的时候就提出未来大数据的重要性和价值"><a href="#这两个作者很牛-在30年前大数据还只是萌芽的时候就提出未来大数据的重要性和价值" class="headerlink" title="这两个作者很牛,在30年前大数据还只是萌芽的时候就提出未来大数据的重要性和价值."></a>这两个作者很牛,在30年前大数据还只是萌芽的时候就提出未来大数据的重要性和价值.</h3><h2 id="大数据是什么了-现在为什么这么火-以前没有大数据吗"><a href="#大数据是什么了-现在为什么这么火-以前没有大数据吗" class="headerlink" title="大数据是什么了,现在为什么这么火,以前没有大数据吗?"></a>大数据是什么了,现在为什么这么火,以前没有大数据吗?</h2><h3 id="大数据的产生就像是科技带来的附带品一样-原来我们把信息存贮在图书馆里面-因为人类的活动产生的信息越来越多-图书馆也越来越大-但是由于管理和存贮的成本越来越多-我们被迫要缩减浓缩我们的信息-我们想出了很多很方法来对付这种情况-比如抽样-只要抽取一部分的代表信息存在图书馆里面然后记录主要特征和平均分量-我们就可以把信息降低几个数量级"><a href="#大数据的产生就像是科技带来的附带品一样-原来我们把信息存贮在图书馆里面-因为人类的活动产生的信息越来越多-图书馆也越来越大-但是由于管理和存贮的成本越来越多-我们被迫要缩减浓缩我们的信息-我们想出了很多很方法来对付这种情况-比如抽样-只要抽取一部分的代表信息存在图书馆里面然后记录主要特征和平均分量-我们就可以把信息降低几个数量级" class="headerlink" title="大数据的产生就像是科技带来的附带品一样,原来我们把信息存贮在图书馆里面,因为人类的活动产生的信息越来越多,图书馆也越来越大,但是由于管理和存贮的成本越来越多,我们被迫要缩减浓缩我们的信息,我们想出了很多很方法来对付这种情况,比如抽样,只要抽取一部分的代表信息存在图书馆里面然后记录主要特征和平均分量,我们就可以把信息降低几个数量级."></a>大数据的产生就像是科技带来的附带品一样,原来我们把信息存贮在图书馆里面,因为人类的活动产生的信息越来越多,图书馆也越来越大,但是由于管理和存贮的成本越来越多,我们被迫要缩减浓缩我们的信息,我们想出了很多很方法来对付这种情况,比如抽样,只要抽取一部分的代表信息存在图书馆里面然后记录主要特征和平均分量,我们就可以把信息降低几个数量级.</h3><h3 id="但是现在不同"><a href="#但是现在不同" class="headerlink" title="但是现在不同."></a>但是现在不同.</h3><h3 id="芯片做的越来越小-容量做的越来越大-我们有能力存贮所以的信息-一些先驱发现我们用另一种思维去对待信息"><a href="#芯片做的越来越小-容量做的越来越大-我们有能力存贮所以的信息-一些先驱发现我们用另一种思维去对待信息" class="headerlink" title="芯片做的越来越小,容量做的越来越大,我们有能力存贮所以的信息,一些先驱发现我们用另一种思维去对待信息."></a>芯片做的越来越小,容量做的越来越大,我们有能力存贮所以的信息,一些先驱发现我们用另一种思维去对待信息.</h3><h2 id="用全体数据而不是随机样本"><a href="#用全体数据而不是随机样本" class="headerlink" title="用全体数据而不是随机样本"></a>用全体数据而不是随机样本</h2><h2 id="用混杂性而不是精确性"><a href="#用混杂性而不是精确性" class="headerlink" title="用混杂性而不是精确性"></a>用混杂性而不是精确性</h2><h2 id="用相关关系而不是因果性"><a href="#用相关关系而不是因果性" class="headerlink" title="用相关关系而不是因果性"></a>用相关关系而不是因果性</h2>]]></content>
      
      
      <categories>
          
          <category> 大数据 </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>Numpy的简析</title>
      <link href="2016/07/18/python/Numpy%E7%9A%84%E7%AE%80%E6%9E%90/"/>
      <url>2016/07/18/python/Numpy%E7%9A%84%E7%AE%80%E6%9E%90/</url>
      
        <content type="html"><![CDATA[<p>numpy简单来说就是python的C版数组实现,因为python原生列表虽然好使,但是生成大量数据时开销很大,而numpy是基于C的,生成大量数组非常简单,而且操作他们速度非常快.</p><h3 id="由于numpy是基于C的-所以numpy是一种强类型的-当然numpy是可以判断数组里面是数据类型-但是我们可以显示声明他们-dtype是声明的参数-一般我们通过下面的方式简单声明一个narray"><a href="#由于numpy是基于C的-所以numpy是一种强类型的-当然numpy是可以判断数组里面是数据类型-但是我们可以显示声明他们-dtype是声明的参数-一般我们通过下面的方式简单声明一个narray" class="headerlink" title="由于numpy是基于C的,所以numpy是一种强类型的,当然numpy是可以判断数组里面是数据类型,但是我们可以显示声明他们,dtype是声明的参数,一般我们通过下面的方式简单声明一个narray"></a>由于numpy是基于C的,所以numpy是一种强类型的,当然numpy是可以判断数组里面是数据类型,但是我们可以显示声明他们,dtype是声明的参数,一般我们通过下面的方式简单声明一个narray</h3><pre><code>import numpy as nparr = np.array([1, 2, 3], dtype=np.int32)</code></pre><p>numpy还有一个强大的地方是多维数组,numpy对多维数组的支持很好.只要简单的使用嵌套序列就能被转化成多维数组.<br>比如<br>        arr2 = np.array([[1, 2], [3, 4]])</p><h6 id="numpy另一个强大的地方是矢量化-这对于科学计算来说非常有用"><a href="#numpy另一个强大的地方是矢量化-这对于科学计算来说非常有用" class="headerlink" title="numpy另一个强大的地方是矢量化,这对于科学计算来说非常有用"></a>numpy另一个强大的地方是<strong>矢量化</strong>,这对于科学计算来说非常有用</h6><p>比如前面的<code>arr</code>,<br>我们可以简单使用</p><pre><code>arr3 = arr * arr</code></pre><p>得到另外一个序列 <code>[1, 4, 9]</code>,数组间的运算应用到了元素级.</p><h5 id="numpy之所以成为数据分析的基本数据结构-还在用取数据的灵活性"><a href="#numpy之所以成为数据分析的基本数据结构-还在用取数据的灵活性" class="headerlink" title="numpy之所以成为数据分析的基本数据结构,还在用取数据的灵活性"></a>numpy之所以成为数据分析的基本数据结构,还在用取数据的灵活性</h5><p>对于一维数组来说,python自身的列表就支持切片处理,numpy不仅支持切片处理,还支持列表取出,比如:</p><pre><code>&gt;&gt;&gt; num =  np.arange(10)&gt;&gt;&gt; num[[3, 1, 0]]array([3, 1, 0])</code></pre><h5 id="在一维数组里面这个并没有什么优势-因为我们可以通过数据简单一个构造器-num-x-for-x-in-3-1-0-构造出来"><a href="#在一维数组里面这个并没有什么优势-因为我们可以通过数据简单一个构造器-num-x-for-x-in-3-1-0-构造出来" class="headerlink" title="在一维数组里面这个并没有什么优势,因为我们可以通过数据简单一个构造器[num[x] for x in [3, 1, 0]]构造出来."></a>在一维数组里面这个并没有什么优势,因为我们可以通过数据简单一个构造器<code>[num[x] for x in [3, 1, 0]]</code>构造出来.</h5><p>当但在多维数组我们使用构造器非常繁琐了, numpy使用了很多技术使我们很方便的取出多维数组</p><h4 id="我们先创建一个多维数组"><a href="#我们先创建一个多维数组" class="headerlink" title="我们先创建一个多维数组"></a>我们先创建一个多维数组</h4><pre><code>arr = np.arange(32).reshape((8, 4))</code></pre><p>生成的arr是</p><pre><code>array([[ 0,  1,  2,  3],   [ 4,  5,  6,  7],   [ 8,  9, 10, 11],   [12, 13, 14, 15],   [16, 17, 18, 19],   [20, 21, 22, 23],   [24, 25, 26, 27],   [28, 29, 30, 31]])</code></pre><p>现在像一维数组一样的取出数据</p><pre><code>In[13]: arr[[0, 3]]Out[13]: array([[ 0,  1,  2,  3],   [12, 13, 14, 15]])</code></pre><p>我们取出了一个二维数组</p><h2 id="试试用这个"><a href="#试试用这个" class="headerlink" title="试试用这个"></a>试试用这个</h2><pre><code>In[14]: arr[arr &gt; 8]Out[14]: array([ 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25,   26, 27, 28, 29, 30, 31])</code></pre><p>我们取出一个一维数组,这个<code>arr &gt; 8</code> 是也是一个二维数组对应原来数组的每个位置都有一个布尔值代替,当numpy判断得到是一个array数组时会广播每个值来判断是否获取,这个可比你用for循环快了很多.</p><pre><code>array([[False, False, False, False],   [False, False, False, False],   [False,  True,  True,  True],   [ True,  True,  True,  True],   [ True,  True,  True,  True],   [ True,  True,  True,  True],   [ True,  True,  True,  True],   [ True,  True,  True,  True]], dtype=bool)</code></pre><h5 id="numpy很善于处理不同的选择-当你直接给一个数组时-如上面的arr-0-3-4-它默认第二维为全部选择也就是想当与arr-0-3-4-当你给确定的值时-他就会在第二维上取响应的值-比如"><a href="#numpy很善于处理不同的选择-当你直接给一个数组时-如上面的arr-0-3-4-它默认第二维为全部选择也就是想当与arr-0-3-4-当你给确定的值时-他就会在第二维上取响应的值-比如" class="headerlink" title="numpy很善于处理不同的选择,当你直接给一个数组时,如上面的arr[[0, 3, 4]],它默认第二维为全部选择也就是想当与arr[[0, 3, 4], :],当你给确定的值时,他就会在第二维上取响应的值,比如"></a>numpy很善于处理不同的选择,当你直接给一个数组时,如上面的<code>arr[[0, 3, 4]]</code>,它默认第二维为全部选择也就是想当与<code>arr[[0, 3, 4], :]</code>,当你给确定的值时,他就会在第二维上取响应的值,比如</h5><pre><code>arr[[0, 3, 4], 1] 或 arr[[0, 3, 4], [1, 1, 1]]</code></pre><p>从上面你可以看到,如果第二维你每个都想取第二个,你可以直接写一个整数就行,numpy会广播过去,假如想你分别再第一维的每个上分别对应取哪个你就可以用数组来分别选择.</p><h5 id="有时候我们想在二维数组上面取出一个矩形块-直接使用-arr-0-3-4-1-2-3-只能取出二维数组矩形块的对角线-我们这时候就可以先取出第一维的矩形列-然后再在取出的列中取出矩形行"><a href="#有时候我们想在二维数组上面取出一个矩形块-直接使用-arr-0-3-4-1-2-3-只能取出二维数组矩形块的对角线-我们这时候就可以先取出第一维的矩形列-然后再在取出的列中取出矩形行" class="headerlink" title="有时候我们想在二维数组上面取出一个矩形块,直接使用 arr[[0, 3, 4], [1, 2, 3]]只能取出二维数组矩形块的对角线,我们这时候就可以先取出第一维的矩形列,然后再在取出的列中取出矩形行"></a>有时候我们想在二维数组上面取出一个矩形块,直接使用 <code>arr[[0, 3, 4], [1, 2, 3]]</code>只能取出二维数组矩形块的对角线,我们这时候就可以先取出第一维的矩形列,然后再在取出的列中取出矩形行</h5><pre><code>arr[[0, 3, 4]][:, [1, 2, 3]]</code></pre><p>当然我们还可以用二维数组来取出数据<br>        arr[[[0], [3], [4]], [[1, 2, 3]]]</p><p><code>[[0], [3], [4]]</code>代表第一维的1, 4, 5列, <code>[[1, 2, 3]]</code>代表第二维的2, 3, 4行.</p><h5 id="numpy给我们提供一个函数将一维数组转换成二维数组我们可以简单使用"><a href="#numpy给我们提供一个函数将一维数组转换成二维数组我们可以简单使用" class="headerlink" title="numpy给我们提供一个函数将一维数组转换成二维数组我们可以简单使用"></a>numpy给我们提供一个函数将一维数组转换成二维数组我们可以简单使用</h5><pre><code>np.ix_([0, 3, 4], [1, 2, 3])</code></pre><p>生成二维数组,这样我们使用</p><pre><code>arr[np.ix_([0, 3, 4], [1, 2, 3])]</code></pre><p>就可以取出矩形块了.</p>]]></content>
      
      
      <categories>
          
          <category> Python </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>&lt;&lt;机器学习实战&gt;&gt;心得.</title>
      <link href="2016/07/18/ai/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%AE%9E%E6%88%98%E5%BF%83%E5%BE%97/"/>
      <url>2016/07/18/ai/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%AE%9E%E6%88%98%E5%BF%83%E5%BE%97/</url>
      
        <content type="html"><![CDATA[<h6 id="先谈谈我对机器学习的理解"><a href="#先谈谈我对机器学习的理解" class="headerlink" title="先谈谈我对机器学习的理解"></a>先谈谈我对机器学习的理解</h6><h2 id="什么是机器学习"><a href="#什么是机器学习" class="headerlink" title="什么是机器学习?"></a>什么是机器学习?</h2><p>我们人类有从婴儿开始就开始学习,父母教我们穿衣吃饭、老师教我们读书写字，我们开始能辨别好人坏人，开始通过自己的经验来判断新事物。</p><h5 id="机器学习很简单，就像人一样，我们教机器通过我们教的来判断新的事物，或者在从新的事物里面学习处理新的事物。"><a href="#机器学习很简单，就像人一样，我们教机器通过我们教的来判断新的事物，或者在从新的事物里面学习处理新的事物。" class="headerlink" title="机器学习很简单，就像人一样，我们教机器通过我们教的来判断新的事物，或者在从新的事物里面学习处理新的事物。"></a>机器学习很简单，就像人一样，我们教机器通过我们教的来判断新的事物，或者在从新的事物里面学习处理新的事物。</h5><p>这看起来很复杂的样子,但是从我们神经网络来看,我们可以把学习当做建立一个神经元连接,通过输入的信号得到一个输出的信号.我们只要简单的把输入的信号分类就可以了.通过无数个分类我们就可以建立复杂的神经系统,进而实现’学习’这个功能.</p><h2 id="如何分类"><a href="#如何分类" class="headerlink" title="如何分类?"></a>如何分类?</h2><blockquote><p>涉及到分类,假如输入的信号种类只要两种,我们就可以简单用if-else来实现分类功能,但是有时候输入信号种类个个都有细微的差别,只是遵循某种规律,这时候我们不能用简单的if-else来进行分类了,下面我就按照书的顺序来解释各种强大的分类方法.</p></blockquote><h1 id="K-近邻算法-k-Nearest-Neighbor"><a href="#K-近邻算法-k-Nearest-Neighbor" class="headerlink" title="K-近邻算法 ( k-Nearest Neighbor )"></a>K-近邻算法 ( k-Nearest Neighbor )</h1><blockquote><p>作为本书的第一个机器学习算法,K-NN算是我感觉原理最简单的一个了.</p></blockquote><p>假设我们有两个点, 红点为(-1,  -1)分为红类, 绿点为(1, 1)分为绿类<br><img src="http://7xsxdn.com1.z0.glb.clouddn.com/knn1.png" alt="图片1"></p><h4 id="接下来我一个点-0-1-这个点应该分为红还是绿呢-我们添加两条辅助线"><a href="#接下来我一个点-0-1-这个点应该分为红还是绿呢-我们添加两条辅助线" class="headerlink" title="接下来我一个点(0, -1),这个点应该分为红还是绿呢,我们添加两条辅助线"></a>接下来我一个点(0, -1),这个点应该分为红还是绿呢,我们添加两条辅助线</h4><p><img src="http://7xsxdn.com1.z0.glb.clouddn.com/knn2.png" alt></p><p>蓝点离红点距离为1,蓝点离绿点距离为2.2,我们很轻松的可以知道这个点应该分为红类.<br>现在我们进一步推广,当有很多种类点的时候,当我们二维扩展到N维,给一个点a我们只要选取距离a最近的K个种类,我们就基本能判断他属于这K个种类的,这就是K-近邻的原理了.</p><h4 id="K-近邻算法是最简单最有效的算法了-但是他也有缺点-比如他必须保存所有训练样本的数据-当训练样本很大的时候就会占用很多内存空间-我们后面会学到的KVM只取支持向量的训练样本来计算可以减少很多占用内存"><a href="#K-近邻算法是最简单最有效的算法了-但是他也有缺点-比如他必须保存所有训练样本的数据-当训练样本很大的时候就会占用很多内存空间-我们后面会学到的KVM只取支持向量的训练样本来计算可以减少很多占用内存" class="headerlink" title="K-近邻算法是最简单最有效的算法了,但是他也有缺点,比如他必须保存所有训练样本的数据,当训练样本很大的时候就会占用很多内存空间,我们后面会学到的KVM只取支持向量的训练样本来计算可以减少很多占用内存"></a>K-近邻算法是最简单最有效的算法了,但是他也有缺点,比如他必须保存所有训练样本的数据,当训练样本很大的时候就会占用很多内存空间,我们后面会学到的KVM只取支持向量的训练样本来计算可以减少很多占用内存</h4><h5 id="而且K-近邻算法对训练数据集都要计算距离值-实际使用可能会非常耗时-我们后面学到的logistic回归能很好解决这个问题"><a href="#而且K-近邻算法对训练数据集都要计算距离值-实际使用可能会非常耗时-我们后面学到的logistic回归能很好解决这个问题" class="headerlink" title="而且K-近邻算法对训练数据集都要计算距离值,实际使用可能会非常耗时,我们后面学到的logistic回归能很好解决这个问题."></a>而且K-近邻算法对训练数据集都要计算距离值,实际使用可能会非常耗时,我们后面学到的logistic回归能很好解决这个问题.</h5><p>######### 总而言之,KNN作为小样本时非常简单粗暴,但是他无法给出任何数据的基本结构信息.接下来我们要学习用概率测量解决分类问题,这个算法能解决这个问题</p><p>(决策树)[]</p>]]></content>
      
      
      <categories>
          
          <category> 人工智能 </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>python的编码问题研究------使用scrapy体验</title>
      <link href="2016/07/18/python/python%E7%9A%84%E7%BC%96%E7%A0%81%E9%97%AE%E9%A2%98%E7%A0%94%E7%A9%B6/"/>
      <url>2016/07/18/python/python%E7%9A%84%E7%BC%96%E7%A0%81%E9%97%AE%E9%A2%98%E7%A0%94%E7%A9%B6/</url>
      
        <content type="html"><![CDATA[<blockquote><p>基于python2</p></blockquote><p>scrapy是一款非常轻量级的爬虫框架，但是由于它隐藏了太多关于网络请求的细节，所以我们有时候会遭遇到一下很尴尬的bug，当然这主要是因为碰到一些不规范的网站。</p><p>python的编码转码网上有很多文章，如果你不了解这个你可以参考下面了解。</p><p><a href="http://nedbatchelder.com/text/unipain.html" target="_blank" rel="noopener">Ned Batchelder 关于python unicode和str的理解，通俗易懂</a></p><p><a href="http://www.tuicool.com/articles/ZbEFnya" target="_blank" rel="noopener">关于scrapy 入门</a></p><p><a href="http://blog.csdn.net/crazyhacking/article/details/39375535" target="_blank" rel="noopener"> 关于 encode的认识</a></p><p>通过上面我们可以很好的理解python的转码译码，在这里我想谈一下我自己对其的认识吧，我一开始接触的c语言序列的基本上都是强类型，比如C里面假如我想写一个函数每个传人的参数都得是有类型的，但是python弱化了类型这一点，python也是面对对象的，但是他的对象就是鸡同鸭讲，照猫画虎就能运行，弱类型适合动态语言，我们不确定下一行代码输入的是什么，自从学python起，一直感觉python对类型一直不严格，这样就给了我一种错觉，只要长得差不多就能一样的比划，比如在两个string，<code>&#39;中国&#39;</code>,<code>u&#39;中国&#39;</code>,看起来差不多但是如果你把<code>u&#39;中国&#39;</code>存入文件中就会出错（假如你没定义编码规则)</p><pre><code>UnicodeEncodeError: &apos;ascii&apos; codec can&apos;t encode characters in position 344-351: ordinal not in range(128)</code></pre><p>unicode字符编码错误，要想理解这个要对unicode字符集和unicode编码有一定的理解，推荐你读一下这篇博客<a href="http://www.techug.com/character-set" target="_blank" rel="noopener">字符编码的知识</a>,python内部使用unicode字符集存贮所以的编码的字符，为什么要用unicode字符集举个栗子吧：</p><blockquote><p>A是米国的程序员，他使用asicc编码的文件上传了一封邮件，<br>B是中国的程序员他使用gbk编码的文件上传了一封邮件，<br>现在C要用程序同时处理A和B的邮件，有两种解决方法他把A的文件译码再编码成B的gbk，或者将B的文件译码成asicc但是中文无法处理，那么只能使用第一种方法将A的文件编码成gbk，但是改天D又来啦，他是俄国人，天啊噜gbk可能没有把俄语编进去，那肿么办，我们迫切需要一种编码可以把所以的字符放进去，所以unicode出现了，Unicode中将字符集按照一定的类别划分到0~16这17个层面（Planes）中，每个层面中拥有216=65536个字符码，因此Unicode总共拥有的字符码，也即是Unicode的字符空间总共有17*65536=1114112，一共有1114112这么多的字符可以用，这下我们不用担心了吧，太好了这下不用愁了，</p></blockquote><p>python 内部使用unicode字符集作为一个译码中转站，因为他编码了所以的字符集，只要你能在自己编码方案上找到自己的字，我就能在unicode字符集找到你的位置，所以使用unicode可以很好的解决多种编码方案产生的问题（比如gbk，utf-8）</p><p>当然其他编码方案如果想使用unicode解码成其他的必须同unicode有一一对应关系，不过现在主流的编码方案如gbk，gb2312，utf-8都是unicode系的。</p><p>了解了这些基础知识就可以知道了为什么存贮<code>u&#39;中国&#39;</code>存不进文件里面去了，因为unicode并不提供给当今字符解析器的方法，就是<code>\u234e</code>一个16进制数字，屏幕上不知道他对应什么图形，所以python系统要求存进文件的必须是字节流，也就是可以unicode是一种更高级的字符流，这个字符流能存贮当今世界所以定义的字符，但是他只是一个规定字符集合，我们只需要把发现的字符放进去占据一个位置，但是我们不需要考虑屏幕是否认识这个字符，这个字符的存贮由编码方案负责，如utf-8这些，假如没有字符编码方案可以存贮这些，我们虽然在unicode上有这个字符但是我们无法print出来，所以我们必须将unicode转换成普通字符流，有人就会问了，假如我真的没有找到一个合适编码方案可以存贮所有语言，我们可以将他编码成unicode—escape类型，这里我们不多讲。</p><p>这就可以解释我们大部分碰到的错误unicodedecodeerror和unicodeencodeerror错误，都是因为字符编码方案不了解造成的，网上很多说碰到这种错误就encode，decode搞一下就行但是不弄清楚这背后的知识就会犯迷糊。</p><p>接下来我谈谈我遇到的错误吧，在爬取<code>http://yjsy.ncu.edu.cn/yjs_showmsg.asp?id=2770</code>这个页面时（这是一个不规范的页面没有设置charset），因为每个spider调用的</p><pre><code>response.xpath(&apos;//xpath&apos;).extract()    </code></pre><p>选择器返回的是一个unicode编码的字符集，但是他是接受的是一个字符流，spider可能调用了<br><code>response.body.decode(response.encoding)</code>进行转码，但是这个response.encoding有时候会判断错误，比如将我一个gbk编码的文件判断成cp1253,这个时候假如我把他解码成encode成其他编码方式的话，我们就会得到乱码，那怎么纠正呢，我们可以这样干<br>先将得到的列表中每个<code>content</code>取出来，然后使用<code>content.encode(resonse.encoding)</code>转码成原始字符流，现在你可以将它用正确的编码转换成unicode了</p><p>下面是我github上的关于这个scrapy的项目，在coding_pitch.py文件里面就是对于这个乱码的处理</p><p><a href="https://github.com/MrZhangLoveLearning/ncuscrapy.git" target="_blank" rel="noopener">南昌大学教务处公告爬取</a></p>]]></content>
      
      
      <categories>
          
          <category> Python </category>
          
      </categories>
      
      
    </entry>
    
    
  
  
</search>
