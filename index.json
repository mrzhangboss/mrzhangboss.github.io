[{"content":"","date":"2024-07-16","externalUrl":null,"permalink":"/posts/backend/software/","section":"博客","summary":"","title":"软件","type":"posts"},{"content":" ","date":"2024-01-01","externalUrl":null,"permalink":"/posts/backend/","section":"博客","summary":"","title":"后端","type":"posts"},{"content":" Hugo 自动化博客示例 mrzhangboss/hugo_blog_demo HTML 0 0 一款自动导入JSON到数据库Python库 mrzhangboss/json2db convert json to relational db Python 25 15 一个快速代理扫描Python库 mrzhangboss/FastProxyScan fast proxy scan project Python 8 2 一个监控Python库 mrzhangboss/hookman Python 0 0 ","date":"2024-01-01","externalUrl":null,"permalink":"/opensource/","section":"开源","summary":"Hugo 自动化博客示例 mrzhangboss/hugo_blog_demo HTML 0 0 一款自动导入JSON到数据库Pyth","title":"开源","type":"opensource"},{"content":"","externalUrl":null,"permalink":"/posts/frontend/","section":"博客","summary":"","title":"前端","type":"posts"},{"content":"","externalUrl":null,"permalink":"/posts/design/","section":"博客","summary":"","title":"设计","type":"posts"},{"content":"","date":"2022-03-05","externalUrl":null,"permalink":"/posts/essays/","section":"博客","summary":"","title":"随笔","type":"posts"},{"content":" TODO # 梳理博客，将历史文章归档分类 已完成 # 部署Hugo博客 ","date":"2024-07-16","externalUrl":null,"permalink":"/roadmap/","section":"🗼","summary":"TODO # 梳理博客，将历史文章归档分类 已完成 # 部署Hugo博客","title":"🗼","type":"roadmap"},{"content":"","date":"2024-07-16","externalUrl":null,"permalink":"/posts/","section":"博客","summary":"","title":"博客","type":"posts"},{"content":"","date":"2024-07-16","externalUrl":null,"permalink":"/","section":"欢迎来到 Blowfish! 🎉","summary":"","title":"欢迎来到 Blowfish! 🎉","type":"page"},{"content":"","date":"2024-07-15","externalUrl":null,"permalink":"/categories/","section":"Categories","summary":"","title":"Categories","type":"categories"},{"content":" 背景 # 博客这次由Hexo 转为 Hugo，我来简单介绍为什么要用Hugo\n首先，Hugo的速度非常快。Hugo是使用Go语言编写的，它使用了一种基于内容的建模方式，这意味着它只会重新生成发生更改的页面，而不是整个网站。相比之下，Hexo使用JavaScript和Node.js，每次生成网站时需要重新渲染整个网站，这会导致生成时间较长。对于那些需要频繁更新和发布内容的用户来说，Hugo的速度优势非常明显。\n其次，Hugo在Windows平台上的友好性也是一个重要的因素。Hexo在Windows上的安装和配置可能会遇到一些问题，需要一些额外的步骤和设置。而Hugo则是一个跨平台的应用程序，可以在Windows上轻松安装和使用，而无需额外的配置。\n最后，Hugo不依赖于Node.js环境。尽管Node.js是一种强大的工具，但它在某些情况下可能会引起一些问题。例如，当Node.js版本更新时，可能会导致某些插件或主题无法正常工作。而Hugo完全独立于Node.js，不受其影响。这对于那些希望减少依赖和简化开发环境的用户来说，是一个非常有吸引力的特点。\n对于我来说，采用Hugo，第一个不想本地部署Node，想开箱即用，也想随时随地能够写博客，并且我想要的一个比较符合我的想法的主体，正好 Hugo 有一个款 主题blowfish 能很好满足我将博客当做我的个人知识库的\nHugo简介 # Hugo是一款流行的静态网站生成器，由Steve Francia于2013年创建。它使用Go语言编写，具有快速、灵活和易于使用的特点。\nHugo的主要目标是生成高性能的静态网站。它通过将网站内容和模板结合起来，生成静态HTML文件。这意味着在访问网站时，不需要在服务器上动态生成页面，从而提高了网站的加载速度和性能。\nHugo具有许多令人印象深刻的特性。首先，它支持多种内容格式，包括Markdown、HTML、AsciiDoc等，使用户可以根据自己的喜好选择适合的写作方式。其次，Hugo提供了丰富的主题和模板，用户可以根据自己的需求选择合适的外观和功能。此外，Hugo还支持多语言网站和多种部署方式，使用户可以轻松地创建多语言站点并将其部署到各种不同的托管平台上。\nHugo的使用非常简单，只需几个简单的命令即可生成静态网站。它还提供了实时预览功能，可以在本地进行网站预览和调试。此外，Hugo还具有快速构建和部署的能力，可以轻松地将网站发布到生产环境中。\nGithub Action 自动化流程 # 假如使用了Hugo之后，我们依然要运行hugo 生成html文件，然后上传到github page，那有什么可以自动化这个步骤呢\n有的 github 提供了 Github Action ，你可以理解为就是Git 钩子，当你上传了这个钩子后，当你部署代码的时候自动执行命令\n可以参考 这个文件\n简单分析一下这个文件\n这个文件其实很简单：\n拉取代码 安装hugo软件 打包 执行git push命令将文件推到 GitHub Page 仓库 怎么写博客 # 那有的人要问了，那我还得打开电脑，用git push 博客 markdown 内容到 仓库，现在连接仓库又不能通过密码连接，那还是得自己配置git 仓库，不要急，贴心的github，给我们提供了web版 VS Code 编辑器\n打开你的个人博客仓库，输入 . 号，就会打开 一个web版的 VS Code编辑器，唯一遗憾的就是不能安装一些如辅助写文章的插件，如通义千问，不过，推荐你一个开源项目AI 匿名编辑器, 你只要打开 网站 就可以开始开心的编写你的文章了，当前目前AI辅助 看起来还有点傻乎乎的，不太符合我的习惯，我个人还是喜欢用通义千问那种AI 辅助工具，你写几行字，AI辅助帮你把其他的字补全\n","date":"2024-07-15","externalUrl":null,"permalink":"/posts/backend/software/hugo%E4%BD%BF%E7%94%A8/","section":"博客","summary":"背景 # 博客这次由Hexo 转为 Hugo，我来简单介绍为什么要用","title":"Hugo 使用教程","type":"posts"},{"content":"","date":"2024-07-15","externalUrl":null,"permalink":"/categories/%E5%90%8E%E7%AB%AF/","section":"Categories","summary":"","title":"后端","type":"categories"},{"content":"","date":"2024-07-15","externalUrl":null,"permalink":"/categories/%E8%BD%AF%E4%BB%B6/","section":"Categories","summary":"","title":"软件","type":"categories"},{"content":" I am a software engineer. # ","date":"2024-07-14","externalUrl":null,"permalink":"/about/","section":"欢迎来到 Blowfish! 🎉","summary":"I am a software engineer. #","title":"关于","type":"page"},{"content":"","date":"2024-01-01","externalUrl":null,"permalink":"/posts/backend/framework/","section":"博客","summary":"","title":"框架","type":"posts"},{"content":"","date":"2024-01-01","externalUrl":null,"permalink":"/posts/backend/database/","section":"博客","summary":"","title":"数据库","type":"posts"},{"content":"","date":"2024-01-01","externalUrl":null,"permalink":"/posts/backend/network/","section":"博客","summary":"","title":"网络","type":"posts"},{"content":"","date":"2024-01-01","externalUrl":null,"permalink":"/posts/backend/middleware/","section":"博客","summary":"","title":"中间件","type":"posts"},{"content":"","date":"2022-07-02","externalUrl":null,"permalink":"/tags/java/","section":"Tags","summary":"","title":"Java","type":"tags"},{"content":"","date":"2022-07-02","externalUrl":null,"permalink":"/tags/","section":"Tags","summary":"","title":"Tags","type":"tags"},{"content":"","date":"2022-07-02","externalUrl":null,"permalink":"/categories/%E6%A1%86%E6%9E%B6/","section":"Categories","summary":"","title":"框架","type":"categories"},{"content":" 最近的业务也做了一个类似秒杀业务的，自己在做的过程中也感悟到没有完美的解决方案，每种方法都有自己的缺点， 要基于业务去设计核心，而不是为了秒杀而秒杀\n什么是秒杀系统 # 秒杀系统就是：抢\n用生活的例子就是，一群人去超市排队买瓜，一人只能买一个，一共100个，抢完了就没有了\n秒杀系统核心 # 秒杀系统最核心就是，如何确保数据一致性，不超买超卖，网上这种文章一搜一大把，你只要安安静静看几篇就能明白的七七八八了\n在你看完之后大概就能设计出一个简单的Redis Lua脚本来进行秒杀，我这里就不介绍怎么写，教程实在是太多了\n我这里想提出一点就是，大部分的教程都不会告诉你，Redis其实很脆弱的，它可能会崩溃，虽然概念很低，但是一旦崩溃，你的数据就不准确了\n假如你完全依赖Redis作为秒杀成功标注的话，在极端case下面会出现超买超卖现象\n怎么解决这个呢，其实很简单，只依赖Redis作为秒杀的过滤器，通过Redis完成秒杀，不一定抢购成功，要等数据库确认之后才能算秒杀成功\n那么我们Redis是不是完全没有用了呢，恰恰相反，作用非常大，Redis将流量过滤到数据库可以接受的程度，假如流量全部打到数据库上面，数据库瞬间就崩溃了\n扩展 # 在使用Redis过程中，我发现一个好玩的事情就是使用Redis做一个消息队列，并且对消息进行压缩， 这个压缩是指在消费的过程中，使用Lua脚本，批量获取队列值，然后清空队列，达到一个消息压缩的情况\n相当于在Redis层做了一个数据处理的工作，优化了数据传输，但是有可能会造成Redis性能下降\n总结 # 秒杀系统说难不难，说容易不容易，新手很容易忽视在高流量下面各个系统崩溃的极端情况，然而这些情况都是必须要重视的\n","date":"2022-07-02","externalUrl":null,"permalink":"/posts/backend/framework/java/%E7%A7%92%E6%9D%80%E7%B3%BB%E7%BB%9F/","section":"博客","summary":"秒杀系统","title":"秒杀系统的思考","type":"posts"},{"content":"","date":"2022-03-19","externalUrl":null,"permalink":"/tags/rpc/","section":"Tags","summary":"","title":"RPC","type":"tags"},{"content":" 最近想系统学一下RPC框架，下面是我学习路径\n阅读一个简易的RPC框架源码\n主要是学习了\n服务端注册中心注册服务 客户端创建动态代理，当调用方法查询对应服务所在地址，序列化请求体并发起远程调用 服务端使用netty，对客户端自定义协议进行拆包，并调用对应注册类方法 com.alibaba.dubbo.rpc.proxy.InvokerInvocationHandler\ncom.alibaba.dubbo.rpc.protocol.dubbo.DubboInvoker\n","date":"2022-03-19","externalUrl":null,"permalink":"/posts/backend/framework/java/rpc/rpc%E5%A4%A7%E7%BA%B2/","section":"博客","summary":"最近想系统学一下RPC框架，下面是我学习路径 阅读一个简易的R","title":"RPC大纲","type":"posts"},{"content":"","date":"2022-03-05","externalUrl":null,"permalink":"/categories/%E4%BA%BA%E7%94%9F/","section":"Categories","summary":"","title":"人生","type":"categories"},{"content":"","date":"2022-03-05","externalUrl":null,"permalink":"/posts/essays/life/","section":"博客","summary":"","title":"人生","type":"posts"},{"content":"","date":"2022-03-05","externalUrl":null,"permalink":"/categories/%E9%9A%8F%E7%AC%94/","section":"Categories","summary":"","title":"随笔","type":"categories"},{"content":" 最近玩王者玩着玩着突然有些感悟，感觉有时候通过玩一个游戏也可以看到自己身上的不足\n追杀还是放弃 # 有的时候会上头，追着残血追到二塔被他们围殴或者杀了人却被搭打死了，其实这个完全没必要，一个残血，其实你不打死它，除了那种后期吸血怪，基本上都是要回泉水补血，差不多就相当于杀了他一次\n有这个追人的时候，你还不如继续扩大优势，推个塔，拿野区资源等等，王者是一个经济游戏，在你装备还没成型的时候，你90%的任务都是快速发育以及阻止对面发育，通过经济差碾压别人\n其实我感觉在现实中我也是，有的时候学一个东西也容易上头，打破砂锅问到底，最后把自己的耐心磨完，忘记了自己真实的目的\n观察局势 # 有的时候，无论是劣势局或者优势局容易梦游，经常“目光短浅”，为了几个小兵，孤军深入，最后送个人头快速回家，或者有的时候打射手，一直就是一抓就死，前期大优势打成逆风\n观察局势说难不难，说简单也不简单，毕竟对面5个英雄，你又要对线，但是其实很简单，基本上关注中单和打野位置就好了，当然假如对面野辅联动也得关注辅助的位置，你只需要关注他们是否是会来抓你\n所以其实高端局经常看到大佬说什么给个假动作，给一个往下路走的信号，其实往上路走了，虚虚实实，真正假假\n其实要想养成这个意思，必须让自己脱离这个游戏，培养自己一种上帝视角，自己控制的这个角色输赢都无所谓，你会开始猜对面在干嘛，去预测对面的动向，所以以前英雄联盟有个主播叫大司马，大部门人特别敬佩就是他的预测能力\n基本上他能知道对方在干什么，所以其实你和大神之间的差距就在这里，大神知道你要做什么，所以他能制裁你，你在他眼中和机器人没什么区别\n总结 # 自己之前其实不怎么喜欢玩这种5V5实时对战的游戏，喜欢玩人机，因为5V5中有些人太在意输赢了，会因为别人的失误会去骂人，在这个团队游戏中，经常有射手骂打野，都被对面打野抓烂了，自己打野没来一次，大部分人只会在别人身上找原因，而从来不在自己身上找原因\n自己一抓就死还不是因为没有注意对面打野动态，人生也是这样，大部分把自己不成功的原因归结于没有一个好爹，起点没有别人高，与其抱怨还不如想想怎么才能混起来，发育起来\n其实感觉人生有的时候和游戏一样，那些成功的人他们只是比普通人看的更长远一点，普通人只看到眼前的芝麻，成功的人看到了远方的西瓜。\n努力不一定会成功，但是不努力补每一个兵，打团的时候就打不出伤害，你最后就会输掉这场比赛，游戏和人生一样。或许你没法决定自己的队友，但是你可以成为那个在队友发起投降的时候不放弃的那个人，只有有一个不想投降的人在你们这边，你们就不会自爆水晶。人生也是，你不放弃，没谁可以让你放弃。\n选择权在你手里。\n","date":"2022-03-05","externalUrl":null,"permalink":"/posts/essays/life/%E7%8E%8B%E8%80%85%E8%8D%A3%E8%80%80%E6%84%9F%E6%82%9F/","section":"博客","summary":"最近玩王者玩着玩着突然有些感悟，感觉有时候通过玩一个游戏也可","title":"王者荣耀感悟","type":"posts"},{"content":" 最近被NIO这个概念弄得有点晕，一会是Linux的NIO一会是Java 的NewIO也叫NIO，然后Linux又有一个AIO的概念，但是Java里面好像没有，所以就好好理理IO到底是个啥东西\nIO模型 # 我们都知道IO就是Input Output，但是IO模型其实是给读取也就是Input来做的模型，因为Output非常简单就是往网卡写数据，但是读数据就有点不一样了\n为了高效的从socket里面读数据，Linux给IO创建了5种模型\n第一种最简单，就是同步模型，执行一个系统调用，然后等内核从网卡读取完，最后讲数据从内核复制到用户空间就完成了一次IO读取，这种方式对内核来说最简单了，但是对于用户来说就很低效了\n一个线程基本上都卡在IO上面了，即使有多线程技术，但是万一用户量很大，线程是有限的，而且有的时候万一用户网络很卡数据包很大，网卡读数据就花了几个小时，几个小时都耗在一个链接上了\n所以提出来第二种IO，同步非阻塞 IO，也就是NIO，这个IO和上面区别在于，执行系统调用之后里面就返回了，不会等待内核堵塞用户了，但是假如内核没准备好，则会返回为空，但是这个也有个缺点就是 假如一直没准备好，你就得用个队列把这些没准备好的存贮起来进行轮询，假如队列里面多了，你会耗很多时间在无效轮询上面\n所以在NIO的基础上，操作系统提出一个多路复用的概念，其实就是相当于操作系统给你维护了一套消息队列，当有IO准备好了，就返回给客户端，所以其实对于客户端来说，现在的IO还是一个堵塞的， 只有当IO准备好了才会返回，操作系统在维护这个队列上面也做了很多升级，从最初的select到poll到最后的epoll，操作系统一直在优化，以达到一个完全消息驱动，节省不必要的CPU浪费\n当然前面的NIO的需要轮询的缺点可以通过信号来解决，所以又引出了第三个模型，就是信号模型，不幸的信号模型在大量IO的情况下会丢数据，导致不可靠，所以这个模型其实用的也少\nJava New IO # 异步到底是什么 # 异步优缺点 # 异步的替代方案 # 总结 # 引用 # https://www.linuxidc.com/Linux/2017-09/146682.htm\n","date":"2022-02-19","externalUrl":null,"permalink":"/posts/backend/framework/java/io%E6%80%BB%E7%BB%93/","section":"博客","summary":"最近被NIO这个概念弄得有点晕，一会是Linux的NIO一会","title":"IO总结","type":"posts"},{"content":" 引言 # 最近在业务代码中经常用到的BeanUtils.copyProperties，有的时候在想，这个东西在Java里面真方便，但是性能怎么样呢，然后找了一篇博文 https://www.cnblogs.com/kancy/p/12089126.html\n大吃一斤，竟然要100多ms，我们接口压测性能也就40ms左右，调用这么个方法竟然耗时要这么多，本着不传谣不信谣的精神，开始下面的测试\n步骤 # 首先我用Python生成两个大类A，B，两个字段数都为254（因为Java最大属性就是255），然后生成一个类C，其中有个静态方法，能将A的值全部都赋给B\n最后我跑了一下结果\n下面是结果\nbean复制 热身🚀 1100次耗时: 264.00000毫秒 速度： 0.24000毫秒每次 正式🚀 1000000次耗时: 14383.00000毫秒 速度： 14.38300微秒每次 硬复制 热身🚀 1100次耗时: 12.00000毫秒 速度： 0.01091毫秒每次 正式🚀 1000000次耗时: 116.00000毫秒 速度： 0.11600微秒每次 因为JVM有热加载技术，所以我运行前都让它热个身，结果很哇塞，的确BeanUtils的速度要比硬copy要慢100倍左右，但是其实对于程序来说，14微妙也是非常快了，其实除非你在一次请求里面要用这个方法上百次，其实问题都不大\n疑惑 # 但是我又有个疑惑了，为啥bean复制最开始的1100次要耗时那么那么长，到底发生了什么，接下来我把热身的时候，蜂刺数据提取出来（上一次执行耗时和这次执行差距1ms），结果如下\n0 cost: 10 1 cost: 0 15 cost: 95 16 cost: 2 17 cost: 0 127 cost: 2 128 cost: 0 我执行了很多次，在第0次的时候，和第14、15、16这些的时候都会耗时巨长，第一次可能是因为建立反射缓存，但是第14、15、16为啥会耗时那么长呢\n接下来我们深入源代码看看，为啥会这样\n源码探索 # 源码在 org.springframework.beans.BeanUtils.copyProperties 中\nprivate static void copyProperties(Object source, Object target, @Nullable Class\u0026lt;?\u0026gt; editable, @Nullable String... ignoreProperties) throws BeansException { Assert.notNull(source, \u0026quot;Source must not be null\u0026quot;); Assert.notNull(target, \u0026quot;Target must not be null\u0026quot;); Class\u0026lt;?\u0026gt; actualEditable = target.getClass(); if (editable != null) { if (!editable.isInstance(target)) { throw new IllegalArgumentException(\u0026quot;Target class [\u0026quot; + target.getClass().getName() + \u0026quot;] not assignable to Editable class [\u0026quot; + editable.getName() + \u0026quot;]\u0026quot;); } actualEditable = editable; } PropertyDescriptor[] targetPds = getPropertyDescriptors(actualEditable); List\u0026lt;String\u0026gt; ignoreList = ignoreProperties != null ? Arrays.asList(ignoreProperties) : null; PropertyDescriptor[] var7 = targetPds; int var8 = targetPds.length; for(int var9 = 0; var9 \u0026lt; var8; ++var9) { PropertyDescriptor targetPd = var7[var9]; Method writeMethod = targetPd.getWriteMethod(); if (writeMethod != null \u0026amp;\u0026amp; (ignoreList == null || !ignoreList.contains(targetPd.getName()))) { PropertyDescriptor sourcePd = getPropertyDescriptor(source.getClass(), targetPd.getName()); if (sourcePd != null) { Method readMethod = sourcePd.getReadMethod(); if (readMethod != null \u0026amp;\u0026amp; ClassUtils.isAssignable(writeMethod.getParameterTypes()[0], readMethod.getReturnType())) { try { if (!Modifier.isPublic(readMethod.getDeclaringClass().getModifiers())) { readMethod.setAccessible(true); } Object value = readMethod.invoke(source); if (!Modifier.isPublic(writeMethod.getDeclaringClass().getModifiers())) { writeMethod.setAccessible(true); } writeMethod.invoke(target, value); } catch (Throwable var15) { throw new FatalBeanException(\u0026quot;Could not copy property '\u0026quot; + targetPd.getName() + \u0026quot;' from source to target\u0026quot;, var15); } } } } } } 借助arthas我们可以知道第一次加载慢是因为耗时都在 getPropertyDescriptors 上面，第一次的时候需要通过反射去获取，这个时间在我电脑上要5-10ms左右，之后再次运行相同的类就不需要这么长，但是为什么在15-16次会有一次比第一次还要长的耗时呢，araths的结果我们就不粘贴了，其中其实耗时都不长，耗时99%的时候都在 java.lang.reflect.Method:invoke 这个方法上\n我们知道反射慢是正常的，为啥到了15次之后就没有速度慢的呢，原因是JVM会对发射做一个优化，当请求同一个反射超过15次之后，就会编译成字节码，所以之后速度就起飞，但是由于第一次要生成字节码，所以它耗时还要比第一次通过反射获取的还要慢\n具体资料为：https://blog.csdn.net/zhang6622056/article/details/98950855\n总结 # 通过这个实验，我们知道，原来即使使用反射，其实JVM的速度也能优化到几微秒，假如你的类不超过50个字段，其实硬编码和Bean复制其实差别不大，你追求的极致优化其实都是优化几微秒，相比于网络的ms优化简直不值一提，而且目前其实也有很多优秀的框架，如JMapper,MapStruct 等可以生成字节码，而不需要你硬编码\nhttps://segmentfault.com/a/1190000040791635\n","date":"2022-01-17","externalUrl":null,"permalink":"/posts/backend/framework/java/bean%E5%A4%8D%E5%88%B6%E7%9C%9F%E7%9A%84%E9%82%A3%E4%B9%88%E6%85%A2%E5%90%97/","section":"博客","summary":"引言 # 最近在业务代码中经常用到的BeanUtils.copy","title":"Bean复制真的那么慢吗","type":"posts"},{"content":" 引言 # 2021年，对于世界来说都是一个魔幻的元年，新冠在全世界肆虐，然而每个中国人都被保护的很好\n这一年对于我来说也是挺魔幻的\n经历 # 记个流水账吧，简单来说，换了家公司，谈了一段感情\n收获 # 在技术上，自己真正算是开始做后端开发了吧，自己对中间件、缓存有了一个新的认识，但是进步缓慢。\n在感情上，自己算是成熟一点，学会了怎么去爱一个人，学会了如何接纳自己，最重要的是很大程度上缓解了自己的内耗吧\n总结 # 以前很喜欢写很长很长的文字，但是慢慢发现，懂得越多不是写的越多，而且经历的越多，思考的越多，感谢2021年的故事，有快乐、有悲伤、有痛苦、有释然，2022年加油，不要让自己犯同样的错误，朝着目标努力吧！\n","date":"2022-01-17","externalUrl":null,"permalink":"/posts/essays/life/2021summary/","section":"博客","summary":"引言 # 2021年，对于世界来说都是一个魔幻的元年，新冠在全世","title":"2021年终总结","type":"posts"},{"content":" 一、什么是泛型 # 二、泛型的作用是什么 # # ","date":"2021-12-10","externalUrl":null,"permalink":"/posts/backend/framework/java/%E6%B3%9B%E5%9E%8B%E7%9A%84%E5%89%8D%E4%B8%96%E4%BB%8A%E7%94%9F/","section":"博客","summary":"一、什么是泛型 # 二、泛型的作用是什么 # #","title":"泛型的前世今生","type":"posts"},{"content":" 最近在学JVM的时候，把ClassLoader部分给过了一遍，谈到ClassLoader少不了双亲委派，谈到双亲委派少不了说破坏双亲委派的SPI，也看到了知乎上一些观点 ，这个时候我就疑惑了，有两个问题围绕在我头上，什么是双亲委派，为什么说SPI破坏了双亲委派，这篇博客就从源码出发，讲一讲我的理解\n一、什么是双亲委派 # 双亲委派我归纳为两点：\n不可以越权 委托链机制 现在我来讲讲为什么这样总结\n不可以越权 # ClassLoader分为四种：启动类加载器 \u0026gt; 扩展类加载器 \u0026gt; 应用程序类加载器 \u0026gt; 自定义\n我们看看Class.forName的源代码\npublic static Class\u0026lt;?\u0026gt; forName(String className) throws ClassNotFoundException { Class\u0026lt;?\u0026gt; caller = Reflection.getCallerClass(); return forName0(className, true, ClassLoader.getClassLoader(caller), caller); } 当我们需要初始化一个类的时候，我们得会通过Reflection.getCallerClass获取到调用方的ClassLoader，通过这个ClassLoader去加载类\nPS： 其中forName0 是一个native方法，最终会调用C++代码find_class_from_class_loader 从classLoader中 调用loadClass方法\n也就是说其实源码告诉我们，我们加载一个类，默认是从加载这个类的ClassLoader去加载别的类，说起来很拗口，其实就是说你这个代码是被哪个加载器加载的，默认只能加载被这个 ClassLoader加载过的\n当然在JDK1.2 支持传入ClassLoader，这个后面会讲 ,接下来我们讲什么是委托链机制\n委托链机制 # 我们来看ClassLoader.loadClass方法\nClass\u0026lt;?\u0026gt; c = findLoadedClass(name); // 从缓存中加载 if (c == null) { if (parent != null) { // 父类不为空 委托父类加载 c = parent.loadClass(name, false); } else { c = findBootstrapClassOrNull(name); // 当父类为空的时候就是根加载器，委托根加载器加载 } if (c == null) { // 最后从自己这里取 c = findClass(name); } 代码很简单，就是首先从缓存中取，没找到然后在从父类里找，找不到就到自己这找，其实一个很有意思的点就是，从缓存中取的是调用的native方法，最后调用的C代码就是下面这个\nKlass* k = SystemDictionary::find_instance_or_array_klass(klass_name, //类名\nh_loader, // classLoader地址 Handle(), CHECK_NULL);\n原理是从hash表中，通过类名和加载器类名进行查找，也就是说，对于一个类来说，java判断是否相同是根据它的类名和加载该类的ClassLoader的实例来实现的\n这两点其实很容易理解，接下来我们看看什么是SPI\n二、SPI是什么 # SPI全名是Service Provider Interface，这个其实就是一个工具类\n原理非常的简单，我归纳成两点\n当调用静态方法ServiceLoader.load 时候根据传入的类或者接口名，读取 META-INF/services/{name} 下面的文件 读取字符串，然后加载 Class.forName(cn, false, loader) (cn代表这个字符串，loader代表加载器）\n其中最核心的就是调用了 Class.forName 这个方法，你可以看到，这里面调用了我们上面提到的一个重载方法，这个重载方法最大的差别就是可以自己传入自己想传入的ClassLoader\n接下来看看为什么我们说它破坏了双亲委托\n为什么说SPI破坏了双亲委派 # 我们回顾一下前面总结，SPI它破坏了哪点呢，其实就是我们之前说的第一点 ** 不可以越权** ，这个SPI竟然可以自己传入classLoader，而且默认它取的是当前线程的ClassLoader\n我们来看看经常提到的JDBC中，为啥说SPI破坏了java.sql.DriverManager的双亲委托呢，因为这个java.*包是根加载器加载的，然而我们却在java.sql.DriverManager 中使用 Class.forName 调用了三方jar包（使用应用程序类加载器加载）\n总结 # 所以其实从本质上来说，为啥我们说SPI破坏了双亲委派破坏了，坏就坏在它的代码在java.sql.DriverManager中，假如我们在我们代码中使用这个Class.forName，我们没有破坏这个双亲委派\n其实即使在java.sql.DriverManager我们不使用SPI，我们也可以破坏双亲委派，因为它会读取jdbc.drivers 这个属性，我们会读取这个字符串，然后使用Class.forName(aDriver,true,ClassLoader.getSystemClassLoader());来加载里定义的三方Driver\n资料\nhttps://enfangzhong.github.io/2019/12/17/%E6%B7%B1%E5%85%A5%E7%90%86%E8%A7%A3Java%E8%99%9A%E6%8B%9F%E6%9C%BA%E4%B9%8B%E7%A0%B4%E5%9D%8F%E5%8F%8C%E4%BA%B2%E5%A7%94%E6%B4%BE%E5%8A%A0%E8%BD%BD%E6%9C%BA%E5%88%B6/\n","date":"2021-12-09","externalUrl":null,"permalink":"/posts/backend/framework/java/spi%E7%A0%B4%E5%9D%8F%E4%BA%86%E5%8F%8C%E4%BA%B2%E5%A7%94%E6%B4%BE%E5%90%97/","section":"博客","summary":"最近在学JVM的时候，把ClassLoader部分给过了一遍","title":"SPI破坏了双亲委派吗","type":"posts"},{"content":" 我记得之前写过Spring AOP相关的文章，但是最近在观看Cat源代码的时候发现 @Aspect 这个注解与AspectJ这个项目，查阅了不少博客，感觉还是云里雾里，这篇博客就是基于博客以及自己实际实验与测试搞懂 Spring AOP和 AspectJ之间的关系\nAOP是什么 # 网上的资料很多，我就不写那些烂大街的总结了，其实很简单，我们先了解一下Python的装饰器\n我们定义一个装饰器\ndef cost(f): def _real(*arg, **kward): start = time.time() reslt = f(*arg, **kward) end = time.time() print(‘cost’, end - start, ‘ seconds’) Return _real\n然后我们可以将这个装饰器放在任何方法上\n@cost def run(): print(“run\u0026quot;)\n我们运行任何代码都可以使用这个装饰器，其实AOP的核心要求也就是这个，只要我定义了一个方法，我能让我想让的函数都能执行这个，这就叫解耦，Python怎么实现这个呢，其实很简单，一个语法糖，编译器会把这个cost函数重新定义为一个新函数，具体原理我就不介绍了，我就把拆解后的代码写出来吧\ndef _run(): print(“run”)\ndef run(): return cost(_run)()\n然后在Java中虽然也有@ 这个用法，但是那个是注解，它的作用就是一个文档，我们可以反编译有注解的类，只要这个注解的RetentionPolicy是RetentionPolicy.RUNTIME 它就会在保留在字节码上面\nSpring AOP怎么实现 # 那Spring AOP如何实现在字节码上面读取到注解，然后实现AOP功能呢，答案就是反射（这也是为啥Spring启动很慢的原因，反射太慢了）\n通过Class.getAnnotation 这个方法，我们可以获取到类上面所有的注解，Spring 作为Bean工厂，对应所有的Bean它都会去读取它的注解，根据注解来灵活组合功能\n拿到注解后，如何实现把你想切入的代码插入到这个被注解到类上面呢。Java是一个面对对象的语言，它要想改变类的方法，只能通过继承\n继承有两张实现方式，假如类实现了接口，则使用JDK动态代理方式，即该类的newProxyInstance方法，第二种就是使用CGLib动态代理，自动生成类的字节码\n为什么能实现呢，因为所有的bean都是Spring来控制生成的。因为获取bean的时候，我们也能知道这个bean的类名，所以Spring 也提供了通过类名来控制切面，这就是自己来实现的好处，Python的装饰器只有被装饰的类才有用，而Spring由于自己实现了这套逻辑，所以它其实也支持通过类名来进行AOP\nSpring AOP的局限性 # 前面说到Spring AOP的原理，我们提到了一个关键就是被代理的都只能是bean，这就带来一个局限性，对应非bean的类，我们无法代理，不信你可以试试在Spring Boot里面切入一个非bean的三方如org.apache.ibatis.executor.BaseExecutor\n那对应这种非Spring Bean的没法做到了吗，这里就要提到一个框架 AspectJ\n这个就是AOP的鼻祖，Spring其实切面名字也叫Aspect,虽然实现不是用的AspectJ，但是用了它的注解类，这个框架强在哪呢，首先它提供了两种AOP方式 编译时候，修改编译的字节码，或者编译后增强 运行的时候，通过agent修改字节码\n编译时候修改字节码，我们比较熟悉的lombok就是这样的，是通过jdk1.6 javax.annotation.processing.Processor 引入的一种agent插入方式，通过SPI技术，你只要实现了这个类即可修改字节码\n编译后修改字节码是通过 JDK1.5 提供的 java.lang.instrument.Instrumentation来实现的，原理其实都很简单，使用SPI进行替换增强，感兴趣可以根据资料自己实操一下\nAspectJ和Spring的关系 # AspectJ 对应Spring 来说只是提供了注解类，Spring用代理类或者CGLIB实现了自己的逻辑\n总结 # 之所以写这篇博文，主要是在学习JVM的时候发现对于JAVA的注解，Spring如何实现的自己不是很清楚，所以顺藤摸瓜，把这条链路给整明白，当然还有很多细节还没有深挖，比如AspectJ是怎么将切面代码进行字节码转编译，它是如何做到不出现循环依赖的\n要想深刻理解AspectJ和Spring实现原理，非常推荐使用Arthas使用jad将运行的类给反编译下来，通过这种，你就不会停留在网上各种花里胡哨的文字上面，Spring的确是一个很牛逼的框架，封装了很多东西，定义了很多规范，要想理解这些，必须深入到源代码层次\nAspectJ原理\nhttps://blog.csdn.net/qq_40378034/article/details/115281684\nSpring Boot AOP 原理\nhttps://juejin.cn/post/6844903721101426701\nhttps://www.cnblogs.com/tuyang1129/p/12878549.html#:~:text=Spring%20%E7%9A%84%20AOP%20%E5%AE%9E%E7%8E%B0%E5%8E%9F%E7%90%86,%E9%87%8D%E5%86%99%E7%9A%84%E4%BB%A3%E7%90%86%E6%96%B9%E6%B3%95%E3%80%82\nhttps://blog.mythsman.com/post/5d301cf2976abc05b34546be/\nLombok原理\nhttps://blog.mythsman.com/post/5d2c11c767f841464434a3bf/\n","date":"2021-12-08","externalUrl":null,"permalink":"/posts/backend/framework/java/aop%E5%86%8D%E6%80%9D%E8%80%83/","section":"博客","summary":"我记得之前写过Spring AOP相关的文章，但是最近在观看C","title":"AOP再思考","type":"posts"},{"content":"","date":"2021-10-30","externalUrl":null,"permalink":"/tags/jvm/","section":"Tags","summary":"","title":"JVM","type":"tags"},{"content":"","date":"2021-10-30","externalUrl":null,"permalink":"/posts/backend/framework/java/jvm/jvm%E4%B9%8Bclassloader%E7%9A%84%E6%80%9D%E8%80%83/","section":"博客","summary":"","title":"JVM之ClassLoader的思考","type":"posts"},{"content":" 压测让人爆炸的事，就算把你写的代码都注释掉了这个问题依旧存在，这个时候你开始怀疑网络，怀疑TCP链接，怀疑系统框架，这篇文章就线上遇到的问题好好梳理了一下一个请求进来究竟会“遭遇”什么\n一、单机系统 # 首先我们先来思考在没有Tomcat这些网络框架的时候，一个单机系统是如何处理请求\n首先一个请求进来我们分成五步：\n建立TCP连接\n读取请求数据（先读取TCP请求头，然后根据TCP请求头读取报文）\n处理请求\n发送响应\n关闭TCP连接\n假如我们客户只有一个，那么只要它网速好，这个模型速度很快\n假如用户一多，其他用户就得排队，他们都堵在第一步建立TCP连接上，我们经常看到连接超时就是这种情况\n假如你处理请求很慢，用户发完请求之后就在那里等，这个时候就是浏览器转圈\n接下来我们看Tomcat是如何实现一个高并发的网络框架\n二、Tomcat原理 # 首先Tomcat是采用线程池来实现一个并发框架，它把上面步骤都放到2、3、4都放在一个线程中进行，你只需要写步骤3处理请求的代码，其他的步骤它来帮你完成\n其中步骤1和步骤5由主线程实现，主线程的工作就是创建TCP连接，这个数量是由 server.tomcat.max-connections 控制的，当超过这个数量的时候，Tomcat会把这个请求放到请求队列中server.tomcat.accept-count ，当超过这个数量的时候，Tomcat会丢弃这个请求，客户端会收到503错误\n由于我们内存CPU网卡的速度是有限的，所以Tomcat规定了，我们同时能处理多少请求，这个是通过server.tomcat.threads.max 来规定的，默认是200，就是同时能处理200个请求，其他的都在连接等待过程中\n这样讲非常的枯燥，接下来我们来看看在Cat监控中，每个步骤处的位置\n三、Cat例子 # 首先简单介绍一下Cat的原理，就是AOP，使用Spring的AOP能够在你想要监控的方法前后添加自己的代码\n上面是我限制客户端网速伪造的一个超长请求，我们来看看第一个 17:10:11.750 URL 这个是Cat在方法\norg.jboss.resteasy.plugins.server.servlet.HttpServletDispatcher.service 进行的一个打点\n你可以看到后面接着一大串同时的Service.client 这些，都是在调用这个方法之前打的，然后最后那个URL是执行完这个函数之后打的点\n我们可以看到在HTTP.UserAgent 和 Service 之间存在近3秒的等待，这么长的时间是在干什么呢？\n￼ @RequestMapping(value=\u0026quot;/process\u0026quot;, method = RequestMethod.POST) public void process(@RequestBody QueryRequest payload) throws Exception { System.out.println(payload); } 我们来看到代码，我们在定义这个这个函数的时候，指定了我们请求的Body是QueryRequest 类型，Tomcat提供了在提供了我们运行线程的同时，会给传给我们一个SocketWrapperBase socketWrapper 对象，我们可以读取这个获取到请求报文，当然你也可以选择不读取（无视客户端请求哈哈），Spring通过反射获取到这个方法的时候，会根据你的类型使用你想序列化的方式将它序列化成对象给你\n所以那么耗时将近3秒的时间都是Spring框架在读取Socket报文，我们通过Arthas可以监控到，耗时99%都在 javax.ws.rs.ext.ReaderInterceptorContext.proceed 上面\n这个是直接请求我们单机的ip地址才能监控到耗时这么长的请求，但是假如我们把请求打到网关上去，由网关路由到我们服务，惊奇的发现Cat上面没有长耗时的请求了\n接下来我们看看为什么接入网关之后读取就没有了\n四、网关的影响 # 其实网关和Tomcat一样，只不过它把处理请求下发到服务去了，所以就算客户端网速很卡，发的很慢，但是服务是无感知的，它是和网关交互，打个比方就是，网关就像一个超市，有很多收银台，就算有一个顾客走的很慢，堵住后面的顾客，但是只要它交给售货员，售货员打完了所有的单子，计算机一下子就给它算完了\n所以对服务来说，他从网关那边读取请求很快，他处理也很快，所以没有耗时\n五、接收的影响 # 接下来我们思考一下，假如客户端不仅发的慢，接收也很慢，在Cat上面会有什么影响吗？\n答案是完全没有影响，Tomcat通过org.apache.tomcat.util.net.SocketWrapperBase.write 来实现发送响应体，无论客户端网络多卡，只要服务端不卡，耗时都非常短\n其实这个和网关一样，对于服务端来说，只要它和外网的交换机不卡，不管客户端网速有多炸，它啪的一下就发完了，客户端读的慢就慢慢读，对于服务端来说，它已经发完了，这个TCP连接的控制权已经交给Tomcat的\n六、总结 # 假如一个服务在网关层后面，如果监控上他有卡顿，如果网关没问题的话，99%的可能就是你程序哪个地方有死锁或者耗时的操作，基本上排除了网络接收发送这些耗时的可能\n我们压测最终发现的也是因为日志压缩打印的锁，导致所有请求全部都卡顿了20S，而且一般都是在压测30分钟之后才会有一次，因为日志量达到1个G了\n","date":"2021-10-23","externalUrl":null,"permalink":"/posts/backend/framework/java/%E5%8E%8B%E6%B5%8B%E5%8D%A1%E9%A1%BF20%E7%A7%92%E5%BC%95%E5%8F%91%E7%9A%84%E6%80%9D%E8%80%83/","section":"博客","summary":"压测让人爆炸的事，就算把你写的代码都注释掉了这个问题依旧存在","title":"压测卡顿20秒引发的思考","type":"posts"},{"content":" 最近在做一个接口的压测，一开始以为自己优化后的代码应该没得问题，没想到中途遇到不少问题\n0x1 # 碰到第一个问题就是，压测前几分钟正常，后几分钟直线下降，且QPS出现诡异的下降，而且响应时间平均4s\n根据Cat的监控显示是数据库响应慢，但是DBA说没有慢SQL，这就很尴尬了\n怀疑是因为我们用的数据库连接池给定的活跃连接数太少了（20），我们使用是Tomcat默认开启200个线程，假如都在同时请求的话那么会有部分等待\n但是尝试加到200，依旧是这个诡异的情况，我们数据库后端使用MyCat分库分表，我这个时候猜想是不是因为MyCat原因\n我单元测试将数据库改成我本地的数据库测试，发现QPS很高，不会像压测结果那么诡异，我开始怀疑是不是MyCat网络有问题，进入MyCat宿主机想装一个监控\n但是发现宿主机磁盘满了，经过DBA确认是因为MyCat日志打满了导致的，清楚磁盘空间后这个情况就消失了\n0x2 QPS抖动 # 我们压测是连续压测15分钟，但是很神奇的是会存在偶发性情况，QPS一直很稳定，突然一抖动直接降为0，然后过了20秒又恢复了\n根据Cat监控和Skywalking监控发现，部分长请求会在请求Redis之前卡一下，但是Redis执行很慢，这个时候猜想是因为我们使用Jedis连接池，由于每个请求进来都会查询一下Redis这个导致都得向Jedis连接池申请连接（超时时间1s），但是由于我们Tomcat线程是200，我们给Jedis最大活跃连接数是100，导致有一半都在等，我们尝试把Jedis最大活跃连接数调到200之后这个情况就消失了，QPS一直很稳定\n这个猜想后面也是证明是错误的，在最终使用Arthas监控之后发现是日志打印死锁了\n具体的思考在这篇博客下面压测卡顿20秒引发的思考\n总结 # 这次压测让我知道，业务代码看起来很简单，但是当极限压力下，我们必须清楚各个组件在干什么，这样才能做到不仅仅只是一个CRUD boy\n","date":"2021-10-10","externalUrl":null,"permalink":"/posts/backend/framework/java/%E5%8E%8B%E6%B5%8B%E5%BF%83%E5%BE%97/","section":"博客","summary":"最近在做一个接口的压测，一开始以为自己优化后的代码应该没得问","title":"压测心得","type":"posts"},{"content":" 引用 # https://zhuanlan.zhihu.com/p/62322204 https://cloud.tencent.com/developer/article/1519813 ","date":"2021-09-19","externalUrl":null,"permalink":"/posts/backend/framework/java/linkedhashmap%E5%AE%9E%E7%8E%B0lru/","section":"博客","summary":"引用 # https://zhuanlan.zhihu.com/p/62322204 https://cloud.tencent.com/developer/article/1519813","title":"LinkedHashMap实现LRU","type":"posts"},{"content":"","date":"2021-09-15","externalUrl":null,"permalink":"/tags/mybatis/","section":"Tags","summary":"","title":"Mybatis","type":"tags"},{"content":" 这篇文章不是介绍mybatis二级缓存，而是基于我们目前业务一些痛点，思考如何使用mybatis二级Redis缓存，首先mybatis也提供了一个Redis缓存类，但是那个类并不能解决我们目前业务痛点\n一、业务痛点 # 首先我来介绍一下我们目前的两个业务痛点，第一个就是热key问题，由于我们一个服务全部使用Redis作为缓存，当业务量大起来之后，发现Redis内存占用量不大，但是程序卡顿丢包，一查发现原来是Redis带宽跑满\n这个解决方法有两种，一种是内存做一层缓存热key，第二种就是减少网络包传输大小，可以采取新的序列号算法（目前我们使用的是JSON）如google的 protocolBuffer，后面我们采用的是第一种解决方法\n我们第二个业务痛点就是缓存失效的峰刺，当大量key失效的时候，同一时间大量相同的请求打到数据库，虽然目前还没有压垮，但是等后面业务量大起来其实这个也是个瓶颈\n二、MyBatis缓存介绍 # 我们这里简单介绍一下Mybatis缓存，它分两张，一种是基于session的一级缓存，一种是基于mapper的namespace二级缓存（也就是全局缓存），一级缓存只能对同一个请求有用，所以其实对于我们目前业务来说用处不大\nmybatis二级缓存是个好东西，你可以理解为单个实例共享的cache，它是基于内存的，非常快，基本上没有延迟\nmybatis默认没有开始缓存，你可以简单的在mapper的xml文件里面加个\u0026lt;cache\u0026gt;\u0026lt;/cache\u0026gt;就开启了或者使用@CacheNamespace注解\n但是目前mybatis提供的全部是基于内存缓存，当数据量大的情况下，很容易内存溢出\n三、实现Redis二级缓存 # mybatis官方实现了一个基于redis二级缓存，这个缓存目前对于我们两个痛点来说，只能解决第二个，只需要在二级缓存中配置blocking=true我们就可以在更新缓存的时候锁住所有线程，避免大量请求数据库\n所以我基于官方实现做了下面修改:\n继承原始cache类，可以兼容使用LRU这些插件 使用普通redis值替换原来的哈希表，支持插入时候添加失效时间 当删除的时候，将数据放置到Redis 读取时候先读内存，在读Redis，假如Redis读到了，再更新到内存 通过上面4个修改，就很能实现第一个痛点，而且通过第四步，你可以解决掉Lru扫表问题 PS： 扫表，假如使用Lru你把所有Key都扫一遍，热key会被删除到Redis上去，假如你不更新到缓存，热key永远在redis上面，导致热key还是会对Redis造成压力\n四、总结 # 当然这种实现目前还有下面两个弊端，不过这两个弊端现在看也很难解决\n使用Java自带序列化对Redis消耗大（理论上使用google的 protocolBuffer是最优的）但是目前除非你能知道Redis键值，否则也是难，目前我测试在20个属性下，使用JSON要比原生序列号小，但是超过这个值其实两者开始拉近，在大量属性下原生序列号还是要比Json要省空间 在执行更新插入删除的时候，会把内存缓存清空，但是不会清空redis，这个会导致丢失部分实时性，但是目前我们使用场景可以在保证业务可靠性情况下，两三分钟的缓存时延 ","date":"2021-09-15","externalUrl":null,"permalink":"/posts/backend/framework/java/mybatis/mybatis%E4%BA%8C%E7%BA%A7%E7%BC%93%E5%AD%98/","section":"博客","summary":"这篇文章不是介绍mybatis二级缓存，而是基于我们目前业务","title":"mybatis二级Redis缓存","type":"posts"},{"content":"","date":"2021-06-20","externalUrl":null,"permalink":"/posts/backend/framework/java/stream/flatmap%E7%94%A8%E6%B3%95/","section":"博客","summary":"","title":"FlatMap用法","type":"posts"},{"content":"","date":"2021-06-20","externalUrl":null,"permalink":"/tags/stream/","section":"Tags","summary":"","title":"Stream","type":"tags"},{"content":" 之前看一些开源项目源码的时候，发现一个问题，假如你贪全，一口气把整个代码看完，由于现在程序架构 比较复杂，很多功能被分成很多个组件来完成，有的时候你会被程序跳来跳去给弄晕，假如你但看一个小功能，你又 不知道为啥要用这个\n所以这次准备尝试从问题入手，首先给自己提一些 问题，然后在从源代码中寻找答案， 在寻找答案的过程中会遇到更多问题，就这样打破砂锅问到底，最终没有问题了，这个时候你就差不多看懂了\n0x00 问题 # distinct 操作过程中是否会将新加入的元素和历史元素一一比较？ 为啥会有这个问题呢，因为在看源码\nReturns a stream consisting of the distinct elements (according to Object.equals(Object)) of this stream. For ordered streams, the selection of distinct elements is stable (for duplicated elements, the element appearing first in the encounter order is preserved.) For unordered streams, no stability guarantees are made. 这句话意思是会依靠｀Object.equals(Object)｀　来去重，我们知道｀distinct｀和｀filter｀都是中间操作 难道distinct会将每个元素和历史元素做一个Object.equals调用吗\n假如这样做的话，那么这个操作就是O（n^2)的时间复杂度了，显然不太靠谱，我们查看distinct源码发现\n最终distinct生成了一个StatefulOp ，而且这个类存在一个reduce函数，其中声明了一个\nTerminalOp\u0026lt;T, LinkedHashSet\u0026lt;T\u0026gt;\u0026gt; reduceOp = ReduceOps.\u0026lt;T, LinkedHashSet\u0026lt;T\u0026gt;\u0026gt;makeRef(LinkedHashSet::new, LinkedHashSet::add, LinkedHashSet::addAll); 我们可以看到这个变量是一个终止操作，其中使用LinkedHashSet来进行聚合，所以看到这我们就猜测 Stream没有这么傻，它声明了一个LinkedHashSet来存贮历史元素，这样只需要将加进来的元素进行 哈希计算，然后跟哈希碰撞的调用一下Object.equals(Object)函数就好了\n我们看到这篇博文的实验也证明我们的猜测了`\n接下来我们又有一个疑惑Stream内部是如何实现的呢？\n0x01 猜测 # 我们首先从最小的代码来看，我们首先来看一个无状态的stream函数\nStream.of(1L, 2L, 3L, 4L).forEach(System.out::println) 我们使用for循环用来实现也非常简单\nfor (long l : new long[]{1L, 2L, 3L, 4L}) { System.out.println(l); } 接下来我们来思考，如何实现一个有状态的Stream流\nStream.of(1L, 2L, 3L, 4L).reduce(0L, Long::sum); 我们如何用for循环来实现呢，很简单，定义一个变量\nlong begin = 0; for (long l : new long[]{1L, 2L, 3L, 4L}) { begin += l; } 我们能很容易写出一层for循环，但是Stream强大的地方在于，他可以穿插很多函数处理 比如：\nStream.of(1L, 2L, 3L, 4L, 4L, 5L, 5L).distinct().filter(x -\u0026gt; x \u0026gt; 2).reduce(0, Long::sum); 我们简单的穿插了distinct和filter操作，我们接下来尝试使用for循环来实现上面的Stream\n首先我们知道distinct 需要一个Set来过滤已经存在的，其中reduce需要一个初始量，那就好做了\nlong start = 0L; for (long l : new long[]{1L, 2L, 3L, 4L}) { if(!set.contains(l)) { set.add(l); if(l \u0026gt; 2) { start += l; } } } 00x02 源码探究 # 接下来我们看看Stream内部如何实现这个for循环的，我们可以看到，其实.distinct().filter(x -\u0026gt; x \u0026gt; 2).reduce(0, Long::sum)对于每一层我们都需要能 创建一个Sink，对于这个for循环来说，都是把每个数据，我们把数据从一个sink到其他的sink\n所有的Sink都实现了Consumer 接口，其中最重要的接口就是\nvoid accept(T t); 这个消费接口，我们可以理解“吃”数据，它会把我们传给它的值都“消化”掉\n当我们在创建.distinct().filter(...)...这些stream的时候，我们做了什么呢， 我们每进行一次中间操作，我们都新建了一个流，其中我们通过upstream 这个变量指向 之前的流\n当我们碰到终止操作比如reduce的时候，我们会进行一个回溯，把所有upstream都进行回溯，反过来把一个sink组装起来（每个sink指向它的上游）\n// java.util.stream.AbstractPipeline.class final \u0026lt;P_IN\u0026gt; Sink\u0026lt;P_IN\u0026gt; wrapSink(Sink\u0026lt;E_OUT\u0026gt; sink) { Objects.requireNonNull(sink); // 回溯之前的stream流，创建sink，并让当前的sink指向上流 for ( @SuppressWarnings(\u0026quot;rawtypes\u0026quot;) AbstractPipeline p=AbstractPipeline.this; p.depth \u0026gt; 0; p=p.previousStage) { sink = p.opWrapSink(p.previousStage.combinedFlags, sink); } return (Sink\u0026lt;P_IN\u0026gt;) sink; } 最终我们得到了最上流的sink，以上面为例就是distinct那个sink\n接下来我们执行for循环，其中最重要的就是\n// java.util.stream.AbstractPipeline.copyInto 函数 // 执行for循环 其中 传入的sink就是 我们上面得到的像葫芦串一样的sink spliterator.forEachRemaining(wrappedSink) 我们只需要给wrappedSink传入for循环的值就好了，由于每个sink都有其上游的引用，比如说distinct的sink， 他会判断是否已经存贮在sink中，如果没有就往上游传，由于上游也是个sink，所以最终如果不传了或者到最上游了就继续下一for循环的值\n0x03 # 总结，我们这边非常浅显的把源代码给介绍了一下，其实要想吃透最好使用debug功能，一行一行代码进行debug，这样就能印象更深刻\n##　引用\n对Java8中distinct()的思考 ","date":"2021-05-23","externalUrl":null,"permalink":"/posts/backend/framework/java/stream/%E4%BB%8E%E9%97%AE%E9%A2%98%E5%87%BA%E5%8F%91%E7%9C%8B%E6%BA%90%E7%A0%81/","section":"博客","summary":"之前看一些开源项目源码的时候，发现一个问题，假如你贪全，一口","title":"Stream源码（2）：从问题出发看源码","type":"posts"},{"content":" 本篇博客是在看代码的时候看到使用Java8使用Stream去重的妙用，从而对Java如何使用Stream实现几行代码 完成一个可支持并行化的流式计算程序\n引言 # 什么是Stream\n简单来说Stream就是Java8引入的一种流式API，让只需要定义一些处理函数就能优雅的对集合的一些操作\n举个栗子，之前我们想遍历打印所有的参数我们得写下面代码\nfor (int i : new int[]{1, 2, 3, 4}) { System.out.println(i); } 假如使用Java8，我们只需要一行代码\nArrays.stream(new int[]{1, 2, 3, 4}).forEach(System.out::println); 我们只需要定义一个函数，其他的Stream帮我们解决\nStream模型 # 简单来说Stream把操作分为两张，一种是中间操作，一种是终结操作\n中间操作你可以把他看做对数据源的一种处理，而终结操作是定义产生新数据过程，简单来说，假如我们有一堆苹果 需要处理，我们使用一个流水线来对经过的苹果进行各种处理（中间操作），丢掉烂掉的（filter），削掉皮（map）等等， 最后的我们需要对苹果进行装箱（终结操作）\n分析功能 # 我们接下来看看我们需要实现的功能，就是去重，我们知道Stream其实提供了一个distinct方法中间操作来帮我们实现去重功能， 对于基本类型比如String，Integer，Long这些我们能很容易进行去重，但是对于复杂类，我们得 重写hashCode和equal方法来支持去重操作了，对于集合内的每个数据都会用equal来进行去重\n接下来我们尝试直接使用终结操作collect来解决掉 怎么做呢，collect其实也很简单就是创建一个容器，把数据装进去，我们就只需要使用一个可以支持去重操作的 容器来做就好了\n支持去重的有Map和Set，所以我们想到的第一个方法就是将流变成一个Set，这个也是网上很多人提供的一个版本\ndishes.stream().distinct().collect(Collectors.toCollection( () -\u0026gt; new TreeSet\u0026lt;\u0026gt;(Comparator.comparing(Dish::getType))) 我们把这行代码给分解成两行\nTreeSet\u0026lt;Dish\u0026gt; container = new TreeSet\u0026lt;\u0026gt;(Comparator.comparing(Dish::getType)); dishes.stream().collect(Collectors.toCollection(() -\u0026gt; container)); 第一行其实就是声明了一个容器，核心代码就是 () -\u0026gt; container 我们传入一个容器，让他来存贮，我们使用 TreeSet的一个构造器，它只需要传入一个函数，他会比较传进来的每个值，假如发现已经存在了它就不会插入了\n但是这个有个问题就是，他只能选择第一个出现的\n接下来我们试试用Map，\ndishes.stream().collect(toMap(Dish::getType, d -\u0026gt; d); 我们写下下面的代码，但是这个代码有点问题就是假如出现重复的，他会抛异常，因为程序默认不知道你想要哪个数据\ndishes.stream().collect(toMap(Dish::getType, d -\u0026gt; d, (oldData, newData) -\u0026gt; oldData)); 我们可以传入一个选择函数，这样当发现重复的时候就可以选择要哪个插入到map中去，这个也解决了上面使用Set来存贮的时候没法选择到底是新来的还是后来的\n总结 # 这篇博客简单的介绍了实现去重功能的三种实现方法，接下来这个系列的博客会深入到源码来探究Java8如何实现 Stream这么多功能的\n引用 # Java Stream 源码分析 JAVA8优雅的去重 JAVA8Stream 源码分析2 ","date":"2021-05-15","externalUrl":null,"permalink":"/posts/backend/framework/java/stream/%E4%BB%8E%E5%8E%BB%E9%87%8D%E5%88%B0stream%E6%BA%90%E7%A0%81/","section":"博客","summary":"本篇博客是在看代码的时候看到使用Java8使用Stream去","title":"Stream源码（1）：如何实现去重","type":"posts"},{"content":"","date":"2021-04-05","externalUrl":null,"permalink":"/tags/mysql/","section":"Tags","summary":"","title":"MySQL","type":"tags"},{"content":" 最近在接手老项目的时候，看到一个SQL： select * from xx group by id， 当时一看到这句就感觉，这个group by是不是多余的，既然select 全部了，那去掉其实也无所谓， 然后询问上一个接手的同事才知道这个是用来去重了，好家伙，以前一直用来分组统计的语句竟然可以用来 去重，涨知识了\nGroup By 到底可不可以去重呢 # 首先我们新建一个测试表，并插入一些测试数据\nCREATE TABLE test.student ( `sno` varchar(20) NOT NULL, `sname` varchar(20) DEFAULT NULL, `ssex` varchar(20) DEFAULT NULL, `sage` int(11) DEFAULT NULL, `sdept` varchar(20) DEFAULT NULL, PRIMARY KEY (`sno`) ); INSERT INTO test.student (sno, sname, ssex, sage, sdept) VALUES ('201215121', '李勇', '男', 20, 'CS'); INSERT INTO test.student (sno, sname, ssex, sage, sdept) VALUES ('201215122', '刘晨', '女', 19, 'CS'); INSERT INTO test.student (sno, sname, ssex, sage, sdept) VALUES ('201215123', '王敏', '女', 18, 'MA'); INSERT INTO test.student (sno, sname, ssex, sage, sdept) VALUES ('201215124', '张立', '男', 19, 'IS'); INSERT INTO test.student (sno, sname, ssex, sage, sdept) VALUES ('201215125', '张立', '男', 19, 'IS'); 然后我们测试一些是否可以去重\nmysql\u0026gt; SELECT * from test.student GROUP BY ssex ; ERROR 1055 (42000): Expression #1 of SELECT list is not in GROUP BY clause and contains nonaggregated column 'test.student.sno' which is not functionally dependent on columns in GROUP BY clause; this is incompatible with sql_mode=only_full_group_by 哎呀出错了，意思是不支持，因为我们sql_mode 设置了only_full_group_by这个，我们来看看我们sql_mode里面有啥\nmysql\u0026gt; SELECT @@SESSION.sql_mode \\G; *************************** 1. row *************************** @@SESSION.sql_mode: ONLY_FULL_GROUP_BY,STRICT_TRANS_TABLES,NO_ZERO_IN_DATE,NO_ZERO_DATE,ERROR_FOR_DIVISION_BY_ZERO,NO_AUTO_CREATE_USER,NO_ENGINE_SUBSTITUTION 1 row in set (0.00 sec) 的确我们sql_mode是设置了ONLY_FULL_GROUP_BY这个值，这个sql_mode是啥呢，首先我们知道 MySQL只是SQL一种实现软件，他为了和其他SQL兼容，就设置了这个变量，你可以通过 修改这个值来修改他的语法支持\n其实group by本来的意图就是一个聚合函数，正常来说，我们一般是使用下面的语法\nselect a, count(*) from xxx group by a 但是MySQL非常开放，它允许你使用非group by 字段，在5.7之前都是默认允许的，但是5.7之后，它默认关闭了 但是你可以修改关闭\n接下来我们临时关闭掉这个，怎么关闭呢，非常简单，把ONLY_FULL_GROUP_BY在sql_mode 删除就好了 接下面执行下面语句\nset sql_mode =(SELECT replace(@@SESSION.sql_mode, 'ONLY_FULL_GROUP_BY,', '')); 然后我们再执行上面sql\nmysql\u0026gt; SELECT * from test.student GROUP BY ssex ; +-----------+--------+------+------+-------+ | sno | sname | ssex | sage | sdept | +-----------+--------+------+------+-------+ | 201215122 | 刘晨 | 女 | 19 | CS | | 201215121 | 李勇 | 男 | 20 | CS | +-----------+--------+------+------+-------+ 2 rows in set (0.00 sec) o了，我们再看看我们目前的sql_mode\nmysql\u0026gt; SELECT @@SESSION.sql_mode; +------------------------------------------------------------------------------------------------------------------------+ | @@SESSION.sql_mode | +------------------------------------------------------------------------------------------------------------------------+ | STRICT_TRANS_TABLES,NO_ZERO_IN_DATE,NO_ZERO_DATE,ERROR_FOR_DIVISION_BY_ZERO,NO_AUTO_CREATE_USER,NO_ENGINE_SUBSTITUTION | +------------------------------------------------------------------------------------------------------------------------+ 的确目前看到ONLY_FULL_GROUP_BY这个了，而且我们发现的确去重了\nDistinct和Group By # 通过上面的实验我们知道了，Group By的确可以去重，但是我们去重一般使用Distinct来， Distinct原理很简单\n我们执行下面语句\nmysql\u0026gt; explain SELECT distinct ssex from test.student ; +----+-------------+---------+------------+------+---------------+------+---------+------+------+----------+-----------------+ | id | select_type | table | partitions | type | possible_keys | key | key_len | ref | rows | filtered | Extra | +----+-------------+---------+------------+------+---------------+------+---------+------+------+----------+-----------------+ | 1 | SIMPLE | student | NULL | ALL | NULL | NULL | NULL | NULL | 5 | 100.00 | Using temporary | +----+-------------+---------+------------+------+---------------+------+---------+------+------+----------+-----------------+ 我们可以看到Using temporary告诉我们，就是创建一个临时表，然后把 Distinct的字段从表里取出来，然后返回给客户端就好了\n我们看看Group By的原理\nmysql\u0026gt; explain SELECT * from test.student GROUP BY ssex ; +----+-------------+---------+------------+------+---------------+------+---------+------+------+----------+---------------------------------+ | id | select_type | table | partitions | type | possible_keys | key | key_len | ref | rows | filtered | Extra | +----+-------------+---------+------------+------+---------------+------+---------+------+------+----------+---------------------------------+ | 1 | SIMPLE | student | NULL | ALL | NULL | NULL | NULL | NULL | 5 | 100.00 | Using temporary; Using filesort | +----+-------------+---------+------------+------+---------------+------+---------+------+------+----------+---------------------------------+ 我们也看到了Using temporary;，其实Group By底层也是用到临时表，把你Group By的值当 做主键，而且我们看到默认它会取第一个出现的值作为最终的值 这个是为什么呢\nGroup By 原理解析 # 我们前面知道，Group By 一开始只是用来做聚合函数的，所以底层上最开始设计的时候就是\n首先建一张临时表，把Group By字段作为唯一主键（下面称作key） 然后把原始表中所有数据按照key进行分组 对分组内数据一个一个进行聚合，比如执行count, avg 等等函数，最后每组跟随主键key产生最终的一列值 最后对临时表进行排序返回 所以其实当时为了支持，不在主键内的值，MySQL只是简单的把组内第一个出现值赋值进去\n那为什么不把最后组内最后一个值赋值进去呢，首先是如果实现这个功能，想当于组内数据要全部执行一遍， 效率太低了，那有没有办法能实现去重的时候取组内最后一个呢\n首先说目前是不支持的，不过能通过下面两种方式来实现\n方法一：\n首先把你想要的数据进行排序\ncreate temporary table test.tt as SELECT * from test.student order by sno desc; 然后我们看正常来按照年龄排序\nmysql\u0026gt; SELECT * from test.student GROUP BY sage; +-----------+--------+------+------+-------+ | sno | sname | ssex | sage | sdept | +-----------+--------+------+------+-------+ | 201215123 | 王敏 | 女 | 18 | MA | | 201215122 | 刘晨 | 女 | 19 | CS | | 201215121 | 李勇 | 男 | 20 | CS | +-----------+--------+------+------+-------+ 3 rows in set (0.00 sec) 接下来我们再看看我们去重取最后一个出现的，我们看到19的张立取代了第一个出现的刘晨\nmysql\u0026gt; SELECT * from test.tt GROUP BY sage; +-----------+--------+------+------+-------+ | sno | sname | ssex | sage | sdept | +-----------+--------+------+------+-------+ | 201215123 | 王敏 | 女 | 18 | MA | | 201215125 | 张立 | 男 | 19 | IS | | 201215121 | 李勇 | 男 | 20 | CS | +-----------+--------+------+------+-------+ 3 rows in set (0.00 sec) 方法二：\n第二种方法其实把上面两个步骤合成一个，首先我们把原始数据进行排序，然后取出组内第一个\nselect sno, sname, sage, sdept from (SELECT @i := if(@sage \u0026lt;\u0026gt; sage, 0, @i + 1) as i, @sage := sage, a.* from (select * from test.student order by sage, sno desc) a) a where i = 0 order by sage; 我们用上面这行函数模仿了，Group By去重，我们可以看到我们在末尾加了一个order by sage 语句，这是因为默认Group By会按照Group By的键值给我们排序， 所以如果你想优化Group By性能，当你对组排序顺序无所谓的时候，你可以在 Group By 末尾加上order by null关闭排序功能\n总结 # Group By去重功能在5.7之后就默认关闭了，所以其实大家用来做去重还是慎用，不过通过这个功能去探索 底层原理对你数据库性能优化还是非常有帮助的\n资料 # MySQL中的Group By是否允许SELECT非聚合列的总结 mysql5.7官方文档 SQL运行内幕：从执行原理看调优的本质 ","date":"2021-04-05","externalUrl":null,"permalink":"/posts/backend/framework/java/mysql/groupbyassort/","section":"博客","summary":"最近在接手老项目的时候，看到一个SQL： select * from xx group by id， 当","title":"MySQL Group By 还可以用来去重","type":"posts"},{"content":" 一开始本来想写一篇反思自己最近文章比较浅显的博客，但是想着想着突然发现其实这样的 反思我已经写过很多篇了，但是效果不好，这篇博客从另外一种角度来思考如何让自己 写出好博客\n什么是因材施教 # 目前我们的教育算不上是因材施教，每个人上的同样的课本，同一老师传道受业\n我上学的那会经常上课开小差，倒不是因为听不懂老师的内容，而是因为老师讲的我都会了，我的思维要比 一般的人来的要活跃一点\n但是那个时候老师只是告诉我们，上课开小差是不对的\n假如我回到过去，给自己上课，我会引导自己思考更多，这道题为什么这样做，还能怎么做，而不是单纯的给 我一个答案，说这道题就是这么做\n其实因材施教就是顺势而为，思维活跃的就是给他们更大的挑战，让他们自己去钻研原理\n反思 # 其实回到现在，我们仍然还是处于学生状态，可能因为我们受了很多年的素质教育，所以 当我们自己开始当自己的老师的时候，去自学，还是延续当初自己的学习状态\n那怎么才能做到因材施教呢，其实很简单，找到一种自己最喜欢的学习方式，而不是强迫自己 去学习，去听课，去学新的东西\n对于我来说，最好的学习方式是激起我的好奇心，学新东西的时候，有些部分可能之前学过，你可以 不断的给自己“抬杠”，激发自己的好奇心，去探究，但是还有一个非常重要的一点就是 得及时把自己拉回来，避免一条路走到黑，因为有的时候你会发现随着你”钻“牛角尖越来越深， 你会精疲力竭，你得稍作休息，这个时候做个总结，总结自己所得，以及评估继续钻下去是否 值得\n总结 # 这段时间其实自己一直处于一种很焦虑的状态，很想一口气吃成大胖子，因为自己目前其实算是转行吧，之前一直在做 数据分析、大数据，目前转成后端，对于我来说我想在这个行业闯出一番事业来\n之前一直以被动的学习姿态去重新学习后端，搞的自己非常压抑，这段时间休息好好思考了一下，有的时候我就像对自己 实施了”棍棒教育“，强迫自己努力学习，但是大可不必，自己本来是喜欢这个行业的，自己这种强势的思想反而会 让我产生不想去学的态度，其实把学习当做一个有挑战性的事，快乐的学习比啥都重要\n反思 # 其实我自己反思，自己最大的一个问题就是犯了一个年轻人经常犯“好高骛远”\n回看我自己从大学到现在，自己一直在拼命的学，自己一直在给自己定目标，碰到新东西一定要把它 快速掌握\n甚至到现在，我还是这样，自己想成为后端专家，所以自己一直在“催促”自己抓紧把知识点掌握， 在知乎上看了很多后端专家需要掌握的知识点，自己为了快速掌握，催促自己赶快把这篇文章给 看懂\n其实最大的一个问题就是我太急了，无论学什么东西都是囫囵吞枣\n","date":"2021-04-04","externalUrl":null,"permalink":"/posts/essays/life/stage/how2writewell/","section":"博客","summary":"一开始本来想写一篇反思自己最近文章比较浅显的博客，但是想着想","title":"从因材施教的角度来看写博客","type":"posts"},{"content":" 一眨眼2周就过去了，在新的公司感触颇多\n改变 # 工作流程 新的公司比原来要大的多，业务也比之前要多的多，业务多之前在小公司都是一把梭，什么测试开发运维都是自己上， 但是这边没法全干，每个人都有自己的工作\n这个带来一个好处就是你可以专心自己的业务，但是也带来一个坏处，就是想做一件事得协调很多个部门\n而且当你想做一个大功能的时候就得开会了，这个也是我的一个很大的感受了吧，会议越来越多，之前半年才开一次会，现在一天就开两次\n部署流程 之前由于一般都是个人负责一个大的项目，一般都是自己选择最方便的发布方式来部署应用，但是随着公司越来越大，你得考虑到 各种情况，所以这边采用了K8S + Dubbo 这种来部署分布式应用\n底层还是普通的运行方式，但是采用了云原生镜像这种来隔离应用，以及对应用动态扩容，当然我们我们采用的是 云服务，基本上所有的组件都是云组件，理论上，我们是一个完全在云上的公司\n不足 # 目前公司是完全2C的公司，而且目前我负责的部分是主要是公共服务，所以其实整体业务并没有太多复杂性的东西， 但是2C最重要的一个问题就是，很在乎用户体验，应用的稳定性非常重要\n经过两周的磨合，我对公司的架构有个大致的了解，但是涉及到的一些组件，大部分只是会使用，目前自己需要加强对组件 的了解，尽快上手业务，目前公司的自动化测试也存在不足（或者是没有），自己接下来的打算就是 尽快扩展自己的知识树，把用到微服务组件给钻透，而不是仅仅停留在会使用的阶段\n这样第一个不会因为自己一些知识的欠缺而引起系统事故（听说P0事故就得背书包走人了哈哈），第二个也能让自己尽快 提升到下一个级别\n总结 # 在这两周自己在leader的建议下也尝试在使用UML来归纳总结自己的知识，目前只能说真香，但是的确得完全掌握透才能 完成整体架构的理解\n目前自己新概念也已经背到22篇了，每天背完并默写还是有一定成就感的，其实刚入职这段时间自己一直有点焦急，首先 是因为，刚入职，害怕自己写着写着就背书包走人了，但是还好其实leader和同事都挺好的，\n但是自己发现自己有的时候的确急了一点，总想把事情给赶完，太过急躁其实也不好，太过松散也不好，尽力而为， 才是最好\n","date":"2021-04-02","externalUrl":null,"permalink":"/posts/essays/life/stage/2weeksummay/","section":"博客","summary":"一眨眼2周就过去了，在新的公司感触颇多 改变 # 工作流程 新的公司","title":"新工作两周总结","type":"posts"},{"content":"","date":"2021-03-27","externalUrl":null,"permalink":"/tags/dubbo/","section":"Tags","summary":"","title":"Dubbo","type":"tags"},{"content":" 繁忙的一周终于过去了，加入小影第一周主要是熟悉后端架构，同事们都挺好，自己的基础还是有点弱，前段时间简单的把Spring Cloud 和 Dubbo学习了一下，但是其实对于工作来说，之前学的都是最新的版本，但是其实公司用的版本很老了，所以需要时间去学习老版本\n历史 # 首先我们来看看Dubbo的历史，看Dubbo的历史的时候我们得先看看Spring的历史,2004年就出来了Spring1.0，随着 一些网站越做越大，例如淘宝，随着用户量越来越大，单一Spring无法支持他们的体量， 所以淘宝基于Spring开发了Dubbo这个架构\nDubbo和Spring有什么不同，Dubbo其实是基于Spring开发的，只不过他和普通的Spring程序不一样，它提供的是 分布式服务框架\n原来的Spring程序放进Tomcat里面就可以跑了，但是Dubbo是将一堆Spring程序组合起来做一个分布式系统， 他提供了什么呢，总结起来就是Spring程序的互识和通信\nDubbo使用起来非常简单，复杂的注册和监控已经帮你写了，你只要启动提供者（Provider）和消费者（Consumer）实例就好了\n架构 # 其实仔细看看整个架构，什么是提供者和消费者，之前我们写web，很简单一个url对应一个接口\n但是想想随着公司越来越大，小公司用户量不大直接发布新代码上线就好了，大公司需要24小时提供服务，所以Dubbo首先 做的就是将后端分层，逻辑需要经常变动的给独立开来，我们把原来的接口分成两个一个是提供者和一个是消费者\n提供者负责承担真正的代码逻辑，消费者只是提供调用的接口，这样每次代码上线的时候，只需要把新提供者注册进注册中心， 然后把老提供者下线，这样就无感知的将新版本替换成老版本了\n微服务区别 # 随着Spring Cloud从2016年的兴起，微服务的风潮开始兴起，那么Dubbo和Spring Cloud有什么不同呢，首先 Dubbo和Spring Cloud的设计理念是不一样的\n首先Dubbo它想做的是一个Rpc框架，它提供Spring 消费者和提供者之间的通信，借此打造分布式集群，而 Spring Cloud做的是一种切分，它将系统切分成每个小的服务，每个服务负责的部分都不一样\n简单来说就是，Spring Cloud提供的积木，你可以自己组装一个系统，它也不关心你用不用什么组件，单独来看， 每个Spring Cloud服务都是一个网站，他们之间用HTTP进行通信\nDubbo呢，考虑的更多是各个服务之间高效的通信，你可以理解为Dubbo是做一层网络中间件，这给他自己带来了一定的限制 而Spring Cloud本质上还是一个个网站，所以你可以很容易在里面搭建配置中心，网关，服务跟踪等等\n总结 # Dubbo出生在2011年，那个时候云服务还没有兴起，网络流量还很贵，Spring Cloud出生在2016年，正是云、Docker、K8S这些组件的大热的时候， 大家发现在一个系统中流量是最不值钱的，反而一个系统更加灵活更加好调试才更符合\n但是很多老系统都在使用Dubbo作为系统架构， 所以阿里推出了Spring Cloud Alibaba，即可以兼容原来的Dubbo系统，也可以很轻松将原来的Dubbo架构 迁移到Spring Cloud的架构\n##　资料\nSpring的历史\n","date":"2021-03-27","externalUrl":null,"permalink":"/posts/backend/framework/java/dubbo/dubbo%E6%B5%85%E6%8E%A2/","section":"博客","summary":"繁忙的一周终于过去了，加入小影第一周主要是熟悉后端架构，同事","title":"Dubbo浅探","type":"posts"},{"content":" 花了半天时间把Spring Cloud Alibaba 的Nacos 、 Sentinel 和 Seata简单的使用了一下，下面是我的一些看法\nNacos # Nacos 干了三件事，第一个就是把服务注册的活给揽过来了，第二个就是把配置中心的活给揽过来了，而且由于他是要监控各个服务的心跳的， 所以他顺便把配置同步这个活也给抢过来了\nNacos做了很多事，美中不足的地方是，Nacos更希望你在网页上进行配置（当然可以通过url），比如说Spring Cloud Config 是希望你更新源代码然后再来将配置给更新 掉\nSentinel # Sentinel是一个流量控制的中间件，作用和Hystrix差不多，不过，相比于Hystrix，Sentinel提供了一个web界面来 帮助你定制限流规则，主要是保护后端\n这些规则其实也很简单，多尝试一下就好了，我其实很好奇，Sentinel在其中做的工作是什么，他是如何保证异常流量不 把自己给打爆的，当然负载均衡可以一定量的缓解这些\nSeata # 这个号称实现了分布式事务的软件，我没搭起来，尝试几个小时无果后，而且网上的资料也异常的少，甚至他们官方写的demo我也跑不起来 我看他们的start虽然多，但是issue也多，每个版本都有坑\n我也尝试去搜索有没有大公司尝试使用这个分布式事务，很遗憾没找到，首先限死了用MySQL，而且非常恶心的事每个数据库上面都得创建一个undo表\n我可以理解你在一个数据库上记录这些东西，但是我没法理解你要所有的数据库都建这么个表\n而且非常坑的事，官网SSL证书当天正好到期，在我看了引用了这么个分布式事务，第一肯定涉及到了很多锁，我看原理介绍上 也说了开始事务前会上锁，对于现在通用解决方案都是消息队列来处理这个东西的,引入分布式锁并发不高，而且万一锁表凉凉了\n所以我就去找有没有什么大公司实践过了，可是很遗憾好像没有，也没没看到有什么性能测试，而且部署半天报各种问题，每个版本都不一样，怕是个PPT项目 还是改天有空的时候在研究研究\n总结 # 其实Spring Cloud Alibaba比较重要的一个部分就是将Dubbo集合到了Spring Cloud，所以我接下来的时间得好好看看Dubbo了，目前Spring Cloud这些组件只能说都上手摸了一会 要想真正的成长还得去实际项目考验\n","date":"2021-03-16","externalUrl":null,"permalink":"/posts/backend/framework/java/springboot/springcloudalibaba%E6%B5%85%E6%8E%A2/","section":"博客","summary":"花了半天时间把Spring Cloud Alibaba 的Nacos 、 Sentinel 和 Seata简","title":"Spring Cloud Alibaba浅探","type":"posts"},{"content":"","date":"2021-03-16","externalUrl":null,"permalink":"/tags/springboot/","section":"Tags","summary":"","title":"SpringBoot","type":"tags"},{"content":" 最近面试的时候一个面试官指出了我对系统架构没有固定的认知，推荐我去好好学习一下UML这个语言，我花了几个小时在B站上看了UML这门课，下面是我对UML最佳实践的一点心得\nUML是什么 # 首先它是一门语言，全名叫 Unified Modeling Language，翻译过来就是统一建模语言，首先为什么我们思考一下为什么我们需要这么一门语言。\n最开始的C语言是面对过程的，我们想做一个什么东西，直接去写就好了，等我们到了比如Java这些面对对象的语言的时候，我们就要思考怎么去建模，所以UML就是在这种情况被创建出来的\n怎么用 # 首先UML主要帮助我们设计一个系统，那一般的流程是什么呢\n确定用例 -\u0026gt; 用例图 确定类模型 -\u0026gt; 类图 确定流程 -\u0026gt; 时序图 确定细节 -\u0026gt; 状态图 软件选择 # 其实UML很核心的一个地方就是用可视化的图表展示出你的系统架构，视频里面使用的IBM Rational Rose这个软件，的确拖拽非常方便，但是有个问题就是只支持window，目前我主要操作系统是Ubuntu和Mac，所以我需要一个跨平台的软件\n我尝试了跨平台的dia，或者目前一些主流的在网页上拖拽的软件来完成UML图，但是这些都有一个痛点，不方便管理，而且修改起来也比较麻烦，然后有人推荐使用plantuml，不但跨平台，而且UML图是根据代码自动生成\n也就是说你不需要拖拽，你可以像写代码一样画UML图，更主要的是这个IDEA上面都有插件集成，你只要安装好插件就可以在IDE里面写UML代码生成你想要的图了\n安装流程 # 首先在插件库查找plantuml intergration，然后点击安装，接下面安装绘图课\nMac\nbrew install graphviz\nUbuntu\nsudo apt-get install -y graphviz\n接下来你重启IDEA然后随便新建一个文件，输入下面代码\n@startuml Alice -\u0026gt; Bob: test @enduml\n点开侧边栏PlantUML就能看到生成的UML图了\n总结 # 以前我只知道IDEA能写代码，后面我知道IDEA能执行SQL，我就把他当成SQL管理工具，后面我发现他也能写Markdown，我就放弃的专业markdown软件，如今我发现他也能写UML，IDEA真的太全能了\nPS：这篇博客简单的介绍了我对UML最佳实践的配合，没有涉及到PlantUML的具体使用方法，可以看我的使用UML实现\nUML实现Github链接\n","date":"2021-03-15","externalUrl":null,"permalink":"/posts/backend/software/uml%E6%9C%80%E4%BD%B3%E5%AE%9E%E8%B7%B5/","section":"博客","summary":"最近面试的时候一个面试官指出了我对系统架构没有固定的认知，推","title":"UML最佳实践","type":"posts"},{"content":" 最近在学SpringCloud，之前一直对用视频学嗤之以鼻，觉得只有弱者才会这样，但是其实对于一些已经非常常见的技术 比如SpringCloud这种，已经出来很长一段时间了，而且其实非常杂，用视频学起来其实非常快，当然前提是你要三倍速播放，而且 你得把视频配套的代码找到，这样你就能很快的掌握这个。\n我把SpringCloud视频过了一遍，代码也运行了一遍花的时间可能不超过 5个小时吧，假如自己按照原来的做法去看别人博客然后自己慢慢搭建可能入门花的时间远远不止\n下面是我学习SpringCloud用到的视频已经代码，推荐你视频三倍速播放，把视频看懂之后去实际运行一下相应代码，这样你能很轻松知道这个东西是什么\n代码 B站视频 目前我就总结一下我入门Spring Cloud的一些心得\n什么是微服务 # 其实微服务这个东西是模块化的一种体现，在企业开发中你会发现，好多时候我们要做一个网站如果要暴露在外网地址， 其实最基础就得要有一个登录，每个网站写一个登录其实非常没有必要\n第一个是安全，第二个是耦合，假如你想完全独立于其他系统，你必须重新使用数据库存储，这样用户在同一家公司可能得注册两个不同的账号\n所以微服务的理念就是模块化，把登录和其他业务给组装成不同模块，这样你就专注于你自己的业务而不用去担心其他的问题了\n怎么做 # 微服务一开始也很简单，就是单独起不同的网站，比如我们把登录单独部署在一个网站上面去， 登录完了就给一个token， 然后其他系统使用token来验证用户权限\n刚开始这样很简单，但是对于系统来说，出现了一个问题，首先是用户登录次数很少（如果他不退出登录），然而其他业务 使用的很多，这时候我们就想多起个实例来跑其他微服务，但是我们之前写死了，所有的业务调用都只是跳转到一个 网址上去了\n服务注册 # 所以我们就想到做一个调度中心来把所有的服务都注册在这里，这样你想要的时候过来查就好了，这个服务中心就是我们所说的 服务注册，就是他来监控所有服务实例的，每个实例都得定时发送心跳，如果一段时间没有发送就会默认认为服务下线了\n对于服务注册中心来说，其实就是一个登记的作用，就是记录一个服务名，以及他们的实际IP地址\n服务调用 # 服务注册核心在于注册，对于真正的调用方来说，他只能从注册中心拿到最新存活的实例的IP，实际上他还得去通过这个IP来远程调用实际服务\n所以Spring Cloud就创建了两个调用框架，第一个用来选择选择哪个实例，比如随机选择一个实例，还是说按照实例响应速度等等来选择\n这个其实是一个拦截器，他只是帮你在远程调用之前选择哪个服务\n第二个就是OpenFeign，其实这个相当于把远程调用伪装成为一个Service，你只要把这个Service名字以及相关配置给配置好就可以像普通 Service一样调用了\n服务调用，这个本质上是给调用方写的HTTP请求库，对于被调用者来说其实什么没有影响\n服务降级 # 其实服务降级，我认为说成异常处理更好，对于微服务来说，最大的区别就是原来一个网站，现在好多网站，我以前实现一个功能\n一个接口就好了，目前可能需调用很多很多接口，这个调用一多就会出问题，第一个时间变长了，HTTP请求其实很有冷启动的缺点的，随着你的调用越来越多 其实花在建立链接的时间也越来越多了，而且HTTP其实非常啰嗦，有很多不需要的头部信息，会重复的发来发去\n回到刚才的问题，就是我们调用方最终要实现一个功能的时候，可能得需要很多步骤，假如正常来写，我们得写很多try catch 我们得考虑这个接口调用失败怎么办，我们得返回什么东西\nHystrix就是一套异常处理框架，他能帮你把接口给管理起来，在调用前他会检查这个接口是否已经熔断，在调用的时候他会检查是否 会抛异常，如果抛异常就调用其他兜底的方法\n辅助 # 网关\nSpring Cloud还给了我们一些其他可以选择使用的辅助功能，比如假如你想自己做一层网关，那可以使用gateway做一个网关， 网关能帮我们做一下杂碎但是非常重要的，比如说权限认证，异常流量过滤啊，日志记录啊\n你可以用Nginx来做这些，但是gateway这些网关是用Java写的，你很容易就可以进行扩展实现你自己想要的功能\n配置中心\n对于一个大的公司来说，代码运行的环境非常多，所以Spring Cloud提出配置中心的概念，你可以让所有的应用接入配置中心 这样当你想临时关闭某些东西的时候，只要修改配置中心，所有的实例都能立刻得到响应，而不需要重启\n调用链监控\n这个有点调试工具了，就是在调用方和被调用方都安上这个就能很轻松的把整个调用的线给记录下来，第一个方便你监控异常， 第二就是运维也能根据这个及时添加更多实例来面对突发流量\nSpring Cloud Stream\n其实我感觉这个东西设计的挺失败的，本来想法是把所有的消息队列框架都能用一个框架去调用，但是目前其实只支持两个，那其实我用原生的也没有什么太大的影响，原生的Spring Boot就支持RabbitMQ等这些所有的消息队列分发和消费\n总结 # 微服务架构其实就是将大web系统给模块话，其实本质上每个小的服务还是一个Spring Boot网站\n这个给系统解耦合的同时也带来一个问题，如果去保证一个复杂多模块的安全稳定的运行以及更新，这个也是后面需要去解决的问题\n其实Spring Cloud引进了微服务架构，也引进了一些其他问题，比如说发起服务调用的消耗，以及多系统日志收集等等一系列问题 接下来就是对Spring Cloud Alibaba的学习，看看Spring Cloud Alibaba在Spring Cloud的基础上做了什么优化\n当然在快速过了一遍实验代码和视频之后我也有一些疑惑，等我把Spring Cloud Alibaba过了之后就得解决下面问题\nTODO # 服务注册中心能否添加报警功能，假如服务实例低于某个值 批量启动所有服务的最佳实践是什么，一般企业是怎么做的 Spring Boot如何实现只添加一个依赖来插入自己想要的插件 ","date":"2021-03-14","externalUrl":null,"permalink":"/posts/backend/framework/java/springboot/springcloud%E6%B5%85%E6%9E%90/","section":"博客","summary":"最近在学SpringCloud，之前一直对用视频学嗤之以鼻，","title":"SpringCloud浅析","type":"posts"},{"content":" 最近在面试，面试之前总觉得有一丝焦虑\n我不知道这丝焦虑是不是学生时代带给我的，对于我来说，这种焦虑是害怕自己不知道这个知识点而很囧。或者对于我来说，我自己一直对自己不够自信。为什么呢，因为对于我来说，我自己一直在干的很杂，相比于其他人来说，我算是一个真“全干”工程师\n原因 # 我在思考为什么会这样，我觉得第一点很重要的是我缺乏一个明确的职业规划\n之所以我会变成“全干”工程师，是因为我一直在“瞎”折腾。为什么会这样，主要是因为好奇心，每个人都有好奇心，区别在于有些人好奇心强非得把这个东西搞懂然后再放手，有些人好奇心弱一会就结束了。相比而来，我是一个好奇心非常重的人\n我觉得这种好奇心可能是在我上小学的时候就养成了。为什么这么说，你能想到一个十二岁左右小屁孩捧着一本右脑记忆的书，把圆周率后面五百位完整的记下来，还能完整的记忆一副扑克吗。\n现在这种右脑记忆培训班在中国现在也挺流行的，但是在十几年前，图书馆里面只有几本这样的书。我觉得这是我第一次开始折腾吧，虽然最终我没能应用在我的学习上（因为我读的理科）。但是这段经历现在回头看还是得给自己竖个大拇指，现在十几岁的小孩只知道玩手机，看电视，玩游戏。当年的我在没人教导的情况下，自己去钻研学习记忆方法，而且直到我上高三，六年时间内我都没有放弃。我高三的时候用它来背古诗，读一遍就背出来，可惜背诵高考只占8分，最终也没能派上啥用场。\n我人生中第二个折腾就是在高一的时候，我不记得我是听老师推荐还是自己在网上搜怎么学好英语，反正那个时候是新概念英语比较火，那个时候不知道是李阳还是谁鼓吹着学好英语就是听和背，大量重复的听和读，等你读到几百遍的时候你就自然而然的背下来了，我那个时候在睡觉的时候都听着新概念的英语文章，就是重复的听一篇文章还是每天重复的读，当你自然而然的背下来的时候就换下一篇文章背。我记得我当初买的新概念英语三，用这个方法背了大概20篇，有点后悔当初没有继续背下去，如果把新概念3和4全本背完，现在英语应该流利到丝滑境界吧。\n当然现在捡起来其实也可以，其实自己毕业后也一直在背单词，但是其实效果也不是很明显。现在看来，自己现在会不满意自己的现状也是有原因的，自己会放弃一些好的习惯。\n其实写之前自己也没意识到自己放弃了什么，因为人无时无刻不在做选择，当初放弃的原因是没有看到效果，相比于虎头虎脑的背，还不如多刷几道题（最终我高考英语考了130分），从分数上看我是挣了的，但是从长远的角度上来看，我是亏了的，我只是拿到了分数，然而那种方法能给我带来一种更长远的回报，好比短期投资和长期投资。长期投资最让人恐慌的是很难在短期内看到效果，必须要有一种信念让你坚持下去。\n所以其实对于我来说，目前我就需要的是一种坚定的信念，首先我得明确自己的规划，然后把这些规划慢慢给他落实掉。\n反思 # 其实回头看看，从大学开始到现在七年时间自己一共尝试了五个方向，按照时间排序\n后端 机器学习 爬虫 数据分析 大数据 每个方向其实自己都钻研成中级和高级之间的实力，等我走完这一个圈后，我发现我更喜欢做后端，我突然发现我好像走了一个圈，最后又回到了原点。有时候在想，好像自己真的在浪费时间，假如自己一直在自己喜欢的圈子里努力，那么自己的实力目前也不会之停留在中级和高级之间了。\n但是回头一想，其实自己也不去尝试怎么知道自己最后喜欢的是什么，没有谁天生就喜欢某一行，放牛郎的孩子没办法一辈子只能放牛，作为新一代的人，我很幸运的生活在这个互联网时代，我可以选择自己喜欢做的事，其实我也很感激毕业后呆的这家公司，虽然待遇上比不上大部分互联网人。但是在这里我没有压力没有开会，每天都在思考怎么学，怎么用。\n这几年就像交了5个朋友，每个朋友关系都不是很亲密，所以我会感到“孤独”，感到焦虑。\n要打破这个也很简单，选择自己真正想交的朋友做真心朋友，这样能解决你的焦虑和孤独\n其实有时候觉得做技术必须学点心理学和经济学。为什么呢。因为做到最后你会发现，99.99%的问题都能通过时间来解决，我们每天都在做选择，这个选择是有代价的，其中最大的代价就是时间，所以如何把人生价值最大化，最重要的就是规划好你的时间\n从大的层次上来讲，你要给自己做一个长期规划，这个一般是以年来做计划，第二个就是中期规划，这个一般以月来做单位，还有一个就是短期的，一般是以天来。\n术和道 # 其实上面讲的做规划很早之前自己就了解过，但是之前没有意识到这个重要性，年轻的时候不珍惜时间，对于任何问题不管是三七二十一搞定再说，所以以前会通宵解决一下很小的bug。虽然成就感很高，但是其实从后面角度上来看，得不偿失。\n我不是说你碰到bug就放弃，而是你要去想自己为什么会碰到通宵才能解决的bug。自己的规划是不是出了问题，是自己哪个地方能力有欠缺。是debug能力，还是编程能力不足，或者没有熟悉某些工具帮助自己快速定位等等。\n其实我自己也在开始尝试实践这种方法，我第一次了解是在知乎上看到一个硬件工程师在分享自己在家办公如何让自己有效的工作12个小时，然后我自己开始用Microsoft的TODO来帮助我自己实践这个。\n下面是我这些天实践的心得：\n规划是非常严肃，你不能抱着一个完成任务的心态去制定它，你要首先把自己想做的事列出来，然后你得给他排序，对于很重要的事是必须要完成的 规划完成之后会有一种成就感，对于你历史规划，你也可以定时复盘，看看自己哪些地方可以做到更好 长期规划非常有必要，你可以每天开始规划之前，或者完成了当天规划之后再思考是否长期规划得做适当的调整，比如加快某项，或者你觉得某项还有更多细节需要深追 总结 # 我觉得最重要一点就是复盘吧，你把每天自己完成的TODO，以及这个TODO对你长期规划的影响在脑海里面思考计量，这样你才能以最快速度的接近你的目标。\n","date":"2021-03-07","externalUrl":null,"permalink":"/posts/essays/life/%E9%9D%A2%E8%AF%95%E6%9D%82%E6%80%9D/","section":"博客","summary":"最近在面试，面试之前总觉得有一丝焦虑 我不知道这丝焦虑是不是学","title":"面试杂思","type":"posts"},{"content":" 为什么会有这篇博客呢，因为随着你阅历的增长，以及各种你即将面临的问题，比如婚姻、跳槽这种，你会有一种焦虑感，你一直在原地踏步，你怎么面对你的未来呢\n所以这篇博客其实很大程度是一种反思\n首先要明确一点就是你是一个什么样的人，自我反思，其实我算一个矛盾的人，做事情急着做完，但是要实现一些我自己的想法的时候又很拖延\n所以其实阻止我继续前进的一个东西就是拖延，其实毫不自夸的来讲我是一个解决问题能力很强的人，但是我不是一个行动力很强的人。\n有的时候我在想，我就像一只牛，假如给我一个红旗，我会卯足了劲往那个地方去，吃饭想，睡觉想，无时无刻不在思考这个。但假如不给我目标，我就像一只猫，只想安静的躺着\n就是这种性格，导致我现在什么都懂，但是什么都不精\n所以技术深入也非常简单，就是找到几个目标，一直钻，但是钻完了你还得思考，下一步往哪里钻，钻这个收获了什么，必须抛弃以前那种，完成了就结束了\n说完了怎么深入，现在来谈谈最主要的这么找目标。我也很迷茫，查找了很多资料，看看前人怎么做的，但是他们只是给了很多知识给你，让你去了解，当然在新手阶段，这个挺实用，但是对于那些懂了的来说，这些知识点都知道，只是不精\n怎么才能精呢？这个问题我想了很久，一直没有答案。后来我回顾我技术生涯，我有了一个比较简单的解决方法\n其实这个好多前辈也说过，就是想深一点\n其实技术这个东西基本功能大家都会，hello world，谁都会写。\n用做网站来打比方，你做一个网站，其实新手也可以做，你设计的时候可能就想着照顾好你自己这个用户就好了，各种框架一把嗦。但是假如你想深一点，假设你的1万个用户用你的网站怎么办，或者假如我们要新增一些东西怎么办\n就带着这些想法，去大胆的写你的代码，去验证你的想法，在实现你的目的去看看大家怎么解决的，然后继续学习你能不能用他们的想法做\n所以概括起来也很简单：\n要“多虑”，想事情想深一点 要“大胆”，想到就大胆的去做 要“虚心”，做完不能沾沾自喜，去看看别人怎么做的 要“总结”，要把你做的这方面形成一个系统 其实仔细回头看，自己其实也有大部分都有的拖延症，其实很早就知道该怎么做，但是总是拖着，被历史的惯性拖着\n这里分享一个 中国比特币第一人 李笑来的一个技巧，就是他以前背单词也是，知道背单词对自己有好处，但是总是拖延，后面他就给自己暗示，背一个单词10块钱，就是这种激励让他打败拖延症，成为新东方的一名非常强的英语老师。\n⛽️\n","date":"2021-01-07","externalUrl":null,"permalink":"/posts/essays/life/%E6%8A%80%E6%9C%AF%E5%A6%82%E4%BD%95%E6%B7%B1%E5%85%A5/","section":"博客","summary":"为什么会有这篇博客呢，因为随着你阅历的增长，以及各种你即将面","title":"技术如何深入","type":"posts"},{"content":" \u0026gt; Spring核心就是IoC（依赖注入）AoP（面向切面）本篇就基于一个开源项目 tiny-spring 来分析Spring框架到底给我们提供了什么东西\n依赖注入是什么呢，说起来非常简单就是系统帮你把依赖组装好，我们来举个简单例子\nclass People { Hand hand;};\n上面是一个非常简单的类，Spring把这个类叫做Bean，这个People 的 Bean 依赖一个Hand的类，我们来看一下这个Hand类\nclass Hand { Finger finger; int num};\n上面的Hand又依赖一个Finger类，而且提供了一个属性：手的num（数目），假如你要使用这个People类，你得新建3个类，而且得把代码写死，而Spring就是通过xml或者注解来帮你把这些类自动创建，所以才叫依赖注入，其实原理也非常简单\n为啥要提供这个功能呢，因为我们希望通过配置这些xml就能实现不同的xml来实现不同的功能类，而不用修改代码重新编译，你可以理解为就是代码是高度松耦合，我们可以提供很多相同接口的组件给用户用，用户用的时候只需要修改配置文件就能实现灵活组合不同模块的功能\n我们知道对于一个HTTP服务器，我们希望调用这个接口方法之前能对用户发的东西做一个核验，就像拦截器一样，但是Java里面怎么实现呢，就是调用一个方法之前调用一个特定方法，我们有两种方法，第一种就是定死类名，在调用所有方法之前就调用这个东西，第二张就是重新继承这个方法，在调用父方法之前或者之后在调用我们想调用的方法\n显然第一种太蠢了，我们得先判断这个方法不存在，第二种可以使用Java Proxy类来实现（当然有局限只能是接口，可以用动态生成的类比如Cglib来做），我们而且可以把那个想注入的方法也看作一个特殊的bean（默认在普通的bean之前就做好），这样最终我们把普通的bean使用IoC容器给初始化，然后在用代理类把这个bean继承出来\n这样我们就实现了一个高度控制的类方法，而Spring最主要就是要组装一个Servlet类来处理不同路径下面的请求，网络处理那些事情都交给Tomcat来处理了，所以其实Spring的职责也很简单，就是组装，最后根据组装模块来响应Tomcat发过来的request，返回一个response,因为Spring需要生成很多类，而且假如你使用为了支持注解，Spring还得使用反射来获取所有类的注解，反射速度也是非常慢的，所有Spring启动一般需要一点时间，甚至比其他框架比如Python还要慢，不过这些类的生成都是一次性的（不是单例除外），生成完之后就可以直接使用这些生成好的类的方法来响应Tomcat请求了\n资料 # https://github.com/code4craft/tiny-spring/blob/master/changelog.md\n","date":"2021-01-07","externalUrl":null,"permalink":"/posts/backend/framework/java/springboot/%E6%B5%85%E6%9E%90spring/","section":"博客","summary":"\u0026gt; Spring核心就是IoC（依赖注入）AoP（面向切面）本","title":"浅析Spring","type":"posts"},{"content":" 这篇博客主要是从web技术发展来探索微服务的起源\n要想了解微服务是什么得从web框架出来之后开始讲起，大部分可能不知道微服务，一定知道写web服务的框架，懂Java的可以用Spring Boot一把梭，懂Python的Flask、Django、Tornado也写的飞起\n微服务也就是在这些框架出来后才慢慢出现的，首先我们就不谈MVC模式的兴起（主要就是把前端和后端解耦合），我们谈谈在MVC后要面临的问题\n第一个就是接口越来越多，用户也越来越多，面临的第一个问题就是某些接口请求非常频繁，面临第二个问题就是某些接口我想升级怎么办，我们知道使用上面那些框架你要是想修改接口得把代码重新发布一下，然而线上就得暂停服务一会等服务升级\n针对上面的问题，就开启了微服务的热潮，解决方法和MVC一样，我们把后端和后端进行解耦合，一个后端系统专门负责认证，一个后端系统专门负责订单，两者之间使用token来确认同一个用户，登录的请求接口不是很频繁，就用2台机子，订单非常频繁双十一的时候还会爆炸，就用10台机子，实现上非常简单，就是把在每台机子上面启动单独后端程序，再使用nginx作为负载均衡这样流量就均匀的打到每台机子上面去了，这样不管你是用Java还是Python都OK\n这种解决方案能扛起很高的并发，但是还是有个问题，升级起来非常麻烦，要是想升级系统，得把每台机子的后端程序单独升级，管理起来非常麻烦\n然后大家开始研究，大家把接口抽象起来，其实作为用户来说，他只是想调这个接口，能不能使用一个中间层把用户想调的接口和后端真正的提供的服务连接起来，这个就是中间件兴起的源头\n让我们想想中间件的职责是什么，其实想想也很简单就是一个后端接口管理系统，能够实现注册接口的功能，并且打通接口和后端的接口，当然说起来简单，但是要实现一个高可用的中间件也是不容易的，具体可以去看看Spring Cloud、Dubbo这些中间件框架\n我们想想这些中间件框架优缺点，优点也很明确，就是性能非常强，每个小模块只负责自己的接口，当业务量大起来的时候能迅速启动上万个进程来分流大流量，但是缺点也有就是只限定了自己的编程语言（Spring Cloud和Dobbo这些只能用Java写），而且随着接口越来越多，管理也越来越麻烦，调用链也越来越长了，有的时候你想加一个接口得涉及很多道调用链，先去请求用户信息，再去查询用户银行卡信息，再去查询商品信息，虽然分模块很爽，但是有些东西越分越累，模块越来越多，实现功能也越来越散\n模块越来越多带来的一个副作用是测试调试越来越难了，而且随着摩尔定律，我们现在的内存越来越大了、CPU也越来越多了，以前写微服务为的就是省点内存，现在大家发现其实他也没有那么省内存，为了保证服务高可用，我们得用上百台机器来搭一个集群，但是大部分时间，业务量没有那么大，其实大部分时间那么大内存都放在那里浪费了，而且当业务量万一突然起来了，然而固定集群却没法承受这么大流量\n后面随着Docker、K8S的兴起，大家慢慢的发现容器化能够解决上面的问题，当业务量小的时候启动少量容器，业务量大的时候启动多一点容器，而且可以实现动态扩容，所以这时候提出了一个Service Mesh模式\n什么是Service Mesh，其实就是我们前面提到的nginx的负载均衡方案类似，就是使用一个nginx来把请求打在后端上面，但是不同点在于，原来nginx是写死在配置里面的，现在k8s里面的nginx（目前主流的方案是Istio）把接过来的请求打到容器网络上面去，而且当流量大的时候会自动新建新的实例来负载新的请求\n兜兜转转我们又回到了最初的样子，你也不用学习什么后端中间件使用，你只要会写一个提供web接口的服务，打包成个容器，在k8s配置一下，轻轻松松就能扛着上千万并发，扛不住加实例就够了\n现在我们来谈谈Service Mesh的原理，其实很简单，就是蚁群的思想，每一个容器都是一只小蚂蚁，功能齐全啥都能干，一只蚂蚁搬不动一座大山，但是上亿只蚂蚁可以搬的起，Service Mesh的思想有点像大数据分而治之的理论，当你的容器能服务1千人，那你1万个容器就能服务1亿个人，所以对于开发者来说，你只需要写好一个能服务1千个人的小web，你不需要去考虑当你要服务1万个人的时候，代码需要怎么改动，对于开发者来说，测试和开发都变得异常轻松，自己也不用去写原来的Dubbo那种大体量代码，你所有的接口都在一个项目中，测试开发都变得异常轻松，而且你也不用思考怎么使用Java调用Python的机器学习框架等等，你可以用Python写，用Go写，甚至可以用C来写，容器不在拘束于语言，你只要能对外提供服务就好了\n总之在K8S容器化兴起之后，我们也不用再怎么思考如何解耦合代码，套上各种中间件把我们各个接口都打散，实现一套高并发系统，我们只需要写一个网站，能服务上千的用户，当我们业务量大起来的时候交给K8S，让它帮我们启动上万个相同网站，把流量给消化掉，所以在一定的程度上，在现在技术发展下，高并发的门槛越来越低，原来一个高并发专家得学会好多框架，dubbo各种组件，现在随便叫来一个小白随便把它从网上复制的代码拷拷，搭起来一个小网站，放到K8S上面，马上就可以服务上千万甚至上亿用户\n所以说技术发展真的是快啊，记得以前一个架构师的道路是慢慢从学习各种nginx负载均衡Redis集群搭建，搭建起一套复杂的系统，你得去学习各种组件使用，现在呢，一个小白，网上拷拷代码，使用上内存作为缓存，使用MySQL当数据库，并发也就上个千，然后数据库配置一下把内存换成redis，MySQL换成Kudu，放进K8S里面并发就能上千万，高并发问题就解决了，原来一个百万年薪才能做的高并发技术专家在新的技术降纬打击下一个月薪1万的菜鸟也能搞定了\n总而言之，高并发随着技术发展会越来越简单，微服务的提出有人说就是干掉架构师，所以有时候也会想，随着技术的发展，是不是不会有工程师这个职位了，的确技术在不断发展，但是只要抱着一个不断学习的心也就不怕了，技术不断推成出新，但是本质是不变的，所以也不用害怕。\n资料 # https://philcalcado.com/2017/08/03/pattern_service_mesh.html\n","date":"2021-01-07","externalUrl":null,"permalink":"/posts/backend/framework/java/%E6%B5%85%E6%9E%90%E5%BE%AE%E6%9C%8D%E5%8A%A1/","section":"博客","summary":"这篇博客主要是从web技术发展来探索微服务的起源 要想了解微服","title":"浅析微服务","type":"posts"},{"content":" Tomcat作为Java老牌web服务器，所以研究Tomcat就能搞懂高并发，本文是查阅大量资料总结的自己对于web高并发服务器的理解，有可能部分理解是错误的，欢迎来纠错，本文尽量不使用代码，网上查到的都是一大段一大段代码，让人看的头痛\n一个最简单的web服务器长啥样呢，就是一个echo服务期\n监听端口 新TCP请求进来，建立TCP链接 向TCP链接发送响应 所以我们扩展一下，一台高并发服务器最重要的就是接受成千上万的新请求，以及管理已经建立的请求，虽然说起来很简单，虽然说起来很简单，但是做起来很复杂\n首先我们从用户端看一次完整的HTTP请求是咋样的，我们用吃饭来打比方，首先用户先想好他去哪里吃饭（构建url，比如abc.com)，然后他通过百度去查这个饭店在哪（查询DNS abc.com 的IP地址是什么），百度告诉他这家餐馆搬到他家附件2公里外（得到IP地址 127.0.0.1），然后用户就按照地图找到了这家店，然后他想了半天决定吃鱼（发起HTTP请求/fish）\n现在用户已经决定好了他要干什么了，接下来就是看这家餐馆的操作了，首先用户会发起一个TCP请求建立连接，因为他得确定这家店今天开不开张，不开张人家也不会卖他鱼吃，接下来我们来看看Tomcat是怎么把这家餐馆给经营好的\n首先他要根据他自己后厨的规模，也就是同时可以做多少个菜（并发线程数），以及做一个菜要花多少时间（TPS性能），来决定他餐馆可以放多少椅子（最大连接数），因为这个餐馆一直有人吃饱就走，所以他也特意搞了个叫号系统，允许顾客来了排个队等一等（也就是Tomcat的排队队列来控制）\n那我们来分几种情况来讨论一下，第一种今天没多少顾客，那么大家来了之后就有椅子做（建立好TCP链接），第二种做满了但是门口还有地方站一站（也建立链接TCP放进排队队列），最后一种是门口都站不下了，为了不影响其他顾客，我们只能说客满了，您不一定能吃上，您去别处看看吧，那么用户就会得到一个拒绝连接的错误（Connection refused）\n接下来我们再看看坐在店里的顾客了（建立好了TCP连接），这个时候顾客都知道今天能吃上就安安静静的等了，餐馆呢就把顾客点好菜发给后厨（Tomcat把HTTP请求发给Servlet容器），有的菜烧得快有的菜烧得慢（每个请求耗时不同），菜好了就端上去了，客户呢觉得吃饱了那就走了（关闭TCP连接），觉得没吃饱就继续叫（继续发送HTTP请求），假如有时候叫了一道特别难的菜，后厨做了很久都没端上来，假如你不急那就慢慢等，假如你急的化那你可以直接走（抛出一个Read timed out错误）\n现在我们总结一下，从用户角度上来看，他碰到的问题就是两种，第一种就是餐馆人太多了，没法吃（拒绝连接），第二种就是餐馆上菜太慢（读取超时）。从餐馆角度上来看，为了服务好用户他首先得安排好足够的椅子，第二个就是要让后台上菜速度要快，上菜快可以通过增加厨师（增大最大线程数），你也可以针对做菜某些步骤慢的地方多派人手（SQL调优、缓存优化等等）\nPS: 这篇文章还有一个重要的问题没有提，用户进了餐馆（建立了TCP连接），他什么时候叫菜（发送HTTP请求）我们是不知道，我们怎么来实现一个高效的系统来服务客户，来满足成千上万的顾客有求必应，我会在另外一篇博客来专门谈这个东西【Select和epoll浅析】\n资料 # https://qiankunli.github.io/2019/11/26/tomcat_source.html\n","date":"2020-12-31","externalUrl":null,"permalink":"/posts/backend/network/tomcat%E9%AB%98%E5%B9%B6%E5%8F%91%E6%B5%85%E6%9E%90/","section":"博客","summary":"Tomcat作为Java老牌web服务器，所以研究Tomca","title":"Tomcat高并发浅析","type":"posts"},{"content":"","date":"2020-12-31","externalUrl":null,"permalink":"/categories/%E7%BD%91%E7%BB%9C/","section":"Categories","summary":"","title":"网络","type":"categories"},{"content":" 这篇文章是【Tomcat高并发浅析】的姊妹篇，专门通过分析Select和epoll两个框架来介绍如何实现高并发\n起因 # 我们知道建立TCP连接之后，就是创建了Socket链接，服务器得监控用户所有的Socket，以防用户发送请求过来\n首先最简单就是用个for循环，遍历每个Socket链接看看有没有数据，如果没有就进程休眠一会再来，这个就非常低效，得一直检查一直检查，所以操作系统就开发了一个接口，因为网卡数据是他控制的，首先他让进程睡觉，只要操作系统接受到了网卡数据，根据网卡数据请求的TCP上面的端口找到对应的进程，然后唤醒它，进程就开始遍历每个Socket链接，里面其中一个一定会有数据，因为操作系统已经明确告诉它了\n这个就是select框架，这个实现起来特别简单，不需要加很多东西就能实现，但是有个缺点就是，进程醒来之后他不知道哪个socket链接有数据，所以他得遍历所有的，假如有成千上万的Socket链接，那遍历一遍得好久，所以操作系统就限制了最多你可以监控1024个Socket接口，所有假如你使用select作为监控，你只可以同时监听1024个socket，意味着你同时只能和1024个用户建立连接\n这样看起来真的太低效了，当然也不是毫无优点，假如你用户少，而且都是活跃用户，这样操作系统叫醒一次你就可以同时处理很多用户了，但是假如你用户很多，那遍历耗费时间也太长了\n所以在Linux内核2.4提供了epoll框架，他终于开放自己的事件驱动核心，专门给他写了个函数，首先他用红黑树存贮了所有Socket连接以及他的回调函数，这样当网卡数据来了之后，他直接去回调那个函数了，这样进程不需要遍历了，直接执行函数，当然也有一定的代价，就是存贮这些Socket连接和回调地址，但是相对于的解除了1024的限制，第二直接找到有数据的Socket，读取数据一步到位，具体怎么实现我就不详细讲了，有兴趣的可以去查阅相关资料\n总结 # 服务器之所以很轻松的实现上万Socket监控靠的是操作系统的帮助，我们可以理解各种并发都是事件，操作系统就是事件并发高手，所以借助操作系统我们能够很轻松的实现管理看起来非常复杂的事情\n","date":"2020-12-30","externalUrl":null,"permalink":"/posts/backend/network/select%E5%92%8Cepoll%E6%B5%85%E6%9E%90/","section":"博客","summary":"这篇文章是【Tomcat高并发浅析】的姊妹篇，专门通过分析S","title":"Select和epoll浅析","type":"posts"},{"content":"","date":"2020-12-21","externalUrl":null,"permalink":"/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/","section":"Tags","summary":"","title":"大数据","type":"tags"},{"content":" 首先说说什么是大数据，最简单用数据量为单位，大于1亿就算大的，因为小于1亿就在数据库做就可以了，所以大数据是公司数据大到数据库处理不了的时候才要考虑的事情，小公司MySQL优化一下索引就可以了\n那怎么解决了，Google给了我们三篇论文作为解决方法，这个也是大数据的基石，所有后面才有Hadoop的横空出世，我们也知道了解决这个问题的方法，就是对症下药，你是大数据，我就是“大”机器，以前在一台服务器就可以处理，现在上十台、一百台\n但是又出来一个问题，怎么管理，搭建过简单的4节点的机器就知道，那些大数据组件配置虽然不负责，但是很繁琐，关键最坑的事，你一开始并不知道怎么配置最好，你大部分时间都浪费在修改配置，更新所有组件配置，重启，最坑的是依赖关，比如Hbase依赖Zookeeper，你更新了Zookeeper，HBase你也得重启。而且最麻烦的是你用的那些开源大数据组件更新频繁，你很多时候要踩很多坑才能搭一个简单的大数据架构\n所有就有了Cloudera和Hortonworks（被Cloudera合并了）这些提供一整套解决方法的公司，他们能提供什么了，就是管理这个大数据架构的一套系统，通过这个系统，你很容易就搭建起来一个可用的大数据集群\n但是一个悲伤的消息就是Cloudera也要恰饭了，6.3.3之后不会提供免费的版本供小公司使用，对于大公司自己可能会考虑做一个类似的系统（其实也不难，就是一套web管理系统以及专业的解决方案）。但是对于小公司来说，做一个这样的系统成本太高了，但是目前来看6.3.2还算良心，支持到2022年，虽然节点最多只能100个，但是超过100个，说明你公司应该负的起钱买新版了\nCDH是Cloudera提供给大公司的大数据一套架构，接下来我们就剖析一下Cloudera为什么能凭这一套解决方案支撑起百亿美元的市值\n这里我就不介绍这些基本组件了，网上都有，我就谈谈这些组件是怎么解决大数据这个问题的\n首先我们得知道我们要搞大数据的原因，很简单我们想从大数据中得到我们想要的结果，比如淘宝双十一大屏，我们就想知道淘宝双十一卖了多少钱，最后只要一个数字就好了。\n接下来我们就以最简单的求和举例子来思考Cloudera提供的CDH来处理这个问题，第一步是存第二步是算\n首先是存，假如你数据是具有唯一性的，你就存到HBase，用Row-key来区分，假如你不具有唯一性那你就存到Hadoop里面去，这是最基本的，也是Google三篇论文中的两个\n接下来就是算，MR也就是Google三篇论文中的Map-Reduce理论。由于MR写起来非常复杂，所以就有Hive把SQL转换成MR程序\n这三板斧下来连Google那样的大体量都能解决，但是这个方法有个弊端就是慢，很多不必要的东西比如数据落盘这些耗时间的东西拖慢了整体的速度，而且大数据是这样万一，必须要等所有的步骤都走完才能得到最终结果，所以就是最慢的那一个步骤拖慢了整体速度\nMR的产生是因为内存放不了所有数据才把中间数据落盘，所以假如你内存比磁盘还要大，那在内存里面跑就可以了，Spark就是这样产生的，用内存来加速计算，Spark就像一个万能工具箱一样，你可以直接用它读取各种数据源，来各种折腾，它提供了一整套的框架来帮你做这些，但是对于企业来说懂Spark的人少，懂SQL的多\n所以Cloudera自己开发了一个框架impala来方便那些懂SQL的人直接来进行数据计算，这个方案好是好，但是有个问题，impala一开始只能读取Hive表上的数据，由于Hive存在一个问题就是它由于存贮在Hadoop上所以她只能新增不能删除\n所以impala只能在离线大数据上做分析，你要全量的数据得把最新的数据覆盖掉后才能开始计算，所以他们又做了一个类似HBase的数据库Kudu，基本上融合了HBase 快速新增更新删除和Hive的SQL优好的两个，这样我们就可以用impala读取最新Kudu数据做实时大数据分析了，而且速度要比Spark还快\n当然也不是说impala一定是一个万能手术刀，能解决所有大数据问题，大数据这个东西吧特别有意思，我们永远也找不到一个完美的解决方法，只能找到一个相对好的解决方案。\n总结 # 以前我刚开始了解大数据的时候，我一直觉得哪个强就用那个，其实恰恰相反，每个大数据组件都有其存在的意义，面对大数据，我们不需要一套方案跑通所有，而是找一个最优雅的方法来解决特定问题，其实大数据有很多开源的框架，这些开源组件都有其特定的能力去解决他们遇到的问题，等真正有一天一套解决方案能解决所有问题，那应该大部分软件工程师都应该要失业了\n","date":"2020-12-21","externalUrl":null,"permalink":"/posts/backend/framework/bigdata/%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%9E%B6%E6%9E%84%E5%B0%8F%E7%BB%93/","section":"博客","summary":"首先说说什么是大数据，最简单用数据量为单位，大于1亿就算大的","title":"大数据架构小结","type":"posts"},{"content":" 去年的年终总结还没有写，2020就来了，已经很久没有写博客，原因有很多吧，这里也不多提了，今天突然想写主要是突然内心有些感想吧\n大学寝室四个毕业后两个考上了研究生，两个出来工作，今天那个工作的那个告诉我，他和班里的一个同学今年三月份辞职开始全职考研\n听到这个消息的时候我挺震惊的，让我重新开始思考“考研”这件事\n假如我回到毕业前，我的对考研的态度是不反对别人考，但是自己不想考，因为我觉得研究生的最大作用就是一个敲门砖让我获得更好的工作，我希望通过自己的努力去达到这个目标而不是去白白浪费教育资源\n但是毕业一年半了，我逐渐变得疑惑了，因为我确确实实感受到了一种无力感，过去一年我无论是工作还是生活都很拼命，但是感觉就像一个拳头重重的打在空气中，一点反应都没有，虽然经常会因为自己做的事情感到自豪，但是似乎害怕自己所付出的努力只是镜中月水中花\n有时候跟同事聊天，他们总会说你还年轻，但其实在内心我却已经非常“苍老”，在学过很多技术很多未知的东西发现，无论是技术还是产品，没有什么是不能实现的只要肯花时间，以前刚出茅庐的时候，总是一种我全部都要的心态，无论是多难的技术多么难实现的功能，只要自己觉得炫酷就一定要去搞懂一定要去实现\n当你排除所有的Bug，消除心中所有的疑惑回头再看好像那个炫酷的技术，那个困扰你很多天的bug只是一个小小的知识点，想想自己当初发了九牛二虎的力量排除到凌晨3点没想到却是那么一个小小的地方出了问题\n经历的多了就发现，重要的不是你会做什么，而是你选择做什么，自己已经不在年轻了，自己做的每个决定都会影响到未来，还是年轻的时候好，只管拼命往前冲，现在“老”了，做事总是畏手畏脚，因为自己已经有经验了，不再是摸着石头过河，总是望着河边，想着到底到底是游过去还是像大部分人一样坐船过去\n也是很巧，过马路的时候绿灯亮了，在我这边只有我一个人要过去，对面马路却一群人要过来，当时瞬间感受到了一种孤独的东西\n在这条人生的斑马线上我可以选择和大部分人一样走上一条康庄大道，路上非常平坦，而且在这条道路上我不会感受到孤独，或者我选择一个人孤独倔强的走下去，虽然路的终点迎接我的不一定是鲜花和掌声，但是当我回过头看看过去的自己的回忆，会感叹这辈子还是很能折腾的哈哈，没白过。\n其实现在有的时候回头看看自己写的博客，还是有一点点自豪的，有的时候甚至会问自己是不是请人代笔的，怎么会写出这么有深度的文章（不要脸哈哈）。过去的2019年自己折腾了很多，可能有些看起来是毫无意义的任性，但是我相信自己每一次的折腾都没有白费，可能现在看来没有价值，但是未来能掀起多大浪花还不知道呢。加油吧，2020！！！\n","date":"2020-01-07","externalUrl":null,"permalink":"/posts/essays/life/%E6%B7%B1%E5%A4%9C%E6%9C%89%E6%84%9F/","section":"博客","summary":"去年的年终总结还没有写，2020就来了，已经很久没有写博客，","title":"深夜有感","type":"posts"},{"content":" 随着2019年8月份，Flink1.9.0正式发布，Flink并入Blink代码，开始同时支持批和流的处理，于此同时，批量同时处理所遇到的问题也就产生了\n首先我们要知道，什么是批，什么是流，批好比一瓶水，我们看到到它在那里，流就像雨水，我们只能看着它慢慢的从天上掉下来，什么时候掉完我们不知道\n批处理解决我们非常好理解，分而治之即可，然而对于流数据的解决该如何呢，当然也是分而治之，但是我们分的是时间，所以对于流数据最重要的第一是不能让历史数据沉淀下来\n我们回到现实中，理想状态我们处理数据总希望数据全部都能获取到，由于数据库三范式的存在，我们的数据不是完整的，比如消费记录只会存一个id代表用户，而我们希望统计用户记录的年龄分布就不能只通过记录表来实现\n所以我们第一个挑战就是Join，把多个维度表汇集到一起，什么是Join呢，就是把不同的表进行分组，我们这里就不讨论大表和大表的Join，因为这个不是Flink的强项，对于这个来说，我只能说MapReduce欢迎您。\n我们就讨论一种情况，大表处理表，小表配置表，因为这代表实际大数据处理的中最常见的情况，由于Flink的批流一致的支持，我们现在处理Join有两张情况\n第一种是批流进行Join，第二种是流和流（双流）Join，当然由于Flink底层其实把批看做一种特殊的流（无后续增加），我们也可以看成一种\n对于批和流的Join，Flink的优化是将批变成State，这样流只需要进行内存匹配而不需要每次进行Join的时候在读取一遍批处理数据，这个方式来说最简单，但是也有一个问题，我们的配置表变成的“一次性”的了，假如我们想更新配置表，但是流计算察觉不到变化也就实现不了一个动态的更新\n接下来我们在看看流和流的Join，对于最简单的双流Join来说，为了维持流数据的更新，必须存贮两个流的所以历史数据，对于小表来说存储耗费不了内存，但是大表来说就不一样的，所以双流Join不适合大表和小表的Join，但是统计的话可以，Flink针对一些东西做了优化不会存储所以历史数据，只保留一个统计State，会根据更新动态来修改，那么大表和小表就没法实现双流Join了吗\n当然不是，我们想想为什么大表小表会保留历史数据才能维持结果正确，我们思考一个例子，我们有很多不同种类的商品，然后我们有一个本子记录客人想买的商品（实时更新），为了做到正确匹配客人像要的商品，我们必须记住我们所以种类的商品，当有个客人想买的时候才能匹配上，假如你忘了你们有什么匹配的商品，那么这个客人就买不到了，因为匹配不上\n这种情况是必须要记住所以种类的，但是我们大部分情况是这样的，我们的大表里面记录了系统某个指标，我们的配置表里面保存了我们想监听的指标，当我们更新我们的配置表的时候代表从这个时刻起我们对某个指标感兴趣，对于历史的数据我们其实是不感兴趣的，所以，你完全可以“忘掉”原来的大表历史数据，Flink对于这个功能使用了Temporal Join来支持\n例如：\nselect * from source a, lateral table (ConfigTable(a.protime)) b where a.id = b.id; 我们只要注册ConfigTable到一个流表上就代表告诉Flink历史数据可以不用记住了，我只关心现在的最新配置能不能匹配上，这样就解决了对大表的时间分片\n","date":"2019-10-16","externalUrl":null,"permalink":"/posts/backend/software/flink%E6%89%B9%E5%92%8C%E6%B5%81%E5%A4%84%E7%90%86%E7%9A%84%E6%80%9D%E8%80%83/","section":"博客","summary":"随着2019年8月份，Flink1.9.0正式发布，Flin","title":"Flink批和流处理的思考","type":"posts"},{"content":" Atlas 是一个可扩展和可扩展的核心基础治理服务集 - 使企业能够有效地和高效地满足 Hadoop 中的合规性要求，并允许与整个企业数据生态系统的集成。[来自百科]\n现在我们就基于最新版2.0.0介绍一下部署细节\nServer安装 # 源码安装 首先在https://www.apache.org/dyn/closer.cgi/atlas/2.0.0/apache-atlas-2.0.0-sources.tar.gz下载2.0.0源码\n解压源码\n选择Atlas架构 Atlas支持多种架构作为后端\nHBase + Solr Cassandra + Solr 你可以选择多种，这里我们采用集成HBase + Solr方式编译\nmvn clean -DskipTests package -Pdist,embedded-hbase-solr 执行代码即可（推荐使用阿里云的maven源加速编译）\n修改环境变量 编译完之后在/distro/target下面有很多tar.gz包，我们需要的是apache-atlas-2.0.0-server.tar.gz包，解压到当前目录\n3.1 修改配置文件conf/atlas-env.sh\nexport JAVA_HOME=/your/java/install export MANAGE_LOCAL_HBASE=false export MANAGE_LOCAL_SOLR=false 我们设定Solr和HBase手动开启，方便我们发现哪个部分启动异常\n3.2 修改admin密码：\n系统默认会生成一个密码给我们，但是官网我也没看到说这个密码，所以我们自己生成一个，然后修改上去\necho -n \u0026quot;password\u0026quot; | sha256sum 使用上面命令生成一个sha256加密字符（你可以把password改成你想要的密码），复制生成的字符串（不需要-），例如5e884898da28047151d0e56f8dc6292773603d0d6aabbdd62a11ef721d1542d8 修改conf/users-credentials.properties 改成\nadmin=ADMIN::5e884898da28047151d0e56f8dc6292773603d0d6aabbdd62a11ef721d1542d8 3.3 修改HBase配置(需要提前安装好java和Zookeeper和Hadoop）\n进入hbase目录夹\n修改 conf/hbase-env.sh\nexport JAVA_HOME=/your/java/install export HBASE_MANAGES_ZK=false 复制Hadoop配置到HBase中\ncp $HADOOP_HOME/etc/hadoop/core-site.xml $HBASE_HOME/conf/ cp $HADOOP_HOME/etc/hadoop/hdfs-site.xml $HBASE_HOME/conf/ 在hbase-site.xml中加入\n\u0026lt;property\u0026gt; \u0026lt;name\u0026gt;hbase.cluster.distributed\u0026lt;/name\u0026gt; \u0026lt;value\u0026gt;true\u0026lt;/value\u0026gt; \u0026lt;/property\u0026gt; \u0026lt;property\u0026gt; \u0026lt;name\u0026gt;hbase.rootdir\u0026lt;/name\u0026gt; \u0026lt;value\u0026gt;/hbase\u0026lt;/value\u0026gt; \u0026lt;/property\u0026gt; \u0026lt;property\u0026gt; \u0026lt;name\u0026gt;hbase.zookeeper.quorum\u0026lt;/name\u0026gt; \u0026lt;value\u0026gt;localhost\u0026lt;/value\u0026gt; \u0026lt;/property\u0026gt; \u0026lt;property\u0026gt; \u0026lt;name\u0026gt;hbase.zookeeper.property.clientPort\u0026lt;/name\u0026gt; \u0026lt;value\u0026gt;2181\u0026lt;/value\u0026gt; \u0026lt;/property\u0026gt; 启动安装好的Zookeeper，使用./bin/start-hbase.sh启动HBase\n使用jps应该能看到HMaster和HRegionServer启动了\n测试HBase安装是否完成，使用./bin/hbase shell 进入HBase 命令行，如果status命令返回正确的话，那么你的HBase就安装好了\n3.4 启动Solr\n进入solr目录，启动solr\n./bin/solr -c -z localhost:2181 -p 8983 打开http://localhost:8983/solr/#/如果能看到正常页面，那么Solr就启动好了\n在apache-atlas-2.0.0目录下创建索引\n./solr/bin/solr create -c vertex_index -d conf/solr -shards 1 -replicationFactor 1 ./solr/bin/solr create -c edge_index -d conf/solr -shards 1 -replicationFactor 1 ./solr/bin/solr create -c fulltext_index -d conf/solr -shards 1 -replicationFactor 1 3.5 启动Atlas\n在apache-atlas-2.0.0目录下启动Atlas\n使用bin/atlas_start.py 或者 /usr/bin/python2.7 bin/atlas_start.py\nPS：第一次启动比较慢，如果前面的HBase和Solr都安装好了的话，一般都没有什么大问题，可以查看logs/application.log查看系统运行情况，等到初始化完成后打开localhost:21000使用admin:password即可登录上去\n当然我们现在系统空空如也，现在我们来使用Hook导入数据到Atlas里面去吧 我测试通过的版本是：Hadoop2.8.1 + Zookeeper 3.4.10 ，其他的都是用的默认Atlas 集成的版本\nHook安装 # Atlas最强大的的地方就是能够把Hive，Sqoop，Kafka这些大数据组件的血缘关系给自动抽取出来，所以钩子的安装至关重要\nSqoop钩子 # Sqoop 我用的是1.4.7版本\n配置Sqoop钩子 首先在conf/sqoop-site.xml中添加\n\u0026lt;property\u0026gt; \u0026lt;name\u0026gt;sqoop.job.data.publish.class\u0026lt;/name\u0026gt; \u0026lt;value\u0026gt;org.apache.atlas.sqoop.hook.SqoopHook\u0026lt;/value\u0026gt; \u0026lt;/property\u0026gt; 复制必要的包 解压distro/target的apache-atlas-2.0.0-sqoop-hook.tar.gz，复制apache-atlas-2.0.0-sqoop-hook/apache-atlas-sqoop-hook-2.0.0/hook/sqoop/目录到 \u0026lt;atlas package\u0026gt;/hook/sqoop\n如：\ncd /where/your/untar/atlas cp -r ../../apache-atlas-2.0.0-sqoop-hook/apache-atlas-sqoop-hook-2.0.0/hook . 创建软链接\u0026lt;atlas-conf\u0026gt;/atlas-application.properties到\u0026lt;sqoop-conf\u0026gt;/\n如：\nln -s ln -s /home/zhanglun/github/apache-atlas-sources-2.0.0/distro/target/apache-atlas-2.0.0-server/apache-atlas-2.0.0/conf/atlas-application.properties /opt/sqoop-1.4.7.bin__hadoop-2.6.0/conf 将\u0026lt;atlas package\u0026gt;/hook/sqoop/*.jar 复制到sqoop lib目录\n如：\ncp hook/sqoop/*.jar /opt/sqoop-1.4.7.bin__hadoop-2.6.0/lib cp hook/sqoop/atlas-sqoop-plugin-impl/*.jar /opt/sqoop-1.4.7.bin__hadoop-2.6.0/lib 测试Sqoop 导入Hive中 sqoop import \u0026ndash;connect jdbc:mysql://localhost:3306/sqoop \u0026ndash;username root -P \u0026ndash;split-by id \u0026ndash;table root \u0026ndash;hive-import \u0026ndash;create-hive-table \u0026ndash;hive-table db.auth_user\n不出意外应该会报错\nCaused by: java.lang.NullPointerException at org.apache.atlas.hook.AtlasHook 因为我们还没有配置好Sqoop钩子，接下来我们来配置Sqoop钩子\n配置Atlas 前面我们创建了软链接，现在我们只要修改conf/atlas-application.properties这个配置即可\n首先我们得配置关闭Kafka作为发送消息缓冲，因为Atlas默认使用Kafka作为消息缓冲，然后我们修改下面的配置（这个后期可以打开，你再配置好kafka的地址）\natlas.notification.embedded=false # 不往kafka里面发送 atlas.graph.index.search.backend=solr5 异常一： Caused by: java.lang.ClassNotFoundException: org.json.JSONObject 包缺失，下载java-json.jar 到Sqoop文件夹\n异常二：Import failed: java.io.IOException: java.lang.ClassNotFoundException: org.apache.hadoop.hive.conf.HiveConf Could not load org.apache.hadoop.hive.conf.HiveConf. Make sure HIVE_CONF_DIR is set correctly 环境变量没有设置对，设置HIVE_CONF_DIR在conf/sqoop-env.sh（确保HADOOP_HOME和HIVE_HOME不是空值）\nexport HADOOP_CLASSPATH=\u0026quot;`$HADOOP_HOME/bin/hadoop classpath`:$HIVE_HOME/lib/*\u0026quot; 异常三： Error when removing metric from org.apache.kafka.common.metrics.JmxReporter java.security.AccessControlException: access denied (\u0026quot;javax.management.MBeanTrustPermission\u0026quot; \u0026quot;register\u0026quot;) 根据stackoverflow 解决\n异常四：java.lang.NoSuchMethodError: com.fasterxml.jackson.databind.ObjectMapper.readerFor Hive包版本和Sqoop包版本冲突（我的Hive版本是2.3.4），可以先备份Sqoop的lib，文件再进行下面操作：\ncp -r lib lib_back rm lib/jackson-* cp $HIVE_HOME/lib/jackson-* lib/ 异常五：Connection to node -1 could not be established 你在conf/atlas-application.properties没有修改atlas.notification.embedded成false，那么你必须配置好kafka地址\natlas.kafka.zookeeper.connect=localhost:2181 atlas.kafka.bootstrap.servers=localhost:9092 PS：每次出现异常，你必须先删掉Hadoop上面的文件，再执行导入，你可以直接安装我的流程进行修复，因为这些都是我在配置的时候顺序出现的问题，走到这里我们就配置好了Sqoop和Hive的导入Hook，如果运行成功，你会看到下面界面\n接下来我们配置Hive钩子来导入Hive中的表\nHive 钩子 # 配置hive-site.xml 在里面加入\n\u0026lt;property\u0026gt; \u0026lt;name\u0026gt;hive.exec.post.hooks\u0026lt;/name\u0026gt; \u0026lt;value\u0026gt;org.apache.atlas.hive.hook.HiveHook\u0026lt;/value\u0026gt; \u0026lt;/property\u0026gt; 解压 hive-hook包 如：tar xzvf apache-atlas-2.0.0-hive-hook.tar.gz\n复制到atlas中 如：cp -r ../../apache-atlas-2.0.0-hive-hook/apache-atlas-hive-hook-2.0.0/hook/hive hook/\n配置Hive环境变量 在hive-env.sh中加入'export HIVE_AUX_JARS_PATH=\u0026lt;atlas package\u0026gt;/hook/hiv\n给创建软链接 像前面一样创建一个atlas-application.properties软链接到hive/conf目录下 如：ln -s /home/zhanglun/github/apache-atlas-sources-2.0.0/distro/target/apache-atlas-2.0.0-server/apache-atlas-2.0.0/conf/atlas-application.properties /opt/apache-hive-2.3.4-bin/conf\n复制Hive包到Hook import-hive.sh依赖Hive的jackson一些包（报java.lang.NoSuchMethodError: com.fasterxml.jackson.databind.util.BeanUtil.okNameForGetter错误），把Hive的依赖包复制到钩子的包目录下面 如：cp $HIVE_HOME/lib/jackson-* ../hook/hive/atlas-hive-plugin-impl/\n现在我们尝试执行hook-bin/import-hive.sh(在apache-atlas-2.0.0-hive-hook/apache-atlas-hive-hook-2.0.0目录下）\n现在Atlas里面有两张表，不过一张是Sqoop导入的，一张是Hive导入的，查看Hive导入的血缘关系时候我们发现，他只有自己的一张表（源表）\n其他Kafka和Storm的钩子比较简单我就不介绍详细过程了\n","date":"2019-10-12","externalUrl":null,"permalink":"/posts/backend/software/apacheatlas%E9%83%A8%E7%BD%B2%E5%AE%9E%E8%B7%B5/","section":"博客","summary":"Atlas 是一个可扩展和可扩展的核心基础治理服务集 - 使企业能够有效地","title":"Apache Atlas 2.0.0部署实践","type":"posts"},{"content":" Flink SQL Client 是一个帮助用户直接运行SQL，而不要进行编译运行的工具，首先他支持多个数据源，在Flink1.9.0开始支持了Hive，并且在Flink 1.10.0时候发布了企业级Hive支持，这样就把批处理和流计算结合了起来，这篇博客详解了Hive在Flink SQL Client中的安装和使用，以及探索在上面做的一些复杂开发实践\nHive安装 # 依赖的jar包如下\nantlr4-runtime-4.5.jar flink-connector-hive_2.11-1.9.0.jar antlr-runtime-3.5.2.jar flink-hadoop-compatibility_2.11-1.9.0.jar datanucleus-api-jdo-4.2.4.jar hive-exec-2.3.4.jar datanucleus-api-jdo-5.2.2.jar javax.jdo-3.2.0-m3.jar datanucleus-core-4.1.17.jar datanucleus-rdbms-4.1.9.jar flink-shaded-hadoop-2-uber-2.7.5-8.0.jar 将这些复制到flink/lib目录下面即可，下面是下载链接 1.10.0 flink hive jar Google Driver 下载地址 1.9.0 flink hive jar Google Driver下载地址 百度云提取码： 1.9.0 flink hive 百度云下载地址 百度云提取码: e9c7 链接: 1.10.0 flink hive 百度云下载地址\nHive使用中遇到的问题 # 1. 目前只支持2.3.4和1.2.1 2. 支持的读取的hive类型有限，时间类型只支持1个Date PS: Sqoop导入数据库表到Hive的时候，只能将DateTime和TimeStamp设置为String，否则无法在Client中使用 3. 跨越Catalog读取表的时候Hive不能存放在default（默认）数据库中，否则会解析异常\n例如：想从Hive默认数据库中获取表a，使用insert b select * from hive_catalog.default.a会解析失败\n4. Hive不支持写入分区表，也不支持Overwrite，只支持append模式\n目前flink-1.10已经支持多个Hive版本并且修复了上面所遇到的问题，但是目前在SQL Client里面使用较为麻烦，需要自己安装缺少的jar包(而且版本不兼容的话会出现各种奇奇怪怪的bug），所以我写了一个项目把所以的依赖打包到一起这样只要一行命令就可以生成所需要的依赖，而且支持多个Hive版本\n下面是这个项目的github地址\nFlink SQL Client Hive 依赖生成器\n未完待续\n","date":"2019-10-12","externalUrl":null,"permalink":"/posts/backend/software/flinksqlclient%E5%AE%9E%E6%88%98/","section":"博客","summary":"Flink SQL Client 是一个帮助用户直接运行SQL，而不要进行编译运行的工具","title":"FlinkSQL Client实战","type":"posts"},{"content":" 引言 # 写这篇博客是因为自己从去年就开始学习大数据，但是当自己入了门之后，虽然懂得一点皮毛，但是总觉得自己没有真正掌握这个，尝试了很多方法（做了不少项目，读了几个月源代码），但是总感觉不得要领，或者来说进步缓慢，下面是自己的一些思考\n进步最快的是做项目，但是做项目其实也有下面的问题：\n项目参杂业务 项目只涉及到大数据某个部分 比如以我前面数据转存HBase的项目为例，我遇到的问题是Spark内存不足，以及优化HBase存贮，在解决问题的过程中我的确查了不少资料，学了不少东西，但是通过这个项目之后我只能说我对上面的问题有一定的经验，谈不上精通\n从理论上讲你想真正精通就是不停的使用它，但是对于每个人来说，并不是都有机会去在每个项目中应用它，比如我上个项目Flink SQL管理系统，虽然它是跟Flink有关，但是它大部分都是和前端以及动态编译有关，随着项目越来越多你会发现你越来越熟练某个东西，但是不是精通\n那怎么才能真正的精通某个东西呢，接下来我就下面几点谈谈我的体会\n心态 # 心态这个东西非常的重要，一开始我接触大数据，我飞快的过了一遍大数据的各个软件，把tutorial都运行了一遍，整个人是一种完成任务的心态把所以大数据组件给过了一遍，做完之后感觉大数据也就那么回事，但是其实你只是会运行几个软件而已，就好比一个会用遥控器的猴子，你会看电视不是你有多聪明，而是发明遥控器的人的聪明才让你能那么轻松的看电视\n所以我们要想深入精通某个东西第一个就是摆正自己的心态，心态这个东西怎么端正呢，其实我觉得第一件事就是让自己变成一个“演员”\n这里我插一句，其实我们这一代虽然压力很大，但是有很大一部分年轻人比如我会有一种“佛系”心态，无欲无求，这种心态可能会让你心态变得更加平静，你不会思考那么多问题（为什么我工资没有他高、为什么我买不起房），但是这种心态也会让你丧失变得更加优秀的可能，比如前面我们提到我们要想精通必须端正心态，把这件事放在你心上，作为一个“佛系”看到这可能会想，为什么要把它放到心上，不就是精通嘛多花点时间就可以了\n这种心态就像在一间教室了，你和你同学都在认真听课，你同学被叫起来回答问题，你的心态非常平静，老师提出的问题你根本不会用心去想，你内心只有这个起来讲的傻蛋会怎么出丑，而对于站起来回答问题的那个同学他会精神高度紧张，他会用100%的精力会放在这个“问题”上\n我们回到前面，前面我们提了“演员”这个概念，我们端正心态的具体方法就是把自己变成一个“演员”，或许学这个东西对于你来说只是一个给自己充一下电，在你心中，玩一把LOL、刷一会抖音和充一下电的地位是一样。\n所以第一步就是“假装”你非常热爱这个东西\n为什么要说是假装呢，就以我们自己举例，我们从小到大爱过学习吗？我敢说一万个里面只有一个会说我好爱学习，那个人很有可能是如果他不学习他爸就叫他下地干活，其实我们从小到大一直在被强迫着学习，如果没有老师布置作业我们很有可能一下课就把书包丢到看不到的地方，打开电视机，假如没有考试要求我们背诵课文，我们可能不会把一段又丑又长的文章读了一遍又一遍，等我们到了社会，我们又被领导用工资压着我们做一些内心厌据的工作\n我还好毕业之后从事的是我喜欢做的，所以我经常下班之后会呆在电脑旁，与之相反我有一个建筑朋友，他也是做电脑设计这方面的，他很好奇我每天下班都待在电脑旁边不打游戏就坐在那编程，对于他来说，他下班之后再也不想打开电脑，因为他觉得平常在公司已经非常辛苦了，为什么还要折磨自己。对于他来说工作只是一个维持生活的手段\n其实对于我来说，编程也没有上升到热爱那一个层次，只是比游戏稍微高那么一点而已，这也可以解释有的时候我敲着敲着代码敲累了又打开了虎牙看起直播来了，热爱这个东西就好比打游戏，你越打越开心，而不会越来越累\n所以其实对于我们大部分年轻人来说，第一件事就是把“佛系”心态收起来，变成一个“演员”，假装我们热爱这个东西\n其实李笑来也提到过类似观点，他提过他是学习英语的窍门，他“假装”他背一个单词可以挣5块钱，所以他单词越背越多，最后英语也越来越好，对于我们每个人来说如果你真的想精通某个东西，找到一种能够刺激你的方法去“热爱”它\n插一句，为什么我在这个心态这里用了很多“演员”、“假装”这些词汇，因为从小到大我没有看到一个人真正的热爱某个东西，无论是工作还是学习大家似乎都是一种“妥协”的态度，因为钱而工作，因为击杀的快感而打游戏，很少能看到一个人因为热爱而沉浸在一件事上一辈子，这也可以解释为什么大师那么少，大部分太“博爱”，无法选择出自己最喜爱的\n把人生当做登山，我们面前有很多座山，大部分人登了一会就觉得这座山不是自己的菜，厌倦了，从头开始换另一座山登，而对于我们来说，最好的方法是登了一会虽然觉得厌倦了，“假装”热爱，拼命往上登，到了山顶，视野广阔才能真正的看到自己想要的，这个时候不是随便换个山登，而是“跳”到那座山去。\n大道理我们也不多说了，现在我们面前只有一个朱丽叶，接下来我们谈谈一些我觉得很重要的“术”来帮助我们维持这个心态\n精通的手段 # 问自己问题 # 帮住我们精通某个东西的最重要的方法的是问问题，比如你想精通Hadoop，这个时候你就问自己，什么是Hadoop，为什么叫Hadoop，它有啥用\u0026hellip;.\n这些问题你可以记录下来，在回答过程中你必须要像对待孩子一样对待这些问题，你要充分激发你的求知欲，其实这个同打游戏是一个道理，假如你喜欢打LOL，你会去尝试各种符文天赋，尝试各种英雄各种套路，你拿到一个盲僧你总会问自己，盲僧装备啥符文最强，R闪会不会撞墙。\n问自己问题还有一个关键就是：打破砂锅问到底。接着Hadoop那个问题，当你回答到Hadoop的组成的时候，提到了Namenode，然后在问自己Namenode是什么，如果文件过多会怎么样，高可用和Namenode有什么关系，如果Namenode挂掉会怎么样\u0026hellip;.\n问问题的过程不但是是一个不断了解的过程，也是一个学习的工程，你在学习的过程中遇到的问题越多说明你的短板也越多，我们就像一个修补匠不断对一艘船修修补补，最终它能坚固的如军舰一般\n系统总结 # 不断的提出新的问题就像不断的在一个树枝上分叉，但是假如你一直问下去，你有可能会在寻找问题的过程中迷失自己，或者偏离主航道，比如问大数据最后深入到硬件最后深入到化学方向去了，所以一个系统的归纳总结也非常有帮助\n方法有很多，你可以像我一样写博客，画画思维导图，甚至手写都可以，一切能够帮助你总结归纳都可以\n总结 # 这篇博客简单的介绍了一下我对于怎么精通的个人体会，我个人也还在路上，之所以写这篇也是自己在摸索的过程中磕磕碰碰走了很多弯路，这里谈一下对以前学习过程中的反思吧，我一开始心态就不对，飞快的过了大数据的组件，然后开始做了些项目，但是做完之后也只是熟练的编程而已，接着开始研究源代码，虽然代码都能看懂，但是不知道怎么去看，好像是为了看源代码而看源代码而已，自己对整个系统一窍不通\n后面突然看到一下大数据的面试题，我尝试回答但是好像都模模糊糊，这个时候我才意识到我的问题的关键：“心态”。我自己一直没有摆正自己的心态，我只是把他当做一个工具来用了，就好比电视遥控器，我只是一只会按遥控器的monkey而已。摆正心态说难也难说容易也容易，假如明天马云跟我说你把Hadoop搞精通我给你1000万，那我心态不需要“假装”，我们需要一种手段帮助我们端正心态，每个人可以采取不同的手段来帮助自己，对于我这个佛系青年来说，上面的方法是最好的。\n其实我记得好像小学的时候我就有过这种思考，当初我妈和我姐聊天说为什么同一个妈生的，一个成绩好一个成绩差，我当时直接就说了假如你想成绩好那必须要热爱它，后面我也用这个办法帮我自己学习一些东西，但是苦于没有想到怎么精通的“术”，所以虽然一开始保持100分的热情但是坚持坚持着就放弃了，所以心态不但重要，后面的精通的“术”也非常重要，就像太极，一阴一阳，光有热情没有行动没有用，只有行动没有热情，你也领悟不到精髓。\nPS：其实回头想想这篇博客也非常简单，很多前辈或者先人已经提出来过，不外乎是：端正心态、多问问题、多总结。我自己一开始也想过别人都写过，自己何必多此一举，但是想想虽然这些问题都是老生常谈，但是我们来说，我们都是第一次做人，每个人都是不一样，对于我来说，端正心态怎么端，为什么要端正，端正后干什么这些都是不一样。其实我这篇博客最主要的想谈一下心态，就以我们健身房为例，一遍健身一遍玩手机的都是没有什么肌肉的，那些肌肉大的都是训练的时候拼尽全力，假如你抱着来健身房玩一玩的心态，你永远没法把你的肌肉练出来，而那些为每次动作拼尽全力的人他们的肌肉是最强壮的。\n","date":"2019-10-05","externalUrl":null,"permalink":"/posts/essays/life/%E5%A6%82%E4%BD%95%E7%9C%9F%E6%AD%A3%E7%B2%BE%E9%80%9A/","section":"博客","summary":"引言 # 写这篇博客是因为自己从去年就开始学习大数据，但是当自己","title":"如何真正精通","type":"posts"},{"content":"","date":"2019-05-05","externalUrl":null,"permalink":"/categories/ai/","section":"Categories","summary":"","title":"AI","type":"categories"},{"content":"","date":"2019-05-05","externalUrl":null,"permalink":"/posts/ai/","section":"博客","summary":"","title":"AI","type":"posts"},{"content":"","date":"2019-05-05","externalUrl":null,"permalink":"/tags/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/","section":"Tags","summary":"","title":"推荐系统","type":"tags"},{"content":" 虽然自己刚解决掉一个TB级数据导入“大项目”，但是感觉自己对大数据流程还是缺乏一定对认识，所以想通过一个完整对项目体会一下大数据如何落地再实际产出对，正好@志斌 提供了一个硬盘给我，里面有12年微博对数据，数据量在3TB左右，微博量为65亿，所以就借助这个数据来捣鼓一下推荐系统，我把自己捣鼓的过程写下来，希望能够给其他想了解推荐系统的人一点帮助。\n什么是推荐系统 # 其实从名字上面我们就知道，我们其实就是要做一件事，“推荐”，并把这个东西做成一个系统，推荐很好理解，但是怎么来做呢，我们拿买东西作为例子，我很喜欢吃苹果，所以我经常去超市买苹果，售货员大妈知道我喜欢吃苹果，所以经常推荐我一些好吃苹果，这个就是推荐，他是根据我们历史的消费记录来给我们推荐的\n这算一种非常简单的推荐系统，你喜欢啥我就给你推荐啥，但是有一个问题，假如你喜欢Apple，你买了一个IPhone X，但是如果我再给你推荐IPhone X其实你不会买的（当然也有可能，但是可能性非常低），那一个“高级”的推荐系统应该怎么做呢，你买了IPhone X，你就很有可能会买AirPod、会买IPod、IMac这些，你脑子里面可能会这样想，做一个归类把一类商品都归到一起，比如IPhone，Mac这些，然后只要他买了其中一个，我们就推荐其他，当然这是最容易想到的一个方法，但是这个方法成本太高而且只适用了Apple产品，假如你买小米，你的手机可能是小米的，但是你的贴膜可能不是小米的。\n得亏于大数据时代的到来，我们记录了很多用户的购买记录，这就相当于我们知道这些商品存在一些联系，假如我们能把这种联系找出来，再用一些算法搅拌一下数据，我们就能产生香喷喷的“推荐数据”\n推荐系统原理 # 首先我们来看一下我们有什么数据，最主要的就三张表，第一张用户表（User表），第二张商品表（Item表），第三张用户和商品交互记录表，这个交互记录可能是评分可能是浏览记录可能是购买记录，我们简称为Rank表\n我们把这三个维度组合起来就是一个二维数组，我们可以用一个矩阵来代表这个，如下面所示\n$$\\begin{bmatrix} 5 \u0026amp; 4 \u0026amp; 2 \u0026amp; 1 \u0026amp; 0 \\\\ 4 \u0026amp; 3 \u0026amp; 1 \u0026amp; 2 \u0026amp; 5 \\\\ 1 \u0026amp; 1 \u0026amp; 5 \u0026amp; 0 \u0026amp; 2 \\end{bmatrix}$$\n有三个用户，5件商品，我们从上面伪造的数据可以看出，用户1、2品味很近，而用户三品味与他们完全不同，所以对于最后一件商品，虽然用户1没有评价或者没有购买，我们可以预测他的评价和用户2 差不多，所以我们可以把东西给推荐给用户1，这个是从我们直觉上来感受的，但是计算机可是没有知觉的，我们必须把这些东西量化才能实现智能推荐\n我们用一个欧几里德距离或者皮尔逊相关度来衡量这种品味，其实你可以把它想象为一个N维空间的上每个点的距离，这里我们就不谈公式，我们直接给结果，对于每个用户来说，自己跟自己的品味是100%相近，其他人则有的相同，有的不相同，所以对于用户1，我们可以算出一个“品味值”（我们其实可以不跟自己算因为绝对是1，但是在大型矩阵运算的过程中，其实少算一个比多算一个要划得来）\n$$\\begin{bmatrix} 1 \u0026amp; 0.8 \u0026amp; 0.1 \\end{bmatrix}$$\n我们得到上面的品味值（加权值），所以对于最后一个商品，我们把其他用户评价与他们与用户一的值相乘，我们就把他们对数据的评价可靠性给量化下来了，所以就可以下一步排序输出我们的推荐。\n我们知道现在的大数据强大在于能给所有用户都推荐他们想要的数据，但是从我们上面的描述来看，我们为了获取一个用户的品味值就得对数据进行大量运算，如果一个用户要1s的，一亿个用户就得要3年才能算完，所以我们得将我们的for循环改成矩阵运算，才能通过大数据将所以用户的推荐给算出来\n现在我们来谈谈怎么用矩阵运算来加速这个过程，我们用$$R_{UI}$$ 来代替上面的评价矩阵，我们首先要把$$R_{UI}$$ 给倒置，然后让两个做矩阵乘法，得到”品味矩阵“ $$W_{UU}$$\n$$R_{UI}$$ * $$R_{UI}^T $$ = $$W_{UU}$$\n这个$$W_{UU}$$ 是什么呢，他是一个行数为用户总数，列数也为用户总数的方阵，每一行代表，第i个用户，他与其他用户的品味差距，有了这个我们可以将$$W_{UU}$$ 与 $$R_{UI}$$ 做一个矩阵乘法，这样我们就得到一个$$V_{UI}$$矩阵，这个矩阵就是我们对$$R_{UI}$$的一个预测值，我们就把一些用户没有评价的值给预测出现，接下来就对每一行先做一个过滤，取出用户没有购买的，然后把没有购买的按照大小排序输出\nPS：也可以用点乘一下矩阵进行过滤，就是将没有将用户与商品购买记录生产一个UI矩阵，购买过置0，每购买过置1，这样点乘后购买过无论分多高都变成0了，把0过滤就ok了\n我们来总结一下上面用到的矩阵公式\n$$R_{UI}$$ * $$R_{UI}^T $$ = $$W_{UU}$$ $$W_{UU}$$ * $$R_{UI}$$ = $$V_{UI}$$ 初略一看没有啥问题，但是你要想一下假如用户数非常大，有一千万（很容易达到，QQ微信都10几亿了），那个$$W_{UU}$$ 矩阵的大小有 1千万 * 一千万 = 1e+14 ，我们根本没有足够的内存来存放这个矩阵，而且有一个问题，就是其实假如我们有一千万件商品，但是一个用户一辈子可能就买了几十件，这是一个巨大的稀疏矩阵\n所以虽然我们这个矩阵算法很高效，能通过两次运算就把用户推荐给全部输出，但是假如用户量多或者商品多第一内存不够，第二个矩阵太稀疏了，在没有大数据之前或者计算机还没有现在这么强的时候，大家的解决方法是对数据进行抽样，把大数据分解成为小数据，但是大数据的魅力就是使用全部数据而不是部分，所以我们要使用一些抽象的算法来帮助我们把这个问题给解决了\n首先我们还是要用所以的用户数据，所以的商品数据，但是现在我们做一个抽象，我们假设用户身上有几十种特征来决定他们的品味（比如年龄，性别，性格等），我们直接跳过前面两个计算步骤，我们假设用户特征矩阵$$U_{UP}$$ (每个用户有P个特征，决定他们品味)，对于商品，我们假设就是因为这些特征导致的一个商品特征矩阵$$I_{PI}$$\n最后我们假设，我们这个$$U_{UP}$$ 矩阵乘法$$I_{PI}$$就是 我们上面得到的$$V_{UI}$$ ，也就是公式为\n$$U_{UP}$$ *$$I_{PI}$$ = $$V_{UI}$$\n假如我们能计算出来这个 $$U_{UP}$$和 $$I_{PI}$$，那么我们就基本上算出所以用户的评价了，这个就是ALS的原理，我们利用这个抽象能将原来的亿亿相乘的计算，改成亿与常数P的相乘的计算\n当然具体的原理没有这么简单，涉及到很多矩阵运算的知识，我这里就不多说了，具体可以看下面给的论文《Collaborative Filtering for Implicit Feedback Datasets》，Spark的Mlib中的ALS实现就是基于这篇论文的，也可以参考一下Python的实现，公式推导比较复杂，但是实现还是非常好理解的\nPython实战 # 首先我们来先试试使用Python来快速搭建我们的模型出来\n{'account': '1920286160', 'content': '你的笑容灿烂了一整个夏天!', 'source': 'http://wp1.sina.cn/wap240/5d738637jw1dpmonay1thj.jpg', 'time': '2012-02-01 13:48:39', 'from': '网页版', 'comment': 0, 'forward': 0, 'forward_account': '1567852087', 'forward_content': '民浩君！！ ','forward_id': 'xdasfd21x-s'， 'forword_nickname': '全球流行风尚', 'is_forward_followed': '0'} 我们原始数据全部都是这种字符串，我们没有用户点赞、浏览数据，我们只有用户所发的所以微博数据，而且假如后面有forward_account的值的话，说明这条微博是用户转发的，所以这里我们把用户转发当做一个用户“喜欢”这篇微博，我们系统就是要推荐更多微博给用户“转发”，所以我们把用户账号，和转发ID提取出来当做一次“记录”\n我们把数据全部变成，user-id：forward_id，字符对，然后我们导入到一个pandas 的DataFrame里面去\n我们这里使用一个Python 实现的ALS包implicit，只需要把我们生成的DataFrame转换成为一个压缩矩阵OK了，但是这里要注意的一点是我们所以的用户id和转发id都得变成Int型整数，最好的做法使用\ndf.user.astype('category').cat.codes 比如上面，我们把user这个字符型数字转换成一个整数序号，这样我们得到的就是每个userid的序号\n完整的代码为下面\norigin_data = [(x['forward_id'], x['account'], 1) for x in data if x.get('forward_id')] # data 为json数组 df = pd.DataFrame(origin_data, columns=['item', 'user', 'num']) df['item'] = df.item.astype('category').cat.codes.astype('int64') df['user'] = df.user.astype('category').cat.codes.astype('int64') from scipy import sparse # convert df to martrix item_user_data = sparse.csr_matrix(df.values) # train import implicit model = implicit.als.AlternatingLeastSquares(factors=50) model.fit(item_user_data) # recommend items for a user user_items = item_user_data.T.tocsr() # recommend user model.recommend(1, user_items=user_items, N = 5) 只要简单的几行代码我们就实现了一个协同过滤的算法实现，从Python代码我们可以知道，最关键的地方就是生成训练数据，然后其他都交给算法了，接下来我们来尝试实现一个使用Spark加载全部数据，来实现对几十亿微博数据的推荐\nSpark实战 # 在前面我们把3个T的微博数据的用户转发数据抽取出来，最后数据量在35亿左右，数据量在100G左右，这个数据量还是有的大的，首先内存肯定放不下，假如我们使用前面Python的方式来实现的话，我们的内存至少要200个G，所以我们得使用Spark的内存加磁盘运算来实现这个推荐系统\n生成用户和转发ID的序列值 # 前面Python的实现100个G的数据排序，基本的原理就是Hash表进行排序，然而我们内存不足以放下这100G的数据，所以我们要使用MapReduce来“消化”这个数据，所以我们要借助Hive来进行一个排序，一方面我们可以生成用户和物品的唯一ID，等我们产生推荐结果后也可以通过HIVE来生成对应的数据\n第一步我们将数据导入到HIVE中\n首先创表\ncreate table record(item string, account int, time int) row format delimited fields terminated by ',' LINES TERMINATED BY '\\n' stored as textfile; 这里我还额外导入了一个时间进去，为的是方便以后做线上预测\n然后我们把文件导入进去（文件存贮在/weibo/all_data.txt中）\nload data local inpath '/weibo/all_data.txt' into table record; 接下来我们生成用户表和物品表\ncreate table items as select row_number() over (order by a.item) as id ,a.item from (select distinct item from record) a ; create table users as select row_number() over (order by a.account) as id ,a.account from (select distinct account from record) a ; 我们使用HIVE的row_number函数来生成一个唯一序列自增值，每一个值对应一个用户，最后我们生成我们训练表，最好使用HIVE2来进行导入，因为HIVE1的row_number会存贮所以的id值，由于我们数据量有35个亿，内存会爆炸，HIVE2优化了使用一个流来存贮\ncreate table train as select account_id, item_id, count(*) as num from (select a.id as item_id, b.id as account_id from record left join items a on record.item = a.item left join users b on record.account = b.account) a group by item_id, account_id ; 接下来我们在Spark里面调用我们Mlib的ALS模型，首先我们从HIVE中获取数据\nval df = sql(\u0026quot;select * from train\u0026quot;) 然后我们从将数据导入ALS模型进行训练，同Python比较类似，我就直接给出代码\nval ratings = df.map(row =\u0026gt; Rating(row.getAs[Int](\u0026quot;item_id\u0026quot;).toInt, row.getAs[Int](\u0026quot;account_id\u0026quot;).toInt, row.getAs[Long](\u0026quot;num\u0026quot;).toDouble)).rdd val rank = 10 val numIterations = 50 val model = ALS.train(ratings, rank, numIterations, 0.01) val usersProducts = ratings.map { case Rating(user, product, rate) =\u0026gt; (user, product) } val predictions = model.predict(usersProducts).map { case Rating(user, product, rate) =\u0026gt; ((user, product), rate) } val ratesAndPreds = ratings.map { case Rating(user, product, rate) =\u0026gt; ((user, product), rate) }.join(predictions) val MSE = ratesAndPreds.map { case ((user, product), (r1, r2)) =\u0026gt; val err = (r1 - r2) err * err }.mean() println(s\u0026quot;Mean Squared Error = $MSE\u0026quot;) 最终我们这35亿数据用时4个半小时跑完模型并计算出MSE （均方误差），当然由于我的核心比较小，假如在一个大集群上面的话速度会快很多，值得注意的是，在Spark运行这35亿数据的时候，物理磁盘占用达到800多个G，所以在哪里省下来（内存）的就得从哪里还回来（磁盘）O(∩_∩)O哈哈~\n总结 # 我们这次只是完成了一个非常简陋的推荐系统，只是具备简单的推荐功能，我们还没有考虑到一些“冷启动”问题，一个“智能”的推荐系统需要考虑到很多方面，我的推荐系统还有很多地方需要完善，但是从我的这个Demo里面也可以看到其实大数据也是非常“简单”的，由于数据量太大，你只能使用最简洁的代码来训练你的模型，你的算法要最大可能性满足分布式要求，推荐系统最核心的地方还是如何将你的推荐算法变成一个优秀的分布式算法，我们这篇博客只是走马观花的谈了一下它的原理，看看有空好好研究一下Spark分布式算法是怎么实现的。\n资料 # ALS原理介绍\nPython实现ALS\nSpark Mlib ALS 论文\n","date":"2019-05-05","externalUrl":null,"permalink":"/posts/ai/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E5%BE%AE%E5%8D%9A%E6%95%B0%E6%8D%AE%E5%AE%9E%E6%88%98/","section":"博客","summary":"虽然自己刚解决掉一个TB级数据导入“大项目”，但是感觉自己对","title":"推荐系统微博数据实战","type":"posts"},{"content":" 项目背景\n这个项目是深圳一家证卷公司一个TB级日志离线导入项目，当数据达到10T左右的时候，他们的存贮架构以及检索系统直接奔溃，在朋友的推荐下，我负责对这个项目进行整体的重构以及优化，对于我一个大数据新手来说这是一个挑战也是一个学习的机会，最终幸不辱命最终完成了系统的优化，由原来的48小时导入优化至7个小时，并且提供亚秒级的查询检索，下面就是我对这个项目的总结\n历史方案介绍 # 集群架构\n集群由8台高性能服务器组成，其中两台负责接收历史日志包，两台提供ES和Spark服务，其他四台提供HDFS存贮服务，总存贮量在200TB，其中我们导入主要在两台高性能服务器上面，每台内存256GB，总计512G内存\n历史方案\n原来的解决方案是日志被解析完成之后导入两份数据分别存贮到ElasticSearch（下面简称ES）和Kafka之中，其中存贮到Kafka之中的数据会被Spark读取，然后写入到HBase\n架构优化 # 读取和存贮优化 # 历史解决方案对日志数据的存贮是一个并行化的，但是由于项目对数据准确性要求严格，假如其中ES导入或者HBase导入失败之后，所以的数据都得重新生成，但是原来架构不提供删除HBase数据的功能（能实现但是不能检测是否删除），所以在读取方面我丢弃使用Kafka并行导入的方案，待ES导入完成并且校对完成后再从ES中获取数据导入到HBase中\n原来的导入是直接使用HBase提供的Put方案，单个插入到HBase当中，由于我们一次导入的量多的话大概在10个亿左右，虽然HBase能够承受住压力但是由于行键设计导致会造成region的\u0026quot;脑裂\u0026quot;，原来解决方案为了避免这个现象进行了region的预分配，由于这个预分配导致了另外的一个Bug，后面会提到。所以为了提高导入效率和避免脑裂，我采用了HBase的Bulk Load将数据直接\u0026quot;存\u0026quot;至HBase\n回头看看，其实大数据解决方案非常简单，当我刚接触项目的时候，在看原来的解决方案，惊讶的发现他们的代码其实非常简单，就像教程里面的demo一样，但是随着在实际接触这个\u0026quot;大\u0026quot;数据的过程中，大数据考验的是你代码的\u0026quot;结实\u0026quot;，能不能扛的住，什么花里胡哨的骚操作都没有，所以大数据的代码非常简洁质朴，如果说那些优雅的代码是面对对象的话，那么大数据的代码就是面对数据的。所以我接下来也不谈大数据框架，谈一谈我在这个项目中学到的大数据处理经验\n提高处理速度 # 读取速度\n大数据处理最核心的点就是分而治之，也就是我们常说的MapReduce中Map，无论数据有多大，只要我们分的够小，每一份我们计算机都很很快处理，那么我同时运行这些，意味着我就能在一个很短的时间内跑一个巨大的数据。\n由于我们是从ES中读取数据，ES的Map的个数是由分片（shards）来控制的ES的分片默认是十片（5个原始5个备份），假如我们采用默认设置，我们读取数据Map的大小最大为10（具体跟你给的Container大小和每个核心数的大小有关，给足的话就是10），这里要谈一下这个项目中的分片设计，分片这个东西对于每一个Index来说，不是越多越好，越多读取速度快，但是写入慢，反之。所以这个项目使用别名的方法来保证分片不会膨胀，当一个Index数据大于20GB的时候，会自动新建一个别的Index，比如原来是es _index_0，写满20G后，再新建一个 es_index_1，然后把新数据导入到里面去，然后每个Index固定分片为4片，这样所有的数据会均匀的分布到ES中去\n这种解决方法带来一个问题就是，我们获取数据的时候只能通过es_index_*，而且我们的ES中每次导入的Map.size只能是4，所以无论给Spark任务多少个Container多少个核心，他的Task任务最大也就是4个，一开始我一直在琢磨是否能够提高这个Task来加速数据读取，我尝试使用ES时间索引把数据切分成多份，并行跑多个Spark任务，但是无论我并行多少个，从ES一个月导出来时间总和单个Spark任务读取速度一致\n出现这个问题的原因主要是磁盘的性能关系，由于Bulk Load有一个很重要的部分就是Shuffle排序，所以数据在导入HBase之前一定会落盘，所以读取由两个条件限制，ES的获取速度和磁盘写入速度，由于ES读取速度也跟磁盘读取速度有关，所以读取基本上由磁盘的性能有关，由于集群Spark跑在机械硬盘上，除非换SSD否则在软件上优化无法取得较大的收益\n所以我放弃在读取上面优化，接下来介绍我在处理上面的优化\n处理速度\n其实导入到HBase的原理非常简单，只要使用Spark生成一个HFile然后调用HRegion将数据导入到HBase中去，HFile你可以看着一个排序好的文件，但是如何将1TB数据排序并在内存中生成HFile文件（可能比1TB小如果采用压缩的话，不压缩比原始文件要大得多）这个步骤就很复杂了\nSpark作为大数据界的神兵利器，处理PB级的数据也不在话下，但是怎么处理呢，简单来说就是三个和尚挑水，现在缸很大，但是和尚很多，如果要像迅速把缸里的水挑完，那就让很多和尚通力合作，同时去挑，但是如果里面10个和尚，9个是不干活，只有一个挑水，那么这个任务就花很长时间来完成，这个就是我们说的数据倾斜\n所以其实提高处理速度的方法只有一个把任务均分给CPU每个核心，接下来我就这个项目来谈一谈我遇到的数据倾斜问题\n首先我们要知道一个前面我们讲了Spark作为大数据上的神兵利器，他不会假设用户内存能装下所有的数据，但是用户的磁盘一定能装下所有的数据，我们其实要Spark做的最主要的一件事就是对数据的RowKey（HBase的行键）进行排序，然后把排好序的文件导入到HBase中，排序这个东西最主要的是要把所有的数据都放到一起排一遍，也就是说1个T的数据至少都得放到内存或者磁盘里面来进行排序\n我们接下来缕缕Spark处理数据的流程，其实你可以在UI界面上看到所以的stage，前面读取数据算一个Map stage，接下来我们要完成，排序的stage，还有保存数据的stage，按照正常流程，每个stage都会做一个Shuffle，也就是理论上我们总共要做两次Shuffle，然而TB级数据的Shuffle在机械硬盘上的速度非常慢（相比SSD），所以我们就的优化Shuffle过程\n我们先来谈一下一个正常大数据的排序怎么样，我们用打擂台来打比方，首先我们将所有选手分成10个小组，等小组赛打完，所以等小组都分好名次，然后我们搞一个10个人的擂台，小组第一名全部上擂台打比赛，打完决定出第一名，第一名成为擂主出擂台，然后这个时候再从其他其他小组中找一个积分最高的上来，继续打擂台，打完再找出第一名，继续上面的循环，最后每个人都出了擂台，名次也排练出来了\n这个排序方法是Spark早期使用的方法，这个方法可以面对超大数据，假设数据有10个T，我只有1M内存，我也能完成数据的排序，只是每次排1M，然后写文件，这种方式可以面对海量数据，但是有一个弊端，在排完序后每次必须要把所有的文件全部都打开读取\u0026quot;第一名\u0026quot;，也就是说我们做完一次Map之后，还的进行一次Reduce\n那怎么来去掉这个Reduce呢，考虑这种情况，我们在分组的时候，由一个\u0026quot;裁判\u0026quot;把每个人公正的计算他原来的历史积分在什么档次，然后把他分在那个档次的组，等所有组打完，排名就出来了，你属于哪个档第几名，也就知道你的排名了\n这个方法就是预先评估你的组数，但是这个也有一个问题，假如选手水平层次不齐，强者太少，弱者太多，那么有些组非常多的人打比赛，有的组没有人打比赛，这样，最后总耗时是那个人最多的小组的时间\n接下来我们回到上面的Shuffle过程，如果我们把那个排序过程使用上面的预估方法，那么我们就能在减少一次Shuffle过程，但是这个也带来了一个风险，就是预估如果失败（HBase会加载HFile失败）或者预估不均会导致运行异常\n所以我采用Spark提供的sortWithinPartitions方法来ES数据进行一个分区并排序，分区的方法是通过账号ID来进行，很不幸线上数据部分账号异常，一个月某些账号占了总数的40%左右，这个就导致我们预估不均，那如何来解决这个问题呢，解决方案由很多，由于我们数据只有少量资金账户异常，所以我们采用最简单的方案，分多批次跑，每个任务分剔除大账户的部分，其他的任务就是单个大账户单独跑，得益ES强大的统计功能，只需要执行几次HTTP请求就能完成设计\n至此整个数据优化导入部分就说完了，当然还有其他优化的地方没有讲，比如使用堆外内存加速排序，使用更大的Shuffle cache优化写入，切分更小的块避免磁盘split …..\nPS：亚秒级的查询主要通过Spark一个聚合生成HBase一张小表来实现，当然也可以使用MySql这些关系型数据库\n总结 # 记得我以前去面试大数据岗位的时候，我问面试官怎么才能提高自己大数据能力，他说必须要通过实践才能体会，经过这次项目我也感受到了，只有真正的面对大数据，你才能提高自己的大数据能力，小数据一下子就跑完了，完全体会不到内存OOM的\u0026quot;乐趣\u0026quot;，大数据的难点在于了解数据如何被处理，以及对大数据各个组件“协作”对经验。\n","date":"2019-04-13","externalUrl":null,"permalink":"/posts/backend/framework/bigdata/tb%E7%BA%A7%E6%95%B0%E6%8D%AE%E5%AD%98%E8%B4%AE%E7%BB%8F%E9%AA%8C%E6%80%BB%E7%BB%93/","section":"博客","summary":"项目背景 这个项目是深圳一家证卷公司一个TB级日志离线导入项目","title":"TB级数据存贮经验总结","type":"posts"},{"content":"","date":"2019-03-17","externalUrl":null,"permalink":"/categories/%E7%BC%96%E7%A8%8B/","section":"Categories","summary":"","title":"编程","type":"categories"},{"content":"","date":"2019-03-17","externalUrl":null,"permalink":"/posts/essays/programming/","section":"博客","summary":"","title":"编程","type":"posts"},{"content":"","date":"2019-03-17","externalUrl":null,"permalink":"/tags/%E9%98%85%E8%AF%BB%E6%80%BB%E7%BB%93/","section":"Tags","summary":"","title":"阅读总结","type":"tags"},{"content":" ","date":"2019-03-17","externalUrl":null,"permalink":"/posts/essays/programming/book/%E9%87%8D%E6%9E%84-%E6%80%BB%E7%BB%93/","section":"博客","summary":"","title":"重构-总结","type":"posts"},{"content":" 很早就听说过思维导图，但是怎么去画，怎么让它帮助你去学习，我以前不得其道，以为就像记老师讲的笔记一样，尝试几次后感觉无用就弃之高阁了，这次在知乎上看到一篇回答感觉很是在理，就实践了一番\n下面是原答案思维导图真的有效吗？ - YJango的回答 - 知乎\n我按照这个方法重新把\u0026laquo;代码整洁之道\u0026raquo;读了一篇，受益匪浅，下面是我的思维导图\n一千个人有一千个哈莫雷特，所以我就不写自己的看法了\n","date":"2019-03-13","externalUrl":null,"permalink":"/posts/essays/programming/book/%E4%BB%A3%E7%A0%81%E6%95%B4%E6%B4%81%E4%B9%8B%E9%81%93-%E6%80%BB%E7%BB%93/","section":"博客","summary":"很早就听说过思维导图，但是怎么去画，怎么让它帮助你去学习，我","title":"代码整洁之道-总结","type":"posts"},{"content":" 由于网上的教程大多数是Windows下的，而且都是介绍怎么打包Java的Jar包，关于Scala相关的比较少，因此我踩了不少坑才成功的把包发布到Maven中央仓库，你们可以才中央仓库里面搜到我的包\n如何你想直接使用ctrl-c + ctrl-v那么你直接去我的github上的最小化打包版本把pom.xml里面的build给复制下来就可以了（前提是你已经把gpg给配置好了）。\nPS：因为这篇主要是介绍步骤所以就用中文写了\n第一步：选择用什么打包 # 由于Scala既可以选择用SBT打包，又可以用maven打包，所以你要先选择使用什么来打包，一开始我使用SBT来尝试进行打包上传，但是一直卡在没法配置好SBT的gpg插件（这个主要是对你的文件进行签名），我甚至到Github去看别人的项目是怎么配置的，但是我搜到的Scala项目大部分都没有选择把包发到sonatype去，有的是自己搭建中央仓库有的是根本没有提供中央仓库下载的。虽然我很想用SBT把Scala打包（毕竟是专门给Scala用的），但是那个插件 实在是卡的死死的，而且文档太少了，官网上的SBT似乎一直是用的0.13.5+，但是我用的是1.2+，在选择低版本还是选择换一种打包方式面前我选择听从大牛的意见，选择使用maven进行对Scala打包（毕竟Scala杀手级应用Spark也是用的maven进行打包）\n而且SBT有一个特别困扰我的地方，虽然我一直用阿里源，但是SBT更新依赖的速度实在是太慢了，无论是新建项目还是增加第三方包，相比maven它的速度都特别慢\n当然我做不到不代表大家做不到，如果有人能够搞定用Maven打包，记得在Github上发个issue告诉我\n第二步：在中央仓库上面新建项目 # 为了将包发到maven中央库，我们得借助【sonatype]4来帮我们上传，首先你得去上面注册一个用户\n相比于Python发包，Java有一个group的概念，也就是说每个包都属于不同的组织，比如说Java的核心包java.lang也可以看做是一个组织，在Pypi发包你只要不重名就行，但是在maven中央库发包，你可以重名但是不能重复组织名，由于我有个zhanglun.me的域名，所以我就新建了一个me.zhanglun.ahocorasick组织，你也可以理解组织就是一个网址，我新建了一个ahocorasick.zhanglun.me的网址，由于名字我们没法规定重名，但是网址可以，所以在sonatype上新建一个项目后你得要有帮你审核一下这个域名是否是你的（如果你使用com.github.xxx来作为域名的话那就不要审核了）\n随便找篇教程新建完项目，等你的项目变成Resolved，接下来我们就来配置上传的秘钥\n第三步： 上传配置 # 相比于Python直接将打包好的egg包发到pypi，sonatype需要你对你上传的文件都签名以验证安全，怎么签名呢，借助一个开源的GnuPG，我们只要自己制作一个秘钥，然后上传到秘钥服务器，然后我们就可以用这个秘钥来对我们的文件进行签名（会产生一个.asc 文件，里面是签名值）\n所以关键就是把这个秘钥产生并且上传到秘钥服务器，在Ubuntu上有两个版本的GnuPG，一个是1.0版一个是2.0版，我试过使用1.0版但是没法传到秘钥服务器上面，所以大家还是最好使用2.0来生成秘钥和上传到服务器。\n首先生成key，全部选择默认进行，填上你的个人信息，最后就会生成一个key\ngpg2 --gen-key 里面会让你输入一个口令，你记住就行，等你上传的时候会弹出一个框让你输入这个口令\n生成完后使用下面命令列出key\ngpg2 --list-key /home/zhanglun/.gnupg/pubring.gpg --------------------------------- pub rsa2048/47DC71B6 2019-01-12 [SC] uid [ultimate] zhanglun \u0026lt;zhanglun.me@gmail.com\u0026gt; sub rsa2048/985EE474 2019-01-12 [E] pub rsa2048/D0516023 2019-01-12 [SC] uid [ultimate] zhanglun \u0026lt;zhanglun.me@gmail.com\u0026gt; sub rsa2048/EE34357C 2019-01-12 [E] 我生成好几个没有关系，随便找到一行pub ，例如第三行的，47DC71B6就是你得公钥\n使用下面命令把秘钥发到秘钥服务器（keyserver你可以在网上随便找一个，只有能用就会同步到全球）\ngpg2 --keyserver pool.sks-keyservers.net --send-keys 47DC71B6 接下来我们就要配置maven来使用这个秘钥，首先在pom.xml里面加上这个插件\n\u0026lt;plugin\u0026gt; \u0026lt;groupId\u0026gt;org.apache.maven.plugins\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;maven-gpg-plugin\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;${version.maven.plugin.gpg}\u0026lt;/version\u0026gt; \u0026lt;executions\u0026gt; \u0026lt;execution\u0026gt; \u0026lt;id\u0026gt;sign-artifacts\u0026lt;/id\u0026gt; \u0026lt;phase\u0026gt;verify\u0026lt;/phase\u0026gt; \u0026lt;goals\u0026gt; \u0026lt;goal\u0026gt;sign\u0026lt;/goal\u0026gt; \u0026lt;/goals\u0026gt; \u0026lt;/execution\u0026gt; \u0026lt;/executions\u0026gt; \u0026lt;/plugin\u0026gt; 它能自动帮你使用gpg来进行签名（默认使用gpg待会我们得改成gpg2，否则会报错），这里要注意一点假如你计算机上面有很多秘钥，如果你不指定那个秘钥来进行签名那也会报错，我们要在~/.m2/settings.xml 上配置一下（注意不是你项目目录下面）\n在settings.xml的settings/profiles节点下面下面加上配置\n\u0026lt;settings\u0026gt; \u0026lt;profiles\u0026gt; .... \u0026lt;profile\u0026gt; \u0026lt;id\u0026gt;oss\u0026lt;/id\u0026gt; \u0026lt;activation\u0026gt; \u0026lt;activeByDefault\u0026gt;true\u0026lt;/activeByDefault\u0026gt; \u0026lt;/activation\u0026gt; \u0026lt;properties\u0026gt; \u0026lt;gpg.executable\u0026gt;gpg2\u0026lt;/gpg.executable\u0026gt; \u0026lt;gpg.keyname\u0026gt;47DC71B6\u0026lt;/gpg.keyname\u0026gt; \u0026lt;/properties\u0026gt; \u0026lt;/profile\u0026gt; ..... \u0026lt;/profiles\u0026gt; \u0026lt;/settings\u0026gt; id就是这个配置的名字，gpg.keyname就是秘钥的名字，gpg.executable就是选择使用gpg2（默认使用1.0版本也就是gpg）\n配置完这个你可以mvn install一下，你可以看到生成的jar都已经有一个.asc文件，接下来就是配置sonatype的账号密码，在pom.xml配置文件里面加上一个\n\u0026lt;distributionManagement\u0026gt; \u0026lt;snapshotRepository\u0026gt; \u0026lt;id\u0026gt;oss\u0026lt;/id\u0026gt; \u0026lt;url\u0026gt;https://oss.sonatype.org/content/repositories/snapshots\u0026lt;/url\u0026gt; \u0026lt;/snapshotRepository\u0026gt; \u0026lt;site\u0026gt; \u0026lt;id\u0026gt;master\u0026lt;/id\u0026gt; \u0026lt;name\u0026gt;Efficient longest keyword string matching\u0026lt;/name\u0026gt; \u0026lt;url\u0026gt;git@github.com:mrzhangboss/ahocorasick.git\u0026lt;/url\u0026gt; \u0026lt;/site\u0026gt; \u0026lt;repository\u0026gt; \u0026lt;id\u0026gt;oss\u0026lt;/id\u0026gt; \u0026lt;url\u0026gt;https://oss.sonatype.org/service/local/staging/deploy/maven2/\u0026lt;/url\u0026gt; \u0026lt;/repository\u0026gt; \u0026lt;/distributionManagement\u0026gt; 在这里我们指定了用oss这个作为上传账号，接下啦我们在~/.m2/settings.xml 加上sonatype的用户名和密码\n\u0026lt;settings\u0026gt; \u0026lt;servers\u0026gt; .... \u0026lt;server\u0026gt; \u0026lt;id\u0026gt;oss\u0026lt;/id\u0026gt; \u0026lt;username\u0026gt;your usename\u0026lt;/username\u0026gt; \u0026lt;password\u0026gt;your password\u0026lt;/password\u0026gt; \u0026lt;/server\u0026gt; .... \u0026lt;/servers\u0026gt; \u0026lt;/settings\u0026gt; 在settings/servers节点下加入一个server节点，填上你的账号密码就ok了，在这里你就可以使用mvn clean deploy进行上传了。\n第四部：发布配置 # 前面我们已经能够上传，但是我们现在上传的时候假如我们的版本号上面有SNAPSHOT这个的话，我们是不能把他发布出去的，带有那个代表那个只是实验性质，虽然我们能够把它上传上去但是不能在maven中央仓库里面看到。\n当然这个只是一个要求，为了能够上传你还必须满足两个条件，第一个要把源文件上传上去，第二个就是你的文档上传上去，而且sonatype采用了一个工作流的概念，你要上传中央库必须要进过 上传release-\u0026gt; 关闭release -\u0026gt; 发布release，当然所以的一切都可以用插件来完成，接下啦我们先介绍两个很重要的插件，打包源文件和打包文档。\n由于我们要打包Scala项目，所以第一步就是把Scala编译成Java字节码，这一步我们得借助scala-maven-plugin来完成\n我们在在build里面加入下面的插件\n\u0026lt;plugin\u0026gt; \u0026lt;groupId\u0026gt;net.alchim31.maven\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;scala-maven-plugin\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;${version.scala.maven.plugin}\u0026lt;/version\u0026gt; \u0026lt;executions\u0026gt; \u0026lt;execution\u0026gt; \u0026lt;goals\u0026gt; \u0026lt;goal\u0026gt;compile\u0026lt;/goal\u0026gt; \u0026lt;goal\u0026gt;testCompile\u0026lt;/goal\u0026gt; \u0026lt;/goals\u0026gt; \u0026lt;/execution\u0026gt; \u0026lt;execution\u0026gt; \u0026lt;id\u0026gt;attach-javadocs\u0026lt;/id\u0026gt; \u0026lt;goals\u0026gt; \u0026lt;goal\u0026gt;doc-jar\u0026lt;/goal\u0026gt; \u0026lt;/goals\u0026gt; \u0026lt;/execution\u0026gt; \u0026lt;/executions\u0026gt; \u0026lt;/plugin\u0026gt; 它带了连个命令一个是compile + testCompile（编译），一个是attach-javadocs（打包scaladoc），加入这个插件后你可以试一下mvn install 它会帮你把javadoc.jar生成出来。\n添加source文件的插件就简单了，Java和Scala都可以使用这个插件来生成jar文件（只是简单的复制文件而已）\n加入下面这个插件就能实现打包源文件了\n\u0026lt;plugin\u0026gt; \u0026lt;groupId\u0026gt;org.apache.maven.plugins\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;maven-source-plugin\u0026lt;/artifactId\u0026gt; \u0026lt;executions\u0026gt; \u0026lt;execution\u0026gt; \u0026lt;id\u0026gt;attach-sources\u0026lt;/id\u0026gt; \u0026lt;goals\u0026gt; \u0026lt;goal\u0026gt;jar\u0026lt;/goal\u0026gt; \u0026lt;/goals\u0026gt; \u0026lt;/execution\u0026gt; \u0026lt;/executions\u0026gt; \u0026lt;/plugin\u0026gt; 加上这两个插件你就可以发布release版本了（第一次你得要跟工作人员comment一下才能上传到中央库，以后就不要了）。现在你就可以只要执行mvn clean deploy就能在管理页面的Staging Repositories下面看到你构件，你只要选择最后一个，然后点击close，close结束之后在点击release就能在maven中央库看到你的开源包了，是不是很激动\n但是每次都要登录上面去点击很繁琐，所以你可以安装下面的插件帮你直接发布，不需要登录网站\n加上下面的插件就能自动帮你发布了\n\u0026lt;plugin\u0026gt; \u0026lt;groupId\u0026gt;org.sonatype.plugins\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;nexus-staging-maven-plugin\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;${version.maven.plugin.nexus.staging}\u0026lt;/version\u0026gt; \u0026lt;extensions\u0026gt;true\u0026lt;/extensions\u0026gt; \u0026lt;configuration\u0026gt; \u0026lt;serverId\u0026gt;oss\u0026lt;/serverId\u0026gt; \u0026lt;nexusUrl\u0026gt;https://oss.sonatype.org/\u0026lt;/nexusUrl\u0026gt; \u0026lt;autoReleaseAfterClose\u0026gt;true\u0026lt;/autoReleaseAfterClose\u0026gt; \u0026lt;/configuration\u0026gt; \u0026lt;/plugin\u0026gt; PS：我在项目里面把这个插件注释掉了，你们可以把注释取消\n总结 # 这篇博文主要是把发布涉及到的库以及知识点简单的介绍了一下，其实发布并不难，很多时候我们只是卡在某一步上，只要我们知道原理，就能把钉子拔掉，顺利上路，在这里顺便推广一下我的项目，这个项目基于Aho-corasick自动匹配关键词，使用统计方法来对输出最匹配路径，项目展示的Demo在http://ahocorasick.zhanglun.me/ ，随意输入地点能够迅速匹配出相关城市，比如打入“樟树”能够找到“江西省-宜春市-樟树市”，假如你服务器内存够的话还可以根据村委会名字来寻找到你的地点，而且搜索的速度只和你的地点名长度有关，不会随着关键词的增长而变慢\n引用 # SNAPSHOT\n","date":"2019-01-13","externalUrl":null,"permalink":"/posts/backend/framework/java/%E5%A6%82%E4%BD%95%E5%9C%A8ubuntu%E4%B8%8A%E5%8F%91%E5%B8%83scalajar%E5%8C%85%E5%88%B0maven%E4%B8%AD%E5%A4%AE%E4%BB%93%E5%BA%93/","section":"博客","summary":"由于网上的教程大多数是Windows下的，而且都是介绍怎么打","title":"如何在Ubuntu上发布Scala Jar包到Maven中央仓库","type":"posts"},{"content":" Recently I found that I\u0026rsquo;m kind of over talking when I speak with others, so I want to train my conclusion of solving problem. Here are some problems I meet when I read books of Big Data.\nNo 1. Why Spark does not use map-reduce? Disk reading is too slow to complete big data analyze.Map-reduce is a shuffle which meanings parallizing.Spark just use memory to complete parallizing a huge problem.\nNo 2. How Hadoop to append data? Just use\n","date":"2018-11-07","externalUrl":null,"permalink":"/posts/backend/framework/bigdata/bigdataquestions/","section":"博客","summary":"Recently I found that I\u0026rsquo;m kind of over talking when I speak with others, so I want to train my conclusion of solving problem. Here are some problems I meet","title":"Big Data Questions","type":"posts"},{"content":" Recently I had finshed reading one book : \u0026laquo; Scala for The Impatient\u0026raquo;, I found a lot intrest thing during reading this book.Cause I had known Python and Java before, I can see mixed fetures of Scala between Python and Java.This blog is my thought of studying a new language from other learned language.\nIntroudction # What is Scala, Scala is a new lanuage ? No technically Scala is Java. They both create *.class file, and run on JVM.The most different between them: Scala use .scala as suffix and Java use .java as suffix.\nThe difference of two lanuage is the syntax which deeply affectting your coding style.This blog will introuce the most different of two lanuage and we will find out why is it.\nStronger Object-Oriented # Scala and Java both generate Java bytecode(we will call it bytecode later) file, and let it run on the JVM.The bytecode is totally a object-oriented format.\nThe basic structure of bytecode is class, but we will see something wrong in Java.For examples.In Java library, they give us two type int and Integer. One is C level data type, Other is a really Object Class.This is a huge problem, do we really need a Object Class like Integer, this Class make a C level data type not so effective (It\u0026rsquo;s huge than int).The only reason we need it for collection data type like List,Set,Map and so on can only load Class Object.So we not only make it ugly in Java but also we waste lot of time packing and unpacking between Class-Object and C-level Object.\nThis problem is fixed or improve by Scala.In Scala libray, there are no int or long or char anymore.There are all to be Class.And in Scala offical collection (scala.collection) we can load this Class-Object as a c-level data type undergroud. This trick is done by Manifest type, using reflect to make the collections to save base type like int，char and so on as its\u0026rsquo; elements.\nIn this way, we not only delete c-level data type, but also delete the un-object-oriented part in a object-oriented lanuage.This is improvement of Scala.We needn\u0026rsquo;t pack and unpack again and again.But we share the speed of C level data type without touch it.\nEnhance Function # Java8 support lambda fuction in 2014,so in a long time if you want use something like function, you can use a anonymous class which implemented some interface. eg:\nThread thread = new Thread(new Runnable() { @Override public void run() { // do something } }); You can see if we want run a thread we must initialize one object.Actually you only need the run function.\nLet\u0026rsquo;s see what Python do with function.If you want to call one object as function you only need give a object __call__ function.Then you can call as obj(*arg, **kwds).The Python will transform it to obj.__call__(*arg, **kwds).\nWe can learn it from Python, if Java want to call obj as function so he need use some unnified rule.So when you can write just function and Java will turn it to some Interface.\nScalabuild 23 generic Class from Function0 to Function22 to help us write our function as a object.By the way, in Java, you maybe use avoid in a method to announcing no nedd to return.In Scala, you must have a return, if you really don\u0026rsquo;t need one, just return Unit which is same as void.\nNow you can use fuction as a veriables now in Scala\ndef rInt(): Int = 1 val k: () =\u0026gt; Int = rInt _ In upper, we use fuction rInt as a virable of k, () =\u0026gt; Int is tell the Scala the type of function.So when call some thing like k(), it will tranceform to k.applay().\nScala build a rule for us like HTTP to Internet.Maybe this is not so meaning just building 23 trait (like interface in Java).But you will a huge power when on the basement.It kind like HTTP, thanks to it, the Internet give a amazing world to us.\nDo More # Scala is shorter than Java.In my word, I think Java is kind of wordy.If you want to print some word, you nedd use System.out.println, maybe you can static import to reduce it(use import static System.out).Also there are too mush strict rule in Java.Such as: Only one public class in a file.Your package need use same physcial address.etc.\nJava is strict lanuage, you aren\u0026rsquo;t trust by the compiler.While Scala give us more freedoom.You can save a lot of time in it.\nLet us see what Scala do for us.\nadd varies and method in builder just use one line like\nclass T(var name:String) it\u0026rsquo;s equal\nclass T{ private String name; public String getName(){ return this.name; } public String setName(String name){ this.name = name; } public T(String name) { this.name = name; } } saving brackets if a fuction use no params, you can save brackets.It maybe confused if you want a function not just call them.Just add _ after your function.Scala will know you just need a function.\nuse symbol as your function In Java, we are not allowed to use *,/，+= as your class method, Scala open it, you can what you like.Sometimes, + will be more clearly than just add.\nadd your patch to other library In the your kingdom of Scala, you can simple add any patch to other class without recompiling Java again.\nimplicit def addKing(w:String) = new { def king()= println(w + \u0026quot; is my king\u0026quot;) } def say = \u0026quot;Scala\u0026quot; king just add one rule use implicit to tell Scala, you can add String a new method king.if you run say, it will print Scala is my king.\nCompare Java, Scala more like a human, he will think a lot for your code.If String have no method call king, he will look up in his scope, is there any definenation of a convertion to a new Class which have a method call king.If he find one, he will do it for you, convert the String to the new Class and call the method again.\nSummary # There are so many secret in Scala, if you realy want to know how Scala do for us, just use scalac to compile to .class file, and use javap to tranceform to Java code.That will help you a lot.There are also a intresting part in new power switch in Scala, it\u0026rsquo;s too important to write a new blog to introducing it.\nScala is a intersting lanuage, if you want to know more about it, you can read the Scala library code .It\u0026rsquo;s really perfect.\n","date":"2018-10-20","externalUrl":null,"permalink":"/posts/backend/framework/java/learningscalafromjava/","section":"博客","summary":"Recently I had finshed reading one book : \u0026laquo; Scala for The Impatient\u0026raquo;, I found a lot intrest thing during reading this book.Cause I had known Python and Java before,","title":"Learning Scala From Java","type":"posts"},{"content":" If you are hadoop novice, I strongly suggest you beginning your study from single node building,you can learn from this website, after you having finshed build one single node, then you can reading my blog to learn how to run a N-node clusters just in your computer.\nAbstract # This blog is introduce using one computer to build a N-node clusters.I suggest you use ubuntu to build. You can also use Windows, but you\u0026rsquo;d better install virtualbox to install one desktop ubuntu as your base server.In this blog, we will try two different way to build hadoop clusters in one computer.\nIntroduction # Before you start learning, you can download these required softwares from Intelnet.\nJDK8(optional) we can also install it by apt tool, but may be slow in China.So you\u0026rsquo;d better download it from website.\nhttps://www.oracle.com/technetwork/java/javase/downloads/jdk8-downloads-2133151.html\nchoose \u0026ldquo;jdk-8u181-linux-x64.tar.gz\u0026rdquo; to download.You can alse install in your master computer later, you can read from this blog\nHadoop(2.85) I choose latest 2.85 version, you can download from this website.\nhttps://www.apache.org/dyn/closer.cgi/hadoop/common/hadoop-2.8.5/hadoop-2.8.5-src.tar.gz\nUbuntu Image In this trip, we choose Ubuntu16.04 server for build clusters.You can use 163 mirrors to speed up your download.\nhttp://mirrors.163.com/ubuntu-releases/16.04.5/ubuntu-16.04.5-server-amd64.iso\nVirtualbox we need virtualbox to create our clusters. It\u0026rsquo;s easy for you to install virtualbox in ubuntu. You can read this article to install virtualbox-5.2\nDocker we will try use docker build our clusters, it\u0026rsquo;s easy install in ubuntu.The install tutorials is https://docs.docker.com/install/linux/docker-ce/ubuntu/#uninstall-old-versions\nClusters On VirtualBox # Now I assuming you\u0026rsquo;re working on a ubuntu16.04 desktop OS.Now let\u0026rsquo;s begining our trip.\nFirst,let\u0026rsquo;s init a master, after we install required software in master, we can use virtualbox clone function to easy to build slave.\nBuild Ubuntu VMS # new a machine named master Choose 2G RAM, VDI then run this image, load the iso file you downloaded.Pay attention to make true install ssh server( Or you can install after installing os by apt)\nBefore we start install hadoop and java skd, let me tell you something about the internet require.\nFor our clusters runing, we need a connected internet between master and slaves.If we have many computers, it\u0026rsquo;s simple, we just need they both have public IP or private IP in one LAN.But if we just in one computer, how can we have independent IP for our master and slaves.\nThis is why we install virtualbox, virtualbox provide our independent computers in just one computer.Moreover, it can provide a simulate NIC for each computer.By using that, each computer can have they own private IP in LAN.\nSo the key our cluster running is the bridge\nwe need choose the bridged adapter to make master and slaves just in same LAN.Pay attention to make true you need choose your real NIC.In ubuntu you just run ifconfig and find out have one line inet addr:192.168.1.12 .Usually it\u0026rsquo;s eth0 in ubuntu.\nWhen you have finished OS installment.You can login in and start installing hadoop clusters.\nStep 1. Configure Static IP\nIn your virtual machine, your IP is changeable when reboot.Because ubuntu use DHCP for init your IP from gateway.We need make true our master and slaves have changeless IP to protect their connection.\nTo do this, first you need make true your installment is ok. Try ping baidu.com to check you connected Internet or not.Then we need know our gateway address.Try run route in shell, you can find a table, in the row Gateway, you can find one or more static IP like 192.168.0.1 , this is your gateway.Now we open our internet settings.\ncat /etc/network/interfaces\tyou can see something like this\nauto eth0 iface eth0 inet dhcp eth0 is your NIC(yours maybe different). and we use dhcp to get IP. Now we need change it to static.\nauto eth0 iface eth0 inet static address 192.168.0.105 netmask 255.255.255.0 gateway 192.168.0.1 PS: make true, you need change the eth0 and gateway IP to yours.The address IP must be subnet of gateway under the control of netmast.eg, you can\u0026rsquo;t set you ip address to 10.1.1.1 if your gateway is 192.168.0.1.The easiest way is set by dhcp format.And just change the last number.If you still can\u0026rsquo;t connect the Internet.Try add one line dns-nameservers 8.8.8.8 .\nifdown eth0 ifup eth0 now run upper commands in your vm(eth0 need your NIC name).If run ifconfig again, you can see our IP address chage to 192.168.0.105 now!\nStep 2. Add Hostname alias\nBecase hadoop need hostname to identify their ID, so now we add Hostname-IP pair to smooth our connection.\nJust edit /etc/hosts/ and add three line below\n192.168.0.105 master 192.168.0.104 slave1 192.168.0.103 slave2 Step 3. Make SSH Login\nBecase hadoop need login by root with SSH , so we need make root can login in in ubuntu.Open /etc/ssh/sshd_config and change line PermitRootLogin prohibit-password to PermitRootLogin yes, then service ssh restart .\nAlso you need use your sudo to set password for root\nsudo passwd root now check you can login in with root\nssh root@127.0.0.1 Step 4. Set Hadoop Env\nFirst, we need install JDK for hadoop, now back to your host computer. And use scp to upload JDK to vm.You can add below to /etc/hosts in your host machine.\n192.168.0.105 master 192.168.0.104 slave1 192.168.0.103 slave2 then you can easy upload your JDK and Hadoop to your vm(you need unpack this tar.gz file first)\nscp -r /path/your/jdk root@master:/usr/lib/jvm/java-8-oracle scp -r /path/your/hadooproot@master:/usr/local/hadoop PS: you can also install Java8 by apt\nNow, we installed JDK and Hadoop in our VM.Then we back to VM and initialize our Hadoop.\nSet JDK Home edit hadoop-env.sh(in /usr/local/hadoop/etc/hadoop/) file add export JAVA_HOME=/usr/lib/jvm/java-8-oracle to tell Hadoop JDK local address.\nSet Core IP We need a boss to handle all employer.So edit core-site.xml(in /usr/local/hadoop/etc/hadoop/) and add a property in configuration\n\u0026lt;property\u0026gt; \u0026lt;name\u0026gt;fs.defaultFS\u0026lt;/name\u0026gt; \u0026lt;value\u0026gt;hdfs://master:9000/\u0026lt;/value\u0026gt; \u0026lt;/property\u0026gt; each cluster will send heartbeat to master:9000.\nSet HDFS replication and file dir The hadoop basement is HDFS, edit hdfs-site.xml and add three property\n\u0026lt;property\u0026gt; \u0026lt;name\u0026gt;dfs.replication\u0026lt;/name\u0026gt; \u0026lt;value\u0026gt;2\u0026lt;/value\u0026gt; \u0026lt;/property\u0026gt; \u0026lt;property\u0026gt; \u0026lt;name\u0026gt;dfs.namenode.name.dir\u0026lt;/name\u0026gt; \u0026lt;value\u0026gt;file:///root/hdfs/namenode\u0026lt;/value\u0026gt; \u0026lt;/property\u0026gt; \u0026lt;property\u0026gt; \u0026lt;name\u0026gt;dfs.datanode.data.dir\u0026lt;/name\u0026gt; \u0026lt;value\u0026gt;file:///root/hdfs/datanode\u0026lt;/value\u0026gt; \u0026lt;/property\u0026gt; The dfs.replication meaning the backups of HDFS, dfs.namenode.name.dir and dfs.datanode.data.dir is optional.If you not set this, it will store under /tmp (when reboot ,it will delete).\nSet Yarn for MapReduce In hadoop2, we use Yarn to manage our MapReduce, run cp mapred-site.xml.template mapred-site.xml and then add property to set Yarn as our MapReduce framework\n\u0026lt;property\u0026gt; \u0026lt;name\u0026gt;mapreduce.framework.name\u0026lt;/name\u0026gt; \u0026lt;value\u0026gt;yarn\u0026lt;/value\u0026gt; \u0026lt;/property\u0026gt; and we also need tell yarn the master of the clusters and our need open MapReduce Shuffle Fuction effective our MapReduce, edit yarn-site.xml, and add two property\n\u0026lt;property\u0026gt; \u0026lt;name\u0026gt;yarn.nodemanager.aux-services\u0026lt;/name\u0026gt; \u0026lt;value\u0026gt;mapreduce_shuffle\u0026lt;/value\u0026gt; \u0026lt;/property\u0026gt; \u0026lt;property\u0026gt; \u0026lt;name\u0026gt;yarn.resourcemanager.hostname\u0026lt;/name\u0026gt; \u0026lt;value\u0026gt;master\u0026lt;/value\u0026gt; \u0026lt;/property\u0026gt; yarn.nodemanager.aux-services open shuffle, and yarn.resourcemanager.hostname set ResourceManager hostname.\nNow we complete the base Hadoop settings, now we can try run hadoop on master\ncd /usr/local/hadoop/ bin/hadoop namenode -format sbin/start-dfs.sh We try format our namenode, and start dfs server, now run jps, you can see NameNode and SecondaryNameNode server started.\nNow we try start Yarn to start MapReduce FrameWork.\nsbin/start-yarn.sh Now, rerun jps, you can see ResourceManager running.You can also try netstat -tuplen|grep 8088, you will find the ResourceManager open some tcp port like 8080,8031,8033,etc.And the 8088 is the website of managing clusters.You can open http://master:8088 to see the clusters status.Now you can only see blank node in clusters, for we have not started one slave yet.\nCongratulation, our master is starting, in the running, we need input our password when start, after complete all slave building, we can use ssh-key to autologin.\nNow let\u0026rsquo;s build our slaves.\nUse virtualbox clone function, we clone master to a new VM named slave1.\nBecause we clone every thing to the slave1, so we need close master and goto slave1 change its hostname and static IP make it to be a slave\nFirst we need do is rename the VM,edit /etc/hostname change it to slave1, then we need do is setting slave1 Static IP, we do like upper.Just replace IP to 192.168.0.104, and then we reboot and start master and slave1 at meatime.\nNow let\u0026rsquo;s check master to start our slave1, in our master VM, we edit /usr/local/hadoop/etc/hadoop/slaves file, and one line\nslave1 and make true you have add slaves\u0026rsquo; hostname alias in master VM. Then we try start our Cluster\ncd /usr/local/hadoop bin/hadoop datanode -format sbin/start-dfs.sh \u0026amp;\u0026amp; sbin/start-yarn.sh After running these command, check http://master:8088 to find the master have one slave online named slave1.\nPS: Now you can generate ssh-key for your login in slaves, just run ssh-keygen -t rsa \u0026amp;\u0026amp; ssh-copy-id slave1, you don\u0026rsquo;t need input your password to start your clusters.\nNow we have one node clusters, if you want more, you can add more slaves repeatting upper produce.\nAfter you build your N-Clusters , you can now run those commands to check the hadoop working or not.\n# create input files mkdir input echo \u0026quot;Hello Docker\u0026quot; \u0026gt;input/file2.txt echo \u0026quot;Hello Hadoop\u0026quot; \u0026gt;input/file1.txt # create input directory on HDFS hadoop fs -mkdir -p input # put input files to HDFS hdfs dfs -put ./input/* input # run wordcount cd /usr/local/hadoop/bin hadoop jar ../share/hadoop/mapreduce/sources/hadoop-mapreduce-examples-*-sources.jar org.apache.hadoop.examples.WordCount input output # print the input files echo -e \u0026quot;\\ninput file1.txt:\u0026quot; hdfs dfs -cat input/file1.txt echo -e \u0026quot;\\ninput file2.txt:\u0026quot; hdfs dfs -cat input/file2.txt # print the output of wordcount echo -e \u0026quot;\\nwordcount output:\u0026quot; hdfs dfs -cat output/part-r-00000 PS: By the way, if you want to running this clusters for a long time, you can try use vboxmanage to manage the vm. You can simple use vboxmanage startvm master --type headless to start master background(change master to other VM name can start them too)\nConclusion # The difficult of build a clusters in virtualbox is know how master and slaves connecting each other.If you set a right network, it\u0026rsquo;s easy to running the cluster.But there\u0026rsquo;re some problem in virtualbox, we can\u0026rsquo;t share our network in the host LAN with virtualbox bridge. So we will introduce you build clusters in Docker and we can run our clusters in a swarm clusters in a real envirment.\nClusters On Docker # Building clusters is much easily in docker, for docker provide a easy network bride in sigle computer or in a swarm clusters.\nwe use kiwenlau/hadoop:1.0 image to our test(which hadoop version is 2.7).Just run\nsudo docker pull kiwenlau/hadoop:1.0 After few minutes, we can have a hadoop images, now we need set our private LAN Net just use this(If you want to run a swarm clusters above many computers, just change bridge to overlay, powerful, isn\u0026rsquo;t it)\nsudo docker network create --driver=bridge hadoop Now let start our master server\nsudo docker run -itd \\ --net=hadoop \\ -p 50070:50070 \\ -p 8088:8088 \\ --name hadoop-master \\ --hostname hadoop-master \\ kiwenlau/hadoop:1.0 \u0026amp;\u0026gt; /dev/null In the command, we set the master hostname to hadoop-master.and we needn\u0026rsquo;t change /etc/hosts to add it like in virtualbox, docker will do it for us.\nNow we start our slaves\nsudo docker run -itd \\ --net=hadoop \\ --name hadoop-slave1 \\ --hostname hadoop-slave1 \\ kiwenlau/hadoop:1.0 \u0026amp;\u0026gt; /dev/null sudo docker run -itd \\ --net=hadoop \\ --name hadoop-slave2 \\ --hostname hadoop-slave2 \\ kiwenlau/hadoop:1.0 \u0026amp;\u0026gt; /dev/null After doing that, we have finshed all softwares build.Now just runsudo docker exec -it hadoop-master bash into master, and then start our clusters bash start-hadoop.sh.\nNow you can enjoy your clusters in few minutes, open http://127.0.0.1:8088/ to see our clusters running happily.\nConclusion # After introducing two way to build a hadoop clusters, you will find it\u0026rsquo;s easy to build a clusters if you know how they work together.In a word, we kind of like using Docker to running hadoop clusters, we can easy move add more Hadoop slaves in just one command.Meantime we can use bridge or overlay network for us building a more safe hadoop clusters.\nReferences # https://github.com/kiwenlau/hadoop-cluster-docker\n","date":"2018-10-14","externalUrl":null,"permalink":"/posts/backend/software/buildhadoopclusterinonecomputer/","section":"博客","summary":"If you are hadoop novice, I strongly suggest you beginning your study from single node building,you can learn from this website, after you having finshed build one single node, then","title":"Build Hadoop Cluster in One Computer","type":"posts"},{"content":"","date":"2018-09-15","externalUrl":null,"permalink":"/tags/python/","section":"Tags","summary":"","title":"Python","type":"tags"},{"content":" Python的GIL一直是被大家攻击其语言的一个弊端，每次在讨论语言特性的时候这点总是会被人们提起，但是这个东西好像就一个“污点”，大家都知道，但是大家都不了解为什么。本片博客就是好好的探索一下GIL，让我们不再畏惧它\n引言 # 其实一开始并没有想到研究GIL，但是在研究如何让你的Python更快的过程中发现我们可以通过这种方式解决掉GIL，让我们的代码不被Python拖累\n这篇博客相比于上面的博客更注重于代码的讲解，我们通过使用pybind11从一个Python调用C++的demo出发介绍如何让Python调用C++并且丢弃GIL\nGIL简介 # 首先我们要知道什么是GIL，为什么它会拖累Python，首先我们看一下Python历史，Python是Guido van Rossum 在1989年发布的，那个时候计算机的主频还没有达到1G，程序全部都是运行在单核计算机上面，直到2005年多核处理器才被Intel开发出来\n多核处理器意味着什么呢，就好比一个工厂，你原来只有一个工人干活，现在有很多个了，一开始设计出来只是为了能在每个核心上跑不同的应用，但是随着大家对多核计算机的使用，大家发现有的时候计算器其实很空闲，大部分CPU都在休息，假如只在一个核上跑一个应用的话，那么其他CPU就浪费了，所以大家就开始设计怎么并行在多个CPU上跑同样的任务\n现在我们来考虑一下怎么能让CPU力往一处使，我们用数据库来做比方，假设我们计算机上安装一个银行数据库，为了让这个“银行”能够服务更多的人，我们把对钱的操作（增删查改）放到每个CPU上运行。假如我们的顾客一个一个排着队来取钱存钱，我们每个CPU查询都是唯一的，存取也是唯一的，那么我们的“银行”就能正常工作\n但是现实的环境往往不是这样的，顾客它可能会因为网络原因个人原因同时进行多个操作，假如它同时取1千万的两次操作（它账号只有1千万），每个CPU上的程序查询时候正好都是账号有一千万，然后依次进行数据的更新，最后我们发现用户的账号变成了0，但是用户却取了两千万出来，你的银行损失了一千万，所以并行任务最重要的就是数据共享\n怎么解决这个共享问题呢，很简单加“锁”，我们给需要共享的东西上个锁，每次你想用的时候你就把锁锁上，然后对共享的东西进行操作，当有别人想动这个东西的时候，他一看哎呀有人在用，那我等会。这样就不会造成上面的冲突了，但是这个也造成了一个问题由于我上了一把锁，每次我们想操作的时候，必须去看一下这个锁有没有被人锁上，假如没有我就锁上，有就等待，这一来一去就会造成一个效率问题（感觉这个也是国企的通病，权利依次掌握在领导上，要想完成工作得不断的进行开“锁”、关“锁”，有时候还会造成“死锁”），所以并行的4个任务运行速度不一定是一个任务的四倍，所以我们经常看到一些库在运行说明里面双核速度会比单核加速一点几倍，之说以达不到双倍就是因为这些“锁”的存在\n“锁”帮我们能让单任务拆分成子任务并行化加速，但是在一定程度上拖累了运行速度，我们回到Python，因为多核是在2005年才出现的，但是在并行化上面，一个比多核更早出现的概率就是：线程和进程\n在还没有多核处理器的时候，操作系统为了让程序并行化跑，就创造了进程和线程的概率。用通俗的话来讲，进程就是一家大工厂，而线程就是工人，为了提高生产力，我们可以开很多家工厂，当然我们也可以开一家工厂，招很多工人。但是线程这个东西相比于进程要消耗的少的多，因为它“原材料”都是从“工厂”里面拿的，假如说工厂少了几个工人还可以生产，但是上万个工人没有工厂他们也办法工作。\n所以对于Python来说首先得支持线程和进程的概率，对于进程来说很简单，就是多开几家工厂(多开几个Python程序)罢了，但是对于线程来说，由于Python是一门脚本语言，它需要一个解释器来执行代码，我们知道这个解释器它可以当做大一个共享变量，假如在不同的线程里面用“锁”来限制一下的话，环境变量就会乱了套\n所以Python对于线程的支持就是给他加一个锁，也就是我们俗称的GIL，由于在操作系统在运行单核的时候就支持线程，一个工人加一个锁其实也没有什么，无非就是多了一点开锁关锁的时间，所以Python在2005前一直没有GIL这个概率，到了2005大家发现Python使用多线程竟然只能使用一个核，完全浪费了其他核，因为虽然Python的线程可以分配到不同的核上运行，但是当他们运行的时候发现这个锁没有被释放，所以每个核上的线程都傻乎乎的在等待，结果最后查看效果多线程比单线程速度还慢（要等GIL释放）\nPython社区逐渐发现这个问题，他们也做了很多挽救工作，比如在线程睡觉（sleep）、等待连接的时候让线程主动释放GIL，这样就能让其他线程继续执行，但是对于纯粹的运算代码而不是IO密集代码总也避不开这个锁的存在，如果允许GIL释放，由于历史遗留问题很多代码都会乱了套（理论上其实就是需要重新修改锁的设计，可以参考MySQL的代码去掉“锁”花了5年时间），考虑到Python本来就运行的慢，Python开发者觉得假如你觉得代码很慢，你可以放到C/C++里面执行，所以对于这个GIL就没有继续啃下去，而是把中心放在Python调用C/C++中，提供了一些很方便的方式让我们在C/C++中控制GIL的释放以及获取\n所以我们接下来通过一个来学习Python调用C++代码，来了解Python如何调用C++，并且通过一些实验来验证线程、进程和GIL\n测试GIL的存在 # 首先我们要做的第一件事就是测试GIL的存在，现在基本上主流电脑都是多核CPU，所以我们这个实验可以很轻松的在多核下进行\n首先我们得安装一些环境：Python3，gcc，htop（在Windows可以用下任务管理器代替）\n首先我得提一下我的一个认识误区，在以前我不太清楚线程、进程与多核直接的关系的时候我有一个误区，我以为C能在单线程里面使用多核（我也不清楚为什么我会这么想，可能是因为了解很少），而Python却不能，后面通过我实验我才发现，无论是C和Python只要你的代码不使用线程、进程那么你的代码只能同时运行在同一个核上\n怎么来测试呢，我们可以在Python的解释器里面输入\nwhile True: pass 然后我们打开htop，我们可以发现某一个CPU始终保持在100%（这个CPU可能会变化，因为操作系统控制每个进程切换CPU时间），假如你没有其他任务过多使用CPU的话，你其他的核心一直保持在很低的利用率，当你ctrl-c你的代码后，那个100%的CUP会立马降下来\n然后你在编译一个C程序，使用gcc a.c \u0026amp;\u0026amp; ./a.out命令编译下面代码然后运行\n// a.c int main(){while(1){};} 你会发现C也只能消耗一个CPU，这就印证了我们前面说过得，如果我们不主动使用线程或进程来，同时只能有一个在运行\n接下来我们看看在多进程的基础上，使用Python来使用多核\nfrom concurrent.futures import ProcessPoolExecutor def f(a): while 1: pass if __name__ == '__main__': pool = ProcessPoolExecutor() pool.map(f, range(100)) 当我们运行上面代码的时候，我们会发现所有CPU会运行到100%，我们只要简单声明一个进程池（ProcessPoolExecutor），Python自动帮我们生成你CPU核数相同的进程，然后我们只要把任务分配到池中就能重复的并行化任务，把所有的核心都用起来。\n然后我们来测试一下线程池，要使用Python线程池只需要初始化ThreadPoolExecutor就行\nfrom concurrent.futures import ThreadPoolExecutor def f(a): while 1: pass if __name__ == '__main__': pool = ThreadPoolExecutor() pool.map(f, range(100)) 我们从htop可以看到在Python线程中，只有一个能达到100%，这就是GIL的“威力”，它让我们多线程没有发挥多线程的力量，重复使用到多核CPU\n接下来我们看看在C++里面使用多线程是否能够发挥多核的威力\n// run.cpp #include \u0026lt;thread\u0026gt; using namespace std; #define NUM_THREADS 50 void f(){ while(1){}; } void run_dead(){ std::thread threads[NUM_THREADS]; for(int i = 0; i \u0026lt; NUM_THREADS; ++i) { threads[i] = std::thread(f); } for (int i = 0; i \u0026lt; NUM_THREADS; ++i) { threads[i].join(); } }; int main(void){ run_dead(); } 我们使用 g++ -pthread -std=c++11 run.cpp \u0026amp;\u0026amp; ./a.out运行上面的C++程序，我们在htop里面能够发现，C++的多线程能够完全发挥多核的威力\n上面的程序都很简单，但是具备一个多线程运行的基本构造，我们可以修改我们的调用的子任务来完成实际的任务，当然你程序越复杂也涉及到了各种锁的使用，这里我们就不谈了\n从上面的程序我们可以知道C++的多线程能够充分使用多核，而Python的不行，接下来我们就开始探索Python调用C++\nPython调用C++ # 在上面的博客我总结了Python调用C++的方式，总的来说Cython是控制能力最好的，效率也是最高的，但是由于存在一个学习新语言的难度，所以我这里就不提了，改天再写一篇关于Cython的博客，我们这里使用pybind11这个库作为介绍\n安装非常简单pip install pybind11就行，接下来我们使用github上这个官方例子做介绍，最后我们以一个实际的C++项目为例子，看看如何在实际的项目使用\n首先我们先把项目下载下来\ngit clone https://github.com/pybind/python_example.git 然后我们新建一个环境（避免安装到我们系统的环境，方便删除）\npython -m venv venv PS: 当前Python版本默认为py3.5以上（你可以使用pyenv安装Python多个版本，目前我在自己使用Python版本，但主要使用3.6以上）\nsource venv/bin/activate 然后我们激活我们的环境，我们顺便安装一下我们接下来要安装的Python包\npip install ipython 然后我们进入项目cd python_example，假如你用Pycharm的话，你可以在项目目录下生成venv环境，然后在Pycharm里面打开会自动设定为默认环境\n然后我们先测试一下代码可以不可以用\npip install . 假如我们安装成功了，恭喜你，我们的环境已经准备好了，打开ipython，我们先测试一下这个C++代码的速度\nIn [1]: import python_example In [2]: python_example.add(1, 1) Out[2]: 2 很好，代码运行正常，就是一个简单的加法运算，我们测试一下平均时间\nIn [3]: %timeit python_example.add(1, 1) 313 ns ± 3.03 ns per loop (mean ± std. dev. of 7 runs, 1000000 loops each) 很好，我们的C代码还是跑到很快，313纳秒就跑完了，接下来我们看看纯粹的Python代码速度\nIn [4]: def add(a, b): ...: return a + b ...: ...: In [5]: %timeit add(1, 1) 113 ns ± 9.04 ns per loop (mean ± std. dev. of 7 runs, 10000000 loops each) 什么竟然比C++还要快，快了近3倍，记得我当时第一次运行出来的这个结果的时候的震惊，说好的快呢，你骗我。\n接下来我们就来分析一下出现这个的原因，会不会是因为类型转换出现问题呢，因为pyblind11使用了很多自动转换的技术来帮我们转换，我们看看原函数（在src/main.cpp)\nint add(int i, int j) { return i + j; } 首先Python调用它，要把第一个参数由Python的int对象转换成C++的int基本类型，C++运行完之后，又得转换将C++基本int类型转换成Python的int对象，这一来一回就得多花三个操作，为了验证我们猜想，我们插入一个nothing函数在add函数后面\nvoid nothing(){ } 然后模仿m.def仿照写一行插入nothing函数（你会发现语法特别简单，这也是我喜欢pyblind11的原因）\nm.def(\u0026quot;nothing\u0026quot;, \u0026amp;nothing, R\u0026quot;pbdoc( do nothing )pbdoc\u0026quot;); 接下来我们安装一下我们的新库pip install .\n然后我们再开一个新的ipython（你可以用importlib来重新加载库）\nIn [1]: import python_example In [2]: %timeit python_example.nothing() 125 ns ± 0.6 ns per loop (mean ± std. dev. of 7 runs, 10000000 loops each) 125ns，我们的猜想成功了，类型转换的确拖累了C++运行的速度，我们再看看原生的速度如何\nIn [4]: def nothing(): ...: pass ...: ...: In [5]: %timeit nothing() 85.1 ns ± 0.262 ns per loop (mean ± std. dev. of 7 runs, 10000000 loops each) 竟然还是比C++快，虽然没有上面那么夸张，但是快了25%，我们再来分析原因，首先现在没有类型转换所以理论上那只能是代码运行问题，我们知道Python优化里面提过一句，少用.，因为Python要搜寻很多东西才能获得到对象的属性、方法等，所以我们这边使用了python_example.nothing来调用nothing函数，假如我们去掉.速度会不会提高呢\n怎么去掉呢，用局部变量\nIn [6]: pn = python_example.nothing In [7]: %timeit pn() 90 ns ± 0.761 ns per loop (mean ± std. dev. of 7 runs, 10000000 loops each) 从上面可以看到的确，.“害人不浅”，我们的速度又快了一大截，基本上同原生没有太多差距了，一开始我以为是概率问题，运行了多次但是结果都是一样，原生就是比C++快了5ns，可能是pyblind11“偷偷”的在哪个地方偷跑了一条语句吧，或者有可能是C++比C（Python是C写的）稍微慢了一点\n一开始我以为C++一定会比Python快，但是我们从上面测试可以看出来，在“起跑”阶段，C++甚至比Python要慢，我们使用C++主要是为了加速大段Python代码，只要在这场“长征”中C++能够胜出，那么我们的努力就没白费，那好我们继续测试，看看在长征过程中C++表现如何\n首先我们把add函数魔改一下，我们让他进行100次运算\nint add(int i, int j) { int s = 0, x = 0; for(;x\u0026lt;100;x++){ s = s + i + j; } return s; } 我们再把模块给安装一下pip install .，重新打开新的ipython\nIn [1]: import python_example In [2]: python_example.add(1,2) Out[2]: 300 In [3]: padd = python_example.add In [4]: %timeit padd(1,1) 282 ns ± 3.9 ns per loop (mean ± std. dev. of 7 runs, 1000000 loops each) In [5]: %timeit python_example.add(1,1) 316 ns ± 3.78 ns per loop (mean ± std. dev. of 7 runs, 1000000 loops each) 我们这次重要见到了C++的威力，我们进行100次运算，相比于上面一次运算，我们只增加了4ns的平均时间，我们来看看原生Python的表现如何\nIn [6]: def add(a, b): ...: s = 0 ...: for i in range(100): ...: s += a + b ...: return s ...: ...: In [7]: add(1, 2) Out[7]: 300 In [8]: %timeit add(1, 1) 4.74 µs ± 40.5 ns per loop (mean ± std. dev. of 7 runs, 100000 loops each) C++完爆Python，4.74us = 4750ns，Python用时是C++的10倍多，只还只是100次运算，假如我们上万次运算，那结果更加夸张，C++在长征的过程中胜利了，但是我们也不能说Python是慢毕竟us的单位其实非常小，1us=1000ms=1000000s，在1s内可以执行上面函数几十万次，只能说C++速度太可怕了\n调用总结 # 我们从上面可以看到，虽然Python调用C++在类型转换上会有速度损失，但是在进入到函数提内运行过程中的速度是不影响的，假如我们的运算量够大，完全可以弥补那一点点性能影响，所以要想重复利用C++的速度，尽量少调用C++，把计算结果竟然一次性返回，而不是我们多次进行交互，这样就能最大化利用C++\n在C++线程中测试GIL # 接下来我们来考虑这么一个问题，前面我们测试了C++的线程能使用多核，我们假如在让Python在调用C++的代码中中使用线程，那么我们的C++的线程能不能使用多核呢进而解除GIL的作用\n我们把nothing函数改成多线程\n#include \u0026lt;thread\u0026gt; #define NUM_THREADS 50 using namespace std; void f(){ while(1){}; } void nothing(){ std::thread threads[NUM_THREADS]; for(int i = 0; i \u0026lt; NUM_THREADS; ++i) { threads[i] = std::thread(f); } for (int i = 0; i \u0026lt; NUM_THREADS; ++i) { threads[i].join(); } } 然后我们再重新编译一下pip install .，我们来跑一下我们这个多线程的nothing函数\nIn [1]: import python_example In [2]: python_example.nothing() 我们在htop里面可以看到在单线程的Python程序中，成功的将所有核心都利用上了，也就是是说假如我们在C++扩展中使用线程的话，是不会被GIL影响的\n说实话当我第一次运行的时候我直觉是还是会被GIL影响，结果最后跑出来的结果大吃我一惊，现在我们分析为什么不会被受影响，因为GIL锁的是Python解释器，当我们的代码进入到C++中的时候，我们已经不在Python解释器中了，这样即使我在C++中声明线程，那也是C++的线程，所以就不会造成无法使用多核的情况\n这里我们学到一点，如果我们想摆脱GIL可以把线程放到C++中，这样线程的不再依赖Python解释器，前面我们知道其实Python底层是用C写的，所以基本上所以的语法都是基于C代码实现加上语法糖来完成的，Python线程也就是C线程，我们能不能模拟一下Python来构建这个GIL\n首先我们知道GIL是一把锁，所以我们第一件事就是查看这把锁，在这里我们通过Python的C头文件来引入一个函数PyGILState_Check这个函数会返回一个1和0值，假如是1那么意思该线程拿着GIL锁，反之。\n所以我们先在头部加上#include \u0026quot;Python.h\u0026quot;，在Linux系统上要安装python-dev或者python-devel开发包才有这个头文件，接下来我们在nothing函数加上这个检测状态\ncout \u0026lt;\u0026lt; \u0026quot;GIL is \u0026quot; \u0026lt;\u0026lt; ((PyGILState_Check() == 1) ? \u0026quot;hold\u0026quot; : \u0026quot;not hold\u0026quot;)\u0026lt;\u0026lt;endl; 提一句为了使用cout，我们得在头部加上C++输出库#include \u0026lt;iostream\u0026gt;\n先在我们重新安装一下并运行nothing函数，程序会输出GIL is hold，为什么会出现这个情况呢，因为Python默认会锁住GIL当运行C++或者C代码的时候，但是为什么我们虽然锁住了GIL但是我们还是能够使用C++的线程来运行多核呢，其实很简单因为我们的线程没有像Python一样每次运行的时候去获取这个GIL锁，为了证明这一点，我们来做个实验\n首先我们得在nothing函数里面释放GIL，然后让线程去获取GIL（如果nothing主函数不释放GIL，会造成死锁，线程无法运行，一直获取不了GIL锁），我们可以用Python的C头文件的函数来释放GIL锁，但是pybind11提供了一个更加方便的函数让我们来释放GIL锁，我们把nothing函数定义修改一下，在后面添加一条语句py::call_guard\u0026lt;py::gil_scoped_release\u0026gt;()\n// m.def(\u0026quot;nothing\u0026quot;, \u0026amp;nothing); m.def(\u0026quot;nothing\u0026quot;, \u0026amp;nothing, py::call_guard\u0026lt;py::gil_scoped_release\u0026gt;()); 然后我们在重新编译安装运行一下代码，我们的结果就会是GIL is not hold，我们通过简单的一条语句就释放GIL锁，接下来我们来测试在线程中获取GIL锁来模拟Python的情况\n要想获取GIL锁，pybind11也提供了一个非常简单的方法来实现这个：py::gil_scoped_acquire acquire;\n我们接下来把f函数改成下面的\nvoid f(){ cout \u0026lt;\u0026lt; \u0026quot;entner F: GIL is \u0026quot; \u0026lt;\u0026lt; ((PyGILState_Check() == 1) ? \u0026quot;hold\u0026quot; : \u0026quot;not hold\u0026quot;)\u0026lt;\u0026lt;endl; py::gil_scoped_acquire acquire; cout \u0026lt;\u0026lt; \u0026quot;GIL is \u0026quot; \u0026lt;\u0026lt; ((PyGILState_Check() == 1) ? \u0026quot;hold\u0026quot; : \u0026quot;not hold\u0026quot;) \u0026lt;\u0026lt; \u0026quot; now is runing \u0026quot;\u0026lt;\u0026lt;endl; while(1) { }; } 我们在获取GIL前后，添加了一些输出，方便我们调试，接下来我们再运行我们的代码，我们发现程序输出50个进入entner F: GIL is not hold（在我的电脑上，因为线程同时运行，获取GIL锁需要时间，所以在我电脑上每次运行f函数时锁都打开着），但是只有一行GIL is hold now is runing，因为当一个线程获取到GIL后，其他线程就没法获取到了，而且看htop我们也能发现只有一个核到了100，在我们强行模拟下C++也没能使用多核\n其实从这里我们可以看出来，GIL问题其实就是一个死锁的问题，线程获取后不释放锁，导致所有线程相互竞争，用一个谚语来说就是：一个和尚挑水喝、两个和尚抬水喝、三个和尚没水喝。\n那么我们怎么来解决这个问题呢，很简单就是在你不需要的锁的时候去释放它，接下来我们来模拟一下怎么释放这个锁达到多线程“和平共处”，首先我们引入C++时间库来使用sleep函数(#include \u0026lt;unistd.h\u0026gt;)，接下来我们引入Python的C头文件中的宏来释放GIL，我们把f函数改成下面的形式\nvoid f(){ cout \u0026lt;\u0026lt; \u0026quot;entner F: GIL is \u0026quot; \u0026lt;\u0026lt; ((PyGILState_Check() == 1) ? \u0026quot;hold\u0026quot; : \u0026quot;not hold\u0026quot;)\u0026lt;\u0026lt;endl; py::gil_scoped_acquire acquire; cout \u0026lt;\u0026lt; \u0026quot;GIL is \u0026quot; \u0026lt;\u0026lt; ((PyGILState_Check() == 1) ? \u0026quot;hold\u0026quot; : \u0026quot;not hold\u0026quot;) \u0026lt;\u0026lt; \u0026quot; now is runing \u0026quot;\u0026lt;\u0026lt;endl; Py_BEGIN_ALLOW_THREADS while(1){ }; Py_END_ALLOW_THREADS } 我们使用Py_BEGIN_ALLOW_THREADS和Py_END_ALLOW_THREADS这一对宏来释放GIL，这样我们重新编译运行nothing函数我们就能看到50个enter和50个runing，而且在htop中我们也能发现C++的线程再次使用所有的核心了（利用率达到不了100%，不知道是因为宏的“副作用”还是其他原因，但是每个核还是能够到70%作用），这种在一个函数中获取和释放GIL锁还是不推荐的，最好在函数一开始的时候释放GIL，在函数结束的时候获取GIL返回到Python解释器中（假如你需要与Python进行交互的话），毕竟获取一次锁的成本还是挺大的，而且一不小心就会造成死锁\n在Python线程中测试GIL # 接下来我们来看看一个已经存在的问题，就是如何解决掉使用Python线程时遇到的GIL问题，其实我们在上面的C++线程已经模拟出来了，解决这个问题的关键就是释放GIL锁，我们先测试一下在GIL锁下，线程调用C++代码的速度\n我们首先添加一个新死循环函数\nvoid run_dead(){ while(1){}; } 然后在后面加上pybind11的定义\nm.def(\u0026quot;run_dead\u0026quot;, \u0026amp;run_dead); 接着我们运行下面函数\nfrom concurrent.futures import ThreadPoolExecutor import python_example pool = ThreadPoolExecutor() for i in range(100): pool.submit(python_example.run_dead) 在这个函数里面我们声明了一个线程池，并且向池蕾加入了100函数，接着我们在htop里面查看CPU利用率，我们可以看到只有1个CPU能够跑满100%，其实从前面的实验我们就能猜到这个结果，解决方案其实前面也给了，有两种方法，第一种就是使用Python的C的头文件函数宏 ：Py_BEGIN_ALLOW_THREADS和Py_END_ALLOW_THREADS，第二种就是在函数声明的地方使用pybind11提供的py::call_guard\u0026lt;py::gil_scoped_release\u0026gt;()来释放GIL，两种方法都可以，但是第二种更加简单一点，在这里我就不测试释放GIL之后的性能了，前面已经做过了\nGIL总结 # 通过前面我们的测试，GIL这个东西其实只是一把锁，我们经常能听到很多人抨击Python关于GIL问题，这就给人一种错觉Python这种语言在设计上有弊端，在前面测试我们也发现了就算是C++或者C假如不正确的使用锁其实也会有这个GIL问题，GIL的问题的并不是“编程语言”的锅，主要是我们自己的代码造成的死锁，所以面对GIL的时候，不需要困惑，它就是一把“锁”，把它打开，而不是碰到它就跑，你会发现它也就是一把“锁”而已。\n","date":"2018-09-15","externalUrl":null,"permalink":"/posts/backend/framework/python/%E8%A7%A6%E6%91%B8python%E7%9A%84gil/","section":"博客","summary":"Python的GIL一直是被大家攻击其语言的一个弊端，每次在","title":"触摸Python的GIL","type":"posts"},{"content":" 引言 # 其实一开始没有想到写关于Python的加速，一开始只想好好了解一下C++这门语言，没想到最后研究来研究去，基本上把所以加速框架都试验了一下，这篇博客就谈谈我对Python加速的看法\n首先我先谈谈C++，虽然我上大学之前就自学过C，但是对于这个C的升级版还是没有过多的了解，花了几天时间学习，发现C++这门语言还是不错的，至少在兼容性上，它能兼容C还有以前的版本。\n然而作为一个用惯了了脚本语言的人来说，C++最麻烦的就是他的第三方库管理，当然对于类Unix系统有自己带的包管理器（如ubuntu上的apt，CentOS上的yum）可以来安装第三方库（就是我们平常为了安装一些软件，比如要先apt install xxx-dev那些库），由于这些都绑定了平台的，所以你经常能看到有些软件自己编译会列出各个平台下依赖的包，然而对于一些比较新的库（比如googletest，就得去Github上掏下来自己编译安装了。\n吐槽完了C++的缺点之后，我们不得不说C++的优点了，虽然比较难装（相比于脚本语言）但是那个速度真是贼快，用腾讯开源的协程库，单台机器就能开启千万协程而且内存不超过2个G，想我大Python开个一万都很嘚瑟了。C++在性能上真的的碾压的。就是因为C++性能上要求到极致，所以它才会有那么多的前面安装的缺点，因为C++是面对硬件的，对于不同的硬件，C++想做到最快，那么通用的代码就不可能的，通用就代表损失性能。然而让我全用C++写代码是不可能的，脚本语言用的多爽呀。所以了解完了C++的强大之后，我就越发的想了解怎么结合两者的方式来提升｀Python｀速度，最后把所有加速手段都测试了一遍，所以就有了这篇博文。\nPS: 之所以花这么多时间介绍C++是因为LLVM就是使用C++写的，而numba依赖LLVM来动态编译出比C更快的机器码，这个也就Python最后能比C还快的主要原因\n（Python）+（C++）难在哪里 # 大家都知道Python有很多实现，我们这里说的Python是CPython也是最常见的实现，它是由C语言编译出来的，我们的目标就是把两种语言给混合起来，C+C++。\n我们看看其他语言，比如Java其实也可以混合C++代码，它是采用JNI的方式来进行交互的，如果你了解这种方式，你会发现也非常麻烦，得先写Java的类，然后再生成C/C++头文件。然后你再写C/C++代码，其实我很讨厌这种方式，我希望能把C/C++和你的语言这两种分离开来，我们能简单通过某种方式桥接一下让两个项目能够连贯起来。\n我们现在来看看Python是如何调用C++的代码。在这之前我先提一下Python与C的关系。\n其实Python和C一直非常友好，相比于其他语言，Python在支持上一直尽最大努力，因为Python开发者也知道Python非常慢（相比于C，C++，而且还有GIL的存在无法使用多线程密集CPU计算），所以Python开发者直接在内库上提供支持：ctypes，一个专为调用C代码的库。你只有编写少量代码就能让Python运行你的C代码。理论上你碰到性能问题直接写C就行了，但是我们为什么还要让Python运行C++来加速呢\n四个字：比C更好，C++由于在性能上与C不相上下，而且比C要高级的多（面对对象等），编写速度与维护上比C更加好，而且要知道现在最流行的Java编辑器都是C++写的，还有很多高性能数据库以及机器学习库都是C++写的，虽然在Python中写C更加简单，但是我们还是希望能够用面对对象的方式来编写代码，毕竟我们主要使用的高级语言也是面对对象的\n也正是因为C++提供了一些C没有的面对对象，以及高级特性，这就让我们融合C和C++带来了一些困难。\nPython为什么能够调用C++代码 # 我们从调用顺序来看，我们其实想用C代码（Python本质其实是C代码）调用C++，C++比C要高级，出生的也更晚，所以C其实是不知道C++这门语言的，所以C能调用C++，其实是C++对C的一种兼容，这种兼容是C++提供的\nC++作为一门偏底层语言，它最终的目的是生成二进制码，C最终也生成二进制码，这个二进制码能直接在CPU里面运行，大家都知道一个代码复用的概念，在二进制层次上，就有这个链接库概率，反正无论谁是最终调用主体，被调用方只需要提供一个规定好的函数库，那么就能实现跨语言的一种交互。\n但是这个交互存在一个问题，C++比C有着更加特性，比如说类，C没有这个概念，假如C++在动态库里面想让C能够调用一个类方法，C根本不知道怎么用，一个类要使用必须牵扯到类初始化，类析构等等。所以C++提供一个关键字extern \u0026quot;C\u0026quot;\n这个关键字就是告诉C++编译器把这个块域里面的东西编译成C可以接受的，当然有个前提条件里面代码声明必须是C式的，也就是只能使用C关键字来声明函数结构体什么的，但是在函数内部你可以调用C++代码，声明一个类什么的，最后返回结果。\n用一句话来总结这个关键词的作用就是：告诉编辑器和用户，里面的函数东西，不管中间过程，只需要在“开头”（函数声明），结尾（结果返回）是C模式的，那么这个函数就能在C里面用\n最后我们总结一下Python能够调用C++的代码的原因：只要C++能够\u0026quot;写\u0026quot;成C代码，我们就能调用。这时候你可能有疑惑了，如果把C++写成C那么我们还不如直接写C代码，何必如此复杂的研究这么久了。但是你有没有想过为什么Python是用C写的，最后却能拥有C++、Java这些语言的一样的类特征这个概念。\n这里我们必须要了解一个名词“语法糖”，在我们看来我们能在Python、Java、Ｃ++中使用一些面对对象的特性，比如类、继承、接口。其实这些都只是一些语法糖而已，在这些实现的底层，比如说Python它就是用C的函数来帮助我们构建这些语法糖，我们看到的一个对象的系统函数，其实它是Python帮助我们把一连串函数绑定在一个module上面，虽然表面上我们新建了一个对象，调用了一个对象函数，其实在C层我们就是调用了一连串的函数来完成一个对象的分配\n我们可以在官方文档中找到这部分介绍，官方文档告诉我们只要将列表的函数赋给一个模块（module）我们就让你的C/C++代码给Python一个模块可以使用，从官方文档我们就可以很清楚看到语法糖\nPython的文档非常丰富，理论上我们能够根据文档完成复杂的C++代码与Python交互，但是我们从文档上可以看到，这个过程是非常繁琐的，相比于调用C的简单，为了实现调用C++的类和数据类型，我们得写很多中间代码进行转换，差不多就重新写了一遍C++的实现\n当然作为以简单为美的Pythoner早就发现这个问题，也就这个问题开发了ctypes、cffi、numba等框架帮助，就连在C++大名鼎鼎的boost库中也提供了boost/python来帮助Python更加简单的调用C++，接下来我就根据我对下面这些库来谈一谈我的看法\n框架简析 # 单纯的介绍这些库的功能太枯燥了，我就按照我对这些的库的理解将他们编成历史故事（真实出现的原因可能不是这样的）\n话说在Python作者设计Python之后，它发现Python实在是有点慢，为了能加速它就把Python的CAPI告诉社区的人让他们自己编写C代码然后让Python去调用它\n但是这个API实在是太繁琐了，要写太多附件的C代码了，有些人就发现这个问题，他们设计了一种脚本程序，你只要把你想调用的C函数包在%{里面就能帮你生成PythonAPI的C代码，这样减少了不少代码量，这个框架叫做Swig。\n大家在使用Swig的时候发现一个问题，这个Swig要生成的一个很大的C函数，C++开发者发现了这个问题，他们跟Python开发者说你们是不是瞧不起C++，这个函数这么不优雅，竟然想跟我们代码混起来，想用C++我们帮你，你要生成什么函数告诉我，我帮你生成你引用一下我这个库就行，这样大名鼎鼎的boost::python就开发出来了\n你开心的用起来boost::python来包装一下代码，这样写完C++代码再引入boost::python把Python需要的函数定义一下，编译，OK，但是Windows用户不开心了，这个boost::python是在boost项目下的一个子项目，为了在Windows安装，还得下几百兆的软件包，要是碰到网络不好得下一天。这个时候Python大牛出来了，啥，这么麻烦，我来开发一个包，把boost::python从boost的掏出来，你只需要pip一下就行\n经过几个\u0026quot;小时\u0026quot;开发，pybind11开发出来了，还是原来的配方还是原来的味道，管他Windows还是Unix，直接pip一下就能使用boost::python 一样的语法来用了\n就这样安安稳稳的过了一段时间，大家很开心用Python包轻轻松松解决生成PythonC API代码的功能。但是随着大家用的越来越多，大家发现怎么我用pybind11调用C++跑的有点慢，Python大牛开始研究，重要他们发现由于pybind11由于秉承Python的简单至上，很多东西它都做了”通用性“，比如它帮你自动把C++的Vector的类型转成Python的list，这样程序在编译时候不会报错，但是由于这种类型转换太多了，严重的拖累了C++的速度，所以pybind11虽然用的很开心，但是速度却比原生的Python C API要慢\n这个时候精通编译原理、Python、C++的大牛出现了，它发现解决这个问题的办法很简单，创造一门中间语言，这么语言可以详细的定义怎么从C++到Python的中间过程，在pybind11 中这个完全是一个黑箱子，只有把这个黑箱子拿出来，这样我们就知道你想怎么调用C++，这样就能设计更加优秀的PythonC API的代码。最后Cython出现了，它的出现让那些苛求性能的人闭上了嘴，它自动出来的生成PythonC API代码近乎人工编写，在这样强的性能加持下，它的速度近乎原生\n至此在生成代码PythonC API的中间代码的三方库尘埃落定，没有人想到有更好的办法来优化这一个方向。但是苛刻的人无处不在，他们攻击不了它的性能，只能攻击它的生成方式\n为了使用Cython必须编译它，要么借用setuptools来简单这个步骤，要么自己手动编译，一些开发者叫嚣着，都说Python是个动态语言，怎么还要编译呀，麻烦死了，这个时候一些开发者就站出来了，他们觉得这是个挑战，他们想解决掉它，于是cffi被开发出来了，你不需要用专门的文件存贮C/C++代码，你可以像调用函数一样把C/C++函数原文作为参数传进去，实现动态加载，但是这种动态性还是付出了代价，速度有了一定影响，虽然还是比Python快，但是远远比不上Cython，有得必有失\n这个时候精通汇编的大佬出现了，他们觉得动态加载这个地方还可以加强，他们觉得不需要我们在Python里面写C或者C++，你写一个Python函数，用一个装饰器包装一下，他们直接从底层出发，反正Python最终会编译成机器码，把Python函数的机器码加上类型（Python函数的参数可以是“鸭子”类型，不是强类型），省掉Python冗余的类型推断，直接从机器码层次上进行优化，最后编译成二进制接口给Python调用（背后使用了LLVM进行编译，这里就不详细介绍了），最终它的运行速度小胜Cython，并且比C还略胜一筹，这个就非常恐怖了，因为C基本上是除了汇编以外的速度标杆，所以懂汇编的大佬不要惹，太恐怖了，这个库的名字叫做numba，现在这个库已经开发6年多了，由于涉及到从Python源代码到了机器码实在太复杂了，所以仍然在开发中（主要适应各种硬件以及平台），目前处于0.40.0版本，基本上在主流平台使用是没有问题的。\n对于各个库速度的测试可以看看这篇博客，可以看到numba完胜C和Cython\nPS: 在这里我没有提ctypes因为它是原生的，而且它对C++支持并不很好\n总结 # 在速度方面numba加持的Python无疑是No.1，但是它也有几个缺点，一个就是目前还处在开发阶段(目前是0.40版本，还没有1.0版本，而且issue有500个open状态，我在试验的时候也发现存在一些在issue的bug），第二个就是它目前支持能在函数内部运行的库只有numpy（当然这个也是它的设计的一个初衷，就是加速numpy与Python的混合代码）\n当然它的优点完全可以盖过它的缺点，优点有很多，首先第一个它的速度，在LLVM加持下比C更快简直让人震惊，第二个是它调试和维护非常方便，都是由Python编写的，去掉装饰器就是Python代码，直接在IDE里面调试不知道多爽，上线的时候加上注释器跑的飞快（还能丢掉GIL）。目前numba还处于开发过程中，现阶段仍然有很多bug（500个Open的issue），不过正是由于大家都对他非常期望，所以它的issue才那么多，也希望numba能够越来越好，让Python真的起飞。\n","date":"2018-09-12","externalUrl":null,"permalink":"/posts/backend/framework/python/%E5%A6%82%E4%BD%95%E8%AE%A9%E4%BD%A0%E7%9A%84python%E5%83%8Fc%E4%B8%80%E6%A0%B7%E5%BF%AB/","section":"博客","summary":"引言 # 其实一开始没有想到写关于Python的加速，一开始只想","title":"如何让你的Python更快","type":"posts"},{"content":" 引言 # 这两个星期的工作主要是对千万文本数据的处理，由于我以前没有接触过类似的数据量，所以我就把我在处理这千万数据的过程中遇到的问题以及解决的方法总结一下\n明确目标 # 完成任务之前我们必须要明确自己的目标，首先谈一下数据，数据是两张表，一张是文章列表，一张是文章内容，每篇文章都牵涉到一些人，我们的目标就是给定一些搜索条件然后把最可能相关文章给找出来\n这个任务有点像实现一个搜索引擎，我们通过输入关键词把相关的网页寻找出来，简单点来实现就是直接使用SQL的Like查询，但是这里存在两个很大的问题\n搜索精度不准，假如我们搜张华可能有关张华硕的人也会出来 搜索耗时太长，在千万级文档中全文搜索速度非常慢 我们希望我们能精确的实现查询，而且我们希望我们的查询能够实现毫秒级的速度。所以我们就尝试使用ElasticSearch来当我们“数据库”，并且放弃系统默认的分词，自己”手动分词“，来实现精准快速查询\n所以我们的目标很简单，将数据从MySQL“塞”到ElasticSearch中，然后想办法再\u0026quot;取\u0026quot;出来\n第一个拦路虎“MySQL” # 我碰到的第一个拦路虎是数据库的响应速度，为了将数据完整的从数据库里面取出来（新数据还在产生），我按照id从小到大的顺序一小块一小块的从MySQL中获取出来\n一开始程序运行的挺Happy，速度一直很稳定，但是我发现跑了十万之后速度突然慢下来，一开始我以为解析有问题，我开始打断点，调试，找了半天原来是数据库返回数据太慢了。\n我们来分析一下这个SQL为什么这么慢\nselect * from a order by id limit 10 offset 100000 我们虽然限制返回了10个但是后面有个条件我们必须要后面10万个，为了拿到这10个，MySQL必须要扫描10万个数据先，虽然我们是在主键上扫描会快一点，但是十万毕竟很大，即使一次主键扫描花0.01ms，乘以十万也是很大的\n基本上每次解决数据库速度的时候，我们第一考虑点就是索引，那么我们这里就多聊两句：索引为什么快？\n数据库其实就是一堆数据的集合，它提供工具我们快速获取我们想要，用图书馆来打比方，数据库就是图书馆，他们把所以的图书分好类，你想买什么书，按照分类去寻找就行，这样假设你图书馆有一千万本书，你要找《安徒生童话》，你只要按照这个索引（童话书\u0026gt;丹麦\u0026gt;安徒生)就能找到，假如你想找一本《无类》的书你不知道他的分类，你就得把整个图书馆逛一遍才能找到你要的书了，这就是没有索引的下场。\n在程序的世界里也一样，你想快速找到一个记录，如果不用索引，那么就得遍历了，运气好一下子就能找到，运气不好一辈子也找不到。在\tMySQL中，索引的背后就是B+树，也就是将数据查找最坏结果降到了一个log2N级。如果你想了解“树”为什么这么快，可以看看我前面写的的博客\n怎么来说明这个索引的作业呢，假设你有一千万数据，你最坏的情况下要进行24次查找，24:1000000 达到惊人的41万倍差距，而且当数据越大这个差距越大，从这里我们就知道索引的威力了。\n我们回到前面，为了使用索引，那么我们只能在id（主键）上做手脚了，我第一个想到的是按照id分块，但是我仔细看了看数据库，id不是全部连续的（可能是因为删除过数据），假如我用id固定的区间来的话，获取到的数据可能部分有部分没有，虽然能够实现但是不够优雅，我还得增加处理空数据的代码。\n这时候我想到了，我们第一次获取的id如果能在后面继续使用，而且更新的话那么我们就能使用上索引了，所以我们只有把每一块数据的最后一个id记住，然后去这个id获取下一批数据，这样就能实现又用索引又不用改太多代码。\n那么我们的SQL就改成下面的语句\nselect * from a where id \u0026gt; 1111 order by id limit 10 通过简单的测试原来需要几分钟才能“掏”出来的数据在几毫秒就取出来了，上万倍的差距。至此我们第一个拦路虎就解决了\nPS：在后面看ElasticSearch文档的时候发现他们也提供了一个scroll（全库获取）的超级翻页功能，在他们的参数里面也要提供一个scroll_id，感觉原理应该也是和这个差不多。通过使用索引id来加速“翻页”\nElasticSearch存贮和排序 # 接下来我就介绍，我怎么优化存储和编写定制动态DSL来实现我们想要的功能。\n在我介绍之前，我先简单的谈一下我对ElasticSearch的理解\nElasticSearch简介 # 在我没有真正使用ElasticSearch之前，我就在知乎上听过它的大名，ElasticSearch真正让我震惊的是当我把上千万数据导入到它里面去，它能在毫秒级别给你响应，而我在MySQL调用SQL进行查询得花几十分钟\n我们可以把ElasticSearch类比成一个数据库，相比于MySQL它在查询性能上做到了苛刻的，我一开始想好好介绍它是怎么做到的，但是我发现有人已经总结的非常好了，可以看看这份资料，它之所以能做到这么快的原因就是这个：索引+内存+缓存\nElasticSearch使用倒叙索引让查询时间复杂度降到logN级，使用内存让物理查询速度达到极限，加上一些过滤缓存让其在复杂查询还是简单查询都能保持在一个很平稳的速度\nElasticSearch相比与MySQL还有一个特点，就是对大文本搜索的支持，ElasticSearch对文本默认自动进行分词，并且通过一些高级分类算法（TF/IDF，5.0后使用更加先进的BM25算法），对匹配的文本进行打分，依次返回得分高低列表，而MySQL在大文本检索只有一个全文索引支持，从实现上来看就是一个加了索引的Like查询，所以ElasticSearch在设计的特定算法加持下被称为“搜索引擎”\n但是ElasticSearch同现在商业的搜索引擎，比如Google、百度、Bing这些又有些不同，ElasticSearch传入的是纯文本，所以它只能使用一些TF/IDF算法来计算给定关键词与文本的相关项，但是现在商业引擎输入的是网页，所以现在商业引擎比如Google就使用Google Page Rank算法来再次计算文档相关性。当然现代商业引擎不单仅仅使用Google Page Rank算法，他还会考虑更加因素进去（比如百度的竞价排行，Google的恶意影响网页排行检测），但是从本质上来说，无论是ElasticSearch和现代商业引擎都在做同一件事，给匹配项打分，这就是他们与MySQL的全文检索的不同（MySQL没有后面打分排行的概率，他只有order by的这个概率）\n设计思路 # 一开始我准备直接使用ElasticSearch的搜索引擎来对文章中的涉及到的人进行检索排序，但是我们来考虑这样一件事，假如文章中存在这么一句话：“刘二能吃两碗饭”（涉及到的人是刘二能）。假如我们使用“刘二”去检索，这篇文章中的”刘二能“也能检索到，而我们的目的就是尽可能返回最可能的结果，对于那些不可能的结果一律不返回\n所以我们就不能让ElasticSearch自动帮我们对文本进行分词，但是我们想利用打分这个机制帮我们完成最可能在最前面返回\n所以我们把每篇文章里面的人物解析出来的属性（姓名，出生年月，民族等）设定为keyword类型，这样ElasticSearch就不会对这个字段进行分词，查询的时候也必须全匹配才能命中，由于一篇文章可能设计到多个人，所以我把它用一个list存到一个document里面\n但是这个又引起了另外一个问题，对与一个document里面的list，ElasticSearch会把它进行转换\n我们用官方文档的例子解释，我们存入了下面这个document\n{ \u0026quot;group\u0026quot; : \u0026quot;fans\u0026quot;, \u0026quot;user\u0026quot; : [ { \u0026quot;first\u0026quot; : \u0026quot;张\u0026quot;, \u0026quot;last\u0026quot; : \u0026quot;华\u0026quot; }, { \u0026quot;first\u0026quot; : \u0026quot;李\u0026quot;, \u0026quot;last\u0026quot; : \u0026quot;四\u0026quot; } ] } ElasticSearch会把它转换成\n{ \u0026quot;group\u0026quot; : \u0026quot;fans\u0026quot;, \u0026quot;user.first\u0026quot; : [ \u0026quot;张\u0026quot;, \u0026quot;李\u0026quot; ], \u0026quot;user.last\u0026quot; : [ \u0026quot;华\u0026quot;, \u0026quot;四\u0026quot; ] } 这样你查询这个人张四，我们发现上面这个文档也返回了（选了user.first列表的第一个值，user.last的第二个值，这个结果明显是错误的，我们怎么才能避免ElasticSearch的“自作聪明”呢，答案很简单我们把user声明为nested对象，这样ElasticSearch就不会把它拆开了而是把它当做两个文档（有些人可能会说这个会不要影响它的速率，恰恰相反，ElasticSearch会经常使用类似技术来加速，详情可以看上面的博文)\n现在我们解决了重重困难终于要到排序的阶段了，然而我们没有使用string类型（支持TF/IDF算法）而使用了keyword类型，导致我们没有办法使用ElasticSearch提供的高级排序算法，所以我们得自己手动进行提分，怎么来提分呢，很简单使用boost\n前面我们提到了我们的文档可能解析出来对象多个属性（姓名、年龄、性别、居住地），但是有些文档也可能没有这些信息，我们查询的时候是有一个信息列表（这个人姓名、年龄、性别等等），所以我们使用boost对命中的信息越多的进行提分，所以我们最终就能完成命中越多信息的排在越前面，当然命中信息少的也会被筛选出来只不过位置稍微靠后\n总结 # 通过这次直面千万数据，让我学习到了不少，虽然一开始目的只想简单搜索出来最匹配的数据，但是在实际过程中，通过不断对产生结果提出问题，最终实现了一个比较满意的产品，整个产品在不断的优化过程中逐渐成型并且稳定，我觉得对我帮助最大就是撰写设计文档，并且在产品成型的过程中把结果反馈上去，最后慢慢迭代一个最好的版本\n","date":"2018-09-06","externalUrl":null,"permalink":"/posts/backend/framework/bigdata/%E5%8D%83%E4%B8%87%E7%BA%A7%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86%E5%B0%8F%E7%BB%93/","section":"博客","summary":"引言 # 这两个星期的工作主要是对千万文本数据的处理，由于我以前","title":"千万级数据处理小结","type":"posts"},{"content":" 毕业近两个月了，然而这两个月对自己的状态不太满意故写下这篇博客反思\n引言 # 大约是去年十一月在室友的带领下走上了健身这条路，说起来也搞笑，当初室友邀请我去健身，然而我每天享受免费操场跑步，反而对于要花钱的健身嗤之以鼻，虽然我知道那些肌肉硕大的大块头都是从健身房出来的，但是我一直没有想过去练出他们一样的肌肉，我运动本身的目的就不是为了炫耀什么的，主要是想要一个健康的身体，然而南昌十一月妖风阵阵，虽然还没到冬天，晚上妖风能让你吹到感觉到冬天的气息，然而中午却非常热，我尝试过早上跑，但是我个人体质太容易出汗了，跑完就需要洗澡，然而早上没有热水（尝试过冷水，差点让我冻僵）\n正好这个时候室友每天去健身，在我的印象里健身房好像有跑步机，于是我一狠心就花了钱办了一张健身卡，刚开始我是奔着跑步机去的，但是我们班长拉住了我开始教我健身动作，我也是在室友和班长的带领下走上了健身这条“不归路”，可怜的跑步机我就没有用过几次，本来一开始是奔着跑步机去的呀。\n为何健体 # 到现在自己已经在三个不同的健身房办了卡了，扯远了，健身的确是一件神奇的事，我一直以为运动会让你瘦下来，没想到健身却让我“胖”起来，当我健身到六个月时，我们班长和室友惊奇的告诉我，我的胳膊整整“胖”了一圈，他们还记得我刚来健身房的时候胳膊细的同木棍一样，然而健身对于我来说却并没有很多不同，我只是找到一个地方可以挥洒我的汗水，找到一个地方可以突破自己的极限\n健体而不健心 # 健身真的是一件很有成就感的东西，它的成就感不是在于别人夸你的肌肉有多大，而是你能清楚的找到自己的极限，我原来是一个脑洞很大的人，每天大脑天马行空，像一个中二少年一样总想着自己是“超级赛亚人”，能像绿巨人一样发怒把汽车给掀开，一拳把树打倒。当你在健身房，你拿起一个5KG的哑铃，然而你倾尽全身力气却没法做一个标准的动作，你这个时候才知道自己不是”超人“。健身让你更加了解你自己，慢慢的你就接受了这样的自己，健身的成就感也就在于你能慢慢的感受自己的成长。\n也可能是过于关注自己的身体的进步吧，而自己一直忽略了自己心理的进步。自从踏入社会，开始工作。对于工作我一直是打着自己十二分的精力去做，然而对于工作结束后的那段时间自己一直没能够充分利用。主要的原因就是自己的不够自律。一方面也有自己身体的原因，自己健身都是在下班后，健身完之后自己总是筋疲力尽，本来需要充分利用的时间全部被自己的羸弱的意志给消磨掉了，虽然自己一开始能够控制自己，但是一不小心大脑发出疲惫的信号，慢慢的自己就变成“葛优躺”，大把时间就在不经意的打开手机玩上两把，我看一下微信，再看一下QQ，大块时间被手机打碎成一小块一小块，搞得一晚上下来该做的没做，不该做的做了一大堆。\n在这里我必须要深刻的反省，即使是因为身体疲惫的原因，最主要的还是自己不够自律的原因，我其实有很多想法，有很多想做的事情，但是总是在不自律的自己把时间给浪费掉了，毕业之后书自己也很少看了，记得以前在图书馆捧起一本书津津有味的看上一晚上的酸爽。然而现在的自己，即使是看书也要打开电脑，左手拿着手机，右手捧着书，眼镜盯着电脑看一会，书再看一会。书走马观花，看了也是白看。原来经常更新的博客也停下来了。\n总结 # 自己也不下什么军令状了，我自己不是一个圣人，我会松懈，我会娱乐，但是我需要自己记住不要因为自己身体的虚弱而导致自己内心的失防，过去自己一直在因为自己过分迁就“健体”而忽视”健心“，自己要牢记只有内心强大才能强大。只要未来的自己不要因为过去的蹉跎的后悔就够了。\n","date":"2018-08-25","externalUrl":null,"permalink":"/posts/essays/life/%E6%AF%95%E4%B8%9A%E4%B8%A4%E6%9C%88%E5%8F%8D%E6%80%9D/","section":"博客","summary":"毕业近两个月了，然而这两个月对自己的状态不太满意故写下这篇博","title":"毕业两月反思","type":"posts"},{"content":" 这段时间没有写博客，一个原因是由于刚毕业没了学校的学习动力反而下降，另外一个方面由于花了很长时间研究编译原理，然而自己却对它没有太多理解，所以也就没有整理自己的知识，现在慢慢稳定下来，会继续像以前一样更新博客\n引言 # 为什么要介绍XPath呢，我一直以为我对XPath还是比较了解的，但是随着我对XPath的了解越来越深，我就对它的越来越敬佩，\n简单来说，我以前认为XPath对结构性文档只能是一把“枪”，指哪打哪，没想到它是一个“巡航导弹”，自动追踪目标。\n接下来我们就慢慢从XPath的基础来谈谈其威力\n什么是XPath # 首先我们要知道XPath是一种语言，你可以理解它是正则、也可以理解它是SQL，他们的目的都是从数据中找到我们想要的东西。相比于SQL从数据库中获取数据，XPath是从一个XML文件中获取数据。\n好的，我们知道XPath要操作的对象，什么是XML，它是一种结构式文档，我们也可以把它看做一种树结构。\n\u0026lt;root\u0026gt; \u0026lt;son\u0026gt; I' m son \u0026lt;/son\u0026gt; \u0026lt;/root\u0026gt; 如上面就是一个简单的XML文档，首先从一个父节点点出发，到最后的一个父节点结束，中间可以有很多子节点，也可以有孙节点，但对于每个节点来说，其父亲只能有一个。\n这种文档的出现是由于我们编码程序中树结构出现而出现的一种数据。相比于正则直接操作文本，XPath要面对的是是一堆有规律的文本，虽然我们也能使用正则来操作XML文档，但是正则无法捕获这种XML的关系，而这个也是XPath最有力的地方。\n在这种关系中，我们最常使用也是最核心的就是父子关系，这个关系简单的通过一个/就能体现，比如现在我们把上面的XML复杂化给他添加一个儿子\n\u0026lt;root\u0026gt; \u0026lt;son id=\u0026quot;1\u0026quot;\u0026gt; I' m son1 \u0026lt;/son\u0026gt; \u0026lt;son id=\u0026quot;2\u0026quot;\u0026gt; I'm son2\u0026lt;/son\u0026gt; \u0026lt;/root\u0026gt; 为了获取第二个儿子我们简单的使用这个XPath语句就能获取到\n/root/son[@id='1'] PS: 当然在XPath中我们可以使用//来代表一个泛指，通过//son[@id='1']我们可以把儿子找出来而不关心他的父亲，甚至更进一步，我不关心它是谁，只要它的id为1就行，我们用node()函数来替代一个节点，这样只要//node()[@id='1']就能拿到id为1的节点了\n我们来看看这个XPath，我们定义了一种关系root和son的父子关系，XPath的威力就是能用很简单的语句来定义一个节点的关系，在这句中，root和son都是节点，我们使用/来约定节点父子关系，使用[]来定义节点与自己内部节点或者属性的直接的关系（@是获取属性）\n要掌握XPath必须要明白，XPath重要的就是“面”和“点”的关系，“面”代表节点，“点”代表属性，对于面来说，它可以包含很多个点，对于点来说，它有可以看做由很多个更小的面组成（微观上）\n就以上面的例子，对于root和son这两个节点，其中root是父节点，我们可以用很多个属性来定义它，比如root[count(son, 2)]（意思是选择有两个son的root），其中对于父节点关系的中son节点来说（有点绕），他又可以用属性来约束比如root[count(son[@id], 2)](意思是选择有两个son的root,并且每个son都有id这个属性）。从这里我们可以看其实节点和属性是可以相互嵌套的。\n从上面这个小例子我们可以看到，XPath的威力就是它可以用来非常详细的约束节点与其他节点或属性的关系，这种关系可以是绝对的，也可以是相对的，一切取决你的取舍，绝对代表严格，相对代表宽松。\nPS：当然我们这里的属性是一种宽泛的理解，在XPath中节点还包括text值，我们可以把它看做节点的一种text属性。\nXPath的其他关系 # 前面我们介绍了XPath中最重要的一种关系：父子关系。这个也是我们使用XPath使用的最主要的一种关系，现在基本上网络上的教程都是基于这种关系的，我们这篇博客主要不详细介绍这种关系，你可以在w3cshool上了解更多内容。\n我们先用问题来引入其他的关系，我们再把上面的简单XML进行修改\n\u0026lt;root\u0026gt; \u0026lt;son id=\u0026quot;1\u0026quot;\u0026gt; I' m son1 \u0026lt;/son\u0026gt; \u0026lt;target\u0026gt; son1 target\u0026lt;/target\u0026gt; \u0026lt;son id=\u0026quot;2\u0026quot;\u0026gt; I'm son2\u0026lt;/son\u0026gt; \u0026lt;target\u0026gt; son2 target \u0026lt;/target\u0026gt; \u0026lt;/root\u0026gt; 我们引入两个目标，现在我们想拿到son（id为1）的旁边target，假如我们使用父子关系，使用/root/target[1](XPath索引从1开始）也可以获取到，但是这里引入了一个约束，必须是root下第一个target节点，假如这个XML它是随机的，son和target是一个集合，但是他们的位置不定，这个时候我们不能仅仅依赖父子关系来确定节点位置。\n这里我们引入兄弟（sibling）这个概率，son和target是一队兄弟，我们能通过知道son的位置从而定位到target的位置，那个这个XPath该怎么写呢，首先我们要确定son的位置\n/root/son[@id='1'] 接下来我们通过定位的son来拿到它后面的兄弟（也有前面的兄弟语法）\n/root/son[@id='1']/following-sibling::target[1] 在这里/following-sibling代表它要找到接下来的兄弟，后面::target[1]是进一步限定我是要拿到兄弟里面第一个target，我们可以通过这个网站在线测试一下我们的XPath\n当然我们可以通过第二个son来找到它前面的兄弟，对应语法是下面的\n/root/son[@id='2']/preceding-sibling::target[1] 在前面我们可以看到这个following-sibling和preceding-sibling他们都是一种寻找兄弟关系的，其实假如我们把-sibling去掉，他们能更加宽泛。\n我们把son和target包起来，这个在现实中可能更常见\n\u0026lt;root\u0026gt; \u0026lt;group\u0026gt; \u0026lt;son id=\u0026quot;1\u0026quot;\u0026gt; I' m son1 \u0026lt;/son\u0026gt; \u0026lt;target\u0026gt; son1 target\u0026lt;/target\u0026gt; \u0026lt;/group\u0026gt; \u0026lt;group\u0026gt; \u0026lt;son id=\u0026quot;2\u0026quot;\u0026gt; I'm son2\u0026lt;/son\u0026gt; \u0026lt;target\u0026gt; son2 target \u0026lt;/target\u0026gt; \u0026lt;/group\u0026gt; \u0026lt;/root\u0026gt; 假如我们还使用上面的语句，我们会发现，我们没法找到语句，这个时候你把兄弟这个约束去掉\n/root/group/son[@id='2']/preceding::target[1] 你会惊奇的发现XPath准确的找到我们的目标，这个令人震惊的是它能实现一种“翻山越岭”的查找。\n假如你使用正则或者普通的父子关系，你必须先找到它的group然后再使用for循环来遍历所以的group找到son\u0026hellip;.\n总结 # 我们使用简单一个前后关系就能轻松实现上百行代码，当我以前不了解这个XPath的关系约束前，为了寻找这个约束写过几十行代码才能定位，而现在简简单单一行就搞定，我们不得不佩服前人的智慧，我只想说一句“真香～～～”。\n","date":"2018-08-09","externalUrl":null,"permalink":"/posts/backend/framework/python/%E4%BB%8E%E5%85%B3%E7%B3%BB%E8%A7%92%E5%BA%A6%E6%9D%A5%E7%9C%8Bxpath/","section":"博客","summary":"这段时间没有写博客，一个原因是由于刚毕业没了学校的学习动力反","title":"从关系角度来看XPath","type":"posts"},{"content":" 引言 # 之所以有这个疑问，是上次阅读Java基础书时碰到讲解char类型没有看明白，并且在代码验证过程中错误的理解了代码的意思，导致我对这么个简单问题产生疑惑并且“恶意揣测”Java内部的黑魔法，这里就把我如何走上歪路，并且最终找到“正确”的道路的故事讲出来\n问题的产生 # 我们知道Java是采用Unicode进行内部编码，但是使用UTF-16作为外部编码。\n怎么来理解这个东西呢。首先你要知道Unicode是在我们熟悉的GB 18030、BIG-5、ISO8859-1之后出现的，它的出现就是为了统一全世界的编码，因为前面这些编码都太片面了，只包含自己国家或者少数几个国家的字符。\nUnicode的目的就是包括全世界的编码，并且给未来可能出现的编码留下位置，你可以理解为它是一张大“表”，一般我们使用16进制来表达它，并且在前面加上U+。例如U+0041代表字母A，但是这里有个历史问题\n一开始我们知道Unicode为了包含全世界的字符从ASCII的一个字节扩展到两个字节，就能包含65536个字符了，但是随着字符包含越来越多，我们逐渐需要更多字符了，最后扩展到U+0000 -\u0026gt; U+10FFFF去了，为了表示这些我们必须使用三个字符，假设我们不考虑内存成本，每个字符都使用四个字符来表示（不使用三个是为了内存对齐），那么问题就解决了，大家都用Unicode来表示，这样我传给你一串字符你就能秒懂了。\n但是学过信息论就知道，单字符越长信息熵也就是信息量就少，其实在日常通信中我们并不是每个字符都会用到，为了提高效率，我们可以使用霍夫曼、香农编码技术对信息重新编码，这个就是UTF-8、UTF-16等现代编码的理论基础。\n这就好比特种部队手势，我们把作战命令（Unicode）需要的指令放到手势（如UTF-8）里面，这样几个手势就能表达复杂的作战计划（假如用嘴巴说的话）。\n接下来我们就从JAVA和Python来看，编码与其关系\n表面兄弟：JAVA # Unicode对于JAVA来说，只能算是表面兄弟，虽然内部支持Unicode编码，但是其本质还是基于UTF-16编码，为什么要这么说呢。\n我们来回顾一下，我们知道Unicode的范围是U+0000-U+10ffff，这意味着我们没法用两个字节来表示，但是在Java里面char类型字节为2字节，而对于字符串类String来说，其组成就是一个char字组，对于小于U+10000的Unicode码来说，String对象最小组成单位就是char，但是对于大于U+10000的Unicode码来说却是char数组，我们用代码来展示一下两者之间的关系。\nchar[] chars = Character.toChars(0x1f121); String s = new String(chars); 而且我们将s输出的话，会发现它是一个字符，但是它的length却为二，而且我们将s每个字符转换成二进制你会发现他们的值依次为0xd83c和0xdd21，他们存贮的值全部以UTF-16的格式存贮，具体编码详细我就不细说了，下面资料介绍的很详细（需要翻墙）。在Unicode里面占一个字符的值，却以两个基本类型存贮，当然为了维持这种“表面兄弟”的关系，Java也使用了“码点”来支持一下兄弟，只要使用codePointAt代替charAt，用codePointCount代替length，我们也能处理超过U+10000的Unicode编码（对于不超过U+10000的字符那就是“真兄弟”）\n当我不知道一个char只能放两个字节的时候，我强行使用char c = (char)0x1f121来“存”一个超过U+10000的Unicode码，结果被Java无情的溢出掉，只取到了部分值，但是我却误以为Java有黑魔法能用两个字节存贮了三个字节才能存下的值，乃至我闹了个笑话。\n总结一下Java是一个非常严谨的语言，规定死的东西就不会变，表面上看Java能够支持Unicode编码，但是实际上他只是编译器支持，比如你写一个🄡（0x1f122）的值来赋给String如下面：\nString ns = \u0026quot;🄡\u0026quot; 表面上看，Java完全支持Unicode码，但在实际的上面他内部还是用UTF-16进行编码，只是在编译的时候帮我们将0x1f122转换成为两个 0xd83c和0xdd21存贮在char字符组里面。\n其实这个表面兄弟是相对的，从Python3``Unicode支持来比较一下就能发现不同。\n亲兄弟：Python # Python3对Unicode是非常友好的，它在明面上完全按照Unicode的编码表使用来存贮Unicode码，对应它的Unicode字符串，最小单元都是Unicode码，多说无意，上代码。\nc = chr(0x1f122) print(len(c)) # = 1 print(type(c)) # str 我们可以看到我们得到的最小的码元是字符串str类型，无论这个Unicode码是否大于U+10000，Python都把它视为一个基本单位，这样避免了你对其进行一些误操作，插句话来讲讲怎么得到这个大小呢，我们使用sys.getsizeof方法就能计算出来\nsys.getsizeof(chr(0x1f122)) # 80 sys.getsizeof(chr(0x1f122) * 2) # 84 由于Python使用一些字段来标注类型，所以直接使用sys.getsizeof得不得一个Unicode码需要的字节，所以我们计算两个的差，很清楚的就能得到一个Unicode码使用四个字节，你可以依次乘下去，而且你发现一个有趣的现象，对于小于U+007F的Unicode码，其大小为一字节，而对于U+0080-U+07FF其大小为两字节。具体可以看参考资料，Python内部是使用UTF-8来存贮Unicode码的，但是Python将这一切都隐藏起来，你从表面上看好像一个Unicode就是一个最小单元，对于其底层我们不得而知，我们可以从侧面来验证一下\ntimeit.timeit(\u0026quot;'中国人'.encode('gbk')\u0026quot;) \u0026gt;\u0026gt; 0.6366317819999949 timeit.timeit(\u0026quot;'中国人'.encode('utf-8')\u0026quot;) \u0026gt;\u0026gt; 0.2109854949999317 我们可以看到将Unicode编译成其他编码方式，其中utf-8速度是最快的，因为基本上是复制一下就行了，而其他的差距到了三倍\n总结 # 通过前面我们知道，Python之所以 Unicode如此“亲兄弟”是因为做了一层封装得来的，相比Java将Unicode码（使用UTF-16作为底层编码）暴露给出来，Java在底层上却是非常“坦诚”，你想直接使用Unicode码值也可以，Java编译器会帮你把Unicode码值转换成UTF-16，你也可以从UTF-16码生成String字符串，这样底层在实现查找的时候也是使用统一的编码进行。但是也正是由于这么“底层”，代码看起来总不是那么“亲”，相比于Python的“一视同仁”，我们也可以理解这就是这两种语言的各自特点所在。\n总的来说如果你想直接接触代码底层，推荐使用Java，假如你只想研究其本质，推荐使用Python来进行自然语言处理，他的封装能让你不需要了解其内部组成。\n引用 # https://zh.wikipedia.org/wiki/UTF-16\nhttps://en.wikipedia.org/wiki/UTF-8\n","date":"2018-07-01","externalUrl":null,"permalink":"/posts/backend/framework/java/java%E7%9A%84char%E7%B1%BB%E5%9E%8B%E5%88%B0%E5%BA%95%E5%87%A0%E4%B8%AA%E5%AD%97%E8%8A%82/","section":"博客","summary":"引言 # 之所以有这个疑问，是上次阅读Java基础书时碰到讲解c","title":"Java的char类型到底几个字节","type":"posts"},{"content":" Spring的IOC理解 # 什么是IOC # 在这里我们不谈Spring的基础知识,我们知道谈到Spring就会谈到IOC,这个IOC是什么呢,中文名叫控制反转，这个东西是伴随着一些编程思想出现,其实同Java的本身也有关\n就好比我熟悉的Python就是一个鸭子语言,你可以随便把一个值丢掉函数里面去,只要他满足一些特性就能正常运行,但是Java是一种强类型语言,你函数给什么参数,必须传什么参数\n这里就不讨论两张语言的设计优劣呢,Java这种特性也做了一些妥协,我们肯定得为语言的扩展性做点事,谁也不知道未来会发生什么,Java里面使用多态来实现这种扩展,只要他是函数参数的家族成员,他就能上去运行\n这个多态是实现IOC的基础,但是造成他出现的原因是因为设计模式里面的单一职责原则,这个要求我们类功能要单一,我们这里给一个例子来说明这个问题\nclass Car { void run() { System.out.println(\u0026quot;Car running...\u0026quot;); } } 首先我们有一个Car的类,一开始我们只让他有run这个属性,很好,接下来我们想知道是谁驾驶这辆车,于是我们便给这个类加一个字段driver\npublic class Car { String driver; public Car(String driver) { this.driver = driver; } void run() { System.out.println(\u0026quot;Driver :\u0026quot; + driver); System.out.println(\u0026quot;Car running...\u0026quot;); } } 很好我们知道驾驶这辆车的人,接着我们又想知道这个驾驶人的驾龄,如果我们继续给Car加入字段,这样我们就违背了单一职责原则,Car类不但承担了车的功能还承担了人的功能\n于是我们就把驾驶人隔离出来\nclass Driver{ String name; String age; public Driver(String name, String age) { this.name = name; this.age = age; } } class Car { Driver driver; public Car(Driver driver) { this.driver = driver; } void run() { System.out.println(\u0026quot;Driver age:\u0026quot; + driver.age + \u0026quot;name: \u0026quot; + driver.name); System.out.println(\u0026quot;Car running...\u0026quot;); } } 我们重新将类分成两个类来实现了这个问题,但是这个时候又来了一个问题,我们有一个飞行员的也想驾驶这辆车,但是这辆车只能司机来驾驶,但是飞行员和司机开车的动作步骤是一样的,为了复用run这个函数,你开始揪起了你的头发.\n你想呀想突然想到,Java的多态,假如我们声明一个IDriver的接口,让飞行员和司机都继承这个类这样我们只要给车一个IDriver对象就能复用run函数\n//IDriver.java public interface IDriver{ String getName(); void setName(String name); int getAge(); void setAge(int age); } // Driver.java public class Driver implements IDriver{ String name; int age; @Override public String getName() { return this.name; } @Override public void setName(String name) { this.name = name; } @Override public int getAge() { return this.age; } @Override public void setAge(int age) { this.age = age; } } // Aviator.java public class Aviator implements IDriver{ String name; int age; @Override public String getName() { return null; } @Override public void setName(String name) { } @Override public int getAge() { return 0; } @Override public void setAge(int age) { } } //Car.java public class Car { private IDriver driver; public void setDriver(IDriver driver) { this.driver = driver; } void run() { System.out.println(\u0026quot;Driver age: \u0026quot; + driver.getAge() + \u0026quot; name: \u0026quot; + driver.getName()); System.out.println(\u0026quot;Car running...\u0026quot;); } } 我们重构代码把Driver抽象为接口,然后让司机和飞行员都继承它,这样不管我们再添加什么其他的人就能适配这辆车.这个就是依赖倒置(DI)的思想\n网上大部分教程就停留到这里了,这里我们继续探索下去,看看Spring是如何让这个DI更加简单的\n首先我们反思一下,我们使用接口参数让我们的代码符合了设计模式,但是也带来了一些繁琐,我们来用代码\u0026quot;开\u0026quot;这辆车\nIDriver driver = new Driver(); driver.setName(\u0026quot;allen\u0026quot;); driver.setAge(18); Car car = new Car(); car.setDriver(driver); car.run(); PS：当然可以把赋值放到构造器中减少代码，但是由于Bean依赖方法接口来赋值，所以为了后面讲解Bean这里就不采用构造器来减少代码\n代码有2行变成了6行，而且我们发现这个代码现在带来两个问题：\n每次运行都得创建一个实现IDriver的对象 每次我们想换人开车的时候都得修改源代码 而且这些工作都很繁琐，作为一个偷懒的程序员，我可不想给每个用户都重新写一套代码，我们的想法很简单，我们希望这个Car能够开箱即用，其实前面我们已经实现了控制反转了，现在就是要解决控制反转带来的“负面影响”\n而且我们发现了一个问题，假如我们把上面函数放到一个代码里面，每次我们“开车”都得创建一个司机，然而我们还是相信“老司机”的手艺，所以我们也希望是否能够”记住“司机，只让一个老司机开车\n接下来就是隆重介绍Spring的Bean的用法了，前面我们知道我们需要某种机制来去除”IOC“的弊端，我们把每个Car当做一个对象，其实我们需要一个配置文件来记录IDriver这些依赖对象，对象的其实在Java里面表现就是一棵树，所以通俗来讲我们需一个”树结构“数据来存贮依赖关系\n我们程序在运行的时候解析这个树结构，然后依次给对象注入你想给他实例话的对象（比如你把”IDriver“设置为飞行员），这样的话，我们把依赖关系成功放到了配置文件中\n这样带来两个好处：\n想给不同用户使用软件时候，源代码不需要改变，只要给他们不同的配置文件就行 我们可以保存依赖实现”老司机“的复用 所以现在我们理理思路，我们需要的有两个东西\n配置文件 一个加载配置文件并保存依赖的对象 在Spring的Bean中这两个分别对应xml文件和实现ResourceLoader接口对象（有多种实现）\n为了更好的理解Bean，接下来我们就从代码出发来测试这个Bean\n最简单的实现 # 首先我们新建一个Spring项目，无论你是用IntelliJ还是Eslipse都没关系，只能你能引用到Spring库就行，我们复用前面的代码，看看使用Spring Bean来如何解决掉IOC的”副作用“\n我们把前面的类分别放到同一路径不同的文件夹中，接下来我们先创建一个xml文件，什么名字不重要，我们这里先命名为driver.xml\n\u0026lt;?xml version=\u0026quot;1.0\u0026quot; encoding=\u0026quot;UTF-8\u0026quot;?\u0026gt; \u0026lt;beans xmlns=\u0026quot;http://www.springframework.org/schema/beans\u0026quot; xmlns:xsi=\u0026quot;http://www.w3.org/2001/XMLSchema-instance\u0026quot; xsi:schemaLocation=\u0026quot;http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans.xsd\u0026quot;\u0026gt; \u0026lt;bean id=\u0026quot;car\u0026quot; class=\u0026quot;Car\u0026quot;\u0026gt; \u0026lt;property name=\u0026quot;driver\u0026quot;\u0026gt; \u0026lt;bean class=\u0026quot;Driver\u0026quot;\u0026gt; \u0026lt;property name=\u0026quot;name\u0026quot; value=\u0026quot;Allen\u0026quot;\u0026gt;\u0026lt;/property\u0026gt; \u0026lt;property name=\u0026quot;age\u0026quot; value=\u0026quot;18\u0026quot;\u0026gt;\u0026lt;/property\u0026gt; \u0026lt;/bean\u0026gt; \u0026lt;/property\u0026gt; \u0026lt;/bean\u0026gt; \u0026lt;/beans\u0026gt; 写入这些东西，接下来我们看看是否能够通过这个xml文件来直接得到一个配置好司机Allen的车\n随便新建一个类在上面的路径中,我们这里就新建一个Main吧\nimport org.springframework.context.ApplicationContext; import org.springframework.context.support.ClassPathXmlApplicationContext; public class Main { public static void main(String[] args) { ApplicationContext context = new ClassPathXmlApplicationContext(\u0026quot;driver.xml\u0026quot;); Car car = context.getBean(\u0026quot;car\u0026quot;, Car.class); car.run(); } } 输出为\nDriver age:18 name: Allen Car running... 我们成功通过一个xml配置文件和一个ApplicationContext对象实现了一个即开即走的车，而且假如我们想换个司机，我们可以修改配置文件把class换成“飞行员”，而且我们可以发现我们得到的司机都是一样的，验证方法很简单，我就不写代码了，假如我们想换司机怎么办，简单在bean里面加上scope=“prototype”就行（默认值为singleton）\n接下来我们又有一个疑问，假如我们有一辆特别宝马车，我们希望只有某一种加上员能能开（假设只有飞行员），也就是是说，我们其实即不想放弃IOC，但是又不想将这个配置写到Bean里面去，有办法能够解决吗？\n当然有，Spring2.5就支持注解来写Bean配置，对于一些固定的类，我们可以把依赖关系用代码写到类中，这样一方面能够保证IOC，一方面又能实现Bean xml文件瘦身\n由于Spring默认不会去扫描注解，所以有三种方式，第一种是在xml里面用加上一个\n\u0026lt;context:component-scan base-package=\u0026quot;....\u0026quot;\u0026gt;\u0026lt;/context:component-scan\u0026gt; 第二种是使用AnnotationConfigApplicationContext来对象来进行扫描，第三种就是SpringApplication来运行Spring程序自动扫描\n这三种方式假如你最后要做一个web程序的话，第三种是非常方便的，这里我们就不谈怎么使用注解来代替xml文件了，本质上是一样的，其实在我没有理解Bean的强大之前，我比较推崇使用注解来写Bean，但是随着对Bean的探索，我发现xml文件才是最佳选择，他将程序依赖与代码分离开来，假如我们还想用程序依赖写在代码里面，那就违背了Bean的设计初衷\n如果你想了解怎么使用注解可以阅读这篇博客\n总结 # 至此，我们从问题的出现到问题的解决探索了IOC背后的故事，但是你可能会有一个疑问，为什么Spring里面会有IOC问题。\n其实这个也跟Web的发展有关，我们知道从Web的发展，一开始是没有前端的，只有后端，慢慢的后端分离出来前端，Web端页面也被分离出视图层和数据层，随着逐渐分离，也就出现我们前面举到的例子，类越来越多，比如视图层依赖数据层，数据层依赖控制层\u0026hellip;..\n这种层层依赖的问题延生出来的IOC的提出，也就慢慢的促进了Bean这个库的开发，也正是因为Bean我们才能享受静态强类型语言的低耦合的酸爽。\n","date":"2018-05-28","externalUrl":null,"permalink":"/posts/backend/framework/java/springboot/%E4%BB%8E%E4%BE%8B%E5%AD%90%E9%87%8C%E8%A7%A3springioc/","section":"博客","summary":"Spring的IOC理解 # 什么是IOC # 在这里我们不谈Spr","title":"从例子里解Spring IOC","type":"posts"},{"content":" 引言 # 本文是学习Tensorflow官方文档的过程中的一点感悟，本文假设你对矩阵运算有一定的了解，具体可以看看下面资料\n加载数据 # 首先我们得先把数据下载下来，Tensorflow给我们提供了一个函数来进行下载，这个函数为read_data_sets\n这个函数read_data_sets函数很简单，查看在目录下面有没有文件没有就去下载，有就解析加载，一方面方便我们获取数据，一方面方便我们直接开箱即食，但是由于这个默认下载地址是需要翻墙，所以我这里提供一个不需要翻墙的地址，你只需要加载下面的函数\nfrom tensorflow.examples.tutorials.mnist import input_data mnist = input_data.read_data_sets(\u0026quot;input/\u0026quot;, one_hot=True, source_url=\u0026quot;http://yann.lecun.com/exdb/mnist/\u0026quot;) 等几分钟，数据就会下载到当前目录的input文件夹中，这样你下次运行就能直接本地文件夹中加载图片数据了\n观察数据 # 首先我们看看下载了什么数据，打开input文件夹，我们可以看到，Tensorflow给我下载好了四个文件，分为两组，一组训练集一组测试集，每组里面2个文件，一个是手写图片文件，一个标签文件（每张手写的图片代表的数字）\n加载图片数据对于新手来说挺麻烦的，为了让我们专注于模型而不是编程，Tensorflow直接帮我们做好了加载数据的事情，我们上面得到的mnist变量里面就存贮了我们这个项目所需要的数据，我们来看看这个mnist有什么\n我们最关心的就是mnist里面训练数据，这里推荐使用notebook来操作这个数据集，我们首先mnist的训练数据是什么\nmnist数据就是上面这些图片，我们把图片把每个像素的二值化，然后把他们放到一个数组中，每张图片对应一个数组\nmnist训练数据存贮在这两个变量中\nmnist.train.labels mnist.train.images 其中mnist.train.images是一个(55000, 784)的二维数组，其中mnist.train.labels.shape是一个(55000, 10)的二维数组，现在摆在我们面前的其实很简单，通过55000个图片像素值来训练我们模型，以便能让模型能给一张图片像素值来预测图片代表的数字\n这些数字在人看来非常容易辨认，但是怎么能让电脑也能辨别他呢，这就要用到卷积神经网络的力量，通过卷积神经网络，电脑的准确率能到99%，这就非常恐怖了，我们人有时候也会看走眼呢。\n在谈卷积之前我们先谈谈我们以前的做法，这样通过对比就能知道卷积到底做了什么优化\n传统做法 # 其实从传统的角度来看，其实图像识别也就是通过图片的特征值来判断图片代表的含义，但是图片这个东西又很特殊，相比于其他机器学习问题，他的特征值很多，这里我们使用28X28的图片就有784个特征，如果我们图片尺寸再大，这个特征值会变得非常巨大，而且我们知道机器学习需要大量数据才能大展身手，然而每个图片如此巨大，训练巨大的数据集电脑也吃不消\n所以我们必须要将数据进行降维，机器学习里面有很多降维的方法，比如PCA，LDA这些，但是这些方法都有一个问题他们必须把一个图片看做一个整体输入，也就是前面的将28X28转换成一个784的数组，这个数组我们知道，他丧失了一个非常重要的东西维度，我们仔细观察上面的图片\n每个图片其实我们关注的都是数字的二维分布，我们通过闭合的圆的个数来区分8和0，我们通过中间的空白部分来区分0和1，所以我们希望能使用一种新的方法来确定图片特征，一方面能够保存图片的空间信息，一方面能最终数据一维的结果（图片代表的数字），这个就是卷积的引入了，卷积从二维的角度来提取图片的特征，相比于传统的一维提取，它能最大程度保留图片的信息，并且进行深度降维\n从项目了解卷积 # 一开始学习深度学习卷积神经网络，看了很多资料，但是总是感觉并没有很深的理解，至到接触这个项目，从代码的层次上再去理解卷积才给我恍然大悟的感觉\n首先先谈一谈Tensorflow这个库的基础知识，由于Python速度有点慢，所以Tensorflow的后端全部由C++写的，你可以这样理解Tensorflow，Python相当于一个客户端，你可以使用一个session（回话）与服务器（C++）进行交互，这样的话，我们在客户端可以享受Python的方便快捷，也可以享受C++运行的高效性，但是这个也带来一个麻烦，原来Python是一个所见即所得的，现在运行一些东西必须使用session来通知服务器来运行，我们很多中间过程就没法知道，只能通过返回的结果来进行推断了。在官方教程并没有讲太多中间过程，只是一笔带过，所以为了更好的理解卷积神经网络，我们将会以一种很难看的方法运行Tensorflow，但是我们能从这个过程中对卷积的理解更加深刻\n所以接下来我们基本上每个操作都会让后端运行并且分析返回结果，为了方便叙述，我们假设你在运行session.run之前都会运行这个session.run(tf.global_variables_initializer())来初始化所以的变量\nPS：之所以要运行这个，因为我们使用session与C++进行交互，如果我们“不声明”变量，c++会报错的\n下面我们就从这个项目一行一行讲起\n准备数据 # 前面我们知道，卷积就是要从二维空间中来提取我们想要的特征，首先我们把数据还原成二维的\nx_image = tf.reshape(x, [-1,28,28,1]) x是上面我们输入的数据，来我们来检测一下，首先我们声明一个session\nsession = tf.Session() 再从数据集中掏出50张图\ndata = mnist.train.next_batch(50)[0] 接下来我们看看这个x_image变成了什么\nsession.run(tf.global_variables_initializer()) x_image_data = session.run(x_image, feed_dict={x: data}) 我们输入两者的shape\ndata.shape, x_image_data.shape (50, 784) (50, 28, 28, 1) 我们很清楚的看到，我们成功将一维的数组图像（784）变成了二维的数组图像（28X28），其实我们生成了三维（28 X 28 X 1），但是由于我们只有有些图片还会有多个色道（RGB），所以我们为了兼容，声明成28 X 28 X 1\n好的，现在我们成功将一维图片还原成二维的，接下来就是将他们卷起来的时候了\n第一层 # 如果你学过一些信号处理你会发现，深度学习使用的卷积其实并不是原始意义上的卷积，他没有“旋转180”的操作，但是他的形式其实是类似的。这个“积”的操作主要是通过矩阵运算来实现的，为了更好的理解卷这个操作，我从网上找了前辈们辛苦做的动图\nPS： 这个图与我们数据有点不同，我们每张只有一个色道，这个有三个色道，这张图有两个卷积核，但是我们这个第一层会使用32个，但是其实原理都一样，如果你实在理解不过来，你可以先值看最上面那一排\n我们回到这种图，最左边就是图像输入，中间是卷积核，最后右边是输出，我们可以从图中可以很清楚的看到卷积的与我们平常操作不同，首先输入上我们是二维数据，通过二维的卷积核进行矩阵运算，最后我们输出二维结果，这就是卷积的强大之处，不但保留了原来的二维信息而且能够使用高效的矩阵运算来加速提取特征\n现在我们回到代码\n首先是要声明卷积核，我们可以使用简单的方法，将卷积核全部声明为全0矩阵，但是这个有可能造成0梯度，所以我们加入一点噪音，我们看看加入噪音的卷积核是什么值\ninitial = tf.truncated_normal([5, 5, 1, 32], stddev=0.1) W_conv1 = tf.Variable(initial) session.run(tf.global_variables_initializer()) W_conv1_value = session.run(W_conv1) W_conv1_value.mean(), W_conv1_value.std() (0.001365028, 0.08744488) 我们使用tf.truncated_normal函数声明了32个5X5X1的随机卷积核，看起来随机性还挺不错哦\nPS：前面（5，5，1）代表输入长、宽、色道，后面代表输出输出数量当然我说它是32个它不一点为32个矩阵，应该是（色道X输出数量）个卷积核，但是我们这里只有一个色道，所以只有32个，我们可以通过W_conv1_value.shape查看真实的维度（当前的维度为（5， 5， 1， 32））\n这个卷积核就对应上面图中间的小矩阵，他的长宽都为5，图中长宽都为3，当然我们可以把这个长宽修改，使用5是我们的经验值，通过这个大小的卷积核能够在模型表现能力更好。\n接下来我们就进行最重要的卷积操作了，由上面图可知，要进行卷积必须要有三维的数据与对应的卷积核进行相卷，其实我们在图中还可以看到一个重要的东西，卷积的步长也就是每个框移动的位置（图中的步长为2）\n还有一个较隐秘的知识，你有没有注意到图中的数据原来是7X7的数据，通过卷积核转换之后就变成了3X3了，影响卷积后图像尺寸不但有步长还有框子的大小，假如你的框是7，那图中只剩下一个值了，所以我们避免尺寸减少，我们使用周围填充0来使最边缘的位置卷积也成为到框子的中心，一方面避免边缘数据流失，一方面也能突出边缘数据（周边全为0）\nTensorflow为我们封装好了上面所以的方法，我们只要通过传参过去就能改变部长，改变填充方式，好了现在就开始来正式“卷”了\nsession.run(tf.global_variables_initializer()) v = session.run(tf.nn.conv2d(x_image, W_conv1, strides=[1, 1, 1, 1], padding='SAME'), feed_dict={x: data}) 现在我们来看看卷完后v的shape\nv.shape (50, 28, 28, 32) 50代表50个数据，（28、28）代表图片维度，这个32就是卷积核数，50和32这两个应该是固定的，不难理解，我们现在来看看为什么通过卷积核的“卷”，图片还是保持28X28的，这个也是在知乎上涉及到的一个问题，现在我们从实验上来解决一下\n首先我们看tf.nn.conv2d函数，他接受四个参数，第一个图片、第二个卷积核、第三个步长，第四个卷积方式\n首先问题是觉得，卷完之后应该是变成24 X 24，这个理解是没错的，我们将pading的值改成VALID再次运行\nsession.run(tf.global_variables_initializer()) v = session.run(tf.nn.conv2d(x_image, W_conv1, strides=[1, 1, 1, 1], padding='VALID'), feed_dict={x: data}) v.shape (50, 24, 24, 32) 我们得到了24 X 24的图片，这个SAME和VALID有什么区别呢，这个区别就是填充0没有填充0的原因，SAME在图像周边填0这样就能得到28 X 28\n我们也发现，这个还有一个参数strides，这个就是前面填的步长，步长的长宽就是中间两位设置的（最边上两位跟输入有关，第一个是输入图片数量，最后一个是图片的色道），我们在这里使用使用1步长，我们来试试2步长试试\nv = session.run(tf.nn.conv2d(x_image, W_conv1, strides=[1, 2, 2, 1], padding='SAME'), feed_dict={x: data}) v.shape (50, 14, 14, 32) 果然输出的图像变成28的1/2了\n接下来我们就要把卷积的值丢到神经元函数里面去了，为了符合实际，我们加入一个偏置量b_conv1\ndef bias_variable(shape): initial = tf.constant(0.1, shape=shape) return tf.Variable(initial) b_conv1 = bias_variable([32]) 这里我们使用0.1来初始化偏置量，接下来就是丢到神经元函数，这里我们使用numpy 的array的传播性，将b_conv1传递给所有的28X28的维度\nh_conv1 = tf.nn.relu(tf.nn.conv2d(x_image, W_conv1, strides=[1, 1, 1, 1], padding='SAME') + b_conv1) v = session.run(h_conv1, feed_dict={x: data}) v.shape (50, 28, 28, 32) 我们可以看到卷积完后从神经元函数生成的数据是（50X28X28X32）的，最后维度由1变成32，所有我们得使用点方法来缩减数据维度，这里我们使用卷积池的方法\n由上面可以看到，其实很简单就是把最大的挑出来\nh_pool1 = tf.nn.max_pool(h_conv1, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME') 这里的参数很简单我就不介绍，这样“瘦身”之后，数据的维度由(50, 28, 28, 32)变成(50, 14, 14, 32)，减少4倍\n到这里我们的第一层卷积就结束了，接下来就是第二层卷积，为什么要多卷一次呢，因为前一层学到的还是太少了，要加强学习，这层和第一层没什么差别，所以我们就跳过这层\n直接贴代码（函数就不复制了，文档里面有）\nW_conv2 = weight_variable([5, 5, 32, 64]) b_conv2 = bias_variable([64]) h_conv2 = tf.nn.relu(conv2d(h_pool1, W_conv2) + b_conv2) h_pool2 = max_pool_2x2(h_conv2) 全连接层 # 当我们完成两层卷积之后，我们的数据变成了（50，7,7，64）的四维数组了，我们知道我们传统的机器学习其实最后都是采用二维数组来当做训练数据（X代表特征，Y代表样本），所以全连接层就是把卷积给“反卷”过来，这样后面你方便对接传统机器学习，而且最后我们需要的数据也是输出的也是二维的（对一堆数据统一进行预测，所以这里称二维），但是这里要注意全连接层不是输出层，所以我们可以随意设置输出的维度，最后输出层对接再进行一次全连接层类似操作就能输出我们想输出的维度，这里我们看看全连接层权值变量\nW_fc1 = weight_variable([7 * 7 * 64, 1024]) b_fc1 = bias_variable([1024]) 这里我们声明全连接层的权值变量W_fc1和偏置量b_fc1，我们可以看看W_fc1的shape是多少\nsession.run(tf.global_variables_initializer()) session.run(W_fc1).shape (3136, 1024) 我们可以看到其实就是一个二维数组维度为（3136,1024），第一个维度跟输入有关，第二个维度影响输出维度，前面我们使用tf.nn.conv2d卷积操作来转换图片，在全连接层我们要使用矩阵运算来转换我们的维度\n矩阵运算非常有趣，我们在前面其实也提到过一点，就是降维的实现PCA就是使用矩阵运算来进行降维，我们把数据分为X（特征），Y（数量），经过一次矩阵运算我们可以实现数量不变，而特征改变，这个就非常强大了，我们可以随便修改矩阵参数来动态修改我们特征数量\n但是矩阵运算也有一定局限性，就是两个运算的矩阵必须是前者长与后者的宽想同，这个跟矩阵运算特性有关，具体可以看看矩阵运算相关资料\n所以为了进行矩阵运算我们第一件事就是改变输入的shape，让它由四维变成二维，以便能够与我们权值矩阵W_fc1进行运算\nh_pool2_flat = tf.reshape(h_pool2, [-1, 7*7*64]) 我们简单的使用tf.reshape就能把第二层卷积后的输出变量转换成（50，7764）的维度，这样我们就能直接与权值矩阵W_fc1进行运算\nh_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat, W_fc1) + b_fc1) 我们这里直接将运算后的值放到激活函数里面去完成全连接层的功能\n输出层 # 其实输出层同全连接层很类似，我们就是把前面的变量转换成我们想输出的维度，在进行这个输出层之前，我们得先搞一层Dropout层，这个能有效的避免神经网络的过拟合问题，具体可以看看这篇论文\nkeep_prob = tf.placeholder(\u0026quot;float\u0026quot;) h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob) 因为同全连接层原理类似，输出层我就不就不详细介绍了\nW_fc2 = weight_variable([1024, 10]) b_fc2 = bias_variable([10]) y_conv=tf.nn.softmax(tf.matmul(h_fc1_drop, W_fc2) + b_fc2) 我们可以看看最后我们输出是什么\nsession.run(tf.global_variables_initializer()) session.run(y_conv, feed_dict={x:data, keep_prob:0.5}).shape (50, 10) ok，我们最后得到一个二维数组，50个预测结果（输出采用OneHot方法）\n反向传播 # 在前面我们得到了在初始话随机权值下得到输出结果，但是这个结果肯定是错误的，我们必须通过修改每层的权值来修正模型，使模型越来越聪明，所以第一步，我们必须“自我反省”，了解自己与真实结果差距多少\ny_ = tf.placeholder(\u0026quot;float\u0026quot;, [None, 10]) cross_entropy = -tf.reduce_sum(y_*tf.log(y_conv)) 我们引入y_作为实际值（我们模型预测值为y），我们这里使用交叉熵来评判预测准确性，但是单单知道“自己错了”没有什么卵用，我们必须要“改正”，这里我们使用AdamOptimizer优化算法来反向传播我们误差，让模型好好“反省改正”\ntrain_step = tf.train.AdamOptimizer(1e-4).minimize(cross_entropy) 到这里基本上差不多了，我们已经形成了一个闭环，预测-\u0026gt;评估-\u0026gt;改正-\u0026gt;预测-\u0026gt;\u0026hellip;\u0026hellip;，只有让它不断的训练下去直到我们能接受他的误差我们的模型就训练好了\ncorrect_prediction = tf.equal(tf.argmax(y_conv,1), tf.argmax(y_,1)) accuracy = tf.reduce_mean(tf.cast(correct_prediction, \u0026quot;float\u0026quot;)) session.run(tf.initialize_all_variables()) for i in range(18000): batch = mnist.train.next_batch(50) if i%100 == 0: train_accuracy = accuracy.eval(feed_dict={ x:batch[0], y_: batch[1], keep_prob: 1.0}, session=session) print(\u0026quot;step %d, training accuracy %g\u0026quot;%(i, train_accuracy)) if abs(train_accuracy - 1) \u0026lt; 0.01: break train_step.run(feed_dict={x: batch[0], y_: batch[1], keep_prob: 0.5}, session=session) 由于我们使用OneHot方法来输出预测变量，所以我们要使用tf.argmax来得到我们想要的真实数字，经过20000轮训练我们正确率可以达到99%，至此卷积神经网络发挥他的威力。\n总结 # 卷积神经网络是深度学习的一个很重要的组成部分，了解卷积必须要知道为什么要用卷积，用了有什么好处。总而言之，卷积并不是一个很新奇的东西，很早在信号处理中就有应用，但是在图像处理上由于他能保留图像维度信息从而在深度学习领域大放异彩，这也可以看做“是金子总会发光吧”\n引用 # http://www.tensorfly.cn/tfdoc/tutorials/mnist_pros.html 矩阵运算 通俗理解卷积神经网络 Dropout\n","date":"2018-04-24","externalUrl":null,"permalink":"/posts/ai/%E4%BB%8Emnist%E4%BA%86%E8%A7%A3%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/","section":"博客","summary":"引言 # 本文是学习Tensorflow官方文档的过程中的一点感","title":"从MNIST了解卷积神经网络","type":"posts"},{"content":"","date":"2018-04-24","externalUrl":null,"permalink":"/tags/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/","section":"Tags","summary":"","title":"卷积神经网络","type":"tags"},{"content":" 由于我的笔记本是农卡，没法安装CUDA加速，而且我的显卡只有2G显存，安装OpenCL费力不讨好，而且由于我有一个Google云的300美元的体验，所以可以在Google云上使用TPU来进行加速，所以我就干脆不安装显卡加速，但是Tensorflow提供了指令集优化，由于默认使用pip安装没有提供这个功能，所以只能手动编译安装\n假如你是用pip安装的Tensorflow你可以会得到下面警告\nthe tensorflow library wasn't compiled to use sse4.1 instructions 安装步骤 # 1. 首先你得先看看你CPU支持什么指令集\ncat /proc/cpuinfo|grep flags\n执行这个指令就能看到你所支持的指令集\n最新的tensorflow1.12不需要这步，可以根据你的CPU进行推测\n~~2. 然后安装bazel\n由于直接从源代码安装比较简单，所以直接从github上面下载0.18.1相对应的版本就行（最好不要使用最新版，最新版没法编译）\n3. 安装完之后下载tensorflow源码\nmkdir github \u0026amp;\u0026amp; cd github git clone --recurse-submodules https://github.com/tensorflow/tensorflow cd tensorflow ./configure 接下来一路选择N就行\n4. 生成whl文件\nbazel build -c opt \u0026ndash;copt=-msse3 \u0026ndash;copt=-msse4.1 \u0026ndash;copt=-msse4.2 \u0026ndash;copt=-mavx \u0026ndash;copt=-mavx2 \u0026ndash;copt=-mfma //tensorflow/tools/pip_package:build_pip_package\n在源码处开始编译，注意copt命令主要是添加指令集支持，这里你要看看上面的指令集（去掉m就是你的指令集，如-msse3指令集为sse3）你的CPU是否支持（一般都支持我的I5 4200U都支持），如果不支持删掉那个就行\n这里你安装的时间比较长，要看你的CPU了\n最新版不需要这么复杂了，直接\nbazel build --config=opt //tensorflow/tools/pip_package:build_pip_package 当然如果你想开启MKL也可以在opt加上改成--config mkl\n最新版使用bazel编译由于依赖网络下载，而我大天朝的网络一直报check-sum错误，官方说是我们网络的问题，我在我香港服务器尝试了一下，相同的环境下使用bazel是能安装的，但是由于安装到后面bazel需要太多内存了，所以直接爆照，而我本地内存大然而尝试了N多方法（尝试使用proxychain由于安装时需要连接本地bazel服务器，而设置过滤本地网络又没有用，也尝试使用全局代理也绕不过去），最终没有办法只能安装一个别人已经编译好的CPU的加速的。现在放出地址\ngithub tensorflow 编译好的版本下载\n你找一个你操作系统对应以及和你Python版本对应的下载下来用pip安装一下就可以了。\n验证 退出安装目录运行python\n执行下面两句\nimport tensorflow as tf;sess = tf.Session(config=tf.ConfigProto(log_device_placement=True)) bazel --host_jvm_args=\u0026quot;-Dhttp.proxyHost=localhost -Dhttp.proxyPort=8123 -Dhttps.proxyHost=localhost -Dhttps.proxyPort=8123\u0026quot;\tbuild --config=mkl //tensorflow/tools/pip_package:build_pip_package 总结 # 如果没有报上面的不支持指令集的warning，那么你的CPU指令集优化版就安装好了，当然这个加速效果因CPU而异，对于Xeon SP系列（100核心以上）已经能加速到50倍，同GPU差距也就2倍了（原来可是100倍），但是对于我的笔记本来说，加速效果可能就在30%左右（核心少），所以当前加速性价比最高的还是GPU加速，骚年还是买个好一点的GPU吧，没事还可以吃吃鸡。\n经过我测试在I7 9700K下相同的mnist数据下运行，不使用CPU指令优化比使用CPU指令优化下速度还要更快，所以在高版本下的Tensorflow CPU版其实优不优化都无所谓，推荐还是使用GPU加速\n","date":"2018-04-19","externalUrl":null,"permalink":"/posts/backend/software/ubuntu14%E5%AE%89%E8%A3%85tensorflow%E7%9A%84cpu%E4%BC%98%E5%8C%96/","section":"博客","summary":"由于我的笔记本是农卡，没法安装CUDA加速，而且我的显卡只有","title":"Ubuntu16.04安装Tensorflow的CPU优化","type":"posts"},{"content":" 引言 # 这篇博客其实写于2016年，最近在重新学了一下正则表达之后，觉得有必要重新整理一下正则的用法\nPython对正则匹配的库是re，re是基于Perl所用的正则表达式,并有 一定的改进.\n正则本质就是搜索所需的文本,正则里面有三种搜索方式\n第一种是知道文本内容直接使用普通字符搜索出来,比如要从abcdefg中搜到cd 第二种就是模糊查询,比如我想从英文中找一个数字,一般借助特殊符号(.+*?)或者转义符号(\\w\\d等) 第三种就是结合前两种,比如我记得一个单词的前两个字母想把那个单词搜出来. 这里不介绍正则基本知识,你想知道可以点这里\nps: 由于在python里面也是用反斜杠做转义字符,所以比如\\\\和\\b这两个特殊字符必须用\\\\\\\\和\\\\b来代替.但是python提供了一个元字符支持re模块,只要字符前面加上r比如r' regex '就能不关闭python的转义.\n正则里面我觉得很重要的一个概念就是组概念,当我们的文本比较复杂的时候将其分成多个小组是利于我们正则的后期维护和改进 # 正则里面使用一个括号来表示组比如(a)(b)就分成了两个组\nre函数里面search和finall都支持组查询,而且findall方法假如里面有组分布会只显示组成员.\nre库支持搜索选项,这几个选项对于正则有时候非常有用\nDOTALL [简S]-------------允许点字符匹配换行符 IGNORECASE [简I] --------忽悠大小写 LOCALE [简L] ----------支持本地化字符 MULTILINE [简M] ---------多行,每行都支持锚点 UNICODE [简U] ----------支持Unicode,\\w也可以是Unicode了 VERBOSE [简X] --------------神器,会无视代码中的注释空格和换行 我们也可以在正则的组里面使用这些搜索选项,只要用上面的简称的小写比如(?is)就可以在组里面使用这些规则.\n正则里面还有一些比较有趣的函数,同string里面的translate函数,sub函数可以替换找到的变量 bold = re.compile(r\u0026rsquo;*{2}(.*?)*{2}\u0026rsquo;) bold.sub(r\u0026rsquo;\\1\u0026rsquo;, \u0026rsquo;this foo and ok')\n\\1代表第一组变量也就是foo和ok 输出为'this \u0026lt;b\u0026gt;foo\u0026lt;/b\u0026gt; and \u0026lt;b\u0026gt;ok\u0026lt;/b\u0026gt;'我们使用成功用加粗了foo和ok,同translate不同这个方法不需要知道要替换的是什么.\n正则的断言 # 我们可以使用一些特殊的符号来执行一些程序判断选择,比如说判断是否特殊字符,如果有 的话就不匹配,这就是断言\n断言有两种一种是前向,一种是后向\n前向是指判断语句在前面,这种就相当于一个if语句,而后向是匹配后判断,由于已经匹配好了文字所以 匹配的字符必须是固定长度的(不能使用*.?).\n前向就是在判断后面匹配的表达式必须与规定相同,比如一个邮箱地址我们要匹配可以用\u0026lt;\u0026gt;包起来的,但是不匹配只要一个的我们就可以在前面加上这个^(?=(\u0026lt;.*\u0026gt;$)|([^\u0026lt;].*[^\u0026gt;]$))通过使用?=来断言后面必须是用\u0026lt;\u0026gt;包起来或者没有\u0026lt;\u0026gt;,我们使用前向断言可以通过正则直接过滤掉不符合的(当然你可以用多个简单正则来做但是效率没有这个高),还有否定前向就是通过?!来声明. 相对应后向断言就是很简单了,直接在匹配后面使用一个?\u0026lt;=(肯定后向)或?\u0026lt;!(否定后向),不过要注意这个是判断前面匹配是否满足的.\n断言只是限定我们想选的文本的范围,他并不会被选择. 断言的一个有趣的应用就是选择字符间的空格,我们知道python其实假设每个字符间都一个空格(这就是我们有时候会选出一些空字符出来的原因),这个空格不是我们自己打上去的.\n举个例子 # 两个字符串a1和a 1,第一个我们称为A,第二个我们称他为B,假如我们想把数字和字母分出来,对于B来说,很简单因为数字和字母之间有一个空格,我们可以直接使用字符自带的split就行,但是对于A来说,就不那么简单了. # 字母a和数字1中间没有字符,我们必须把字母和数字之间的\u0026quot;空格\u0026quot;给选择出来,这时候就可以用到断言了. # r = re.compile(r'(?\u0026lt;=[a-z])(?=\\d)') 这个r就可以字母和数字直接的隐形空格给选择出来了\n遗憾的是由于python的正则并不把隐形的空格当做字符,所以我们不能简单的使用正则的re.split方法(选择字符分割)直接将字符串分解开. # 我们就得写几步 # 第一先把空格换成 $$$(或其他) # \u0026gt;\u0026gt;\u0026gt; s = r.sub('$$$', 'a1') \u0026gt;\u0026gt;\u0026gt; print(s) 'a$$$1' 然后在分割 # \u0026gt;\u0026gt;\u0026gt; s.split('$$$') \u0026gt;\u0026gt;\u0026gt; ['a', '1'] 成功分割好了,当然这个只能处理字母在前数字在后的\u0026quot;隐形空格\u0026quot;,只要加一个\u0026quot;|\u0026quot;在把前向改成后向,后向改成前向就可以选择任意字母和数字直接的\u0026quot;隐形空格\u0026quot;了.\n正则的变量 # 我们可以使用?P来声明一个组(用括号,当然其实我们每使用一个括号re自动帮我们将组取一个 名,依次从1-n\n有时候我们可以要求上面的匹配组,下面也要相应匹配组,我们就可以通过两种方法来引用这个变量,假如你没有使用\u0026lt;?P\u0026lt;name\u0026gt;来声明组你只能通过\\n来引用,n是这个变量的序号,第二种是通过(?P=name) 来引用这个变量,name为你自己定义的组的名字\nre还提供了一种机制来让你修正你的正则,简单来说就是能判断一个组存不存在来约束匹配,语法为\n(?(id)yes-expression|no-expression) id为组的编号或者name.\n正则的起源 # 正则这个东西其实很简单，我们\n","date":"2018-03-17","externalUrl":null,"permalink":"/posts/backend/framework/python/%E6%AD%A3%E5%88%99%E8%A1%A8%E8%BE%BE%E5%BC%8F/","section":"博客","summary":"引言 # 这篇博客其实写于2016年，最近在重新学了一下正则表达","title":"Python正则表达式","type":"posts"},{"content":"","date":"2018-02-20","externalUrl":null,"permalink":"/tags/mit6.828/","section":"Tags","summary":"","title":"MIT6.828","type":"tags"},{"content":" 通过前面的学习我们知道，在前两个实验中最主要的程序就是kern/init.c文件中i386_init函数，但是我们看到最后却是一个while循环结束。\nwhile (1) monitor(NULL); 这个monitor就是简单读取用户输入然后通过字符串调用给定的几个函数。我们可以把这个函数看做i386_init的子函数，也就是程序一直在i386_init中运行，也就是一直以内核形式运行。实验三就开始完成一个正常的操作系统的用户模式的建立，以及两者直接转换。\n引言 # 什么是用户呢，我们把内核当做一个大程序，用户就是小程序，两者基本上没有差别，但是为了让众多用户和内核平稳运行，我们必须要区别对待用户和内核\n首先我们要相信内核，内核是我们精心设计的，相反用户我们不能相信它，任何人都可以犯错误，我们知道计算机程序其实非常精巧，当他们完整的时候，他们可以完成你无法想象的工作，但他们有时候少掉一行指令时，他们就会瞬间崩溃，所以我们必须要保证就算用户怎么折腾都不会把内核搞垮\n所以我们来考虑一下怎么来限制用户不影响整个系统，第一用户不能动内核在内存上的程序，甚至连查看的资格都没有，这就是为什么前面我们已经绞尽脑汁把内核放到高地址上确保其“高高在上”；第二个用户不能在CPU一直跑，假如它有权限一直在CPU上跑，那么内核就废了，没有CPU可以用相当于“断了胳膊”；第三用户必须要有一些内核的权限，比如说申请一些内存啥的。\n接下来我们就从这几个问题出发来探索一下xv6为什么要这样设计用户空间。\n用户是个程序？ # 在完成用户设计之前，首先我们先做个实验，来验证一下用户是个程序。首先我们在kern/init.c文件中i386_init 的头部加上#include \u0026lt;inc/elf.h\u0026gt;然后在#if defined(TEST)代码前加入几行代码\nextern uint8_t _binary_obj_user_hello_start[]; struct Elf* header = ((struct Elf *) _binary_obj_user_hello_start); assert(header-\u0026gt;e_magic == ELF_MAGIC); ((void (*)(void)) (header-\u0026gt;e_entry))(); 这段代码你如果仔细观察过前面操作系统载入的源码（boot/main.c中)你会发现基本上差不多。首先我们声明了_binary_obj_user_hello_start外部变量，这个变量是通过ld编译器将内核中用户程序hello.c的起始地址定义来的，由于我们现在没有文件系统，内核就把用户程序一股脑链接到自己身上，在以后有了文件系统就不需要了。但是它给了我们一个便利，我们现在可以直接在内存上运行它\n当然虽然它的确是个程序（assert那不会出错），但是它还是跑不起来，因为它的虚拟地址不在内核内存上，必须要像前面映射内存物理空间才能运行，但是他的确是一个程序，从这里我们得到一个结论，用户其实也就是一段程序，接下来我们就看看如何用软件与硬件的结合的设计解决上面的问题。\n权限 # 首先我们来看看权限问题，x86系统硬件给我们提供了一个分段式权限功能，在开启内存分页后，CS寄存器16位里面拿出两位来当做权限管理，分为四级（0b00、0b01、0b10、0b11），最小的权限最大，当然xv6只用了两级，如下图所示\nPS：当然CS寄存器拿出来三位来当权限管理，但是xv6将第三位用户和内核都设置为1，所以我们也不介绍这一位\n我们在CS寄存器的这两位称为CPL，用来区分权限。在引导和操作系统的交互这篇博客我们知道，在x86系统中，这个CS寄存器是标志段选择子，在GDT表中每个表项也有两位用来标志权限，我们称他们为DPL，他们的意义也就是CS寄存器选择这个段时他们最低拥有的权限。\n当然在GDT表中的段选择子也有一个表示权限的称作RPL，这三者的关系在这里面介绍的很详细，RPL是历史遗留问题但是操作系统基本上没有使用这个功能，所以这里我们也不解释，感兴趣可以看一下前面的介绍。\n在kern/env.c的结构体gdt我们声明了一个GDT表，并且声明了一个系统段和用户段。\n这两个段就是我们权限的基础我们接下来通过实验来验证权限位对操作系统的保护\n我们首先在user/badsegment.c中加入一句汇编\nasm volatile(\u0026quot;ljmp %0,$1f\\n 1:\\n\u0026quot; :: \u0026quot;i\u0026quot; (0x08)); // 将CS设置为0x08 我们尝试将CS设置为0x08（内核段）结果直接引发一个General Protection的异常，但是当我们尝试执行这句\nasm volatile(\u0026quot;ljmp %0,$1f\\n 1:\\n\u0026quot; :: \u0026quot;i\u0026quot; (0x18)); // 将CS设置为0x18 我们发现程序没有引发异常（这段的含义是跳转到用户段，由于已经在用户段，所以没有触发异常），这说明x86通过CS寄存器很好的实现了内核和用户的保护（用户不能直接跳到内核去执行代码）\n在user/faultwritekernl.c和user/faultreadkernl中我们发现尝试写入或者读取内核也失败了，这是通过前面章节的内存分页权限实现的，这里就不介绍了。\n现在我们知道，通过我们不懈努力内核和用户直接存在一个鸿沟，但是这时候摆在我们面前很大的问题，内核拥有一些非常重要的权限比如申请内存，如果用户没有这个权限那么功能非常受限，所以我们就要提到一个内核和用户交互的手段：Trap。\nTrap # 其实这个Trap包括内部中断、软中断、异常。但是xv6统一将他们叫做陷阱（Trap），这个Trap主要完成从用户到内核的一种跳转，我们就不介绍内部中断和异常，因为这些都是系统完成，我们来介绍了一下软中断，这个我们人为可以操作，而且我们可以把其他中断、异常当做系统去帮我们调用这个指令\nint $n 很简单x86共定义了256个中断向量，存在物理内存0x000-0x3ff之间，每个存贮一个cs值一个eip的值，这个n只要是在256直接就行，当调用这个的时候我们直接调到那里读取cs和eip的值。\n在正式介绍Trap之前我们要简单的介绍一下两个知识点\nIRET # 这是iret是一个机器码，我们前面介绍了无论是操作系统还是用户没有办法直接跳段（会引发General Protection fault），而且我们知道前面只是测试了跳代码段，我们知道两个不同的程序，不仅仅是代码段不同，堆栈段也要不同，才能保证各个程序的独立。所以系统为了解决这个问题，直接提供一个iret指令，这个全名应该叫中断返回，它的功能其实很简单，就是实现上面的跳代码段和堆栈段还有恢复EFLAGS寄存器值，这个主要应用在从系统跳转到用户上面。\n感兴趣的可以看看这个详细资料，接下来我们看看在xv6里面如何使用iret来实现跳段\n在kern/env.c的env_pop_tf函数实现了切换程序段的功能\nasm volatile( \u0026quot;\\tmovl %0,%%esp\\n\u0026quot; \u0026quot;\\tpopal\\n\u0026quot; \u0026quot;\\tpopl %%es\\n\u0026quot; \u0026quot;\\tpopl %%ds\\n\u0026quot; \u0026quot;\\taddl $0x8,%%esp\\n\u0026quot; \u0026quot;\\tiret\u0026quot; : : \u0026quot;g\u0026quot; (tf) : \u0026quot;memory\u0026quot;); 首先第一个就是把tf指针指向的值赋给esp寄存器，要想知道这个操作的意思就必须知道，一个结构体其实在程序里面就是一块连续的内存值，所以这里就是把esp的值指向tf代表的Trapframe结构体的开始位置，所以我们得看看这个Trapframe组成是什么\nstruct Trapframe { struct PushRegs tf_regs; uint16_t tf_es; uint16_t tf_padding1; uint16_t tf_ds; uint16_t tf_padding2; uint32_t tf_trapno; /* below here defined by x86 hardware */ uint32_t tf_err; uintptr_t tf_eip; uint16_t tf_cs; uint16_t tf_padding3; uint32_t tf_eflags; /* below here only when crossing rings, such as from user to kernel */ uintptr_t tf_esp; uint16_t tf_ss; uint16_t tf_padding4; } __attribute__((packed)); 第一个是PushRegs结构体，这个对应我们第一个操作popal，将PushRegs所有值全部从堆栈中取出，我们发现当到iret操作时，正好对应取出代码段还有堆栈段和EFLAGS寄存器，所以其实iret的操作非常简单，只是将三个取出指令和成了一个。\nTSS # 前面我们捋了捋从系统跳转到用户，因为我们给用户的都是刚新建的值，所以这个并没有什么问题，但是你要想一想，当用户想跳转到系统的时候，这个时候它也必须要切换堆栈，因为用户可以还没有准备好（有时候可以是因为堆栈内存不够，如果把寄存器值强行插值到用户堆栈会引发二次异常）。但是假如使用操作系统的堆栈，在我们切换之前系统必须要知道内核堆栈在哪\n所以x86提供了一个tr寄存器给我们使用，这个寄存器的位数为16位，专门存贮一个选择子（类似前面段选择子），这个选择子base地址必须指向一块空余空间。前面知道我们要想从用户跳操作系统，必须知道操作系统的堆栈（中断向量表提供了代码段cs：ip），所以x86干脆规定了这个空间名字为TSS，并声明了一大串寄存器的值备用，当然这些是硬件规定的，软件用不用无所谓，但是硬件会在跳转的时候，读取里面固定位置的值，也就是我们需要的堆栈段ss：esp，在inc/mmu.h的Taskstate结构体中声明了这段内存规定的字段\n所以我们在kern/trap.c中声明了（ss：esp）\nts.ts_esp0 = KSTACKTOP; ts.ts_ss0 = GD_KD; 这样我们在跳转的时候就能获取到内核的堆栈段，从而进入内核模式，当然硬件设定TSS的作用一开始为了考虑任务切换，它也提供了相应的指令，但是需要200左右时钟周期来完成，所以我们xv6嫌弃其速度太慢，只是使用了其保存内核堆栈地址的作用，所以在xv6中TSS只是一个“数据库”而已。\n介绍完前面的基础知识之后，接下来的就是这个Trap，前面我们知道在得到int这个指令时候这个时候就开始一个Trap开始。\n这个时候硬件接管程序控制，你也可以认为跳转到硬件代码去了。在给的资料第三章的X86 Protection里面介绍了这个过程，我们将前面几个步骤很我们前面给的资料联系起来\n后面推寄存器值就不解释了，同Trapframe的反向结构相同，之所以会这样，是因为堆栈地址是向下生长，而程序地址是向上生长的\n硬件这部分操作为\n获取中断向量表第n个的地址（n为int后面整数） 然后开始权限检查，这个也是为什么程序不能随意调用系统资源的原因\n检测DPL与CPL的大小关系 这个DPL不是前面GDT里面的，这个是存贮在中断向量表中CS里面的值，这个设置可以让我们给每个中断向量设置一个权限，看它允许是用户调用还是系统调用，如果只允许系统调用，那么用户调用直接引发General Protection Fault（13号中断），在我们的xv6中，我们只允许用户调用Debug和System Call中断，其他只能由系统调用\n判断保留esp和ss，检测目标的段与当前段 当我们在操作系统中出现中断的时候，如果硬件还傻乎乎的将操作系统的堆栈归位那么就进入死循环了，所以这个目的就是避免同属一个段的时候不会切换堆栈\n当跳段的时候，从TSS中取到esp和ss切换堆栈段 这个就是前面弹到的TSS的功能，接下来我们所以推出去的值全部存贮在要跳的段的堆栈上面了。\n推完Trap程序的寄存器值，保留好“犯罪现场”后，就可以进入内核进行处理这个异常了。\n至此我们介绍完了严格有序的处理从用户到内核的跳转，当我们处理完后就可以通过iret回到用户程序，这样就完成了用户拥有内核权限的设计。\n用户时间控制 # 接下来的用户使用CPU时间设置也是通过这个这个Trap的中断实现，你可以理解为硬件在每个CPU上在每隔个10ms的时候就触发一个中断。这样到内核的时候，内核发现是时间中断，就保留用户的寄存器值到用户自己空间（Env里面有一个env_tf变量可以存贮这些信息），然后调用其他或者这个程序，这样的话，我们就实现了一个时间控制。\nPS：这里我们提一下这个Env结构体，内核初始化了1024个这个结构体，这说明我们可以同时拥有1024个用户程序运行，这个结构体就是用户环境，也就是我们在Unix系统上输入ps中的pid（每个pid代表一个程序）\n总结 # 至今我们介绍了通过一个简简单单的Trap操作就实现了复杂的权限管理和空间隔离，虽然操作系统看起来非常复杂，但其实都是由简单干练的概念搭建起来的，因为Unix的哲学就是简单至上。\n引用 # https://stackoverflow.com/questions/6892421/switching-to-user-mode-using-iret\nhttps://stackoverflow.com/questions/36617718/difference-between-dpl-and-rpl-in-x86\nhttps://en.wikipedia.org/wiki/Task_state_segment\n","date":"2018-02-20","externalUrl":null,"permalink":"/posts/essays/programming/mit6_828/%E7%94%A8%E6%88%B7%E7%A9%BA%E9%97%B4%E9%9A%94%E7%A6%BB/","section":"博客","summary":"通过前面的学习我们知道，在前两个实验中最主要的程序就是ker","title":"用户空间隔离","type":"posts"},{"content":" 概括 # 这个问题主要在这本xv6-ref的第一章的练习题2中提出来\n问题如下\nKERNBASE limits the amount of memory a single process can use, which might be irritating on a machine with a full 4 GB of RAM. Would raising KERNBASE allow a process to use more memory? 根据前面的知识可知这个问题翻译过来就是：在32位处理器下，当KERNBASE为0x80000000时用户最多可以用2GB内存，我们是否能够增加KERNBASE让用户使用超过2GB内存呢？\nps： 下面KERNBASE简写为KE\n引言 # 要想解答这个问题，我们必须要知道xv6是如何分配内存的，以及如何区分用户和内核，以及KE的作用。\nxv6使用内存分页技术，如果你对这个不了解可以看这篇引导中的静态表和这篇内存分页设计细节，了解完这个我们谈谈用户和内核的内存分页应用。\n注意我们所以计算的前提是在32位处理器的条件下\n用户和内核应用内存分页 # 其实用户和内核都是程序，对于用户来说内核只有一个，对于内核来说，用户可以有无数个。为了隔离内核和用户我们必须在内存上就进行隔离，所以单一内核和每个用户都拥有一个内存目录。\n内存分页的意义 # 其实我们可以把内存的目录对应的所有内存表看做一张大的内存映射表，我们使用1024 × 1024个表单项代表4GB内存地址，每个代表4K连续的内存空间，当然大部分程序不会使用全部表单（一般就是内存目录一个，一到三个内存表）\n当然好处有很多，比如任何程序都可以使用相同的虚拟地址但是在实际运行的时候不会相互影响，这个好处就是通过映射来实现的。\n映射就是虚拟地址到物理地址的一种转换，这是一种多对一的映射，也就是一个虚拟地址只能对应一个物理地址，然而物理地址可以对应多个虚拟地址 这个转换带来了一个问题，给你一个虚拟地址，你可以查表然后得到物理地址，但是如果给你一个物理地址，你除了遍历没有很好的办法获取虚拟地址（而且有可能你会得到多个虚拟地址）。\n这个问题非常重要，因为它影响了我们后面内核与用户切换的时候地址的搜索，接下来我们看看内核初始化后内存是一种什么样的存在。\n内核内存利用 # 看xv6源码我们知道，内核这个程序将程序虚拟地址起点定在KE上，我们知道一个程序由三部分组成指令、数据、堆栈。堆栈是向下生长，而数据、指令向上生长。而xv6把堆栈固定为32KB，所以内存只能向上生长，所以理论上内核最大占用空间只有2^32(4G) - KE\n当KE为0xf0000000时，只有256M 当KE为0x80000000，有2G。 由于用户内存空间不能超过KE（实际系统中为UTOP，值为KE减去一些为系统功能预留的空间，这里我们忽略他们），所以用户所能得到的内存空间最大为KE，也就是\n当KE为0xf0000000时，为 3840M（4G-256M） 当KE为0x80000000，有2G。 通过上面简单的理论值我们推断出答案是：\n增大KE可以增大用户内存 这个结果看起来是正确的，但是我们知道作为一个操作系统不能固定用户的内存，我们接下来就看看在不同的内存下的实际应用以及xv6自身设计问题，由此来探索这个问题的本质\n我们就分两种情况来考虑\n物理内存小于（4G-KERNBASE） # 这种情况在很早的时候经常出现，我们计算机的内存可能就几M，这个时候内核假如真的占了4G-KE那连内核都跑不起来何况用户程序。\n我们接下来进入源码来看xv6将实际占用的空间体现在内存表中，我们看到kern/kernld.ld\t配置文件，其中最主要的一行\nPROVIDE(end = .); 这行位置出现在最后一行，它的作用是提供了一个变量end，我们在C中就可以用这个变量代表机器码的最后一行的虚拟地址，有了这个地址之后，我们就获取了一个重要信息内核程序总长度，假如没有这个变量，我们只能人工设定一个操作系统的占用空间，假如定大了就是浪费，定小了程序就可能崩溃（内存溢出）。分配的细节可以看这篇文章，通过这个变量，我们把内核占用的物理空间给腾出来了。接下来的剩余的物理空间就留给用户了，所以用户能用到的内存最大为实际内存 - 操作系统占用的内存。\n而且我们能够知道操作系统还将内核的内存映射直接固定了，将KE：4G的地址映射到0:4G-KE，公式为虚拟内存=物理内存+KERNBASE，这个映射存在于kern/pmap.c的mem_init函数中，这个带来了一个便利，就是程序变得很简单，假如我们得到一个物理内存地址，我们就能知道虚拟内存地址，但是这个也引来一个巨大的问题，就是当内存大于4G - KE的时候。\nPS：关于知道物理内存能知道虚拟地址在源码的好处主要体现在用户内存空间创建的时候，这个时候处于内核态，当我们得到一个空的物理空间页，我们只要简单的使用上面公式就能获取到虚拟地址，然后内核就能在直接通过指针修改程序。假如没有这个映射，我们就后面动态建立，但是这里涉及到一个非常棘手的成本的问题，我们其实只需要一个值映射，然而我们却要浪费4K的连续内存空间，而且给程序带来了很大的复杂性。\n我们接下来考虑系统内存大于那段映射最大值（4G-KE）时候会造成什么情况\n物理内存大于 （4G-KERNBASE） # 在现在大家内存愈来愈大，这种情况比较常见，由于xv6在程序设计中是动态创建用户空间，当用户比较少占用空间比较小的时候，程序不会用到物理地址比较大的空间，但是当物理内存超过这个阀值（4G-E），在这里我们考虑KE=0xf0000000,也就是物理内存地址大于256M的时候，由于内核没有映射到这么高的地址，所以我们用上面那个物理地址转虚拟地址公式就失效了。\n由于xv6只是一个教学系统，所以为了简化系统，降低程序复杂性所以使用了最简单的直接映射的方法来处理内存，也正是由于这个原因提高KE并不能增加用户最大内存，当然解决的方法也有，我想了两种。\n在内核空间中分配用户内存目录空间，这种最为简单，但是会浪费一定的内存空间（必须在内核一开始运行就占用空间） 在内核空间中映射一段固定的地址来存放用户内存目录地址，这个程序设计比较复杂，但是只需要初始化一个内存表单就行 总结 # 通过上面的分析，我们得到一个结论：\n由于xv6设计的局限性，增大KERNBASE并不能增大用户最大可用空间，反而会减少。但是如果修改xv6设计可以实现增大KERNBASE增大用户最大可用空间。\n引用 # ","date":"2018-02-19","externalUrl":null,"permalink":"/posts/essays/programming/mit6_828/kernbase%E5%AF%B9%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F%E7%9A%84%E5%BD%B1%E5%93%8D/","section":"博客","summary":"概括 # 这个问题主要在这本xv6-ref的第一章的练习题2中提","title":"KERNBASE对操作系统的影响","type":"posts"},{"content":" 引言 # 前面已经通过lab1的这篇博文了解了内存分页的实现细节，接下来就谈谈如何具体实现内存分页\n物理载体 # 通过了解KERNBASE对操作系统的影响这篇博文我们知道，其实内存分页就是完成对物理存在的一种分割和隔离，所以我们在完成内存分页系统设计之前必须要构建一个载体，完成对物理存在的一种表示。\n在xv6中声明一个动态数组来代表物理内存，每一个值代表一块4k的内存页。我们主要通过offset - base得到偏移倍数，每个偏移量为4K，也就是通过(offset - base ) \u0026lt;\u0026lt; 12 得到物理地址，我们看一下这个数组成员：PageInfo结构体\nstruct PageInfo { struct PageInfo *pp_link; uint16_t pp_ref; }; 主要存在两个值：一个为下一个可用的地址指针，一个为引用次数。\n引用次数比较好理解，但是这个pp_link有什么用呢。其实你可用把这个结构体看做成一个由链表组成的堆栈，我们只需要保留栈顶值（page_free_list)，由于它保存下一个值地址，这样通过不断的push、pop，就能维持一个可用物理内存栈。\n二级指针的妙用 # 由于前面的博客原理已经介绍的很详细了，我就不再累赘了，在这里我提一下源码中二级指针的妙用，虽然它只有短短几行，但是运行的结果却是让人大开眼界，体会到指针的神奇威力。\n这段代码出现在kern/pmap.c的check_page_free_list函数中\nif (only_low_memory) { // Move pages with lower addresses first in the free // list, since entry_pgdir does not map all pages. struct PageInfo *pp1, *pp2; struct PageInfo **tp[2] = { \u0026amp;pp1, \u0026amp;pp2 }; for (pp = page_free_list; pp; pp = pp-\u0026gt;pp_link) { int pagetype = PDX(page2pa(pp)) \u0026gt;= pdx_limit; *tp[pagetype] = pp; tp[pagetype] = \u0026amp;pp-\u0026gt;pp_link; } *tp[1] = 0; *tp[0] = pp2; page_free_list = pp1; } 主要的作用是将“栈底”的元素移到“栈顶”，首先它使用了两个一级指针（pp1、pp2），还有两个二级指针分别指向（pp1、pp2）\n首先int pagetype = PDX(page2pa(pp)) \u0026gt;= pdx_limit;判断物理地址是否为大于4M还是小于4M，我们把物理内存页分成两组\n小于4M A组 大于4M B组 对于小于4M的组，分两种情况\n第一个小于4M的内存页（page1） *tp[0] (也就是pp1） 存贮了pp的值，也就是pp1 = page1 tp[0] 存贮了pp -\u0026gt; link 的地址（这个没有什么用） A组第二个以及以后的内存页（page2） 上一个地址的值等于pp（没什么用） tp[0] 存贮了下一个空闲地址的值 对于B组来说也是一样的，最重要的是for循环结束后实现的交换\n*tp[1] = 0; B组最后一个的pp_link地址地址设置为NULL，也就相当于把他放到栈底\n*tp[0] = pp2; A组最后一个变成pp_link地址设置pp2，也就是B组的第一个接到了A组的最后面去了\npage_free_list = pp1; 栈顶变成A组第一个，通过这样的“乾坤大挪移”术就将A组部分移到B组前面去了，也就实现了先使用低地址的物理内存的作用\n总结 # 理解内存分页必须要了解背后的原理，了解了原理看具体实现的时候才能事半功倍。\n引用 # ","date":"2018-02-05","externalUrl":null,"permalink":"/posts/essays/programming/mit6_828/%E5%86%85%E5%AD%98%E5%88%86%E9%A1%B5%E8%AE%BE%E8%AE%A1/","section":"博客","summary":"引言 # 前面已经通过lab1的这篇博文了解了内存分页的实现细节","title":"内存分页设计","type":"posts"},{"content":" 引言 # 本来自己查了很多资料，想自己写出来，结果下笔的时候发现别人已经把我想写的部分全部写出来了，而且比我想的还要具体，所以我就不写了，把链接放出了，顺便我补充一些\nhttp://leenjewel.github.io/blog/2015/11/11/%5B%28xue-xi-xv6%29%5D-nei-he-gai-lan/\n总结 # 由于内存分页在后面的lab2中也会设计，而且给出的资料写的非常详细，所以我就不多提了。这里提一下为了更好的理解什么内存分页的段选择子与CS的关系，我写了一篇关于CS寄存器的发展史\n最后我提一下kern/entrypgdir.c手写的内存分页表\n第一个entry_pgdir是页目录，第二个entry_pgtable页表，而且注意entry_pgtable是代表0-4M物理内存可写可读的真实内存地址\n前面知道了目录和页表的作用，在现实系统中，一个目录下有1024个页表，但是这只有一个页表，而且它代表从0-4M连续的内存空间\nPS：后面12位是权限位，只看前三位，可以知道是从（000-3ff）的高20位物理地址，由于我们声明了__attribute__((__aligned__(PGSIZE)))（地址4K对齐，其实意思是它真实物理地址后12位一定全部为0），所以直接取(uintptr_t)entry_pgtable地址就是页表地址，虽然后面加上权限位PTE_P + PTE_W，但是在寻找真实地址时都会屏蔽（只取前20位如），当初想了很久感觉取不到页表的真实地址，还以为有什么神奇的编译设定，后面查了很久资料才明白，所以这里提个醒后面我们会在lab2中正式接触内存分页技术，这里只是相当于设定了一个“常数”，后面会将器扩展变成“函数”。\n引用 # ","date":"2018-01-31","externalUrl":null,"permalink":"/posts/essays/programming/mit6_828/%E5%86%85%E5%AD%98%E5%88%86%E9%A1%B5/","section":"博客","summary":"引言 # 本来自己查了很多资料，想自己写出来，结果下笔的时候发现","title":"内存分页","type":"posts"},{"content":" 引言 # 本来自己查了很多资料，想自己写出来，结果下笔的时候发现别人已经把我想写的部分全部写出来了，而且比我想的还要具体，所以我就不写了，把链接放出了，顺便我补充一些\nhttp://leenjewel.github.io/blog/2014/07/29/%5B%28xue-xi-xv6%29%5D-cong-shi-mo-shi-dao-bao-hu-mo-shi/\nhttp://leenjewel.github.io/blog/2015/05/26/%5B%28xue-xi-xv6%29%5D-jia-zai-bing-yun-xing-nei-he/\nELF文件 # 首先你要知道什么是ELF，可以看一下这篇博客，简单来说就是编译完C后的机器码，前面加了一些数据表记录程序的分布情况\n为了帮助我们逆向分析这些代码有什么，我们必须要借助两个工具\nobjdump readelf 我们接下来就用实际例子解读mit6.828里面的引导和核心\n引导 # 首先我们查看一下引导文件，在boot/目录下有两个文件\nboot.S main.c 两者是通过gcc编译器将汇编和C编译成为一个elf文件，具体在Makefile文件中（boot/Makefrag)，我将它简单翻译一下\ngcc -N -e start -Ttext 0x7C00 -o boot.out boot.o main.o boot.o和main.o就是boot.S和main.c编译后的文件，-e start意思程序从boot.S的start中开始运行（这样就能从汇编开始执行），-Ttext中text代表代码段，也就是说直接指定代码入口地址为0x7C00，我们可以用readelf验证一下\nmit-6.828-2014 ➤ readelf -h obj/boot/boot.out git:lab1* ELF Header: Magic: 7f 45 4c 46 01 01 01 00 00 00 00 00 00 00 00 00 Class: ELF32 Data: 2's complement, little endian Version: 1 (current) OS/ABI: UNIX - System V ABI Version: 0 Type: EXEC (Executable file) Machine: Intel 80386 Version: 0x1 Entry point address: 0x7c00 Start of program headers: 52 (bytes into file) Start of section headers: 4868 (bytes into file) Flags: 0x0 Size of this header: 52 (bytes) Size of program headers: 32 (bytes) Number of program headers: 2 Size of section headers: 40 (bytes) Number of section headers: 9 Section header string table index: 6 我们读取了一下生成的elf文件头部，我们可以看到Entry point address这个字段，就是我们设定的0x7c00，要了解这个地址的含义我们必须知道虚拟地址和物理地址的区别，可以看一下这篇博客，接着我们看一下反汇编的汇编代码obj/boot/boot.asm中，我们知道现在0x7c00代表程序把自己当做在内存上的真实内存上面，但是不一定会真的存在这块上，所以我们称它为虚拟的\n首先我们了解一个系统知识，因为电脑启动后会按照启动盘顺序，把每个盘第一个扇形区512B取出来，如果最后两个字节为0xAA55的话就把它放到内存上面的0x7c00上去，我们看一下我们生成的obj/boot/boot\n使用hexdump obj/boot/boo可以看到（我把最后一行复制出来）\n00001f0 0000 0000 0000 0000 0000 0000 0000 aa55 最后连个字节为0xaa55，且文件大小刚刚好512B，这时候你可以有一个疑惑了，我们知道gcc我们生成的boot.out的elf文件，但是这个文件还有头部存贮数据，我们如果把整个文件放到磁盘上，当程序在内存0x7c00处执行时，那么文件头部碰到的就是elf头了，所以 为了把机器码提取出来并生成合适文件（512B尾部为0xaa55），程序干了两件事\n用objcopy将elf文件中执行代码提取出来（相当于去掉elf头部） 用脚本修改尾部两字节（在boot/sign.pl用了perl程序来将生成512B且尾部为0xaa55的boot文件） 总结 # 后面将核心加载到内存，上面的给出资料写的很详细，我就不多说，只不过由于当前2014的版本同资料有点不同，这里我提一下\n当前版本是将内核头加载在0x10000上，然后在把内核代码加载到0x100000上（前面4个0，后面5个0，我当初看错了，百思不得其解），并将内核地址映射到f0100000上。\n由于资料给的操作系统与2016的操作系统实现细节有点不同，其中最主要的就是一个很重要的KERNBASE常量值由0x80000000变成0xf0000000，这个变量牵扯到了在给出的book-rev8.pdf资料中第一章的最后一个问题，我就把我对这个问题的思考放到这篇文章中。\n引用 # http://leenjewel.github.io/blog/2014/07/29/%5B%28xue-xi-xv6%29%5D-cong-shi-mo-shi-dao-bao-hu-mo-shi/\nhttps://my.oschina.net/u/864891/blog/87965\n端口信息\n","date":"2018-01-31","externalUrl":null,"permalink":"/posts/essays/programming/mit6_828/%E5%BC%95%E5%AF%BC%E5%92%8C%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F%E7%9A%84%E4%BA%A4%E4%BA%92/","section":"博客","summary":"引言 # 本来自己查了很多资料，想自己写出来，结果下笔的时候发现","title":"引导和操作系统的交互","type":"posts"},{"content":" 引言 # Intel作为作为微处理器的航头老大，一直引导CPU的进步发展，也正是因为Intel是一个有着历史包袱的企业，所以站在现代CPU看起来，有一些非常奇葩的设计遗留下来，这些设计一开始是为了兼容，慢慢的将这种兼容又发展成新的功能，把“包袱”转换成“亮点”，段设计就是其中的一个很重要的代表，要想搞懂这个设计在不同的CPU的如何保持兼容和强化，我们必须要慢慢的把CPU的历史给捋顺。\n前言 # 我们要介绍第一个CPU是8086，虽然Intel前面也生产了4004、8008、8080微处理器，但是8086基本上后面所以现代化CPU始祖，Intel后面生产的CPU都对8086兼容，所以可以说段这个“祸端”就是从8086种下来的\n8086作为第一款的16位处理器，由于技术的问题，出现了一个严重问题寄存器只有16位，而地址总线却有20位，要想只通过一个寄存器表达地址的值显示无法实现，所以8086采用了一个段寄存器，segment × 16+offset（CS:IP），采用这个方式就能实现20位地址索引\n段设计对程序影响 # 前面简单的介绍了段设计的原因，现在我们来看看这个设计的对16位程序的影响。\n兼容性 # 由于段的存在，我们设计程序的时候可以更加自由，我们可以假设我们是在任何低16位置的内存上，只有到时切换程序时候切换段就可以\n安全性 # 我们可以用相同的低地址存代码和数据，只需要切换段就行，而且我们也不需要考虑数据要多少空间，代码要多少空间，以便设定跳转地址\n可以说段设计是16位程序的一个很优雅的做法，但是到了３２位的时候程序不行了\n32位处理器的改变 # 我们知道32位处理器的诞生带来了两件事,第一个就是寄存器有32位了,说明最大内存地址可以到4G了(0x000000-0xffffff),而且总线地址也有32位了。这个时候就很尴尬了，原来的段设计到这里就成鸡肋了。但是为了兼容，我们还是给他一个功能，段选择子，当然现代Unix操作系统没有用这个东西，只是在初始化的时候意思意思一下，具体可以看一下这篇博客的GDB表\n总结 # 最近学操作系统感觉有很多疑惑，了解很多知识，总想好好总结一下细节，结果发现总有大神们早就写出来了，而且非常详细，我就不班门弄釜了，把下面的lab好好思考，提炼出自己的知识。\n","date":"2018-01-30","externalUrl":null,"permalink":"/posts/essays/programming/mit6_828/%E4%BB%8Ecs%E5%AF%84%E5%AD%98%E5%99%A8%E7%9C%8B%E6%AE%B5%E7%9A%84%E5%89%8D%E4%B8%96%E4%BB%8A%E7%94%9F/","section":"博客","summary":"引言 # Intel作为作为微处理器的航头老大，一直引导CPU的","title":"从CS寄存器看段的前世今生","type":"posts"},{"content":" 引言 # 本文是基于mit6.828 的lab1对操作系统的思考，网上有不少关于lab1的博客，大部分都是介绍如何完成lab1的问题，介绍的比较详细的有这个博客，在这里我就不从问题出发，建议大家看完上面的博客在看我这篇博文，我这篇博文就是从把我遇到的疑惑提炼出知识点，然后再把这些知识点串起来\n实验目的 # 作为操作系统的第一个实验，在完成lab1我们主要目的就是要了解什么是操作系统，后面的lab都是在这个基础上搭建完善好系统的各种功能，就好像你画一幅画，第一步你先把人物的轮廓外形画出来，接下来你在把人的鼻子耳朵眼睛等等慢慢画出来，所以第一个实验非常重要，如果不把骨架搭好，你的操作系统再怎么豪华强大也是空中楼阁，所以我们在这个lab上一定要花上很多时间，万事开头难，只要攻克了这个，操作系统也就离你不远了\n前言 # 谈操作系统之前必须得谈谈什么是计算机，因为我们的操作系统是运行在计算机上面的，操作系统与计算机关系密切，我们可以用一个很贴切的比方，假如你想了解手套为什么长成那么奇怪，那么你得先举起你的双手看看。\n闲话不多说，进入正题，计算机最核心部分就是CPU，记得电脑刚发明出来的时候，编程只能通过穿孔卡片，当然计算机发明这么久，CPU的核心功能基本上没有变，还是只能通过“穿孔卡片”，只不过我们编程语音将我们的使用的语音如C，Python等变成“穿孔卡片”，这里有个非常重要的概念：指令。也就是每个CPU的是每次只能执行一条指令。当然第二个重要的就是内存了，前面说了CPU是必须要通过”穿孔卡片“来编程的，但是随着程序愈来愈大，我们必须要使用一个东西来存贮“卡片”，所以内存孕育而生了，其实很多感叹电脑这个词实在是太贴切了，人脑可以处理问题，电脑也可以处理问题，人脑有短期记忆长期记录，电脑有寄存器（短期记忆）内存（长期记忆）。\n简单的谈了谈计算机，我们提出两个重要的前提\n计算机通过顺序读取一条一条指令来工作 计算机从内存获取指令 所以操作系统的就是在内存上面运行自己并帮助其他程序的一个大程序，总的来说它就是内存上面的一段程序，当然它的功能就是好好在它一亩三分地上面“分封诸侯”，维护自己的“王位”。\n由于一开始它只是待在软盘或者磁盘等存贮设备上的一段程序，接下来我们就看操作系统如何“上位”内存的“宫斗大戏”。\n上位篇 # 在操作系统“上位”之前，其实计算机还进行了一个小操作系统（BIOS）运行，具体的流程可以看这篇文章，如果把电脑看做人的话，BIOS就是我们的脊髓，它控制我们的身体四肢（硬件），但是却无法灵活的操作他们（比如非条件反射的膝跳反射），但是它却不可少，少了它就成植物人了。BIOS的作用总结起来就是感知硬件，调控硬件（比如CPU电压，内存频率）。\nBIOS与操作系统最密切的一个地方就是设置启动盘，大家可能大多时候都是听文件文件比较多，但是文件这个概念其实是操作系统的，在没有操作系统之前是没有文件这个概念的，只有盘这个概念，比如一块硬盘、一张软盘、一个U盘，所以BIOS的一个重要工作就是选定一个存储硬盘，然后读取上面的程序运行。\n所以我们从这时候可以得到一个结论，安装操作系统，只需要按照BIOS的规定把程序放到某一个存贮设备（磁盘、U盘、软盘）专门位置上（一般是头部开始），开机后然后就静等BIOS把程序加载到内存，就完成了“上位”过程。\n我们已经知道操作系统的外部条件，接下来分析一个操作系统到底该有啥\n操作系统结构 # 首先我们要回顾一下操作系统的发家史，刚开始是没有操作系统的这个概念的，随着在电脑上要运行的越来越多，每个程序都要写大量底层代码来操作硬件这样无疑每个程序越来越来臃肿，所以我们需要让操作系统管理来管理硬件，这样不但能够提供程序兼容性（一次编译、到处运行），而且能高效利用硬件（操作系统专供）。所以操作系统也就是程序的分工的产物\n接下来我们看看操作系统的发展历史，大家都知道汇编语言是除了机器语言最接近底层的一种语言，第一个Unix系统一开始也就是使用汇编编写的。但是汇编语言不易于维护，所以1973年汤普逊和里奇用C重构了Unix，但是部分与硬件接触太大的地方还是必须使用汇编（in、out端口，操纵指定寄存器等），所以现代操作系统还是用C与汇编混合编写的（C占大部分），当然最后都会编译成机器码，所以我们从机器码角度来看，最后的结果都是相同的，不同的是我们使用不同的工具来生成机器码\n前面我们介绍了启动盘，其实BIOS给我们只是从启动盘上面取了一小段数据（第一个扇形区）放到内存上面（512B），然后执行那小段上面的机器码，所以接下来就有一个问题，随着我们的操作系统功能越来越完善，取出来的代码肯定不是全部代码，代码只有放到内存上面才能跑，所以前人就把代码分成两部分，一部分称作引导，另一部分才是核心代码，引导作用就是把核心代码放到内存上面去\n所以操作系统被分成两部分\n引导 核心 这里比较有意思的地方就是引导和核心如何交换程序控制权，由于要讲解必须要牵扯代码，所以我把这部分分离出来，放到这篇博客上面，要了解细节可以看这篇博客。\n现在我们假设你已经知道引导将核心代码加载在了内存上面，现在我们就从这里开始分析，堆栈作为程序的基础，首先我们要提一下堆栈\n堆栈 # 什么是堆栈呢？看下面这张图\n堆顶为大地址，我们使用的时候只要不断把栈顶往下推，就能将线性内存变成一个数据结构。当然有个问题，我们必须要知道什么地方是栈顶，现在我们想想操作系统必须要放到内存的某一个地方，假如放到低位置，那么我必须要记住操作系统的最高位置，如果不这样做当堆栈顶到操作系统的存贮地址，那么操作系统就被破坏了。所以我们只能把操作系统放到高的内存地址，这样我们既能安全的使用堆栈，而且保证了操作系统的安全\n所以我们接下来就谈谈由于这个设定引发的一系列问题\n操作系统的高地址 # 在lab1中，操作系统的起始代码被设定为0xF0100000的高位\n从这个地址我们可以分析出来什么0xF0100000，因为我们知道32位操作系统最高只能有4G内存，也就是2的32次方内存，这个地址代表的是系统最后面的255M内存的空间，也就是操作系统给自己留了255M剩余内存，给前面保留了（4G-255M）空间\n操作系统这个设计是非常好的，不仅给自己留下扩展空间，也给其他代码带来便利，就是从0-(4G-255M）这部分内存随便用，怎么改都不会出问题\n从现在角度来看，个人电脑随便都是2G、4G内存，但是在70\\80年代，几M内存都很很大的存在，如果我内存没有4G这么大，那么这个操作系统就无法使用了，而且你留下256M给操作系统，有的时候我整个电脑内存都没有这么大，留下那么多，就是浪费\n所以最后他们想出来一个办法，就是现实和理想之间放一个转换器（MMU），也就是动态内存管理。操作系统也不管你机主有多大内存了，我假设你有4G，当你用一块，无论是什么位置，我从空余的地方给你取出来，假如你没有4G，我就把一部分不常用内存值的放到磁盘上面，这样通过这样这样的操作的实现内存的高效利用。\n操作系统对内存这种骚操作就相当于在更高的维度上建立了一种抽象，不论底层硬件怎么变（内存大小不同），我高层都不需要变，只需要底层把根据实际情况依次映射，这样无论你换多大内存，我都不需要重装系统。\n这种映射不通过代码很难将清楚，所以我这里也不提太多，如果你想知道怎么映射，硬件如何配合，你可以看我这篇博客，详细介绍了内存分页的骚操作。\n总结 # 至处到这里我们的操作系统之旅就结束了，我们成功在引导帮助下上了内存，也使用了提高操作系统的地址保证了堆栈的正常使用，接下来就是操作系统的各种附加功能，我们也会在接下来的lab中继续讲解。\nlab1不止介绍了这两个地方，但是在lab1中我觉得最重要的地方就是要知道操作系统是什么，它从哪里来，它要到哪里去，操作系统毕竟牵涉到硬件，很多硬件知识我们不一定能搞懂，比如说要切换哪几个位到保护模式，怎么输出端口才能读取扇形区，但是我们必须要搞懂他这样做的原因，这也是我在这篇里面谈了很多操作系统历史的原因，希望在理解操作系统上面能给大家带来一点启发。\n在最后的话，我谈一谈我对入门这门课的感受吧，我算编程基础还算比较扎实，但是在一开始学习的过程中一脸懵逼，C语言的各种骚操作，汇编与C疯狂混合，看惯了脚本语言的我，对这种改一下就编译不过的“硬骨头”异常难受，一开始根本看不下去，有的时候看几行代码，查资料好几个小时，而且资料特别难找，你不知道这种骚操作有啥意思，在这里还得感谢学习过这门课的学长学姐们，在网上无私的把自己的感受分享出来，慢慢的根据他们的资料和自己的调试还有自己找资料，慢慢的把这个代码一行一行搞懂，搞懂之后特别感慨，其实也不是很难，难的是要把你知道的所以知识点串起来，然后你就能搞懂为什么要这么做，所以我推荐大家还是多看代码，多去思考为什么要这么做，必须要自己的思考在里面，如果单纯的知识为了完成实验其实很简单，但是难就难在把这门课琢磨透，搞懂他们为什么要这样考你，毕竟大家最后的目的都是自己做出自己的操作系统，只有了解的透彻，才能“破而后立”搞出自己的创新，而不是复制他们的壳子，所以加油把每一行代码都搞懂，多思考，不要骄傲，毕竟这只是教学类的操作系统，要想自己写出来还得不懈努力。\n接下来还有6个lab需要完成，终于迈出的第一步，希望在年前能够完成所有的实验吧！！！加油！！！\n引用 # \u0026ldquo;C语言虚拟内存表”\n","date":"2018-01-29","externalUrl":null,"permalink":"/posts/essays/programming/mit6_828/%E4%BB%80%E4%B9%88%E6%98%AF%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/","section":"博客","summary":"引言 # 本文是基于mit6.828 的lab1对操作系统的思考，","title":"什么是操作系统","type":"posts"},{"content":" 引言 # 一开始想直接做一个操作系统，但是万事开头难，学习操作系统需要太多基础知识了，所以就按照网上推荐先学习mit6.828的课程，先把xvf6操作系统搞懂，然后在来实现自己的操作系统，下面就是学习这个课程的体会，按照各个lab的顺序，介绍自己的心得体会\n课程的地址是： mit6.828 PS：由于mit6.828课程仓库需要翻墙，所以我把clone下来放到我的github仓库，我的仓库里，大家可以clone下来（我会逐渐完成所以的实验）\n目录 # 什么是操作系统 lab1 内存分页设计 lab2 用户空间隔离 lab3 并行多核设计 lab4 文件系统设计 lab5 网络设计 lab6 课程总结 # 学到一半才发现这是一门研究生课程的学习，的确难度非常大，而且每一个实验都可以拿出来大做研究，但是课程给的资料非常详细，基本上每个硬件细节都给出了链接，但是对于最大的困难还是英语资料实在太多，有点“吸收”不过来。不过一路学习下来，感觉还是收获很大，基本上每个实验都环环相扣，每个实现细节都需要反复思考，为什么要这样做，还能怎么做，最终实现的xvf6还有些许跟不上时代的脚步，但是基本上框架已经有了，就是按照这个骨架完善更多细节，所以这个课程还远远没有结束，期待接下来对这个操作系统的进一步改进！！！\n","date":"2018-01-09","externalUrl":null,"permalink":"/posts/essays/programming/mit6_828/mit6.828%E8%AF%BE%E7%A8%8B%E6%80%BB%E7%BB%93/","section":"博客","summary":"引言 # 一开始想直接做一个操作系统，但是万事开头难，学习操作系","title":"mit6.828课程总结","type":"posts"},{"content":" 引言 # 最近逛知乎的时候看到一篇知乎回答很有趣，也给了很深的感触，自己对于操作系统、数据库、HTTP用的都非常熟练，但是也从来没有去想怎么怎么实现或者背后深层次的原理\n以前刚开始学习的时候自己也看过很多关于操作系统、数据库的书，但是就像走马观花一样，很多书就是过了一遍脑子，然后就没有了，在2017年下半年自己也开始尝试把自己以前只是了解只是会用的东西拿出来咀嚼咀嚼，并在这个过程中写出一些自己觉得还是有点干货的博文。而且我也比较享受把复杂的问题简单话的过程。\n操作系统、计算机、数据库、编程原理在我看来都是非常复杂庞大的东西，在2018年，希望能够将上面这些复杂的东西搞的简单一点，当然在这个同时我也会将我将问题简单话的过程中分享出来。\n当然这些东西都太过宽泛，比如操作系统。很多人刚开始学习编程了解了操作系统概念，还有听到很多牛人都说自己写了一个操作系统都会在心里暗暗的立下一个Flag，自己也要做一个操作系统，但是很多人的热情慢慢的被无数的复杂概念慢慢磨掉，最后能坚持到做出一个完整的系统没有几个。作为一个普通人，我们没法一口吃下一个很大的“饼”，当然你如果叫爱因斯坦给你造一个原子弹，它也只能拍拍手跟你说我只能证明它能造出来。所以有效的解决问题的方法是将问题分解，或许最终我们最终造不出操作系统，但是我们对操作系统也能更深一步了解。\n操作系统 # 首先谈谈操作系统，其实我们对操作系统的理解大致都停留在开关机。我们要分解的问题就是我们对这个“黑盒子”的疑问\n操作系统如何管理硬件，比如说鼠标移动点击、键盘输入 操作系统如何让多用户使用 操作系统如何在多用户的情况下区分每个用户 操作系统如何分配内存，如何限制用户的内存 操作系统如何管理文件，如何避免文件被同时写入 \u0026hellip;.. 当然我这里只是列举了一些直观的问题，在解答这些问题的时候，我们会逐渐衍生出各种问题，最终我希望所有的问题都能被简单话\n数据库 # 数据库我算使用的比较多了，但是对数据库背后的机制也是懵懵懂懂，只知道他能保证这些功能，比如事务的原子性、一致性、分离性、持久性。\n我觉得了解数据库就是要了解它是如何实现这些特性的，当然这个问题也很宽泛，我们可以尝试分解成\n数据库如何将输入复杂的查询分解成为数据库的操作 数据库如何避免死锁 数据对于插入删除异常如何解决 \u0026hellip;.. 其实在我看来数据库可以分解成两大块，一个是编程语音解析（SQL），一个是高效的存贮（Database）\n所以了解数据库也可以尝试了解编译原理，了解如何实现解析SQL，所以接下来我就不提编译原理了\n总结 # 其实想想自己的这种把复杂问题简单话就是华罗庚先生曾说的一句话：“把书由薄变厚，再把书由厚变薄”，首先我们得把这些看起来很简单的问题复杂话，最后慢慢的把这个复杂的问题简单话，这样你才能真正的把这个问题给搞懂。\n对于我来说，从小或许受应试教育的影响，自己只学会了复杂问题给解决，但是不懂得解决后再把复杂问题简单话，所以看起来我懂很多复杂问题，但是其实换一个场景，换一个问题就一知半解了。所以自己慢慢的开始改变自己的方法，不满足于解决问题，而着重于简化问题，希望自己的一点经验也能给后来人一点启发。\n","date":"2018-01-06","externalUrl":null,"permalink":"/posts/essays/life/2018%E6%96%B0%E5%B9%B4%E5%B1%95%E6%9C%9B/","section":"博客","summary":"引言 # 最近逛知乎的时候看到一篇知乎回答很有趣，也给了很深的感","title":"新年展望","type":"posts"},{"content":"","date":"2018-01-05","externalUrl":null,"permalink":"/tags/%E7%BA%A2%E9%BB%91%E6%A0%91/","section":"Tags","summary":"","title":"红黑树","type":"tags"},{"content":" 本篇博客主要基于这篇博客的扩展，建议阅读前先阅读这篇博文，这篇博文详细介绍红黑树的实现原理，完整代码在github的rbtree.go文件中\n浅谈\u0026quot;树\u0026quot;这种数据结构 Github 可视化页面\n引言 # ","date":"2018-01-05","externalUrl":null,"permalink":"/posts/essays/programming/%E7%BA%A2%E9%BB%91%E6%A0%91%E5%AE%9E%E7%8E%B0%E5%8E%9F%E7%90%86/","section":"博客","summary":"本篇博客主要基于这篇博客的扩展，建议阅读前先阅读这篇博文，这","title":"红黑树实现原理","type":"posts"},{"content":" 本篇博客主要基于这篇博客的扩展，建议阅读前先阅读这篇博文，这篇博文详细介绍AVL树的实现原理，完整代码在github的avl.go文件中\n浅谈\u0026quot;树\u0026quot;这种数据结构 Github 可视化页面\n引言 # AVL树是在对二叉搜索树的一种优化，通过构造一棵高度平衡的二叉搜索树从而实现提高空间利用率，所以在了解如何实现之前，必须了解如何构造一棵二叉搜索树，你可以阅读我的这篇博客了解如何构建一棵二叉搜索树，虽然我是用Go来实现的，但是不必了解太多Go方面的知识，我在博客中尽量使用图片的方式来介绍实现原理\n二叉搜索树实现原理\n由于AVL树本质上也是一棵二叉搜索树，查找并不会改变树的性质，所以AVL的查找也是同二叉搜索树的查询相同，所以这里就重点介绍如何实现“增”和“删”\n树的字段 # 在开始介绍如何实现“增”、“删”之前，我想提一下AVL树的存贮字段，在wiki上面的AVL和很多资料上面，AVL的树结构分别由下面四个组成： Val、 Left*、Right*、Parent*。\nVal代表树存贮的值，Left、Right、Parent代表三个指针，分别指向左子树，右子树，父亲，而在我的实现中，我去掉了父亲指针，对于讲解来说，使用一个父指针能很好的解决从子遍历到父亲的经过，但在实际的应用中，我们完全可以使用一个列表存贮从父亲到儿子之间的值，这样的话，能实现一样的功能，而且能节省存贮空间（每个子树都少了一个父指针），但是相应的程序也变得比较复杂起来。\n虽然我在讲解的过程中会直接获取节点的父节点，但是在实际的代码实现中，是通过获取从列表中获取该节点的父节点。\nAVL树的“增” # 引用 # https://en.wikipedia.org/wiki/AVL_tree\n","date":"2018-01-04","externalUrl":null,"permalink":"/posts/essays/programming/avl%E6%A0%91%E5%AE%9E%E7%8E%B0%E5%8E%9F%E7%90%86/","section":"博客","summary":"本篇博客主要基于这篇博客的扩展，建议阅读前先阅读这篇博文，这","title":"AVL树实现原理","type":"posts"},{"content":" 本篇博客主要基于这篇博客的扩展，建议阅读前先阅读这篇博文，这篇博文详细介绍二叉搜索树的实现原理，完整代码在github的binary.go文件中\n浅谈\u0026quot;树\u0026quot;这种数据结构 Github 可视化页面\n引言 # 之所以使用Go来实现，个人还是比较喜欢Go的，作为一个基础数据结构，Go用来实现这个速度比Java、C++都快，而且相比Java也能节省内存，而且我也不喜欢换使用冒号。\n二叉搜索树的基本功能 # 作为一个树结构，主要必须实现三个功能，查增删（当然有改，不过改是删和增的结合所以就不算在里面），当然二叉树还能帮我实现一些额外功能，比如寻找最大值最小值等等，实现一颗二叉搜索树必须要保证在“增”和“删”的时候保持树结构不变，也就是保证还是一颗二叉搜索树\n什么二叉树是二叉搜索树呢，二叉搜索树只需要保证一点\n任何一个节点，它的左子树（不为空）所有值小于这个节点值，它的右子树（不为空）所有值大于这个节点值 所有我们在实现增删的时候必须要牢记这点，而且我们也可以知道在你开始对这颗二叉树进行增删的时候，它一定已经满足了上面这个条件\n查 # 如何在上面这颗标准的二叉树里面查询到我们想要的值呢？我们知道一颗二叉树只需要知道根节点（上面的10）就能通过遍历得到所以的节点的值，所以我们在实现中声明一个二叉搜索树的时候只保存一个root节点，通过这个节点，我们就可以实现所以的增删查改功能了\n为了找到每个值a，我们依次要在每个节点上对值进行比较，因为每个节点的值代表已经将这堆数据分成两个部分，一堆比这个值大在右边，一堆比这个值小在左边，这样只要最终能找到一个那个值的最小区间的对应值（找到）或者找到一个为空的叶节点（代表没有找到），\n通过最简单的查找，我们可以反推出一个理论，假如我们需要插入一个值，我们只要先尝试去寻找到那个值，假如找到了，因为已经存贮了就不插入，假如没找到就会找到一个为空的叶子节点，那样只要在父节点将这个为空的叶子节点替换成这个值为a的节点，我们就完成了插入\n所以查找和插入是相对的，知道怎么查找就知道怎么插入，接下来我们详细介绍如何删除，这个非常关键，因为它是后面要介绍的高级树实现高效的删除的关键\n删除 # 我们还是以上面那种图举例子，删除我们最先想到的是删除12,17那些节点，直接删除掉对二叉树没有什么影响，但是如果我们要删除那些带有节点的呢？我们先简化问题，先看一个只有两个节点的树\n我们要想删除10，有两种可能\n一种是将左边值移过来，一种是将右边值移过来，这两种都不会破坏二叉树的平衡，现在我们假设我们倾向将左边的值移过来，我们再回到上面那棵标准的二叉树，如果我们要删除掉10节点，我们要把那个值移过来，我们直接在可视化界面进行这个操作\n我们发现，我们只需要将7和放到10的位置上就完成了一次删除的操作，而7与10的关系是，7是10左边的那堆值最大的，用一个通俗的话来说，完成改朝换代的关键就是要找一个能镇的住手下的人，由于“10”挂了，在左边只有“7”能镇住左边，所以\u0026quot;7”升官直接跑到“10”的位置了\n根据这个原则我们将情况分成三种\n左右子树都为空，直接删掉 左边子树或者右边子树有一个为空，将不为空的子树提上来当做删除的节点 左右子树不为空，将左边最大的值提上来替换删除的节点 对于第三种情况，为了编程方便，我们考虑一种情况，如果左子树的右节点有没有值（比如说上面的5节点），如果它右节点有值，那么左边的最大值肯定出现在，左子树的右节点上（就像上面左子树出现在5的右节点的7节点上面）。然后我们继续判断“7”节点有没有右子树，这样循环下去，我们总能找到一个节点没有右节点，这样我们就找到左子树的最大值了。\n在上面这种情况下，我们将这情况分成两种\n如果左子树的右节点有值（循环下去找到一个节点右节点没有值） 如果左子树的右节点没值（那么第一个左子树就是最大值） 具体的代码在binary.go的Delete(寻找节点）方法和delete（删除节点）方法中\n总结 # 总的来说，二叉搜索树的插入是最容易理解的，但是删除的话要考虑不同四种的情况所以还是稍微需要点时间理解一下的，小小的一个二叉搜索树也花了近200行代码实现，但是相比后面的AVL树、红黑树的近千行代码来说，二叉搜索树还是比较简单的，而且后面实现的高级树结构基本上都是依赖二叉搜索树的实现（必须要按照二叉树的增删满足二叉搜索树的原则），所以我们对二叉搜索树的增删必须理解透彻。\n引用：\nhttps://en.wikipedia.org/wiki/Binary_search_tree\n","date":"2018-01-04","externalUrl":null,"permalink":"/posts/essays/programming/%E4%BA%8C%E5%8F%89%E6%90%9C%E7%B4%A2%E6%A0%91%E5%AE%9E%E7%8E%B0%E5%8E%9F%E7%90%86/","section":"博客","summary":"本篇博客主要基于这篇博客的扩展，建议阅读前先阅读这篇博文，这","title":"二叉搜索树实现原理","type":"posts"},{"content":" 一直以来我对树这种数据结构就比较头疼，随便找一个红黑树的博客，大部分都是在谈怎么旋转怎么插入怎么删除，将算法讲的头头是道，但是就算你看懂了也不懂为什么要这样做，所以我们这篇博文就从可视化的角度，慢慢的介绍这些树的来世今生。\n引言 # 首先给大家推荐一个神奇的网站，我们这篇博文很大程度上都是依托这个网站从可视化的角度分析各种树\n网站的网址是http://www.cs.usfca.edu/~galles/visualization/Algorithms.html\n接下来我们从易到难分别谈谈下面这些树结构\n二叉搜索树 AVL树 红黑树 Splay树 Trie树 B树 B+树 你可以点击下面高亮的标题尝试这种树结构的可视化操作，当然我在谈的过程中默认你已经打开可视化界面网站。\n在开始介绍各种树结构之前，我们先谈谈为什么我们需要树这种结构。\n假设我们一开始都有一个“柜子”存放东西，当我们东西很少的时候，我找东西的时候只要在柜子里面翻一翻就能找到，当我东西越来越多，在柜子里面找东西消耗的时间越来越多，这时候我就把柜子分类，让每一个柜子的东西都是固定的，这样我们查找时候不会在一个柜子里面寻找很久，这样无论你有多少东西进来，我查找的时间基本不会太长。\n所以树这种数据结构就是为了解决当数据量越来越大的时候相比与线性查找时间基本不会太长。对树查找和线性的区别我们可以看一下这个可视化界面\n线性搜索与二分查找\n通过输入查找344，我们发现线性搜索花了15步才搜索到，而二分查找只需要2步，而且随着数据越来越大，线性搜索花的步数直线上升，而二分查找只会维持用Log(N)的步伐增长。\n树的结构基本上都是基于二分查找，但是在维护这个查找树的方法上各种树有各种优化方法，接下来我们就从可视化角度来依次介绍各种树的原理和优缺点。\n二叉搜索树 # 这个树可以称作所以后面变种树的鼻祖，基本上所以后面的树都是基于这个树的不足之处进行改良达到其各自的目的。\n我们现在尝试构造一棵二叉树，我们依次插入3,2,4。然后一棵标准的二叉树就出现在我们面前，在我们插入的过程中，我们可以清晰的看到，第一个插入的变成了母节点，而且插入程序实现的非常简单，首先跟母节点进行比较，如果比母节点大，从母节点右边下滑，然后依次找到最后一个为空的节点把自己插入进去。\n我们接下来尝试一下如果输入一个连续递增的数组进去，比如1， 2， 3， 4， 5，我们发现这个二叉搜索树会逐渐把所以的值放到右节点上。这时候我们我们尝试搜索一下5，我们发送这个二叉搜索树需要5次查询才能得到最终结果，这个基本上等同与线性搜索。\n这里我们就发现二叉搜索树一个致命缺点，它容易造成一种空间的浪费，虽然这个树看起来很“高”，有5层，但是利用率极低，每层只有一个树，这样不但查找的时候效率不高，而且插入的时候效率也不高（很可能需要“爬”很多层）。\n二叉树搜索树实现起来非常简单，只要用一个while循环就能完成插入查找等功能，但是由于它结构控制太低，它对数据要求要很高，它时候依次插入那些无序数字，而对那些有序数据来说，二叉搜索树等同与一个数组。\n所以我们接下来介绍下面改良树，看看其他树是如何提高的空间利用率的，由于这篇主要是简单的谈一下这些树的优缺点，如果你想了解如何实现这样可以看一下我写的这篇博文，详细介绍了如何使用Go语言实现一个高效的二叉搜索树，如果你想彻底了解下面高级的树的实现，强烈建议一下了解一下二叉搜索树实现的方法，因为后面所以的树实现的前提必须满足是一个二叉搜索树，这点非常重要。\n二叉搜索树Go实现\nAVL树 # 1962发明的AVL树现在还生龙活虎的活跃在Windows进程地址空间管理，虽然作为最早的平衡二叉树现在有点干不过后面的年轻的小伙，但是作为前辈，后面小伙很多地方都是借鉴前辈的，所以我们有必要隆重介绍一下这个老前辈。\n首先要说明一下什么是平衡二叉树，前面我们也看到了，二叉搜索树有个弊端，它有可能子树高度不一样，比如说1,2,3,4,5那个母节点1的左子树高度为0，右字数高度为5，平衡二叉树就没有这个问题，它规定所以节点子树高度差不能超过1，这样规定的话我们的树空间利用率就能基本上达到100%了。\nAVL树就是一颗高度平衡二叉树，它的要求也非常简单，就两个\n必须是二叉搜索树 必须是平衡二叉树 成为一个二叉查找树非常容易，但是为了必须满足平衡二叉树条件，这里我们给每颗树定义一个高度，默认空为0，这样就可以计算左右子树的高度差是否在-1和1之间判断是否是平衡树。\n了解了这个之后，我们回到web页面，我们输入1,2,3看看AVL树如何避免像二叉搜索树一样过度生长\n我们从这两幅图可以看到，插入前AVL树高度为3，经过单旋转后，将2旋转到1的位置，然后让树又满足平衡二叉树的条件和二叉搜索树的条件。让我们思考一下旋转的意义。\n旋转的意义 # 第一旋转并没有改变二叉搜索树的特性，我们将二叉树看做一个一个小区间组成的区域，旋转将区间整体移动了，所以我们保持住了区间的稳定，而且我们把旋转的支点看做中心，我们发现通过一次旋转后，支点左右之间的数量发生的变化，其中一方增加一个，其中一个减少一方，通过这种方式，把高度差由2转成0\n所以我们在插入和删除的时候造成的高度不平衡可以通过正确的旋转达到一个高度差的减少。而且我们可以很轻松的推断出在插入和删除的时候只需要通过有限的旋转就能实现树的平衡，但是我们也会发现这个平衡是非常严格的，每次插入删除的时候，删除或者增加的那条“路径”（从根节点到修改的节点）的各个节点的高度都会发生变化，所以我们需要检查和更新整条路径上高度差（也称平衡因子）是否满足平衡条件。\n直接纪录每个树的高度并且约束高度差的方法能够很好的规避像上面二叉搜索树造成的空间利用率低问题，但是对于增添和删除操作频繁的数据来说，AVL树并不是一个很好的解决方法，因为每次删除和增加都必须检测路径上的高度差，虽然旋转比较高效，但是每次检测和修改高度会让这个算法花费太多时间在这个严格的条件上面。\n总的来说，AVL树是比较好理解的一种平衡树算法，通过这个树的各种旋转操作，我们能很清晰的发现二叉树是一种很神奇的数据结构，一方面它可能是混乱的（分支混乱），一方面它又是整齐的，通过“扭动”它的身躯，将数据平均的分摊在根节点上面。我非常推荐你在这个可视化界面上通过一步一步将单独递增或者递减的数据插入AVL树中，你会发现AVL通过一次一次的“扭动”，将数据向根节点另一个方向传输过去。\n这里我也不详细介绍AVL树实现的具体原理，如果你想了解更底层的实现，可以看我下面这篇博客\nAVL树的实现原理\n红黑树 # 在AVL树发明10年后鲁道夫 贝尔发明了红黑树，直到现在红黑树还依旧在C++的STL、Java的HashMap中发挥着重要作用，只所以要先介绍AVL树再介绍红黑树，因为从我的理解上来看，其实红黑树和AVL树原理都是类似的，只不过红黑树修改了AVL树的缺点并且将AVL的平衡原理进行了抽象话。\n从数据结构上来看，红黑树相比于AVL树来说，只是将存贮高度的值换成了存贮颜色的值，从空间的角度上看，红黑树用一个二进制bool值（1Bit）换掉一个存贮int值（4Bit），节省了3Bit的空间，从增删所需要的操做来看，AVL树旋转可能需要lnN 步操作，而红黑树增不超过2次旋转，删不超高3次旋转，在增删效率上远远优于AVL树。所以我们有一个疑问为什么红黑树能用更少的空间实现更高的效率呢？\n回答这个问题之前，我们再捋捋AVL它的实现原理，前面介绍了AVL实现平衡的原理的就是使的左右子树高度差维持在[-1,1]之间，高度差这个东西，你想想高度这个东西其实它就是母节点到叶子节点的路径，所以高度差我们翻译一下就是左右子树路径差，现在我们把平衡后的AVL树条件更加严格一点，我们规定左右子树路径差为0，也就是左右子树高度一样。\n在上面那个严格的条件下，假如我们要新增一个到树末端，假如它加在像上面这样满二叉树上面，那么左右子树高度就立刻不相同了，但是对于一个AVL树来说，在这颗树上末端插一颗树并不会影响树的平衡，所以我们要想一个办法让插入这个值“不算高度”，这时候我们一想，如果我们把树节点分成两种颜色，一种红色一种黑色，红色不算高度，黑色才算高度，那样虽然我现在条件很苛刻“左右子树高度必须一样”，但是如果插入的是红色，那么就能很轻松的实现上面的条件。\n好了我们通过上面的操作成功的将高度变成了黑高（黑色节点的高度），而且满足了一种更加严格的条件“左右子树黑高必须相同），假如说前面AVL树的高度差为[-1,1]之间都能实现高空间利用率的，那么在这种更加严格的条件下也能满足，但是现在又出现一个问题，我们看下面这棵树\n当我们插入节点3的时候，由于插入一颗红色你把红色节点不算高度，所以在前面的约束条件下，插入3满足条件，但是实际上这棵树并不是一个平衡AVL树，为了避免这种情况的发生我们必须要做一个约束，我们要约束红色节点不能太多，所以我们就约束红色节点后面只能是黑色节点，这样路径上最少都会有一半都是黑色（红色黑色相同）。\n我们现在得到两个约束，现在我们来说思考一下这两个条件的约束到底给我们带来了什么\n黑高相同 红后面必须为黑 首先对于黑色节点来说，这棵树是高度平衡的（假设它只能看到黑色节点），高度差为0，但是对于红色节点来说，这棵树有可能是不平衡的（假设有N个黑节点，红色高度差可能有N-1），所以从整体上来看，红黑树牺牲了一定的平衡性换来了插入删除的高效性\n这个高效性表现在哪呢？第一它不在需要更新所有的高度差，它只需要对受影响的路径上的有限节点进行染色就能实现再次的平衡，其次由于它不在需要高度的平衡，所以它能容忍树上的红色的不平衡现象，来减少旋转变化的次数。\n前面我们分析了AVL树旋转的意义那么对于红黑树来说，旋转的意义是什么呢？\n红黑树的旋转意义 # 前面我们知道了在AVL树中每一棵树都是独立的个体，所以每次旋转都是一次运输，将树按照节点扭动，但是对于红黑树来说，它要处理的情况是黑树高度的不同，当需要搬到的也是黑树，它的目的就是将黑树的高度平衡，对于红色的树来说，它更像一种胶水，它能保证当数据不平衡的时候（所以的树不可能全部变成黑树）还能使黑树保持一种平衡，从而使一棵树变得相对平衡。\n所以红黑树的旋转的意义同AVL树是类似的，只不过红黑树在旋转的时候要更加注意，它不但要考虑我们要如何在不平衡的节点上平衡黑高，而且要考虑我们平衡了这个节点的黑高有没有对其父亲的黑高造成影响。\n对于红黑树来说，由于颜色是可变的，所以着色（改变颜色）也是一种手段，而对于AVL树来说高度是固定的，只能通过旋转来改变，所以单独讲红黑树的旋转是没有意义的，或许有的时候只需要通过旋转就能实现黑高的统一，或许通过简单着色也能实现黑高的统一，但是只使用一种手段是不能高效的完成所以的条件。所以在我看来着色和旋转是相辅相成的\n也正因为我们多了一个着色的手段，所以红黑树比AVL树在某些方面更加高效，随着而来也是更加复杂难懂，但是总体来说AVL和红黑还是类似的，在这里我就不深谈如何实现红黑树，如果你想了解如何实现红黑树，可以看我这篇博客\n红黑树的实现原理\n剩余的四种树的简析会待代码实现后陆续更新 # 引用 # https://en.wikipedia.org/wiki/Binary_search_tree\nhttps://en.wikipedia.org/wiki/AVL_tree\nhttps://www.cnblogs.com/vamei/archive/2013/03/21/2964092.html\n","date":"2017-12-27","externalUrl":null,"permalink":"/posts/essays/programming/%E6%B5%85%E8%B0%88%E6%A0%91%E8%BF%99%E7%A7%8D%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/","section":"博客","summary":"一直以来我对树这种数据结构就比较头疼，随便找一个红黑树的博客","title":"浅谈\"树\"这种数据结构","type":"posts"},{"content":"","date":"2017-12-27","externalUrl":null,"permalink":"/tags/%E6%A0%91/","section":"Tags","summary":"","title":"树","type":"tags"},{"content":" 这篇博文是从问题理解动态规划的练习篇，通过几个动态规划的问题剖析进一步理解动态规划\n找零钱练习题 # 给定一个零钱数组比如[1, 2, 5],每个值代表一个面额的纸币，给定一个总数(aim)，求换钱有多少种方法（每种面额纸币不限量） 这个问题非常经典，所以我就从最先容易想到的算法出发慢慢推导出动态规划\n正向暴力搜索 # 面前一大堆钱，分成三堆，我们必须要从这三堆中抽取出来所以可能的方案，看看能够凑到总数。\n我们第一个想到的就是正向暴力搜索，先从第一堆取出0张、1张、2张\u0026hellip;.，然后递归下去，让取出0张、1张、2张凑剩下的总数，等到取到最后一个钱堆或者总数正好相同的时候递归停止。\n我们可以很轻松的写出下面的代码（Java）\nint violenceSearch(int[] level, int index, int aim) { if (aim \u0026lt;= 0) { return aim == 0 ? 1 : 0; } if (index \u0026gt;= level.length) { return 0; } int sum = 0; for (int i = 0; i \u0026lt;= aim / level[index]; i++) { sum = sum + violenceSearch(level, index + 1, aim - i * level[index]); } return sum; } 这个函数接受三个参数，第一个是钱堆的面额数组，第二个是当前是拿第几个钱堆的序号，第三个是剩余要凑的总数\n这个算法的核心就是sum = sum + violenceSearch(level, index + 1, aim - i * level[index]);，我们分别从钱堆里面取出想0张、1张\u0026hellip;，然后计算剩下的总数和剩下的堆数方法总数和。\n这个算法也可以优化成记忆搜索，总共有多少种方法拼钱，主要与index和aim有关，我们只要用map记录一下这个值就可以优化成为记忆搜索。\n反向暴力搜索 # 反向的话比较难想到，但是正向暴力搜索没有办法分解成子函数，也就无法实现动态规划，所以我们必须要反向思考\n首先我们假设纸币面额为1， 2， 5， 我们要凑10块钱，我们假设已经得到了所以的次数F(x, y)(x为由多少种纸币组成，y为凑多少钱），所以我们得到这个我们想要的结果F(3, 10) （也就是由3种面额组成10块）\n假设我们得到了F(3, 10)所以可能的组成结果结合如下面10种\n10张1块 8张1块、1张2块 6张1块、2张2块 5张1块、1张5块 4张1块、3张2块 3张1块、1张2块、1张5块 2张1块、4张2块 1张1块、2张2块、1张5块 2张5块 5张2块 现在我们把这10种可能按照5块的张数分成3份\n0张5块 10张1块 8张1块、1张2块 6张1块、2张2块 4张1块、3张2块 2张1块、4张2块 5张2块 1张5块 5张1块、1张5块 3张1块、1张2块、1张5块 1张1块、2张2块、1张5块 2张5块 2张5块 分别为0张5块（6种），1张5块（3种），2张5块（1种），也就是我们成功将F(3, 10)分解成 F(2, 10), F(2, 5), F(2, 0)\n通过这种方式我们成功构造出子函数，我们很容易就能写出递归函数\nint lowBackVS(int[] level, int index, int aim) { if (index == 0) return aim % level[index] == 0 ? 1 : 0; if (aim \u0026lt; level[index]) return lowBackVS(level, index - 1, aim); else { int count = 0; for (int i = 0; i * level[index] \u0026lt;= aim; i++) { count += lowBackVS(level, index - 1, aim - i * level[index]); } return count; } } 这里我们要看到第二个if，我们判断剩下的余额是否够当前的一张，如果不够，那直接就是前n-1种纸币能够组成的次数。（也就是假如你剩下4块钱要用一张5块来组成，肯定不可能，直接返回前面的n-1种货币能够凑出4块的种数）\n这种时间复杂度为O(n) = n X aim X aim， 假如aim比较大还是比较耗时间的，我们看看是否能够优化一下\n反向暴力搜索优化 # 我们还是回到上面那个例子，我们这次按照能够减去一张5块的进行分类，这样我们就分成了两类\n无法抽掉一张5块 10张1块 8张1块、1张2块 6张1块、2张2块 4张1块、3张2块 2张1块、4张2块 5张2块 能够抽掉一张5块 5张1块、1张5块 3张1块、1张2块、1张5块 1张1块、2张2块、1张5块 2张5块 一张是无法抽掉一张5块（6种），能够抽到一张5块（4）种 我们回到函数定义，这样我们成功将函数F(n, aim)分解成两个F(n-1, aim) 和F(n, aim - level[index])\n我们成功的将子函数进行化简成两项\nint backVS(int[] level, int index, int aim) { if (index == 0) return aim % level[index] == 0 ? 1 : 0; if (aim \u0026gt;= level[index]) return backVS(level, index - 1, aim) + backVS(level, index, aim - level[index]); else return backVS(level, index - 1, aim); } 我们可以看到我们成功将时间复杂度降到了O(n) = n X aim，至此我们写出了比较完美的反向暴力搜索方法，当然我们也能够像正向暴力搜索优化成记忆搜索\n动态规划 # 前面我一直没有怎么提记忆搜索法，因为这个方法基本上等同与动态规划，只不过动态规划使用数组存贮，记忆搜索法用字典存贮。\n前面我们知道我们能通过分解函数将问题划分成前面的子问题，所以我们只需要构建一个二维数组，x，y分别代表由几种货币组成（index），组成的总数（aim），这样就能通过慢慢“填”写动态规划表，最后求出由N种货币组成钱数（aim）总数\nint dynamic(int[] level, int index, int aim) { int n = level.length; int[][] dp = new int[n][aim + 1]; for (int i = 0; i \u0026lt; n; i++) dp[i][0] = 1; for (int i = 1; i \u0026lt;= aim; i++) dp[0][i] = i % level[0] == 0 ? 1 : 0; for (int i = 1; i \u0026lt; n; i++) { for (int j = 1; j \u0026lt;= aim; j++) { if (j \u0026lt; level[i]) dp[i][j] = dp[i - 1][j]; else dp[i][j] = dp[i - 1][j] + dp[i][j - level[i]]; } } return dp[n - 1][aim]; } 这里要注意的是初值的初始化，默认当钱数为0的时候值为1（比如说前面的2张5块的方法F(2, 0) = 1），当只由一种钱币组成时，只要总数能够整除第一种钱币面额就为1（全部由这种货币组成）\n动态规划的优化 # 当然这个优化可有可无，因为我们观察上面的函数，我们发现其实N层只与前面N-1层一个有关，这样的话，我们如果覆盖掉上一层的值对后面的计算结果也没有影响。所以我们就不使用多维数组，直接使用一维数组，节省了（N-1）* (aim + 1)的空间\nint advanceDynamic(int[] level, int index, int aim) { int[] f = new int[aim + 1]; f[0] = 1; int n = level.length; for (int i = 0; i \u0026lt; n; ++i) for (int j = level[i]; j \u0026lt;= aim; ++j) f[j] += f[j - level[i]]; return f[aim]; } 所以我们成功的将代码改的更短，而且更省空间\n总结 # 一开始本来想多讲几道动态规划的问题，但是写着写着发现其实大部分都是大同小异，找到子函数，构建表存贮计算过程，其中最大的挑战就是找到分解子函数，所以我就不介绍其他的问题了，我就把这些问题留在下面，你可以试试挑战一下自己\n背包问题 一个背包有一定的承重cap，有N件物品，每件都有自己的价值，记录在数组v中，也都有自己的重量，记录在数组w中，每件物品只能选择要装入背包还是不装入背包，要求在不超过背包承重的前提下，选出物品的总价值最大。\n最优编辑 对于两个字符串A和B，我们需要进行插入、删除和修改操作将A串变为B串，定义c0，c1，c2分别为三种操作的代价，请设计一个高效算法，求出将A串变为B串所需要的最少代价。\nLIS 求出序列的最长上升子序列的长度\nLCS 给定两个字符串A和B，返回两个字符串的最长公共子序列的长度。例如，A=\u0026ldquo;1A2C3D4B56”，B=\u0026ldquo;B1D23CA45B6A”，”123456\u0026quot;或者\u0026quot;12C4B6\u0026quot;都是最长公共子序列。\n走迷宫问题 有一个矩阵map，它每个格子有一个权值。从左上角的格子开始每次只能向右或者向下走，最后到达右下角的位置，路径上所有的数字累加起来就是路径和，返回所有的路径中最小的路径和。\n","date":"2017-12-26","externalUrl":null,"permalink":"/posts/essays/programming/%E5%87%A0%E4%B8%AA%E6%9C%89%E8%B6%A3%E7%9A%84%E5%8A%A8%E6%80%81%E8%A7%84%E5%88%92/","section":"博客","summary":"这篇博文是从问题理解动态规划的练习篇，通过几个动态规划的问题","title":"几个有趣的动态规划","type":"posts"},{"content":" 网上关于动态规划的资料，大部分直接给结论，所以一开始我一头雾水，搞不懂为什么要这么做，这篇博文就从实际问题出发，简单的剖析动态规划\n引言 # 现实生活中总能找到一些问题你没法给出具体答案，比如给你一堆1块、5块、10块的零钱，要你找出多少种能够拼出100块的方法。还有就是迷宫问题这种。这种问题都有一个特征，我们没有办法立刻给出答案，而且我们对这种问题的想到的第一种解决方法就是暴力搜索，把所以的可能方案列出来然后得到答案。这种暴力搜索最终能够解决问题，但是他们在计算的时候花了很多时间在相同的计算上面。为了节省时间，所以我们使用动态规划“优化”暴力搜索\n为什么要使用动态规划？ # Example # 我们先举一个简单的例子，大家都知道走楼梯问题，这也是教科书一个经典的递归程序\n有N阶台阶，一个人一次上两个或者一个，问有多少种走法? 我们拿到这道题，我们首先会想这样想，从第一个台阶开始，我们使用递归让这个人走一步或者两步，这样每次分解成为两种可能，最后直达走到N阶台阶，或者迈过去了，然后计算这种所以迈到N阶台阶的可能路径。\n这种正向思维很容易理解，但是最终它直接得到的是所以可能的路径，但是这道题我们需要求的是N阶的走法，所以我们从正向思维必须要反过来思考，假如我们从一个台阶出发有两种可能，那么我们到达第N个的台阶来看，也有两种可能，第一种是N-1（迈了一步到达），第二种是N-2（迈了两步到达），这样我们就很清楚了，假如我们要想得到到达N阶台阶的走法总数，那么我们只需要把到达N-1和到达N-2的次数加起来就可以了\n这是一个很重要的思想把一个复杂的问题，分解成为其他的子问题，这也是我们完成动态规划的设计的核心思想\n从更好的理解动态规划的优点和源头，我们就从这个简单的例子使用不同的算法来解释为什么要用动态规划\n暴力搜索法 # 我们成功的完成了问题的分解，为了完成计算，但是我们还得计算子问题的结果，上面得到一个很重要的公式F(N) = F(N-1) + F(N-2)\n我们可以很轻松的写出代码（Python）\ndef f(n): return f(n-1) + f(n-2) if n \u0026gt; 2 else n 我们只用一行代码就能将这个问题解决掉，而且效果看起来还不错，我们可以试不同的n都能获取正确的结果，但是n大于30之后，当我在我的电脑上运行起来非常慢，需要几秒钟才能返回结果，而且当n越大，消耗的时间也越多。\n这是为什么呢？我们现在来思考一下这个暴力算法有什么弊端\n暴力搜索的弊端 我们现在假设N=10，那么我们现在就把F(10)转换成为F(9)与F(8)的和，那么F(9)又分成了F(8)和F(7)，而F(8)被分成了F(7)和F(6)\n从这里可以看到，F(7)在第二次分解的时候计算了两次,而每次计算的结果都是一样的，所以我们相当于重复了一遍耗时的操作，知道这个问题，我们就必须改进了，我们可以用一个东西存贮计算结果，这样就不需要重复计算了\n记忆搜索法 # 我们修改我们算法，加一个参数map\ndef map_get(map, n): v = map.get(n-1) if not v: v = f(n, map) map[n] = v return v def f(n, map): if n \u0026lt; 3: return n if n in map: return map[n] return map_get(map, n - 1) + map_get(map, n - 2) 我们添加一个辅助的字典存贮我们中间计算过程，虽然让我们的代码臃肿了不少，但是让我们代码速度有了质的变化\n我们尝试使用运行让N增大到100多都能迅速返回，但是当我们逐渐增大到1000的时候我们会发现Python直接报了超出最大堆栈限制的错误\n堆栈超出最大层数的原因 由于我们使用了递归，递归函数是在递归的时候当前堆栈再次申请一个堆栈待，运行递归函数，为了避免一直无限调用下去耗空堆栈空间（申请也需要一点空间），Python限制了递归层数，由于为了计算超过1000的我们必须至少要递归超过1000次（从1000减一减到小于2），所以我们光荣的被当错程序错乱被误杀了。\n动态规划 # 反观我们这个函数，使用递归，我们很容易理解，但是对于计算机来说，只是为了计算一个数而使用递归是非常不划算的，所以我们要思考这些中间值保留有什么共同点，我们从头开始看，对于第三个来说，它只需要知道第一个和第二个的值就行，而第一个第二个我们知道分别为1和2，对于第四个来说，它只需要知道第三个和第二个，如果我们先把第三个计算下来并保留下来，我们就能知道第四个。\n从头开始思考我们知道，我们只需要保留前面计算的结果就能知道后面的值，我们使用一个列表保存这个中间计算过程，我们将函数改写成下面这个\ndef f(n): if n \u0026lt; 3: return n values = [1, 2] for i in range(2, n): values.append(values[i-2]+values[i-1]) return values[-1] 接下来我们运行这个函数，我们会发现就算N为10000都能迅速返回\n来看看我们动态规划的“损失”，我们使用了一个列表存贮中间过程，使用了空间“换回”了速度\n总结 # 我们使用了一个很简单的题目来介绍动态规划，由于这个问题太过于简单，你或许自己在不知道动态规划的时候都能写出来，但是这个从暴力搜索到记忆搜索最后动态规划的算法优化过程中，我们能够清楚的知道设计动态规范其实也非常简单，将大问题分解成小问题，然后思考小问题的如何细分，最后反过来思考从小问题逆向到最终的大问题，这就是动态规划。\n当然这道问题并不是很经典的动态规划问题，为了让大家更好的理解动态规划，我在下面这篇的博文中介绍若干中经典的动态规划问题\n几个有趣的动态规划\n","date":"2017-12-25","externalUrl":null,"permalink":"/posts/essays/programming/%E4%BB%8E%E9%97%AE%E9%A2%98%E7%90%86%E8%A7%A3%E5%8A%A8%E6%80%81%E8%A7%84%E5%88%92/","section":"博客","summary":"网上关于动态规划的资料，大部分直接给结论，所以一开始我一头雾","title":"从问题理解动态规划","type":"posts"},{"content":" 引言 # 最近重温《TCP/IP协议簇》，读到子网这个部分，概念都能弄懂，但是不明白子网存在的目的，很多资料都说有两个好处，一是能够判断IP存在局域网还是远程网，另外一个将大的网段分成多个小子网。\n这样就搞得我一头雾水，原来我对互联网的认识是从TCP、HTTP高层协议理解的，我原来对互联网信息传递的理解就像这篇回答，网络就像一个神奇的大网，你只需要把电话线插到“网”中就能同别人连起来。原来我对这个解释并没有什么疑问，但是我越来越深入“互联网”，我就我对“互联网”越来越疑惑。\n举个例子，我们知道这个IPV4理论上总共有4294967296（256*256*256*256）个，按照当前的理论，过几年就要分配完了，那意味这至少有50%已经分配好了，也就是说至少有20亿根“网线”要连到一起，我们知道局域网只要交换机就能搭起来，假设我们一个交换机能插20根网线，那么要搭这个20亿“网线”，至少要用一个亿的交换机，你能想象一个亿的交换机堆在一起组成“互联网”吗？就算假如我们使用数字信号，作为一个程序员，我也很难想象原先的程序员是在有限的内存和硬盘，如何设计强大计算机有条不紊的处理20亿的并发。\n所以这篇文章就要从OSI底层协议出发让我们从底部掀一掀“互联网”的老底，将一个有血有肉的“互联网”展现在我们面前。\n触摸“网” # 我们一直在说着互联网、互联网，由于无数在前人的不懈努力下，其实很多时候我们根本感受不到这张网的存在，比如打开浏览器，输入www.baidu.com，我们直接就能连上千里之外的服务器，其实在浏览器后面，我们发的“包”正沿着网跨越千山万水到达一个机房的服务器中。\n所以为了触摸到网这个东西，我们得用一些工具，我们就在打开的百度中输入traceroute，根据你的操作系统安装好，然后我们打开命令行输入traceroute -I www.baidu.com\ntraceroute to www.baidu.com (14.215.177.38), 30 hops max, 60 byte packets 1 192.168.1.1 (192.168.1.1) 2 182.96.180.1 (182.96.180.1) 3 68.248.177.220.broad.nc.jx.dynamic.163data.com.cn (220.177.248.68) 4 53.251.177.220.broad.nc.jx.dynamic.163data.com.cn (220.177.251.53) 5 182.98.159.73 (182.98.159.73) 6 202.97.75.117 (202.97.75.117) 7 113.108.208.194 (113.108.208.194) 8 * * * 9 14.29.121.194 (14.29.121.194) 10 * * * 11 * * * 12 14.215.177.38 (14.215.177.38) 由于百度存在负载均衡，所以你们看到的最终IP地址可能不会同我一样（PS：我去掉了时间），虽然中间还有一些***的存在，但是不管怎么我们终于触摸到这个网的存在。\n我第一次看到这个非常震惊，原来在我的心中，“网”上最多有两个端，一个是我们的客户端，一个是服务器端，现在突然冒充这么多个“中间人”，这些东西是什么呢？\n接下来我们通过回答下面两个问题来慢慢了解互联网的构造。\n为什么第一个IP是局域网内的IP（内网）？ 为什么中间有那么多IP端，他们的作用是什么？ 内网与外网的区别 # 解答第一个问题前，我们必须要知道什么是内网什么是外网。\n从IP的角度上来看，刚开始创建以太网时，由于避免连在一起的电脑认错人，就用IP用来做每个电脑的“身份证”，一开始要连在电脑比较少，组织只需要拿个小本本记住哪台服务器对于的IP，但是随着想连在一起的电脑越来越多，这个小本本满满的一本快写满了，虽然可以在多买几本本子记住他们，但是本子越多每次要查他们的IP的时候消耗的时间越多，所以他们决定不再一个一个分IP了，于是他们把40亿IP分成A、B、C、D几类。\n这样组织成功用一个小本子记录了几十亿的IP分配，这里组织指的是因特网协会（ICANN），协会自己分好IP后开始发本子，找到下面的运营商，让他们自己搭网线光钎把他们自己负责的国家区域连起来，但由于地方太多，一个本子也记不下来，所以他们又把组织发给他们的本子分发到地区运营商，这样慢慢缩小，最终每个地区的局域网的本子都不会太大，这样查起来速度快而且网络的压力也平摊下去，但是摆在我们面前有一个问题\n假设小明和小华分别住在同一条大街的街头和街尾，小明想给小华写信，小明然后写了一封信信放到邮箱，然后邮递员过来把信取走，在邮局分配再让邮递员送到小华家，本来两个人住在同一条街，邮递员只需要把信从街头送到街尾这次传递就结束了，但是由于不知道小明和小华住在同一条街，这封信绕了一个很大的圈才到小华手中。\n从这个故事告诉我们，要解决不必要的传输，我们必须要提供一种机制让“邮递员”知道这封“信”是直接“送”还是发到总部发出去，这种机制就是确定是否是内网还是和外网。\n大家回头看一下IP，假如我们按照组织（ICANN）的本子来分内网还是外网，那会造成极大的浪费，比如说A类，从1.0.0.0 到126.255.255.255，共分了126个，每个分类下有1658万台电脑，可能现在最大的云服务商都没有几千万台机器，假如你就几十台电脑，你申请一个B类IP（子网可以容纳6万多台），那么子网的利用率约为为20/6000，这么大一个网段却只有几个IP有效，这是对IP的极大的浪费，所以我们需要再次切片，将一个IP段智能的切分成很多块。\n有没有什么好的方法能够切分IP呢，我们知道在TCP、HTTP这些高层协议栈中并没有子网这个概念，他们只负责连接和解析，所以我们得从数据链路层里面查看，在这层有一个非常重要的概念：子网掩码\n子网掩码 # 首先我们要了解一个概念：路由表\n这个就是我们前面提到过的“小本本”，这个路由表就记载了我们主机上面有关子网划分的重要数据，我们可以通过在Linux上的route -n命令显示电脑上的路由表\nKernel IP routing table Destination Gateway Genmask Flags Metric Ref Use Iface 0.0.0.0 192.168.1.1 0.0.0.0 UG 0 0 0 wlan0 192.168.1.0 0.0.0.0 255.255.255.0 U 9 0 0 wlan0 在我的电脑的得到的结果是这样的，这里有两个很重要的参数，Gateway（网关），Genmask（掩码），网关和掩码是什么呢？网关就是我们连接上外网的关键，掩码就是区分内网外网的钥匙。\n子网是什么呢？你可以看做是IP与掩码的按位与运算得到新IP，比如说我们上面第一个192.168.1.1与0.0.0.0的运算结果是为0.0.0.0，而且你会发现所以的IP跟0.0.0.0得到结果都是一样的0.0.0.0，这说明对于网关来说，所有的外网IP都是属于同一子网，接下来我们看看第二行，这个网关为0.0.0.0说明这个是局域网，当我们的IP与这个局域网掩码运算后得到的地址与这个局域网IP相同时，说明这个IP属于局域网，我们可以看到我们子网大小为256，由于我用的是路由器，所以说明这个路由器最多可以允许254(.255为广播地址）台设备连接\n接下来我们看看在我的云服务器上的路由表\nKernel IP routing table Destination Gateway Genmask Flags Metric Ref Use Iface 0.0.0.0 10.10.6.1 0.0.0.0 UG 0 0 0 eth0 10.15.6.0 0.0.0.0 255.255.192.0 U 0 0 0 eth0 你可以清楚的看到在第二行我们的内网的大小约为16128=256*(255-192)，如果你感兴趣对子网计算，可以看看我下面引用，在这里我就不解释太多计算细节，你可以看到，通过改变内网掩码，我们可以轻轻松松的将局域网分成不同的大小。\n由于IPV4数量较少，所以在局域网内每个主机并不能都分到单独的独立IP，只有网关需要一个独立IP来访问互联网，在局域网内我们一般使用本地局域网IP（组织特地保留下来不分配给运营商，只在局域网内使用）。所以我们这就能解释第一个问题，traceroute第一个发送的地址不是我们单独IP（路由器IP），而是发送给局域网上的网关。\n包的逐级转发 # 解决上面第一个问题之后，我们知道在一个局域网内我们能够通过子网掩码知道当前局域网的子网范围，接下来的包的“旅途”是什么呢，为什么在traceroute的路径中发现那么多IP地址呢。\n回答这个问题前，我们从物理角度上先了解互联网是怎么搭建起来的。\n当第一台计算机出现的时候，我们不需要互联网，但是随着计算机原来越多，我们想把计算机都用网线连接起来，一开始电脑都放在一起，只需要找一些长长的网线把他们连接起来，但是随着全球各地的人都有个人电脑，这时候只能靠网络运营商，也就是比如电信、移动、联通这些运营商，他们埋光钎搭网线，慢慢的将网络在各地连接起来，最后将子网络连接到骨干网，最后互联网就这么连接起来了\n但是就像送信举的例子一样，如果邮递员能够直接把信从街头送到街尾，那么直接节省了很长一段在路上的花的时间。所以运营商就在各个分支网络搭建大大小小的“局域网”，就一层层代理一样，当一个包请求过来，首先先查看这个IP是否属于当前地区的局域网，如果是就查表找到地址发送过去，如果没有就发送到它的更高一级代理（网关），最终这个IP包到达机房区域的局域网的主机上。\n所以我们能在traceroute查看到一个IP包要传递的那么多的IP地址，那些IP地址都是勤劳的网关，不过相比我们第一个网关，也就是我们在网上冲浪的IP地址，那些网关更像一个一个交通指挥员，指导着我们的发送的“信”一步一步走到指定地点。\n总结 # 互联网就像一个乐高拼成的巨人，刚开始不了解它，远远的观察它，它就像文明遗迹一样让人赞不绝口，等到你慢慢走进它，你就会发现它的组成其实也很普通，也就是一个一个乐高模块组成，但就是这种无数普通搭建我们的“万里长城”，这或许就是互联网的伟大之处。\n引用 # 子网\n","date":"2017-12-14","externalUrl":null,"permalink":"/posts/backend/network/%E6%BC%AB%E8%B0%88%E7%BD%91%E7%BB%9C/","section":"博客","summary":"引言 # 最近重温《TCP/IP协议簇》，读到子网这个部分，概念","title":"从子网到“互联网”","type":"posts"},{"content":"","date":"2017-11-25","externalUrl":null,"permalink":"/tags/git/","section":"Tags","summary":"","title":"Git","type":"tags"},{"content":"","date":"2017-11-25","externalUrl":null,"permalink":"/series/git%E4%BD%BF%E7%94%A8%E5%A4%A7%E5%85%A8/","section":"Series","summary":"","title":"Git使用大全","type":"series"},{"content":"","date":"2017-11-25","externalUrl":null,"permalink":"/series/","section":"Series","summary":"","title":"Series","type":"series"},{"content":" 以前听学长提过Git钩子，但是自己一直没有仔细了解过，记得我还写过一个github更新的Python包，现在想想其实用自带的钩子就能很好的完成\n什么是钩子？ # 我们知道Git是迭代式开发工具，我们的开发流程都是git add、git commit，git push，钩子呢就是你完成每一步Git给你的“回调”，举个例子假如你想让服务器每次上传完新的代码后更新网站，如果你没有钩子，你只能自己ssh登录上服务器，自己更新软件，一次两次还好，多了的话你会骂娘的，所以钩子是给我偷懒的脚手架，我们可以很轻松的写一些脚步帮我们完成一些重复的步骤\n介绍玩钩子的作用，我们来介绍一下钩子的分类\n我们知道Git核心是commit和push两个命令，一个对应客户端，一个对应服务端，所以钩子主要分客户端和服务端，由于Git步骤分的很细，所以每个大分类下面还有很多小分类，比如pre-commit，post-commit这些。\n钩子的全部放在.git/hooks下面，在新建一个项目仓库的时候，Git已经在这个文件夹下给我们生成了很多个.sample后缀的钩子，这些钩子只要把.sample去掉就可以运行了，我们可以在这些sample上面修改完成我们自己的钩子\n客户端钩子 # 客户端钩子很好理解，你commit之后想做其他事，比如说编译一下程序啥的，这里我就不多讲，主要由下面几个钩子组成\npre-commit 提交之前 post-commit 提交之后 pre-rebase 变基之前 post-rewrite 替换提交记录之后 pre-push 推之前 详细的可以看官网链接钩子\n客户端钩子我觉得一般没有太多作用，因为我在提交之前就会运行脚步进行开发调试什么的，我把介绍重点放在服务端钩子\n服务端钩子 # 服务端钩子就是你push之后的事情服务器要运行的脚步，有用推的步骤只有一个，所以钩子只有四个\npre-receive 接受之前 update 更新之前 post-update 更新之后 post-receive 接受之后 服务器接收到客户端请求时，pre-receive先进行调用，如果返回值为非0就会拒绝推送，所以我们写钩子的时候一定要记住最后要返回0才能正常接收更新，update主要处理多分支推送，有的时候你一次更新，推三四个分支到服务器，pre-receive只会调用一次，update会对每个的分支调用一次，后面两个都很容易理解\n一般我们就是要在服务端更新代码之后运行脚步，所以我们要修改的就是post-update或者post-receive\nbash脚步大家都会写，但是大家可能会很陌生什么是Git服务端，接下来我们就来介绍一下Git服务端是什么\nGit 服务端 # 大家一般使用Git都是使用的客户端，但是Git这个工具的确很强，它不但可以当做客户端，也可以当做服务端，为了让大家更好的理解Git服务端，我们先来拿本地文件做”服务器“\n首先我们先新建一个文件夹为server，在新建一个文件夹为local，假设文件夹都在/root文件夹下\n我们执行下面的命令生成服务器\ncd /root/server git init --bare 只需要在init后面添加一个--bare选项告诉Git，Git就会帮我们生成一个空的“服务端”，我们可以查看一下文件，我们发现Git 给我们生成下面几个文件夹，其中就有我们的hooks\nbranches config description HEAD hooks info objects refs 但是服务端和客户端生成的位置不一样，客户端是给我们生成一个.git文件夹，里面放了这些文件夹，然而服务端直接将这些文件夹放在主目录了\n行我们已经生成了服务端的，接下来我们生成客户端的钩子\ncd /root/local git init 很简单，同我们往常操作一样，我们这时候添加一个README.md 然后commit一下准备开始往服务端推代码了\n在 linux 下直接执行下面命令就行\necho \u0026ldquo;local update\u0026rdquo; \u0026raquo; README.md git add README.md git commit -m \u0026ldquo;Add ReadME\u0026rdquo;\n接下来我们就要向”服务器“提交代码了，我们先添加本地文件作为远程服务器\ngit remote add origin file:////root/server 然后直接推代码\ngit push origin master 这样我们就向我们文件提交了代码，这时候我们回到我们”服务器“\ncd /root/server ls branches config description HEAD hooks info objects refs 我们惊奇的发现服务器并没有我们新建的README.md文件，原来Git服务端并不像SVN一样只保留一份代码大家共同修改，Git服务端只是记录文件变化和分支变化\n这里插一句我为什么会去了解Git钩子，由于一开始实现服务器自动更新我的FastProxyScan项目代码，但是我又不想使用Github钩子（push后发送http请求），太麻烦了，后来我一想干脆直接推到我的服务器上，但是推到服务器上的代码只是记录了分支和提交信息，不包含源文件，所以我只好在在服务器上部署这个项目，并添加一个服务器钩子，当服务器更新完成后，再用钩子把服务器上的项目代码更新\n如何写服务器钩子 # 通过上面对本地文件新建仓库，我们知道Git“服务端”新建很简单，我们一般接触比较多的是Github服务端，但是Git非常强大，他可以支持多种协议来连接“服务端”，比如说我们上面用到的本地文件（file协议），假如你用ssh连接远程服务器，你也可以使用类似git remote add origin ssh://username@ip/file/path添加ssh远程仓库\ngit 支持的协议有ssh、http、https、file、git等协议，你只要确保你能连接上远程服务器就行，接下来我们谈谈如何写服务器钩子\n在使用git init --bare新建了一个Git服务端之后，在服务端文件下面有一个hooks文件夹，我们要做的就是把脚本放到hooks文件夹里面（当然你要确保它有执行权限），如果你更擅长写Python，Ruby那些脚步也可以，不过要确保前缀后后缀正确。\n这里要提到很重要的一点，由于在执行钩子的时候，环境变量GIT_DIR被设置为服务端当前目录，如果你像我一样想更新在另外一个文件夹下面的项目代码，你必须使用uset GIT_DIR清除变量名，否则只会更新服务端，而不会更新你的项目代码\n这里我提供一个模板\n文件名为 post-update或者post-receive\n#!/bin/sh cd /project/path/ || exit unset GIT_DIR git pull origin master exec git-update-server-info 你只需修改项目文件路径和仓库名即可\n总结 # 通过这个Git钩子了解了Git服务端，也让自己更加了解Git这个软件，以前一直懵懵懂懂，只会向Github提交文件，一直以为Git只是一个版本记录工具而且，现在看来神器之名不是浪得虚名，通过一个小小的钩子，摇身一变成部署神器。\n","date":"2017-11-25","externalUrl":null,"permalink":"/posts/backend/software/git/%E5%B7%A7%E7%94%A8git%E9%92%A9%E5%AD%90/","section":"博客","summary":"以前听学长提过Git钩子，但是自己一直没有仔细了解过，记得我","title":"巧用Git钩子","type":"posts"},{"content":" mrzhangboss/FastProxyScan fast proxy scan project Python 8 2 为了给我的站点增加人气，我把这个项目的介绍放到我的博客，如果你觉得这个项目还不错的话，请不要吝啬你的star\ngithub传送门 Demo传送门\n引子 # 一开始自己只想做一个代理池，于是搜了搜Github发现类似的项目，大多数都是爬取网上的一些代理商的免费代理，这部分代理大多都是没有用的，可用性非常低，于是我自己就干脆做一个“代理商”，自己扫描主机把可用的代理扫描出来。\n但是现在网络主机实在太多了，至少几百万台，所以这个项目的核心就是快速扫描，在最短的时间内检测更多的代理，目前项目的速度最好只能完成1000代理每小时的速度（日扫描两万代理），希望能继续优化代码，加快速度，如果你对这个项目感兴趣可以Fork下来，欢迎各位的Pull Request\n项目依赖 # 项目基于Python3.5+开发\n软件依赖\nnmap 项目结构 # 项目主要由三个部分组成\n主机扫描 端口检测 代理检查 项目结构为\nproxy_pool scanner display database 现在依次介绍在搜索速度上的优化\n主机搜索 全球的IP都是有ISP统一分配的，ISP主要由下面几大洲分配，我们中国处于亚太区，所以我们的IP由亚太互联网络信息中心（APNIC）分配IP，目前中国分配的IPV4总数为3亿左右，这个数量还是比较大\n我们可以从 http://ftp.apnic.net/apnic/stats/apnic/delegated-apnic-latest下载最新分配的IP地址\n但是代理服务器只存在特定的服务器上，所以现在版本还没有发布V1.0主要是因为搜索的效果不是很好（搜索的主机代理转换率太低），目前还在想其他方法，等有更好的解决方法就会发布V1.0\n端口检测 使用nmap的“TCP SYN scan”最大化加快端口检测速度（需要root权限）\n代理检测 使用nmap先验端口与Python异步最大化代理检测速率\n安装教程 # 项目采用Django做后台管理，所以只需要一点Django基础知识就能在这个项目上做二次开发，如果你只想获取最新的可用代理，可以通过http://115.159.146.115 调用API接口获取最新可用代理（我的站点带宽有限，所以只开放最新100个代理，并且只是20分钟更新一次）\n环境安装\nnmap 安装 python3.5+ 安装 运行：\ngit clone https://github.com/mrzhangboss/FastProxyScan.git cd FastProxyScan python3.5 install -r requirement.txt 主机检测\ncd pool/proxy_pool sudo python3.5 manage.py scan \u0026ndash;vps\n端口检测\nsudo python3.5 manage.py scan \u0026ndash;proxy -m 100\nm是并行参数，值越大速度越快\n代理检测\nsudo python3.5 manage.py check \u0026ndash;start -c http://115.159.146.115/ip\nc是检测ip头网址，可以使用我提供的 http://115.159.146.115/ip 返回请求头，可以参考我的上一篇博文 代理的前世今生\n在我搭建的DEMO站点上，我使用supervisor让这三个程序循环运行，你可以使用crontab定时调度也可以像我一样。\n数据库当前采用的Sqlite3，但是数据库模型全部使用ORM开发，你可以很方便的修改settings.py来放入其他数据库，如果你懂一点Django的话\n总结 # 如果你像了解更多开发这个项目背后的知识的话，可以看看我上一篇博文 代理的前世今生\n","date":"2017-11-23","externalUrl":null,"permalink":"/opensource/fastproxyscan-introduction/","section":"开源","summary":"mrzhangboss/FastProxyScan fast proxy scan project Python 8 2 为了给我的站点增加人气，我把这个项目的介绍放","title":"FastProxyScan项目介绍","type":"opensource"},{"content":"","date":"2017-11-23","externalUrl":null,"permalink":"/categories/opensource/","section":"Categories","summary":"","title":"OpenSource","type":"categories"},{"content":" 引言 # 自己对代理认识不深，也只是会使用而已，由于最近想做一个代理池，于是查了很多资料，发现代理这个东西还是非常有趣的\n代理是什么？ # 从编程上来看，requests只需要在请求里面加上proxies参数例如requests.get('http://www.baidu.com', proxies={'http': '127.0.0.1:3128'})，我们就能连上代理进行访问，由于requests包装了太多细节，我们无法知道用了代理和没有用代理的区别\n接下来我们来看一下Python3的内库是如何使用代理的\n首先要申请一个ProxyHandler\nfrom urllib import request proxy = request.ProxyHandler({\u0026lsquo;http\u0026rsquo;: \u0026lsquo;127.0.0.1:3128\u0026rsquo;})\n然后我们通过这个proxy创建一个opener，然后用opener打开页面，最后输出结果。\nopener = request.build_opener(proxy) resp = opener.open('http://www.baidu.com') print(resp.read()) 然后我们看一下如果没有使用代理的请求是什么\nresp = request.urlopen('http://www.baidu.com') print(resp.read()) 你可以很清楚的看到，如果我们不需要代理，之前打开url就行，也就是说我们的请求，其实全部发给代理，交给代理了\n这里就稍稍谈点感想了，以前经常在书上看到人歌颂互联网的伟大，但是自己一直不明白这个伟大在哪，以前一直从表象感受互联网，有了互联网，不用打开电视就可以看影片，不用去图书馆就可以看书，感觉互联网神奇的地方就是给自己带来方便，但是没有去互联网本身的架构的伟大，通过一根根网线交换器，无数主机“连”在了一起，构成了宏大的互联网，或许你通过浏览器打开的网站主机离你几百公里，但是你不要做飞机轮船，你直接在家里就能通过层层代理传递你的请求将千里之外的“敌将首级” 探入囊中。\n互联网神奇的地方就是看起来各个部分非常分散，但是他们却能通过一根一根线紧密的联系起来，只有有“距离”的时候你才能感受到他的美丽\n以前刚学习网站的时候，在本地调试的时候，你在本地跑一个web，直接打开浏览器访问，这时候我就有一种错觉，web就是两端，客户端和服务器端，在本地调试可以这样理解，但是一旦拿到互联网，这就是不完整的，完整的应该是客户端-代理端-服务器端，当然我们的代理端有时候可能是网关、路由器、交换机等等。\n了解这些有什么用呢，因为我自己以前一直对代理没有什么很深理解，用爬虫的时候使用代理就不会被封IP，感觉代理是很BUG的东西，但其实代理很普通，而且无处不在，我们要想真正理解代理，就必须把它拖下圣坛。\n代理其实很普通 # 前面我们知道代理其实很普通，但是要深入了解代理必须要先了解IP，IP是什么呢，IP就是互联网的身份证，要想在互联网上“混”，必须要有“身份”\n那为什么我们要用代理呢，比如说假如你是未满18岁的小朋友，你想要买上网，你必须要借一张大人的身份证去上网，这个时候代理的作用就是帮用它的身份证帮你干事。\n我觉得中国文化博大精深，其实一听代理这个词，我们就能知道代理是干什么用的。其实把代理吹得神乎其神没什么用，那些作用都是它的他自己瞎几把搞的，从我们客户端来看，代理就是服务端，了解这点非常重要，因为他能让我们把所以的事情都简化，而且从客户端来看，代理就是这样的\n客户端、代理、 服务器三者之间的关系 # 前面我们已经谈了客户端和代理之间的关系，对于客户端来说，代理就是服务器端，我们啥都不管，把请求发给代理，相信它就是我们请求的服务器\n对于代理来说，其实它自己最清楚，自己就是个代理，它必须要把请求转发给服务器，然后在把服务器的响应发给客户端，代理就是一个中介人，有的时候我们也可以把它看做一个双向中继，把请求传递一下，再传回来，所以在这三者之间，只有代理是个明白人，它必须清楚这次任务所以细节，所以有时候虽然说代理是安全的，但其实它也不安全，只要把代理攻克了就能了解到底是哪个家伙干的坏事，所以网上干坏事的人，一般都用很多个代理，层层代理，就算你攻克了一个，也找不到坏人\n对于服务器来说，代理就是客户端，它只负责响应就行，对于代理和客户端来说都是一样的策略。但是时候很奇怪，服务器为什么知道你是个代理，原来全是代理自己的锅，我们细谈一下代理的分类\n代理的分类 # 代理也分很多种，有的时候代理也不老实，把客户信息暴露了，这个时候我们就说它是小透明（透明代理），有的时候它不告诉你客户信息，但是告诉服务器我是个代理，我们就说它是匿名代理，但是有时候它连它自己是啥都不告诉你，它伪装成它是客户端，这个时候我们称它为高匿代理，所以这些代理根据暴露信息的不同可以分为这三种\n透明代理 匿名代理 高匿代理 当然我们最喜欢高匿代理，你可以把它当做你的分身，除了身份证不一样，两个人长得一模一样。\n所以我们判断一个代理的类别，必须要检测它向服务器发的报文，所以在我项目FastProxyScan，我搭建了一个服务器，返回客户端向服务器请求头，主要是HTTP_X_FORWARDED_FOR和HTTP_VIA头来分别暴露客户端信息和代理端信息，所以我们只要请求头检测有没有这两个字段就可以完成检测，原理非常简单\n在这里我介绍用Nginx高效返回检测信息\nlocation ~ ^/ip { default_type application/json; return 200 '{\u0026quot;REMOTE_ADDR\u0026quot;:\u0026quot;$remote_addr\u0026quot;,\u0026quot;HTTP_VIA\u0026quot;:\u0026quot;$http_via\u0026quot;, \u0026quot;HTTP_X_FORWARDED_FOR\u0026quot;: \u0026quot;$http_x_forwarded_for\u0026quot;}'; } 在Nginx配置里面加上这个端口，我们只有请求/ip，就能直接从Nginx返回请求头信息，速度贼快\n总结 # 当然网上还有很多对代理的分类，缓存代理，正向代理，反向代理，但是这些都是代理自己的额外功能，我们前面介绍的代理都是傻呼呼，客户端要什么，它就做什么，这些如缓存代理高级的代理就是很聪明，它的目的就是最快返回客户端需求，比如说虽然说这个傻客户端傻乎乎一个请求请求了几十遍还没记住，代理自己拿个小本子记好，你下次来，正好对上号，直接抄给你，不用再跑几千里去拿了。但是其实本质上它还是逃不出上面的分类，只不过它有的自己的不同罢了。\n","date":"2017-11-20","externalUrl":null,"permalink":"/posts/backend/network/%E4%BB%A3%E7%90%86%E7%9A%84%E5%89%8D%E4%B8%96%E4%BB%8A%E7%94%9F/","section":"博客","summary":"引言 # 自己对代理认识不深，也只是会使用而已，由于最近想做一个","title":"代理的前世今生","type":"posts"},{"content":" 0x00 引子 # 排序是很多算法的基础，简简单单的排序前人就归纳出很多种算法，但是这些算法多多少少都有着相同的原理\n排序算法有很多，这里我们就简单的谈谈下面7种排序的特点\n冒泡排序 选择排序 插入排序 希尔排序 堆排序 归并排序 快速排序 0x01 Summary # 从算法的抽象程度上来看，冒泡、选择和插入是比较好理解，我们能用我们生活中的常见事物来理解，后面四种比较抽象，而且相对于前三种平均时间复杂度O（n） = n ^ 2 的来说，后面四种的平均复杂度都比前面的小，尤其是面对大量数据排序来说，后面四种能比前三种跑的更快\n0x02 冒泡、选择和插入的特点 # 这三种算法空间复杂度都为O（n） = 1，也就是说在给定一个列表的前提下，无论列表数有多大，额外的排序所需的空间都为常量。但是这三种算法的平均时间复杂度为O（n） = n ^ 2， 也就是说在给定一个长度为n的数组，必须要经历 k × n × n 次操作才能排序，当我们的n比较小的时候，我们无法察觉这个算法与更高效的算法的差别，当n很大的时候，比如一个亿，这时候的要进行的操作就瞬间爆炸了。\n这三种算法很大程度上是牺牲了运行时间换取运行空间，我们可以从桶排序上面得到相反的例子，桶排序的时间复杂度为O（n） = 1，空间复杂度为O（n） = n，也就是说在排序上面他的速度是最快的，但是它所花费的空间也是巨大的，有时候时间空间就是两个双刃剑，你如果想节省空间必须浪费时间，你如果想节省时间必须浪费空间\n这三种算法原理很简单，而且有一个相同的地方，就是他们每一节排序就会“删掉”一个数字，接下来就是对剩下的排序。当然我这里的删掉就是代表已经排好，然而接下来的过程中不会再涉及到这个数字\n这个非常好理解，随便给我们一副牌让一个小朋友把他排出来，小朋友一般就是先找出最大的牌放到最前面，然后在剩下里面找到最大的，依次排下去，最后手里就剩一张牌了，这个牌组就排好了\n这三种算法都是基于这个核心，但是具体的算法细节不同。冒泡排序就是先从头到尾依次把最大的交换到最后面；插入排序的话就是我们从第一个数字开始从后面把小的数字插入到前面去；选择的话同冒泡有点相似，不过它并不会把数字传递过去，它直接将未排序的最大值与未排序的末尾值交换。\n这三种排序我们都非常好理解，但是他们有一个缺点，就是未排序前必须遍历全部数组，我们都知道现在大数据时代，对于上亿数据执行一次遍历就已经非常耗时间了，为了排一个数字要几乎就遍历一遍（排到后期遍历的越来越少），所以这三种算法在面对巨量数据的时候，花在遍历上面的时间比排序时间要更多。\n0x03 希尔和插入排序 # 希尔排序是插入排序的更高效改进方法，说到改进我们就要谈谈插入排序的优缺点\n优点 我们给定 [ 1, 3, 2, 5, 8 ] 数组，这个数组基本上已经排好序了，如果使用插入排序，我们只要在插入2的时候，将2和3交换就可以，设我们挪动的距离就为1\n我们在看这个数组 [ 1, 3, 5, 8, 2 ] ，我们可以看到这里如果使用插入排序，我们会在插入2的时候，要将2依次与8、5、3交换，这样移动的距离就为3\n希尔排序改进的地方就是步长，如果它的步长选择的好，它的排序效果越好。这个步长是什么呢，插入排序的步长一直为1，也就是每次遍历的时候步子迈一步，假如步长为2，也就是迈两步。在[ 1, 3, 5, 8, 2 ] 数组中，比如说我们数字2，它在步长为1的时候，它下一步要比较的是8，假如步长为3，那它下一步就直接与3比较了。\n所以希尔排序改进就在于他能直接移动多位，在上面的例子里面，步长为3，我们能直接将数组从3的位置移动到2，如果直接使用插入排序，必须移动3次才能达到希尔排序的效果。\n希尔排序原理同插入是一样的，不同在于，插入的步长希尔是可变的，这样就为一些“调皮”的数字的移动加快了速度，一步一步的移动他们太累了，直接把步子迈大，一步到位。\n","date":"2017-11-12","externalUrl":null,"permalink":"/posts/essays/programming/%E6%BC%AB%E8%B0%88%E6%8E%92%E5%BA%8F%E7%AE%97%E6%B3%95/","section":"博客","summary":"0x00 引子 # 排序是很多算法的基础，简简单单的排序前人就归纳出很多","title":"漫谈排序算法","type":"posts"},{"content":" 引言 # 编程开发有时候也像雕刻一件艺术品\n以前一直有一种错觉，觉得编程开发就是会用库会用框架，这阶段的感悟只是停留在库的使用上面，然而当你持续工作在一件产品上的时候，你就把思维聚焦在产品，这时候你的感悟就会是架构的搭建，库只会变成你的工具\n所以慢慢明白编程届的前辈们一直劝我们在大学不要为了钱选择做一些外包项目，外包项目这种东西就像一次性编程产物，你写完之后就再也不会code review了，而对于编程来说，编程就是在写BUG，只不过对于大神来说写的少，对新手来说就是写的多\n诚然大神们也是从一个一个BUG中慢慢走过来的，然而对于我们菜鸡来说，很多时候我们并不能发现自己的BUG，所以让自己成长最快的方法就是立马纠正自己的BUG\n当然这里我们所说的BUG有的时候并不是我们经常说的系统无法运行的BUG，我们这里说的BUG可以算成缺陷，有时候是在特殊情况下才触发。比如说运行时，这时候我们称它未漏洞；重构时，我们称它为SHIT代码；测试时，我们称它为过耦合。\n这里我也不想过多谈技巧如何去发现这些BUG，以为技巧是死的人是活的，我们不需要太多技巧去避免或者去查找这些BUG，还有重要的一点在于BUG是无法避免的，我们要关注的是产品本身。有幸在实习几个月的时间一直专注于一个产品的开发，期间一直经历了大大小小的重构，随着产品的成型，自己也慢慢感悟到一些方法加速查找系统BUG和如何快速开发。\n接下来就介绍一些我自己的浅显感悟\n迭代开发 + CODE REVIEW # 如何从零开始搭建一个产品，除非你是超级大牛，几个小时就能搞定一个完整的代码的开发流程，普通人都是一步一步来迭代开发，但是这个迭代开发也有讲究，有些人喜欢从头写到尾，然后看看能不能跑起来，再疯狂DEBUG，也有些人喜欢先写局部，慢慢测试，最后把所以组件都串联起来。\n这两种方式萝卜青菜各有所爱，第一种速度最快，但是不适合团队合作和CODE REVIEW，第二种速度慢，但是灵活可靠，容错率更高，对于新手来说，选择第二种能让自己的错误不会对系统造成系统性崩塌，而且可以慢慢发现自己的BUG，从而从BUG中提高自己\n对于迭代式开发我们就不得不提一下git，作为一个版本控制工具，它在迭代开发的作用堪称神器。然而这个神器我却一直没有找到正确的打开方式，只是把它当做上传服务器的工具，最近才开始慢慢掌握一点小小的技巧。\n我原来的git的工作流程用命令概况下来就是\ngit add -A git commit git push 这三条，然而我大部分时间都只是发挥了git push的功能，纯粹把它当成代码的备份，然而git的核心在于git add和git commit这两个命令上，这里要检讨一下我以前的做法，以前一般完成一个组件的功能就直接快速git add -A然后git commit，虽然我是遵循迭代开发，但是我很少去REVIEW自己这次提交的commit\n所以最主要的问题在于如何在快速迭代开发的时候慢下来，好好思考和REVIEW一下自己这次提交的代码，所以在这里不得不介绍git diff这个命令了，对于在每个新修改的文件来说，在你执行git add之前，你最好git diff一下这个新提交的文件，git会把你所做的修改和原始代码做一个对比。\n有的时候我们并不能记得原始代码是什么，我们到底对代码做了什么改变，幸亏我们有这个神器，只需要git diff一下，我们所做的修改和原始版本的差异就会显示出来，REVIEW代码的过程也就是我们发现错误的过程\n我们要想提高自己的就要不断的改变修正，所以正确的git的工作流程应该是这样\ngit diff git add git commit git push 当然每个commit可以由很多个git diff + git add 组成，但是我们必须要保证自己对git add的每一个文件都要REVIEW一遍，而且我们在每次git add之前，要思考这次的改变是否能够改进，是否必要等\n产品开发就像爬山，你不可能一步登山，所以我们要做的就是，在每次停下来的时候确定方向，修正自己，甚至回头\n拥抱变化 + 快速开发 # 前面我们谈了在每一个commit的时候我们都得慎重再慎重，小心又小心，但是这种思想有时候如果把它带入开发过程中则会让你寸步难行，具体是什么呢，我来根据我自己经历来介绍\n我自己是一个有一点叫做代码“洁癖”的人，由于看了不少编程理论的书，我容忍不了自己写出很SHIT的代码，面对新东西，我一般喜欢研究个透再下手，我要确保我的设计是万无一失的，所以这就造成如果我接触一个新的库或者新的功能我会花上很长时间在上面，而且由于我自己思考原来越深，我可能会把原来简单的问题搞得越来越复杂，等到我觉得开始CODING完的时候，我发现自己把一个超简单的问题搞得那么复杂，牺牲了太多时间却适得其反\n造成这种原因主要是吸收太多而没有消化，我看过很多编程理论的书，技术大牛用他们的开发经验告诉我们要模块话开发，要注意设计模式，要考虑系统灵活性耦合性，这种大牛经验是很宝贵，但是这些经验就好像最高的武功秘籍，假如你没有相应的基础，贸然去练的话你会走火入魔，对于新东西新功能，我们就要想爬山者先驱一样，我们不是要找一条最锻炼自己的路去走，而是找一条最简单的路，只有爬到山顶我们才需要考虑其他问题\n这种快速开发的思想还有很重要的一点就是“拥抱变化”，我们在快速开发的过程中无法避免由于快速开发造成的部分SHIT代码，当你写出这部分的时候，其实对于菜鸟来说，这部分代码才是你最宝贵的代码，因为它暴露了你的缺点，要想提高自己，必须发现自己的缺点，所以快速开发的过程中不但激发自己的潜能，而且让自己对自己的缺点有了更好的了解，了解了自己缺点，才能慢慢改进，所以快速开发第一能够节省时间，第二个就是能缓慢提高自己，当然前提是去修正它，所以快速开发你必须要把自己的代码”洁癖“和速度结合起来，抱着一颗永不满足的心去不断锤炼自己的”代码“\n总结 # 在我看来编程开发就像是打太极，一方面我们得快，以目标为驱动，快速开发；一方面我们得慢，以变化为驱动，迭代开发\n看似快慢是两个极端，其实两者相得益彰，快促进慢，慢促进快，两者相互促进\n当然这只是我的自己小小感悟，编程开发博大精深，这些只是技巧，关键在于自己，思考并转换才是最核心的\n","date":"2017-10-28","externalUrl":null,"permalink":"/posts/essays/life/%E7%BC%96%E7%A8%8B%E5%B0%8F%E7%BB%93/","section":"博客","summary":"引言 # 编程开发有时候也像雕刻一件艺术品 以前一直有一种错觉，觉","title":"编程小结","type":"posts"},{"content":" 曾经有人问过我一个问题什么是TCP复用，我当时没有回答上来，后面我又遇到一些并发性能问题的时候，我才开始慢慢明白为什么会有这个问题，以及这个问题背后的秘密\n其实当时应该他想考我的是爬虫的请求优化，准确来说是HTTP持久连接（HTTP persistent connection），并不是TCP复用，这才导致我当时查阅很多资料，并没有发现TCP复用能优化客户端，因为TCP复用是服务端的事，现在就让我从源头开始慢慢解读这个问题\n起因 # 我们知道我们每次发的HTTP请求在底层都是一个套接字的通信，我们可以从底层开始做一个测试\n我们使用个for循环，申请1024个socket\nimport socket l = [ ] for i in range(1024): sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM) sock.bind(('www.baidu.com'),80)) print(i) l.append(sock) 这个过程有点慢，但是你会发现在申请到1000左右的时候，会直接报Open too many file 这个错误，但是我们并没有打开文件，为什么会报这个错误\n原来在Unix系统下，我们申请的套接字也就是socket在底层是以文件的形式存在的，客户端通过申请一个socket来写入和接受服务端的请求，这是一个非常重要的概念，对我们后面解析库函数有很大的帮助。\n由于系统资源有限而且打开很多文件系统响应会变慢，所以Unix系统或者Windows都对单个进程申请套接字有限制，在Unix系统下我们可以通过ulimit -n查看这个值，在笔者的Ubuntu上这个值为1024，基本没有修改过都是这个值，我们可以通过我们可以在命令行执行ulimit -HSn 4096临时增加至4096,\n所以我们一般来说单台机器单个进程最多只能并发1024个请求，在不修改配置的情况下这个值是固定的，所以我们提高并发数只有两种方法\n修改系统配置 使用多进程 在写这篇文章之前，我一直以为HTTP复用能在作用在并发上提高爬虫性能，但是其实并不是，它能提高性能但是却不是在并发上提高，接下来我们仔细介绍HTTP复用是怎么提高爬虫性能的\nHTTP复用 # 说道HTTP复用，我们不得不介绍一下HTTP和TCP协议，我们都知道Internet是由OSI七层协议构成的，但是OSI只是规定了框架，具体协议我们是通过TCP/IP来实现的\n我们先来说说这个TCP，我们都说互联网能够发展到现在这么稳定可靠多亏了这个TCP可靠协议，但是这个可靠是要付出代价的，建立一次连接的过程要经过三次握手，断开的过程也得四次分手，而且这个连接的过程完全不涉及我们要请求的内容，我们知道爬虫一般请求一个站点只有通过一两次请求就行，如果每次请求都得握三次手，还得分四次手，这样的代价也太大了\n所以HTTP的复用优化的方向就是减少TCP的连接，谈到如何减少TCP连接，我们就得说说HTTP长连接（HTTP persistent connection）\nHTTP长连接 # 在HTTP1.1规定了默认都是长连接，TCP不断开，并且在请求头添加一个Connection的header，如果是值为keep-alive则保留TCP连接，假如为Close请求完成之后就会关闭，在HTTP1.0的下默认为关闭状态\n怎么来理解这个长连接呢，我们都听说过HTTP是无状态的这句话，从HTTP协议上来看，服务器客户端就是一个“Request”，“Response”组成，无论多复杂的页面都是由一个个“Request”组成\n为了更好的理解上面的话，我们回到那个套接字，我们把HTTP请求比作打电话，对于每个电话，我们只需要先拨号，然后滴滴滴三下后确定我们同对面连上了（服务器“协商”好），然后我们把我们要说的话通过话筒传给对方，等我们说完之后，由于信号差，对面听完还要想怎么回，然后我们安安静静的在听筒那等，等他想好说什么，在慢慢的说给我们听。\n在HTTP1.0的时代，我们每次拨完一次好，说完一句话，听完对面的回应后，我们就会挂断电话，如果我们还想说就得再重复这个过程，在HTTP1.1下我们增加了长连接这个概念，就是如果你想这个电话里多聊几句，那么就在最后加上“你等下不要挂了，我还要说”（在header加上“Connection: Keep-alive”），那么对方就不会挂断电话，等它说完之后也想你一样在听筒那而等着，这样我们就省掉了一次拨号的时间\n我们现在了解为什么HTTP复用能够节省爬虫的性能了，接下来我们就从编程语言对HTTP复用的实现上了解如何实现HTTP复用\n存贮单元\u0026mdash;ConnectionPool # 在介绍ConnectionPool之前我们先简单介绍一下HTTP复用的具体表现\nTCP与URL的关系 # 我们知道HTTP复用的是TCP的连接，而TCP连接由四个部分组成\n本地ip 本地port 服务器ip 服务器port 简单来说就是两个二元组（local_ip, local_port), (server_ip, server_port)\n但是我们发一次的请求是一般是通过URL，也就是类似“http://www.baidu.com”，这样的url来请求的，这个同我们TCP有什么关系呢？\n首先介绍一下“http”代表通信协议，这里使用的是HTTP协议，“://”后面的就是请求的域名，域名后面如果有冒号就是我们请求的端口号这里没有，根据HTTP协议这里默认是80端口（HTTPS是443），域名后面的就是请求路径，这里也没有就默认问“/”，也就是我们通过这个“url”就知道我们这次请求的具体位置了，现在我们找到了端口，但是请求的IP在哪呢？\n这里就要介绍一下DNS了，我们为了让我们的站点更好记，我们使用域名代替ip地址，通过在DNS服务上注册我们域名，以及绑定我们域名对应的IP地址，我们就能让计算机通过域名来转换成IP地址，这里就不详细介绍了\n所以呢我们现在了解了，一个TCP连接只是涉及到URL的域名和端口号，我们请求站点的时候主要是通过不同的路径来获取内容，所以我们可以很清楚的知道，只要我们URL的域名和端口一样，那么我们所以的URL都能共用这个TCP接口\nConnectionPool的实现 # 简单来说为了实现HTTP复用，我们只需要保存TCP连接就行了，但是通过前面我们知道，我们保留的TCP连接必须和你要请求的url要域名端口一样，有时候一个站点的服务可能由多个域名多个端口组成，所以原本我们只要用一个变量保留上一次请求的TCP连接，为了程序更加健壮，我们需要一个TCP连接池，存贮不同的TCP连接。\n每次新的URL来的时候我们就是先从TCP连接池中查看有没有相同的域名和端口，如果有就用它发请求，如果没有就新建一个TCP连接，这就是TCP连接的基本原理，当然还要一点编程的时候要注意，我们从池子里面取出一个用完必须放回，否则池子用完了又得新建，那就完全丢掉了复用这个概念了\nHTTP复用在Requests的具体表现 # 前面介绍了一大堆概念，但是从头到尾如果让我们自己来做一个实在太难了，幸好我们有Requests这个库，它的Session对象在文档介绍了它就维护了一个TCP连接池并且能够复用TCP连接\n接下来我们就从代码入手来更好的理解这个进程池的高级用法，我们为了更好看到每一次请求底层的操作，我们这里自己先自己搭建一个本地服务器，我们使用Flask来搭建一个本地服务器 新建一个web.py文件，在运行\nfrom flask import Flask, request from werkzeug.serving import WSGIRequestHandler app = Flask(__name__) WSGIRequestHandler.protocol_version = \u0026quot;HTTP/1.1\u0026quot; @app.route('/') def hello_world(): return '%s %s' % (request.remote_addr, request.environ.get('REMOTE_PORT')) if __name__ == '__main__': app.run(host='0.0.0.0', port=8000) 这里我们在8000端口开了一个服务器并且设置为HTTP/1.1协议，我们返回用户请求的ip和端口\n接下来我们开一个Python解释器来看看这个进程池的用法\n\u0026gt;\u0026gt;\u0026gt; import requests, logging \u0026gt;\u0026gt;\u0026gt; logging.basicConfig(level=logging.DEBUG) \u0026gt;\u0026gt;\u0026gt; session = requests.Session() \u0026gt;\u0026gt;\u0026gt; session.get('https://baidu.com') DEBUG:requests.packages.urllib3.connectionpool:Starting new HTTP connection (1): www.baidu.com DEBUG:requests.packages.urllib3.connectionpool:http://www.baidu.com:80 \u0026quot;GET / HTTP/1.1\u0026quot; 200 None \u0026lt;Response [200]\u0026gt; 看我们可以从打印的logging日志看到我们在进程池中新建了一个TCP连接，我们在试着再请求一次\n\u0026gt;\u0026gt;\u0026gt; session.get('https://www.baidu.com') DEBUG:requests.packages.urllib3.connectionpool:https://www.baidu.com:443 \u0026quot;GET / HTTP/1.1\u0026quot; 200 None \u0026lt;Response [200]\u0026gt; 看我们的HTTP复用实现了，在同一个TCP连接中我们请求了两次\n深入requests的ConnectionPool # 在上面我们验证了requests的Session对象的确实现连接池，但是似乎requests并没有给我们接口来操作这个值，通过分析代码和资料，我们发现在Session初始化的时候，绑定了一个\tHTTPAdapter对象，这个对象就是requests封装了urllib3.connectionpool.ConnectionPool来实现TCP池\n我们查看这个HTTPAdapter文档发现它的用法是这个\n\u0026gt;\u0026gt;\u0026gt; import requests \u0026gt;\u0026gt;\u0026gt; s = requests.Session() \u0026gt;\u0026gt;\u0026gt; a = requests.adapters.HTTPAdapter(max_retries=3) \u0026gt;\u0026gt;\u0026gt; s.mount('http://', a) 我们可以通过创建将一个TCP池绑定到一个session对象上，我们可以看一下这个创建一个HTTPAdapter的参数\nHTTPAdapter(self, pool_connections=10, pool_maxsize=10, max_retries=0, pool_block=False) 我们主要看这两个参数pool_connections和pool_maxsize，通过一番测试（比较长就不演示了，可以参考引用来进行实验），我们发现这个pool_connections主要控制TCP池的种类数，我们知道在进程池中我们可以有很多相同的TCP连接（主要是并发新建的），这些连接有些是连接相同的域名和端口，这个pool_connections就是控制有多少种类的站点（域名和端口）同时能够存在池中，那么这个pool_maxsize代表的就是池中不管种类有多少总共的TCP连接数\n假如你只写单线程程序那么你只要考虑pool_connections这个参数，因为单线程你发出一个请求只会占用一个TCP连接，在你每次开始请求时，池中不同站点的连接只有一种，所以你可以把pool_connections当做池的大小，但是假如你写多线程程序，每个时间点需要的TCP连接同你多线程的个数有关，由于requests不会限制当池中无可用连接时新建TCP连接，所以你一个站点的TCP连接可能有多个，这时我们就要用pool_maxsize来限制池子的容纳量，为了避免无限制存贮TCP连接，TCP连接池会把超过总数的连接按照时间顺序踢出去，让池中保持不大于限制总数的TCP连接。\n当然这里有个非常重要的知识点，requests的TCP池并不会限制新建TCP连接,它只是限制存贮量和种类，这个知识点非常重要，这对后面我们理解aiohttp异步请求时候为什么要限制并发数有非常大的帮助（它只限制TCP连接总数）\nTCP连接池的作用 # 经过上面的探索，我们知道TCP连接池一方面能够实现HTTP复用达到减少TCP连接时耗的作用，另一方面我们通过复用TCP连接可以节省套接字，避免经常碰到”Too many file“的错误，顺便提一下，由于TCP连接具有冷启动的特点，在刚连接上TCP时，速度会非常慢，只有系统发现负载不多才会恢复正常速度，所以这就是我们有时候用浏览器打开一个新页面要加载很久的原因。\n前面一直在介绍HTTP复用的理论基础，最后我们实战演练一下在异步框架aiohttp使用HTTP复用\n异步框架下HTTP复用 # 在这里我们使用Python的aiohttp异步请求框架（在这里我们要求Python的版本必须大于等于3.5），aiohttp也提供了TCP连接池的功能，要想共享TCP连接池，我们先新建一个Session对象\nconnector = aiohttp.TCPConnector(limit=50) session = aiohttp.ClientSession(connector=connector) 我们直接创建了一个最大容量为50的TCP池，并把它绑定到session对象上，接下来先试试跑个200个请求（要先在按照前面的代码搭建本地服务器）\nasync def fetch(url, session, semaphore): async with semaphore: async with session.get(url) as response: print(await response.read()) loop = asyncio.get_event_loop() 接下来我们就可以直接使用aiohttp框架\nsemaphore = asyncio.Semaphore(20) tasks = [fetch(url, session, semaphore) for x in range(nums)] begin = time.time() try: loop = asyncio.get_event_loop() loop.run_until_complete(asyncio.wait(tasks)) except: pass finally: end = time.time() loop.close() session.close() print('cost', end - begin, 'speed', nums / (end - begin), 'req/s') 在我的电脑测试测试下421.73 req/s,基本上达到异步的效率（可以调节limit至100左右达到最大）\n在这里解释一下为什么要使用semaphore（asyncio锁），由于当前版本（aiohttp==2.2.5）下aiohttp的HTTP连接池无法在没有锁的情况下复用TCP连接（具体可以看一下我提的这个issue,这里由于牵扯到太多异步框架的知识，我就详细不介绍异步库，如果想了解更多的话就看我上一片博文Python异步的理解\n总结 # 在如何提高请求效率和速度上，HTTP复用算是从协议出发上的一种优化，他主要利用方向是在单个站点多次请求上面，假如每个站点都只是一个请求的话，那他就无用武之地，不过现在站点不可能一次请求就完成交互，所以了解这个HTTP复用如何是非常有帮助的。\n引用：\nRequests\u0026rsquo; secret: pool_connections and pool_maxsize\nMaking 1 million requests with python-aiohttp\n","date":"2017-10-10","externalUrl":null,"permalink":"/posts/backend/network/http%E5%A4%8D%E7%94%A8/","section":"博客","summary":"曾经有人问过我一个问题什么是TCP复用，我当时没有回答上来，","title":"HTTP复用","type":"posts"},{"content":" 起因 # 异步的出现主要是单线程的io等待，由于任务大部分是io处于等待，假如让一个线程工作，所有任务按照流水线形式执行，假如一个请求需要1秒，五个请求需要五秒，那么如果能让他们同时运行的话，那么速度就能增加五倍\n如何让五个任务同时进行有两种方法\n多线程 异步 调试过过多线程的人都知道，线程就是从头到复制主线程一遍，开多个线程不仅成本高，而且调试成本高，异步就不一样呢，你可以把它当做一个单线程来进行编程，而且比多线程更加高效\nPython异步的多种实现 # Python实现异步的框架有很多，但是核心思想大概是基于下面两种方式\ntwister gevent twister思想是将异步操作封装起来，通过回调的方式来操作，我们看scrapy里面中间请求的实现就是twister方式\nscrapy.Request(url='xxx', callback=func) 通过传递封装的request，当框架帮我们请求完后，会通过callback进行回调，如果你的请求很简单那还好，只需要回调一次就可以，假如你的请求较复杂，那么你就会进入回调地狱（callback hell）\n而且你还要写处理各种回调产生的异常，你可以看看scrapy中间件的实现就知道scrapy的异常处理有多繁琐了。但是中间间的存在的确让我们代码模块话更加容易，这里暂且不谈。\ntwister这种回调比较反人类，它必须依赖背后的核心进行调度，离开了背后核心的支持，这个根本跑不起来，而且由于它依赖回调来进行后续步骤处理，所以我们的代码必须被切分为不同的部分，假如我们不知道背后的核心如何回调函数或者约束，我们根本不知道这两个函数是有关联的\n这种编程方式比较有利于模块话开发，但是对于我们熟悉顺序编程来看，这种回调方式显然是一场噩梦，相比于twister这种回调方式，gevent采用的是绿色协程的方式进行回调。\nPEP-380定义了yield from的语句，Python3.3开始使用，为了区别协程和生成器，Python3.5开始使用await代替yield from，这样协程就有了一个专门的方法来声明（await和async），后者用来标记异步函数\n协程之所以能够在异步中大方光彩，其中很大一部分就是协程天生就是异步的，理解协程我们可以从一个简单的生成器与普通函数来对比\na = (x for x in range(10)) b = [x for x in range(10)) 我们来看这样一个生成器a，一般我们来用这个生成器必须加\tfor循环才能得到里面的值，假如我们尝试使用a.send(None)，我们会发现，我们依次从返回值得到了b里面的序列\n就是这么一个send与接受的功能让我们实现了一种”绿色“回调，就是协程这个性质让他写异步变得更加顺理成章了，而且相比twister回调，协程的回调更为彻底，它把”自己”包装起来全部回调回去了。\n了解异步基础 # 前面简单的聊了协程的性质，现在谈谈异步存在的基础，异步的存在最关键的在于等待，为了了解这个等待意思和后面解读asycio库，我们先使用selectors （Python3对select的封装）来做个演示\nimport selectors sel = selectors.DefaultSelector() 声明一个select对象sel，现在我们要调用这个核心函数\nsel.select(10) 这个10是代表timeout的时长，也就是最长等待时间，10秒之后我们发现，这个结果返回了一个空列表，这是显而易见的，我们并没有指明让它等待什么\nselectors这个库的功能非常好理解，类比寄信，你如果想等别人回信，假如你没有寄出去你自己的信，你一直在邮箱那等，除了等到你不想等，否则你是收不到你的回信的，所以这个库的核心在于，“寄信”（register）和等信（select），然后自己选择处理信件\nimport selectors import socket sel = selectors.DefaultSelector() def accept(sock, mask): conn, addr = sock.accept() print('accepted', conn, 'from', addr) sock = socket.socket() sock.bind(('localhost', 8000)) sock.listen(100) sock.setblocking(False) sel.register(sock, selectors.EVENT_READ, accept) while True: events = sel.select() for key, mask in events: callback = key.data callback(key.fileobj, mask) 这个程序最关键的地方在于sel.register、sel.select和callback那里，前者是注册函数，后面是等待，最后就是回调\n上面就是twister式最简单的回调，你可以看到，为了得到连接sock的连接，我们必须把处理注册到等待中去，但是这只是得到sock连接，为了成功建立一个TCP连接，我们还得进行三次握手，还得处理每次回调时的错误\n而且你可以看到回调函数与核心驱动select.select()耦合度非常高，我们必须完全了解系统如何回调，处理一件事被回调分割成一段一段\n接下来我们来看看基于gevent的asyncio实现\nasync def wget(host): connect = asyncio.open_connection(host, 80) reader, writer = await connect header = 'GET / HTTP/1.0\\r\\nHost: %s\\r\\n\\r\\n' % host writer.write(header.encode('utf-8')) await writer.drain() while True: line = await reader.readline() if line == b'\\r\\n': break writer.close() 我们成功的用一个函数描绘了建立一次连接并且进行通信的过程，假如你懂一点asyncio，你就会发现它与twister回调的不同，使用await关键字把函数挂起，然后等待回调，根据回调接着进行下面的操作，我们成功的用同步的语句把异步写出来，而且是使用Python的原生实现，所以当asyncio出来的时候Guido（Python之父）是多么自豪，你可以看下面引用 Tulip: Async I/O for Python 3演讲的视频\n浅析Python异步实现 # 前面我们知道了异步的基础就是等待，那么Guido是如何在协程的帮助下将异步实现出来的呢，接下来我们就简单的谈一下这个实现基础\n我们先将上面twister改成gevent方式的\nsel = selectors.DefaultSelector() @asyncio.coroutine def get_connection(sock): sel.register(sock, selectors.EVENT_READ) yield True async def create_connection(): sock = socket.socket() sock.bind(('localhost', 8000)) sock.listen(100) sock.setblocking(False) await get_connection(sock) conn, addr = sock.accept() print('accepted', conn, 'from', addr) event = create_connection() event.send(None) events = sel.select(100) for key, mask in events: try: event.send(None) except StopIteration: pass 我们稍稍修改一下上面的twister函数，我们创建一个get_connection函数把sock绑定到我们的sel上面，然后回调一个True，当然这个回调没有处理异常什么的，然后我们将得到的协程向其发送一个None让它启动，这时候你在在另外一个ipython客户端执行\nimport socket socket.socket()..connect(('localhost', 8000)) 然后你就会发现在主线程里面打印出来客户端的连接信息\n通过这个小例子我们知道，实现异步要解决的问题就是一个公用注册器（能够注册所以的io等待），一个容器（能够存贮所以的协程），一个核心能够一直执行等待回调和处理回调（多个协程）\n深入asyncio了解Python异步 # 通过上面我们简单的知道了，如何通过协程与select合作完成异步操作，然而我们上面写的只是最最最基本的实现，接下来我们来深入asyncio源码了解如何让异步变得更加简单\n引用 # Python异步并发框架\nPython 中的异步编程：Asyncio Tulip: Async I/O for Python 3 【译】深入理解python3.4中Asyncio库与Node.js的异步IO机制\n","date":"2017-10-09","externalUrl":null,"permalink":"/posts/backend/framework/python/python%E5%BC%82%E6%AD%A5%E7%9A%84%E7%90%86%E8%A7%A3/","section":"博客","summary":"起因 # 异步的出现主要是单线程的io等待，由于任务大部分是io","title":"Python异步的理解","type":"posts"},{"content":" 平常在Django项目中大量使用自增这个键，平常都是使用ORM，很少去了解这个东西在数据库中具体使用，最近遇到要备份和复原数据的事情，趁着这次好好探索一下这个自增键的使用\nDjango里面大部分都是将其作为Int自增主键来使用，第一个不需要维护一个唯一值，第二个使用Int作为主键的话，搜索和外键关联速度比较快。\n我们这次从原生SQL出发，探索一下这个自增主键在数据库中的具体使用\n新建数据库 # 我们先新建一个数据库\ncreate table inc( id serial not null, name text ); 在PG里面简单的使用serial关键字就会生成一个自增键，默认会在数据库新建一个索引表，例如上面就会新建一个inc_id_seq的索引表，这个字段类型为int，如果数据库很大，我们可以使用BIGSERIAL键申请一个bigint类型的字段\n我们可以看一下这个索引表里面有什么\nSequence \u0026quot;public.inc_id_seq\u0026quot; Column | Type | Value ---------------+---------+--------------------- sequence_name | name | inc_id_seq last_value | bigint | 1 start_value | bigint | 1 increment_by | bigint | 1 max_value | bigint | 9223372036854775807 min_value | bigint | 1 cache_value | bigint | 1 log_cnt | bigint | 30 is_cycled | boolean | f is_called | boolean | t 我们可以这个索引表其实就是维护了一个参数，通过字段我们可以知道，这是一个自增为1的键，下一个值为2，目前没有插入一个值\n增删查减 # 我们通过一些基本操作来看看这个自增键的作用\n首先是插入\ninsert into inc (name) values ('1'),('2'), ('3'); 我们插入三个值，我们再查看索引表，发现last_value变成了3\n这个是没有指定id的值插入，我们试试显式声明插入\ninsert into inc values (1, '1'),(2, '2'), (3, '3'), (4, '4'); 我们惊奇的发现，在我们显式声明自增键的值的时候，索引表并没有变化，last_value还是3,这说明只有在不声明自增键，让数据库自己新建的时候，索引表才会更新\n我们可以把自增键看做一个默认值，当没有给自增键赋值的时候，这个自增键会从这个键的索引表中得到下一次自增的值\n所以我们再尝试使用不声明自增键值的方法插入一个新值\ninsert into inc (name) values ('4') 我们发现索引表中last_value变成了4\n主键自增 # 由于我们在Django里面使用自增，一般都是将其声明为主键，设为唯一值，所以如果我们将声明表的结构变成\ncreate table inc( id serial not null PRIMARY KEY, name text ); 上面的情况就不可能发生了，因为我们把自增键声明为主键，不过有意思的事就是如果你像上面一样指定了一个自增主键值为4，然后不指定再插入4，你会发现第一次会报主键不允许重复的错误，第二次则会成功插入，而且索引表的last_value变成了5\n看来并不是每次成功的时候才会更新last_value值，只要让系统自己去申请自增值就会更新索引表，我尝试了对表的增删查改，发现只有insert并且申请自增值的时候才会更新索引表，而且这个索引表之后增加，不会减少，所以有时候你删掉最大的值，自增键默认又从最后一次开始更新\n总结 # 在对单个数据表备份还原的时候，由于简单的使用了COPY命令进行备份还原，通过上面的探索我们发现如果涉及到自增主键的导入导出，在新表导入旧数据是不会出错的，但是由于我们没有考虑自增键的影响（我们导入自增键是显示赋值），在后面插入数据的时候有可能会报主键重复的错误\n为了避免以后插入入数据出现这样的错误，我们有两种措施\n使用COPY命令导入导出时候不获取自增键值 在COPY导入新表后自己更新索引表 第一种的话SQL比较繁琐（必须写出表所有字段值），推荐使用第二种\n我们可以简单的使用\nSELECT MAX(id) FROM your_table; 先获取自增键最大值，然后更新索引值(999为上面你获取的最大值）\nSELECT setval('your_table_id_seq', 999, false); 当然我们可以将这条语句合正一句话\nSELECT setval('your_table_id_seq', COALESCE((SELECT MAX(id)+1 FROM your_table), 1), false); 这样我们就可以开心的完成单表导入导出了\nps： 在使用COPY命令时必须是superuser才能从文件中读取和导入数据，最简单的方法是用superuser账号加权使用alter user xxx superuser，待倒完数据后再降权alter user xxx nosuperuser\n","date":"2017-10-08","externalUrl":null,"permalink":"/posts/backend/software/postgresql%E7%9A%84%E8%87%AA%E5%A2%9E%E9%94%AE/","section":"博客","summary":"平常在Django项目中大量使用自增这个键，平常都是使用OR","title":"PostgreSQL的自增键","type":"posts"},{"content":" 其实这篇文章很早就像写了，但是自己也一直没有明白自己想写什么，直到最近自己慢慢才有一点思路\n这篇文章并不想高谈阔论，只是自己的一些碎碎念，把自己对人生的一些看法的小总结。你可以把它当做一篇小说来看，我也想把它当做小说来写。\n引子 # 中秋回家，同自己表妹聊天，她突然问了一句,大学哥哥没有谈一个女朋友，接着说道没有恋爱的大学是不完整的，我楞了一愣，一本正经的对她说道，大学其实就是培养自己一个完整人格的过程，在这个过程中我们学习并且养成自己一个完整独立认知。\n回去之后我仔细想想，我好像并没有回答表妹的问题，但是我自己也陷入了深思，大学这几年到底对我干了什么。\n错觉 # 先不说大学对我做了什么，我仔细想了想我自己对自己定位。\n你觉得你很努力? # 我一直觉得自己在大学还是很努力的，我没有沉迷游戏超过两天，我没有放弃学习新知识，在大学图书馆借了几百本书，可能比全班人加起来都多，大一到大三我经常去图书馆读书，尤其是大三，有时候会在图书馆读一天书\n但是上面的上面全部我自己的错觉，我虽然不沉迷游戏但是经常会被游戏分心，我虽然读过很多书但是我一直是读那些别人认为是必读的书，而且我读过的书大部分都没有转换成为我真正的源泉\n我就像一个饥渴的行者，在大河面前用手拼命的往口里塞水，我的确看了很多的书，但是这么多书就像流水一样，全部都流走了;这些知识对于我来说只是解渴之物，当我非常饥渴的时候，我会拼命的想得到它，但是当我满足的时候，这些东西就像泥土一样对我一文不值\n所以这就可以解释为什么我每次借书的时候都是兴高采烈，但是当借回来时候往往翻了几页，然后就束之高阁，然后循环往复\n你觉得你懂很多? # 没有出去之前，我在大学社团里面干过不少项目，所以我有时候觉得自己技术很牛逼，我是大神级别的人。我懂很多别人不知道的知识，我用过很多框架，我知道怎么搭集群，我知道什么是机器学习，什么是分布式，什么是代码规范。\n然而出去之后才发现这都是错觉，你做过很多项目，你经历过百万规模的并发吗，你了解很多框架，随便挑一个框架出来，你能说出它的优点和不足吗，你看过源代码吗，你知道如何保证上千集群的容错率，你知道什么是大规模机器学习吗\n挑出任何一个你会发现自己一直处在皮毛阶段，有时候你会用你还年轻但是学习能力强来掩饰你的不足，但是这只是你的错觉吧，不懂就是不懂。\n你觉得你不需要明确方向? # 有段时间我一直很困惑自己未来发展方向，我搞过UI，搞过前端、后端，搞过机器学习、数据分析，搞过分布式、爬虫。编程语言更是多，C、C#、Python、Java、C++，node，JS。我对我自己的定位一直很模糊，我不知道我未来到底想干什么，我很羡慕那些从小就明确目标的人。\n我一直为此苦恼，我也看过很多人的书籍、博客，我也看到过很多人写的相同的文章，在很长一段时间我都认为它是正确的，它告诉我你不需要明确你的职业规划，它给了很多有名的人例子，奥巴马、马云、马化腾、李开复他们在大学都不知道自己要干什么。在很长一段时间我都觉得大学就是应该多学东西，把东西学杂。\n但是我仔细想想，这个也是错觉。\n我从大二就开始有转行的念头，当时我是web后端开发，我当时觉得有没有方向无所谓，只要你多学就行，就这样陆陆续续混杂看了一年多书，直到七八个月前，我才开始反思。\n我开始明确我的目标，把它当做我要干一辈子的方向去搞，我开始扣书，像一个干涸的大地一样汲取天空飘下的雨滴，我这时候发现知识是那么的宝贵，自己是多么的“native”；从一窍不通到入门到小小成就只花了短短几个月的时间，完成了一年多都完成不了的入门。\n当然我最终没有选择这个方向，但是这个过程我从来没有后悔过，而且在这个学习过程中，它帮助我更加了解我自己，而且节省了我选择的时间。\n总结 # 好像我的大学一直全部都是由错误组成的，但是这些错误真的对我来说毫无作用吗。其实未必，当我学完第一门编程语言C的时候，所以的“错误”都在默默的发挥的作用，我用静态语言的辩证思想学Python，我用动态语言的思想反过来学习Java，好像全部的“错误”全部融合成为一个圈，\n我们好像一直在害怕自己出错，其实慢慢的发现那些没有错误的人生不是完整的。\n小时候很羡慕那些一直走在正确的道路上的人，也有时候会幻想成为他们一样的人。没错那样的人生固然完美，但是我更喜欢一直跌跌撞撞的我，或许我经常走在错误的道路上，但是我享受了沿途的风景，不论最终结果如何，人生的意义还是沿途的风景吧!\n","date":"2017-10-07","externalUrl":null,"permalink":"/posts/essays/life/my_college_mistakes/","section":"博客","summary":"其实这篇文章很早就像写了，但是自己也一直没有明白自己想写什么","title":"大学的错觉","type":"posts"},{"content":"","date":"2017-08-09","externalUrl":null,"permalink":"/tags/%E6%AF%94%E8%B5%9B/","section":"Tags","summary":"","title":"比赛","type":"tags"},{"content":" 最近打了两个比赛，一直忙着工作和打比赛，没有时间总结，今天抽空好好总结一番\n先说一下比赛结果吧，队名全为OfferGo唯品会购买预测第五名，携程房屋预测复赛第六名,两个比赛打的都不算太好,只能算勉强及格,虽然离大神的距离还有十万八千米，不过总算可以称的上入了门，现在来总结一下我入门的经验吧。\n观察数据 # 我参加过很多群,发现很多新手缺乏观察数据的能力,他们每次进入一个群总是嚷嚷这让大神发baseline\n这一点对于新手来说很不利的，比赛考的就是你对数据的掌握能力,你对数据把握的越好，你的比赛成绩就越好，要真正掌握数据就要从观察数据入手\n在我看来观察数据主要从四个方面来，我总结为望闻问切\n望 # 观察数据缺失值，缺失值对数据影响很大，有时候我们能够从缺失值里面了解很多信息,而且对于缺失值，后期我们对不同的缺失值要采取不同的手段，比如补全、统计占比、丢弃等等。\n对于缺失值我一般从两个方面来观察\n全局观察 一般采用datafram.info(null_counts=True, verbose=True)方法来观察全局数据缺失情况 局部观察 一般采用series.isnull().count()和series.loc[series.notnull()]观察单一列表缺失情况 这个阶段我们主要从大的方向远远的望一下数据，主要建立对数据的全局观。\n闻 # 对于数据来说，一般分为三种，一种为数值型数据（整数、浮点数、时间等），一种为字符型，最后一种为图像型，三种类型数据处理难度依次增强\n对于大多数比赛都是设计前两种数据，第三种只有牵扯到图像处理才能遇到。对于前两种数据，我们在闻的阶段，主要是探查数据分布情况，了解数据分布情况，我们才能对症下药。\n了解数据分布情况有两种方法\n图像观察 数学统计观察 图像观察主要使用Pandas的Matplotlib绘图接口，或者使用seaborn（一个友好的封装了Matplotlib的包），一般我们可以从直方图、饼图、频率图方向来观察数据\n数学统计我们主要采用Pandas的describe方法，对于数值型数据，主要从平均值（mean）、中位数（50%）、标准差（std）、最大（max）、最小（min）、非空总数（count）来探测数据，对于字符型我们主要从最频繁的值（top）、最频繁的值的个数（freq），非空总数（count）、不相同的值（unique）。\n通过上面两种方法，我们能够从数据分布的角度大致勾画出数据的轮廓。\n问 # 比赛的目的就是找到最优解，而最优解的跟相关特征紧密联系的，你的特征对结果影响越大，你就要审问这个特征\n举个例子，我们要预测三组数据\n1 1.8 2 2 3.5 4 3 5.4 16 第一行为我们要训练的值，我们发现第二行的数据是第一行的1.8倍，而第三行只是2的次方，对于这两个特征来说，第二个特征就是最好的特征，我们只要建立一个映射，准确率能接近100%，而第三个特征对预测结果毫无联系，这个特征不但对结果没有作用，而且有时候会起到反作用\n当然在这里我们举这么一个例子在实际中不可能遇到，我们遇到是更多数据，而数据之间的联系并不是这么简单的线性关系，但是线性关系有的时候能让我察觉到特征与预测值的关联，毕竟如果特征值是随机值那么与预测值之间的相关性是非常低的。\n在Python里面探测线性关系最简单的方法是调用特征值和预测值的相关性系数（corr），我们可以简单的使用df[['feature', 'target']].corr()就可以得到线性相关系数，这个数的绝对值越接近1，相关性越大，一般来说相关性越大和越好对结果都不好，最好的特征相关性处于中间位置。\n相关性低我们可以理解，为什么相关性高反而不好呢，因为数据比赛里面给我们的数据大部分都是不平衡的数据，正负样本失衡，一般相关性很高的值一般为分类同预测值相同，比如一个二分类问题，预测值为0和1，给的样本正负比为1000:1，那么如果有一个特征全为0或者其他，那么他与预测值的相关性会达到90%以上，然而这个值是毫无作用的。\n所以我们通过简单的相关系数并不能很好的观察特征真正相关性，一般我们要辅助图像法和统计法。\n图像法就是通过将特征值分布与预测值相关性图表画在同一个图表里，具体可以参考可视化特征\n统计法类似图标，使用统计方法观察，特征值与预测值的相关性，一般使用groupby方法对两个特征进行统计就可以进行简单的观察\n问只是一个简单的手段，一般我们在大量添加特征的时候，为了节省模型训练时间，在将特征放入管道之前进行一个简单的过滤删除的工作，真正重要的步骤在切这个方面\n切 # 切这个步骤放在最后是因为，这个步骤也是我们一趟循环下来的最后一步\n数据比赛中前期大家最喜欢用的模型是树模型，比如随机森林、Xgboost，LightBoost等，这些模型属于弱学习器组合模型，我们最后可以从训练结果得到每个特征在模型占的比重\n对于这个比重，是非常重要的，他代表了每个特征对应在模型中占的权重，也可以理解特征与结果的相关性\n对于相关性很强的不同的特征，我们可以将他们组合，有时候这种强强组合生成出来的特征会比原来母特征相关性更强，当然组合的方法有千种万种，如何验证他们有效就要从头开始对数据进行望闻问切了\n总结： 数据比赛就如同问诊，我们不断对特征进行望闻问切，对于高手来说他们能很快的从原始特征中挑选出病根，对症下药，而新手的话，一阵摸瞎，经常会碰到在比赛中期做出一个很好的结果，接下来很长一段时间都没有进步的情况。掌握科学有效的挑选特征方法需要一个“医者心”，必须学会对特征“负责”，要学会望闻问切。 # 并行化算法 # 由于Python本身对多核利用不好，如何利用多核加快特征生成对于比赛来说意义重大\n就拿我来举例子，我每天下班打比赛的时间不超过8个小时，前期算法没有并行化的时候，走一遍管道要四个小时，这意味着我一天只能跑两次，而进行并行化优化以后，我跑一遍四线程全开（笔记本双核四线程）只要十分钟就能跑完，每次生成新特征只有10分钟就能拿到特征相关性数据，来验证特征的好坏。\n下面我从三个方面来谈谈怎么实现并行话算法\n1 . 使用系统自带函数，拒绝for循环 # 举个例子，作为新手，实现对两个个特征求平均，一般采用for循环将每一行两个特征值加起来然后除以2，假如有1000万行，每行加法和除法运算花0.001ms，那1000万也要10秒钟，只是进行一个最简单的求平均，你就花掉10秒钟，上百个特征你得运行几天\n学过矩阵的都知道，矩阵就是一种高效的并行化结构，它将集合统一进行计算，可能一个大矩阵运算要比单一计算要慢，但是单一计算要1000万次的话，大矩阵运算只需要两次就够了，这个效率比就出来了\n而Python由于是一门解释性语言，比其他静态语音速度要慢许多，你一方面使用for循环加大运算次数，一方面执行一次时间长，这相重叠加你的算法会跑的比蜗牛还慢\n所以我们避免使用我们写的函数，尽量使用库系统函数，因为库系统函数底层是使用C或C++实现的，而且他们在底层进行使用矩阵话运算代替单一浮点计算，我们使用库的函数（比如mean，groupby等）一方面能底层能使用C加快速度，一方面使用矩阵运算加快速度，两个叠加你的算法跑的比飞机还快。\n2 . 使用多进程，充分发挥使用多核性能 # 由于Python的GIL锁，使得Python无法利用多核进行计算，所以我们只能使用多个进程来充分利用多核\n实现多进程有两个要点（具体可以参考我携程比赛代码 Github地址)\n特征提取模块化 进程池的搭建和维护 我在携程比赛中的mult_run.ipynb中搭建了一个进程池，通过第三方调度和监控进程内存CPU等信息，达到充分“榨干”每个核的功效\n3 . 压缩数据，让矩阵运算更快 # 由于在对特征进行提取过程中，Python会自动将低位制值转换成高位制值，比如float16在进行一次groupby之后就会转换成float64，由于在矩阵运算时候，高进制值会占更多内存和运行时间，所以为了加快算法运行，我们要将其压缩，一方面节省内存，一方面能够让算法运行的更快\n在携程的比赛中，原始数据有一个G，我将其压缩之后只占用300M内存空间，这为我后面在一台12G内存的笔记本实现并行化算法提供了巨大帮助，当然我每次在生成新特征的时候也会进行压缩，具体可以参考我携程的utils.py文件\n总结： 这两次比赛，我从菜鸟出发慢慢的从一个程序员变成了数据挖掘机，在模块化和并行化方面，我觉得我的进步不错，但是在数据特征挖掘方面我与大神之间的差距还是巨大的，这也是我止步于前五的主要原因，接下来我要加强对数据方向的锻炼，希望能够在工作和比赛之中得到更好的进步 # 在最下面贴一下我的携程比赛代码（基于Notebook） # https://www.github.com/mrzhangboss/ctrip_room_predict\n","date":"2017-08-09","externalUrl":null,"permalink":"/posts/ai/competition/vip_ctrip_competitions_summary/","section":"博客","summary":"最近打了两个比赛，一直忙着工作和打比赛，没有时间总结，今天抽","title":"唯品会预测和携程房屋预测总结","type":"posts"},{"content":"基于:大航杯“智造扬中”电力AI大赛参赛经验\n赛题背景\n主办方为大航集团提供21个月江苏省杨中市1454家企业日用电量,来估计下一个月日总用电量\n从给的数据分析,这次给的数据只有历史企业日用电量,用来估计日总用电量,是一个典型的时域分析问题\n但是这同我们以往的时序问题不一样,向往常时序问题预测的是每个企业的未来每日的用电量,而这个比赛却是求全部企业的总数.\n由于我报名比赛时候比较晚,比赛已经接近尾声,比赛5月18号开始,6月8号中午切换数据,13号截止,我6月8号晚上下载数据,由于我以前已经做了几个类似的比赛,但是一直没有系统的做一个,抱着锻炼的自己的态度,决定系统做一次,权当练手.\n首先分析一下提交的结果,预测一个月的日总用电量,总共为31个数据,给的历史数据只有21个月的,按月的比例来看,只有21个值去训练值去预测一个值,根据往常的比赛经验来看,这种比赛适合使用规则方法来做,然而我剩下的验证机会不多了,只能用模型,但是过拟合的危险非常大,如果不能找到一个好的方法克服过拟合,复赛都进不去.\n当然最后还是没有找到一个很好的办法,止步于复赛,不过这次比赛让我学到很多,主要通过这次比赛自己琢磨出来自己如何搭建基于IPython Notebook的管道结构,这个管道帮我自动生成上万特征.\n管道 # 什么是管道,在数据挖掘比赛中很多大神都着重讲了一定要搭建一个自动化的架构,我们暂且称他为\u0026quot;管道\u0026quot;,这个\u0026quot;管道\u0026quot;我们要能够把数据倒进去,结果倒出来.\n这个管道用专业的术语来看要有以下几个功能\n能够自由添加Feture 能够自动评判得到添加的Feture的效果 管道能够自己选择合适的参数训练模型 能够输出结果 其实简单来说,我们要做的是一个能够非常方便的扩展的脚手架,我们不可以第一次就把所以的特征全部找出来,所以我们要搭建一个能够实时添加Feture的框架.\n其实很早以前就看过类似文章,也有很多人推荐大神开源的一个脚手架,然而找到的大多是用python实现\n我因为一开始就是使用Notebook进行数据挖掘,主要Notebook能够提供一个实时的反馈,而纯python,对于复杂多变的数据来说,显得非常笨重,你经常有个好想法想验证一下,又得重新跑一遍,尤其是对于我的机器配置来说,重新跑一边的时间都够我喝杯茶了.而且notebook有个特点,可视化特别方便,有时候从数据上看不到,可以画个图表\n好了,夸了这么久,现在就来仔细讲讲脚手架如何搭建.\n我们先回到赛题,第一步审题,当时我看到日平均两个字,直接把日字省略,看成平均用电量,结果白白浪费了两个验证机会\u0026hellip;..\n审完了题我们来看我们要提交的数据,换数据后要预测十月日用电总量.我们来看看给我们数据,只有一份数据,表头如下\nrecord_date,user_id,power_consumption 2015/1/1,1,1135 2015/1/2,1,570 2015/1/3,1,3418 2015/1/4,1,3968 2015/1/5,1,3986 解释一下字段,record_date\u0026ndash;日期,user_id\u0026ndash;企业id, power_consumption\u0026ndash;日用电量 非常简单,就这么简单单单的数据,我现在要教大家怎么从这么简单的数据上抽取6000维度的\n我把代码已经推到Github上了(由于数据比较少,我把数据也推上去了,方便大家本地跑跑,看完如果对你有帮助的话,请不要吝啬你的star哦),我就对照我的代码解释如何搭建一个可以跑出上万维度的脚手架\n数据划分(split_samples.ipynb) # 首先要搭建本地预测集,也就是线下样本(这个很重要,有时候线下的结果很大程度对应你线上的结果)\n给的数据要我们从前面21个月预测下一个月的日总用电量,我们很容易就能想到,那我们用前面20个月预测第21个月来做线下测试,但是这样我们就只有30个训练样本,要来预测30个,99.999%过拟合啊,首先我们要扩大样本,我们采用滑动移窗的方法把预测的样本按照月份推移,也就是分别预测9月8月7月等等\n这种方法在实现Notebook有几个难点,首先你划分了预测集,那么就也要划分训练集,就相当于把一份数据切分成好几份,切分完之后有个问题,你必须要隔离每个部分\n举个例子,我们把训练集划分成为2份,1月到7月预测8月,2月到8月预测9月,训练1-7月数据集的时候,我们不能让这个训练集接触到2月到8月的数据,因为8月对于前一个训练集来说是未知的, 如果我们让第一个训练集接触倒第二个训练集我们称为信息泄露,很影响线上的结果\n我们知道这个问题之后,我们就要用巧妙的方法来解决,首先我们要考虑我们代码的复杂度,以前我的解决训练集隔离的方法采用的是循环法,使用一个列表存贮所以训练集,然后使用for循环分别传参到函数里面,这个方法能解决隔离训练集,但是有几个问题\n在单个ipy文件中训练所以的样本,在测试的时候跑起来太慢,而且要把数据全部加载在内存里面,这次数据量还算小,但是对于某些小内存的电脑来说,这种方法时不时就得报Memory Error,而且感觉调试起来特别麻烦,所以一直在寻找更好的解决方案.\n这次想到了一种巧妙的方法,虽然有点取巧但是效果我很满意.\n我们先看到split_samples.ipynb文件,首先我把数据划分为9个样本,一个预测样本.分别放入不同文件夹进行物理隔离.但是名字相同.\n再其次我让ipy能够获取参数,这样我通过外部参数就能更换数据集,平常添加Feture的时候默认选取一个训练集,这样我开发的时候调试就非常方便,而且可以丢掉for循环,还我一个清新脱俗的ipy.\n这里说一个小细节,因为我传参必须要外部调用这种,对于运行ipy我使用了runipy这个工具,然后我死活没有找到,如何使用runipy把参数传倒ipy里面去的方法(如果找到了请告诉我),我一拍脑袋那就转换成py文件传过去,通过sys.argv很轻松就能获取到,所以我又用jupyter nbconvert的工具把ipy转换成py文件\n所以绕了一圈最后又回到了py上(手动滑稽).不过我们工作还是在ipy上进行,生成的py文件我好像没打开过\u0026hellip;.\n特征提取(extract_fetures.ipynb) # 聊完如何划分数据集,现在我们进入如何特征提取,我们可以看到这次数据其实就三个特征:时间-企业-用电量.由于企业的信息只有一个id,所以我首先提取的是时序的特征,首先把时间分解为八个维度\ndayofweek dayofyear days_in_month quarter week weekofyear month year 我们可以通过pandas轻松提取出来\n然后我们再从两个方向来看,第一个就是我们日总用电量特征,从全部企业日总用电量\n第二个就是日用电量特征,从每个企业日用电量来看,这些特征我们使用简单统计又可以得到10个维度数据(mean,std,等等)\n看完这些之后我们又可以从多个时间维度来看这些特征,比如30天前,90天前等等(我划分了30,60, 90,180,360五个),\n这样我们就有了 8 * 2 * 5 * 10个特征,但是这远远达不到我们说的上万维度,\n现在我们从业务逻辑上来思考,因为我们知道,其实我们中国节假日和周末,天气这些对用电量影响非常大(我们老家打雷就停电\u0026hellip;..)\n所以我们要引入外部数据集,我采用两个爬虫分别是weather_crawl,holiday_crawl爬取了天气和节假日的数据\n我们按照前面的思路,从天气节假日的角度又可以划分出n多特征(这时候我的特征已经达到3000了)\n完了这些基础特征后,我发现有些特征重要性特别大(使用Randomforest得到),这时候我们又要请出我们第二大神器,交叉特征,比如月和假期的特征融合,这一波操作直接让我的特征到了6000+维度(如果将窗口扩大轻轻松松上万)\n在这里要介绍一个特征生成的方法,有时候我们特征少,我们会采用自己命名的,自己生成,然后这个由于规律性比较大, 如果我们自己手动一个一个写的话,这上万Fetures够你写的,所以要让他自己生成特征,我们只要建好模子就行,由于这次 时间仓促,基本上我没有自己手动命名feture,全部都由程序生成,省掉很多代码量,具体可以看看代码实现,原理很简单.\n训练模型(train_model.ipynb) # 训练模型的话,一般比赛都推荐先使用树模型,一方面速度快,第二个可以看到feture的重要性,这对于你挑选交叉特征非常有用,模型调参的我这里就不讲,一方面我自己也不是很懂,第二个方面也网上教程也多,我讲的不一定比他好\n这里要推荐一个发现有趣的包,mlxtend,我用他来进行stacking特别方便,有意思的时,我用他融合了四个模型,最后我的训练结果竟然为1,完全拟合了\u0026hellip;\u0026hellip;\n这个包可以很简单的进行模型的stacking,然而这个比赛我没有把他用好(手动滑稽)\n通过训练模型后我们把模型存到pkl文件中,然后在用他来预测数据,这样在文件夹里转一圈的原因,因为原来打过部分比赛数据量太大,训练模型后内存不足,只能先del,清空内存,再预测,存到文件夹后,结束进程,清空内存,这样就能省下空间来读取下一步数据.\n总结 # 其实在这个脚手架上可以扩展很多东西,比如最后搏一搏单车变摩托的时候,我就在分割数据和训练数据之间加了一个过滤清洗数据层,在训练模型和融合特征之间加了一个降维的中间层.\n建立一个好的脚手架只是能让你在增添特征,选择特征时更加轻松,其实比赛看的还是你对数据的一种掌控力,建立这个脚手架主要是为了节省更多时间给提取特征、选择特征上.特征决定你的上限.\n这次比赛比较特殊,模型在这个比赛效果可能没有规则好,因为数据量太小,我stacking一下直接完全拟合了.可惜验证的次数还是 太少,除去前面两次错误的提交,我只有三次验证机会,如果次数多一点的话,选择特征降维或者模型调参一下遏制拟合结果可能会好很多吧.\n但是这次比赛自己学到了如何搭一个ipy的管道和增加了一些特征调参、特征降维的经验。因为以前看到的搭建管道资料都是基于py,很少基于ipy的,所以把自己搭建ipy管道经验分享出来,也希望自己写的这篇博文能够抛砖引玉,帮助大家搭建自己的完美管道.\n附上我的开源示例: https://github.com/mrzhangboss/electricAI 大家觉得有帮助就给我点个star吧\n","date":"2017-06-13","externalUrl":null,"permalink":"/posts/ai/competition/%E5%A4%A7%E8%88%AA%E6%9D%AF%E6%99%BA%E9%80%A0%E6%89%AC%E4%B8%AD%E7%94%B5%E5%8A%9Bai%E5%A4%A7%E8%B5%9B%E5%8F%82%E8%B5%9B%E7%BB%8F%E9%AA%8C/","section":"博客","summary":"基于:大航杯“智造扬中”电力AI大赛参赛经验 赛题背景 主办方为","title":"如何搭建自动生成上万特征的管道","type":"posts"},{"content":"","date":"2017-05-16","externalUrl":null,"permalink":"/tags/%E6%84%9F%E6%82%9F/","section":"Tags","summary":"","title":"感悟","type":"tags"},{"content":"自我介绍\n前端 - \u0026gt; 后端 -\u0026gt; 数据挖掘机\nML DM AI 的区别\n我的自学之旅\n给新手的推荐\n机器学习课程(MOOC) Kaggle、天池、数据城堡 掌握的技能\nJava + Python 数据可视化 训练团队感 未来的发展方向\n全栈数据挖掘工程师 增长黑客 ML算法工程师 ","date":"2017-05-16","externalUrl":null,"permalink":"/posts/ai/%E5%A6%82%E4%BD%95%E6%88%90%E4%B8%BA%E4%B8%80%E5%90%8D%E6%95%B0%E6%8D%AE%E6%8C%96%E6%8E%98%E6%9C%BA/","section":"博客","summary":"自我介绍 前端 - \u0026gt; 后端 -\u0026gt; 数据挖掘机 ML DM AI 的区别 我的自学之旅 给新","title":"如何成为一名数据挖掘机","type":"posts"},{"content":" 由于最近在一家数据服务公司实习，项目需要了解分布式，所以在这里基于scrapy的分布式总结一下爬虫的分布式实习\n分布式起因 # 单机无法完成全部工作任务所以要使用集群加速完成工作任务\n分布式有点像蚁群，一只蚂蚁举不起一只卡壳虫，但是几百只就能轻松的把他运回家\n但是分布式设计必须科学，否则就像下面一样，一个和尚挑水，其他和尚围观\n分布式设计 # 分布式设计原理在于分工\n首先我们来看看爬虫怎么进行分工，单个爬虫运行根据url获取响应报文，然后通过解析报文返回结果或者下一次爬取目标，如果单个爬虫我们只要在内存维持一个set变量记住爬取过的url，这就是scrapy默认的方法。\n但是我们无数个爬虫由于不在同一个进程，无法共享变量，所以我们只要让一个“variable（变量）”能够被被所以爬虫共享到就完成了主要功能\n现在我们来完善具体细节 要求：\n爬虫能够轻松读取所以已爬取变量 爬虫能够加入已读取变量 爬虫能够获取下一次请求具体参数 原则上我们可以使用内存映射来构建这个变量，但是读取，修改都不便利，所以可以先使用redis作为存贮变量的地方，使用redis提供的set我们替代scrapy框架的set变量。\n现在我们已经决定我们要使用什么容器来存贮变量，接下来我们要考虑存什么变量。\n我们先看scrapy-redis存贮了什么，分析源代码可知，scrapy-redis将返回的Requestpickle话存入数据库，并且计算这个Request的32位hash值存入redis的set中过滤列表。\nscrapy-redis通过修改scrapy的调度器（scheduler）让其当爬虫没有Request需要处理时在redis中提取Request，实现分布式。\n我们来分析一下这种方法，爬虫在爬取的过程中从master端获取Request，并不断生成Request到master端，master只是一个redis数据库，负责对url去重，分发任务。\n我们来比较一下直接存取url这种方法，这种方法好处在于，slaver能够从上一个Request中获取全部信息，假如上一个Request需要存取获取的表单提取地址，我们下一次爬虫发起Request就能从上一个Request中获取参数。\n当然由于我们存贮的是Request，一个Request pickle化之后的字符串比较长，当我们的任务列表里面有很多Request的时候，redis占用的内存会非常巨大。\n当然如果爬虫启动的够多，生成一个就能把任务被调度下去，那么这个任务列表就能稳定在一个可控的范围。\n总结\n每个爬虫即负责爬取数据，又负责生成下一个任务，即无主次之分，我们可以一次性在docker中启动上百个实例，我们只是用redis充当一个存放变量的地方。\n但是这种方法也有一个缺点，我们不能自由的添加初始url，要想添加新的爬取任务，必须新建一个爬虫更新初始url，我们如果是想搭建一个自由添加url的爬虫，这种实现方式不大优雅。\n分布式改良 # 我们要修改程序框架，达到随时可以添加要爬取新任务，然而不影响爬虫集群\n我们独立出来master，master负责生成Request去重以及任务调度，而slaver只负责从master获取任务爬取。\n这种方法我们可以很轻松对master改良而不影响slaver，通过让master定时从数据库中获取新的任务生成到任务列表，我们可以轻松添加新的任务到slaver集群中去。\n下一步我们就介绍如何修改scrapy-redis达到我们新框架需要\n重构scrapy-redis # 参考： 基于Redis的三种分布式爬虫策略\n","date":"2017-04-04","externalUrl":null,"permalink":"/posts/backend/framework/python/%E7%88%AC%E8%99%AB%E5%88%86%E5%B8%83%E5%BC%8F%E6%80%BB%E7%BB%93/","section":"博客","summary":"由于最近在一家数据服务公司实习，项目需要了解分布式，所以在这","title":"爬虫分布式总结","type":"posts"},{"content":" 引言 # 递归是高度抽象化问题的一个好东西，我们能从很多算法书里面看到这个， 但是递归虽然对于人来说好理解，但是计算机执行起来会损失性能，一个差的递归可能会耗光计算机的资源\n接下来我们来看一个非常经典的算法问题Fibonacci数\nf(n) = n (n \u0026lt; 2) f(n) = f(n-1) + f(n-2) (n \u0026gt;= 2) 我们可以很轻松的用递归解决掉它\ndef fibonacci(n): if n \u0026lt; 2: return n else: return fibonacci(n-1) + fibonacci(n-2) 当n比较小的时候很快就出结果了，但是当n大于100时候要很久才能出结果，如果n大于1000，直接报出超出迭代深度的错误（python默认迭代深度是1000）\n现在我们来解决两个问题\n为什么n大于100时候就很久才能算出结果 为什么n大于1000就报迭代深度的错误 首先我们要知道一个概念就是堆栈段，每个进程开始运行时都会初始化一个堆栈段，这在物理上就是一小块内存，初始化堆栈段的时候计算机要做一些看起来同程序毫无关系的事情，比如说将寄存器的值推入堆栈里面等等\n当你在运行主程序的时候你调用一个子函数，系统又会在当前堆栈段新建一个堆栈段，你子程序运行完了后会删掉这个堆栈段回到主程序，但是递归有个问题，就是他调用子程序的时候不会立即返回又会再调用自己\n没办法因为子程序还没返回，所以计算机又初始化一个堆栈段，一个n为10的fibonacci函数就会初始化掉 2 ** 10 = 1024个堆栈段，n越大值会指数型增长，虽然1000个初始化在当今计算机上发不了多少时间，但是当我们n大于20就要 百万次初始化了\n这就是为什么n很大的时候要很久才能算出结果，在一些单片机上面，循环调用空函数就是延时的功能，原理也就是堆栈初始化耗时间，而且不但耗时间假如像递归这样调用上百万次初始化而不返回将会耗掉大量内存在堆栈段上。\n对策 # 要解决这两个问题，一种方法是改算法，使用非递归算法，这个网上有很多，感兴趣的可以去搜一下，第二种是使用协程解决递归问题\n如何使用协程来解决递归呢我们先改主程序，将return换成yield\ndef _fibonacci(n): if n \u0026lt; 2: yield n else: yield ( (yield (_fibonacci(n-1)) + (yield (_fibonacci(n-2))) 接下啦我们运行一下函数\n\u0026gt;\u0026gt;\u0026gt; _fibonacci(10) \u0026lt;generator object _fibonacci at 0x00000013A74779E8\u0026gt; 没有返回结果,返回一个生成器，那我们用list简单的试一下吧\n\u0026gt;\u0026gt;\u0026gt; list(_) ...... ...... TypeError: unsupported operand type(s) for +: 'NoneType' and 'NoneType' 生成器小知识 # 这里补充几点生成器的知识，懂得可以跳过\n生成器大家都用过，无论是Python2或Python3都不陌生,最简单的生成器是这种\n\u0026gt;\u0026gt;\u0026gt; items = ( x for x in range(10)) 我们一般搭配for来使用\n\u0026gt;\u0026gt;\u0026gt; for i in items: ... print(i) ... 我们也可以用协程来实现这个生成器\ndef iter_func(n): for i in range(n): yield n 像上面一样使用for就能实现一样的功能，在这个例子里面yield好像变成了一个return的作用，在for语句中，随着每次请求都会return一个数过来\n在这个里面yield好像就是这么个功能，但是yield的作用远远不止于此\n我们现在来改一下这个函数\ndef iter_func(n): for i in range(n): r = yield i print('result', r) 我们用list来运行一下这个函数\n\u0026gt;\u0026gt;\u0026gt; list(iter_fun(2)) 0 result None 1 result None r返回了一个None，我们尝试自己实现一下for循环,有两种方式\nnext(generator) generator.send(msg) 先尝试用next\n\u0026gt;\u0026gt;\u0026gt; it = iter_fun(2) \u0026gt;\u0026gt;\u0026gt; next(it) 0 \u0026gt;\u0026gt;\u0026gt; next(it) result None 1 我们介绍一下next函数, next接受两个参数，第一个是生成器，第二个是返回的默认值,next函数在这里相当于下面这个函数\ndef next(iterator, default=None): try: iterator.send(None) except StopIteration: if default: return default else: raise StopIteration() 为什么第二个执行了print函数而第一个没有执行?\n生成器工作原理 # 这里我们介绍一下生成器的工作原理\n当我们使用调用一个函数的时候，一般是碰到return或者执行全部函数就会返回父函数\n但是生成器不同，假如他执行函数碰到yield，他就会直接返回一个生成器。\n这个生成器我们可以把它看做是邮递员，我们必须写好目的地,他才会帮我们把信寄出去。\n现在我们分析一下生成器的具体流程，我们先定义一个简单的生成器\ndef mygenerator(n): while True: r = yield n n -= 1 print('result', r) 然后我们调用这个生成器\n\u0026gt;\u0026gt;\u0026gt; i = mygenerator(10) \u0026gt;\u0026gt;\u0026gt; i \u0026lt;generator object mygenerator at 0x7f420a339d00\u0026gt; 我们得到一个生成器，我们先尝试发送一个地址给“邮递员”\n\u0026gt;\u0026gt;\u0026gt; i.send(0) ... TypeError: can't send non-None value to a just-started generator 我们得到一个错误，必须传递一个None，我们先不管，先送一个None值过去\n\u0026gt;\u0026gt;\u0026gt; i.send(None) 10 我们得到一个10，再送一个地址过去\n\u0026gt;\u0026gt;\u0026gt; i.send(None) result None 9 我们现在来分析一下代码，第一次调用的时候直接返回了，第二次调用我们从r = yield n那行开始执行，并且运行到第二个r = yield n那里停止了\n就可以解释上面为什么要第一次传递None过去，因为第一次调用它会直接返回yield后面的值给我们，第二次调用 我们可以根据第一次生成器递给我们的值，决定我们第二次想寄的“信”，因为第一次传递过去“信”并不能被处理，所以Python强制我们传递一个None值过去\n我们回到上面的函数\ndef _fibonacci(n): if n \u0026lt; 2: yield n else: yield ( (yield (_fibonacci(n-1)) + (yield (_fibonacci(n-2))) 我们来分析一下流程，为了解决上面的问题我们先把函数简化，去掉递归\ndef f(n): yield (yield n) + (yield n - 1) 我们先创建一个生成器i\n\u0026gt;\u0026gt;\u0026gt; i = f(5) \u0026gt;\u0026gt;\u0026gt; i \u0026lt;generator object f at 0x7f4a421d8f10\u0026gt; 我们先启动i\n\u0026gt;\u0026gt;\u0026gt; i.send(None) 5 我们再把得到5传给i\n\u0026gt;\u0026gt;\u0026gt; i.send(5) 4 我们得到yield n -1返回的4，我们再把4传给i，得到最终结果\n\u0026gt;\u0026gt;\u0026gt; i.send(5) 9 假如我们把后面两个send的值换成其他值我们会得到不同的结果，这里我们可以看到我们，要实现上面函数必须要依靠一个栈，保存我们返回的生成器，然后依次调用生成器返回结果，具体代码如下\ndef fibonacci(n): stack = [ _fibonacci(n)] last_result = None while stack: last = stack[-1] try: if isinstance(last, types.GeneratorType): stack.append(last.send(last_result)) last_result = None else: last_result = stack.pop() except StopIteration: stack.pop() return result 我们这里用stack作为我们的堆栈，用last_result保存上一个生成器返回的值\n总结 # 我们使用协程解决掉了递归错误，但是这个方法并不可以给我们算法加速，虽然n为1000以上不会报递归错误，但是等待的时间还是很长很长。。。\n虽然协程在这个方法里面并没有起到多大作业，协程在算法方面还是没有太多帮助，协程在计算机I/O还有网络请求方面有更好的效率，但是这次尝试让我们对协程如何使用有了一个清晰的了解\n有兴趣的可以去了解一下协程在异步网络请求的应用\n","date":"2017-03-15","externalUrl":null,"permalink":"/posts/backend/framework/python/%E5%8D%8F%E7%A8%8B%E8%A7%A3%E5%86%B3%E9%80%92%E5%BD%92%E9%94%99%E8%AF%AF/","section":"博客","summary":"引言 # 递归是高度抽象化问题的一个好东西，我们能从很多算法书里","title":"协程解决递归错误原理","type":"posts"},{"content":" 第一次听说这个比赛还是去年在知乎上，当时也不知道这个比赛具体是怎么回事，当时自己还是一个小白，忙着搞懂各种主流的机器学习模型算法。\n当时在我心中，模型算法是数据挖掘的最重要的组成部分，搞懂这些才能真正搞定数据挖掘。我当时对算法模型和数据的理解是：模型就是风车，数据就是流水。我要做的事就是撘一个强健的风车，让数据流过。\n当我还没接触实际的工作前，我还没有没有从编程工转向挖掘工。我太注重编程本身了，而忘记我自己真正要挖掘的宝藏。\n我以前在Quora上搜如何成为数据科学家，我发现很多有经验的数据科学家他们都把“对数据的敏感和兴趣”作为数据科学家最重要的特征，而“了解各种算法模型并能应用到数据上”才是第二重要的。我当时不是太理解，我觉得后者才是更重要的。\n参赛感想 # 这次参赛算是我学习数据挖掘第一次实际的挖掘，以前学习各种算法模型都是准备的很好的数据，只要套上算法模型就能跑的很好。所以我一开始就拼命的去找类似的大赛，看看获胜者他们用的模型是什么。\n这几天我好像抱着一堆瓶子，拼命的想把巨大的石头（数据）塞进瓶口里，看起来工作量很大，流了很多汗，其实什么都没有干。今天在看一个类似的比赛选手答辩的时候的视频，突然明白自己好像走了一个死胡同。自己拼命的想这找一个合适的瓶子（模型），其实我更应该做的是把石头（数据）磨碎。\n模型本身不重要，他只是一个载体，更重要的是数据。 # 第一次参加这样大型比赛，有点激动也有点惶恐，如何将所学的应用到实际，还有在实际中提高自己还有待自己“挖掘”。虽然这个比赛奖金“丰富”，但是我觉得在这个比赛中得到的体会乐趣比奖金更诱人。\n比赛还有一个月，在这里立个小目标，争取跑到到前五页，我也会尽量抽时间把自己感想写出来。 未完待续。\n","date":"2017-02-17","externalUrl":null,"permalink":"/posts/ai/competition/%E5%A4%A9%E6%B1%A0%E5%A4%A7%E8%B5%9B-%E5%8F%A3%E7%A2%91%E9%A2%84%E6%B5%8B%E5%8F%82%E8%B5%9B%E6%84%9F%E6%83%B3/","section":"博客","summary":"第一次听说这个比赛还是去年在知乎上，当时也不知道这个比赛具体","title":"天池大赛-口碑预测参赛感想","type":"posts"},{"content":" 这是我从Quora上看到的一篇非常简短但详细的数据科学家的‘技能点’ 来自eBay的一个数据科学家的回答 翻译来自Quora回答\n这是面试谷歌、英特尔、脸书等大的世界五百强公司的数据科学家相关岗位常见的技术要求，在我看来主要有七点\n基本的编程基础 你应该了解一门统计学相关的编程语言，比如说R或Python（同时要了解Numpy和Pandas库），还要一门数据库查询语言比如SQL\n统计学 你应该要能解释零假设、P值、最大似然估计和置信空间这些短语，统计学在非常巨大的数据库里压缩数据和从挑选最重要的特征非常重要，在你得出结论和设计实验过程中也帮助巨大\n机器学习 你必须能够搞懂K-近邻、随机森林和集合方法等机器学习算法，这些算法基本上都在R或Python中得到实现，这些算法能告诉你雇主你能够将计算机科学运用在实际的管理中。\n数据重组 你应该要能够“清理”数据。比如数据库中\u0026quot;California\u0026quot; （加利福利亚）和“CA”是一样的，数据库里面可能出现用负值代表人口。这个总的来说就是识别坏（或者不正确）的数据然后校正（或删除)他们。\n数据可视化 数据科学家不能就只是自己搞懂就行，他们需要把他们发现告诉你的产品经理，这样就能确保数据能很好的应用到程序里面去。所以，熟悉数据可视化工具比如说ggplot非常重要（这样你就能展示你的数据而不是仅仅谈谈而已）\n软件工程 你应该了解算法和数据结构，因为这些东西在你写高效率的机器学习算法时非常重要，知道如何使用分支和使用高效的数据结构：队列、数组、列表、堆栈、树等等。\n产品管理 这个绝对是有争议的，但是那些了解产品的人将会知道什么指标是最重要的。这里有很多数据可以用来做A/B测试，但是产品导向的数据科学家将会把最好的指标用来做测试。你要知道这些的意思：可用性测试、线框、保留和转换率、流量分析、客户反馈、内部日志、A/B测试。\n","date":"2016-12-23","externalUrl":null,"permalink":"/posts/essays/programming/book/how-to-be-data-scientist/","section":"博客","summary":"这是我从Quora上看到的一篇非常简短但详细的数据科学家的‘","title":"怎么成为数据科学家（翻译）","type":"posts"},{"content":" 看过好几把关于机器学习的书,但是很多书只是停留于算法原理阶段,或者更着重介绍算法原理, \u0026laquo;集体智慧编程\u0026raquo;这本书更多的是从实践来介绍书,比如你要撘一个推荐系统你怎么做,,还有怎么来做一个垃圾邮件过滤系统等\u0026hellip;. 接下来介绍一下我对于做一个推荐系统的理解.\n前言 # 推荐系统是一个什么东西呢 简单来说就是两个字\u0026mdash;-推荐\n推荐系统的出现是伴随评价系统出现而出现的,评价系统就是我们的打分制,比如对一部电影每个人都对他进行打分\n用户 电影A评价 电影B评价 新电影C评价 新电影D评价 张三 5.0 3.5 4.9 2.0 李四 2.5 5.0 2.0 3.0 王二 3.1 4.7 比如现在三个人张三李四王二,王二有两部电影没有看过,但是张三和李四看过了,现在就是要把电影C或D推荐给王二,推荐系统就是从其他数据分析来确定推荐次序\n相似度 # 推荐系统如何根据其他用户的数据来推荐呢\n这里就要介绍一个相似度的概念,也可以加权系数,对于张三和李四,这两个人都看过电影C和D,但是我们该听谁的呢,我们可以把两个人的评价加起来然后排序,但是假如张三和李四品味相差太多,而且张三的评价可能会把某个王二喜欢看的电影评价总值拉低\n这个时候我们就要考虑一个相似度问题,就是尽量避免一些品味同王二不同的人对排名造成影响\n这里我们引进相似度这个概念,我们看张三和李四对电影A、B同王二的差别，这里我们使用距离这个概念，我们使用距离公式 l=（（r1^2 + r2^2+....rN^2)^1/n)\n然后通过距离算得S(相似度)=1/(1+l)\n当l=0时,两者相似度为1(最大),当l很大时,相似度几乎为零\n用户 电影A评价 电影B评价 相似度 张三 5.0 3.5 0.23 李四 2.5 5.0 0.59 通过求相似度(权重)我们就把同我们臭味相投的人的评价提高了,那些同我们不一样品味的人评价作用降低了\n选择 # 基于人物的评价推荐有个缺点就是人的评价有时候会比物品多,而且变化频繁,假如使用上面的方法的话,每次每一个用户评价过后,就要重新算一次相似度了,假如用户不多或者电影不多还好,一旦数据偏多服务器就扛不住了\n所以我们要换一个角度出发,计算物品的相似度,这样图表就变成这样了\n电影 用户-张三 用户-李四 相似度 A 5.0 2.5 B 3.5 5.0 这样有个好处就是可以离线处理数据,我们可以搭建一个离线处理系统,让一个系统专门处理数据,当服务器需要的时候再拿取过去.\n总结 # 搭建一个简单推荐系统并不会你想象中那么简单,但是要搭建一个功能强大速度快的推荐系统需要的不仅仅是这么一点,还需要考虑系统的稳定和速度.\n","date":"2016-09-17","externalUrl":null,"permalink":"/posts/ai/recommendations-programming-colletive-intelligence-impression/","section":"博客","summary":"看过好几把关于机器学习的书,但是很多书只是停留于算法原理阶段","title":"集体智慧编程之推荐系统(Programming Colletive Intelligence)","type":"posts"},{"content":" git是当今流行的版本控制工具,一般我们可能只要会push, pull就可以了, 但是当我们同别人共同工作的时候,我们必须要了解git协同开发的一些重要流程.\n前言 # git作为当今最流行的版本控制工具之一,当时开发出来就是为了管理Linux庞大源代码的分布式版本控制工具. 由于Linux源代码过于巨大,仅靠一个人的力量是完成不了的,那就必须把工作分配下去,然后将代码合并,所以git一开始设计的时候就是一种分布式的、多分支的\n概念 # 所以git最重要的就是分支这个性质,分支是什么呢.\n要了解分支必须要了解git工作原理.\ngit工作原理很简单就是add、commit，add、commit\u0026hellip;.,简单来说就是添加记录,添加记录,保存快照,添加记录,添加记录,保存快照\n如上图,随着master分支快照的一个一个建立,软件就慢慢的迭代下去了\n分支工作流程 # 接下来我们要着重讲一下分支,我们看到master分支的v0.1版本,我们已经开发出稳定的v0.1版,这时候我们决定开发一个新功能.\n在这里我们分了一个Develop分支,我们在Develop分支开发新代码.\n这时候我们发现v0,1的一个bug,假如是没有使用版本控制的话,一般人会停下手中的活,然后从当前的新代码处来修复这个bug,当这个bug很简单的时候,我们不会遇到很大困难,但是当bug藏的很深,而且新代码隐藏了这个bug,或者被这个bug影响,这时修复工作就变得很困难.\n还好我们有git,我们从v0.1直接分一个Hotfix分支,这两个分支的父都是v0.1\t,我们直接从稳定版本修复,不牵涉到新代码,这样修改好后我们就能很快从Develop分支继续工作了\n而且这样有一个好处我们将master和Develop分支合并的时候很大可能不会产生冲突.\n冲突(coflic)是什么了,怎么能避免呢?\n从两个分支的父亲v0.1看起,我们每次改动一个保存文件就会产生一个modify(修改)的动作,我们假如分支里面都对同一个文件产生了modify(修改)动作,当我们合并的时候这就是一个冲突,git无法理解采用哪个分支的modify动作,这时候就要你人工来修改采用哪个分支.\n假如没有相同的文件有modify(修改)动作,git就会聪明的知道采用每个分支的最新的modify(修改)修改出一份所以文件的最新版.\n那我们怎么来避免这个冲突呢,这就要求我们分支要分的合理,分支只要完成特定的工作,不要越俎代庖,那有些人会说我这个分支一定要改父的耦合地方否则我的代码工作不了,这时我们要好好思考自己的分支的功能,把合并耦合的代码放在主分支里面,次分支只要完成特定功能就可以了,这样合并分支时候不但可以安全的merge(合并)了, 而且修改bug的时候也可以对症下药,直接在问题开始的地方修改.\n","date":"2016-09-07","externalUrl":null,"permalink":"/posts/backend/software/git-working-streamline/","section":"博客","summary":"git是当今流行的版本控制工具,一般我们可能只要会push,","title":"git 工作流程","type":"posts"},{"content":" 在牛客网刷了16道题了,在这做个总结\n概况 # 编程题无非两种一种考算法,一种考数据结构\n算法的话,考验你对事情的分析程度和脑袋的灵光,用好的算法还是又大又重的算法,用算法复杂度来看,一般能到 o(n)就算勉强可以,当到了o(n * n)你就要考虑是不是你算法有问题了.\n数据结构的话,队列和链表和二叉树是比较常见的,当然有些奇怪的一般算法反倒很简单.\n算法 # 谈算法的话不得不谈递归了,递归其实我感觉更想一种思想\n话不多说请看题,斐波那契数列一直是递归的代表\n0 n= 1\tf(n) ={ 1 n= 2 f(n-1) + f(n-2) n\u0026gt;2 虽然\n","date":"2016-08-21","externalUrl":null,"permalink":"/posts/essays/programming/%E5%88%B7%E9%A2%98%E7%AC%94%E8%AE%B0/","section":"博客","summary":"在牛客网刷了16道题了,在这做个总结 概况 # 编程题无非两种一种","title":"刷题笔记","type":"posts"},{"content":"","date":"2016-08-21","externalUrl":null,"permalink":"/tags/tdd/","section":"Tags","summary":"","title":"TDD","type":"tags"},{"content":" 翻译自TDD-byexample 作者Kent Beck, Three Rivers Institute 有删减\n表现 # 测试驱动开发核心: # 除非你有失败的自动化测试千万不要写一行新代码 拒绝重复 这两个的简单原则构成了TDD的核心,但是他能规划一个复杂的项目乃至一个团队.这里有一些TDD的建议.\n你的项目设计不能太过全面,只要有一个模型或者相应的功能,然后你让你的测试代码测试你模型,通过反馈来完善你设计. 你必须自己写测试代码,你不能依靠别人来每天帮你修改无数次测试 你的开发环境必须能监控到代码的微小的变化 你测试代码必须要非常简单,复杂的测试代码说明你的程序有问题 根据核心我们总结了一种具体的测试方法:红绿重构法\n红 --- 写一段测试代码让他无法通过,有时候可以编译都通过不了 绿 --- 写尽可能少的代码让测试代码通过,通过后保存一下系统状态 重构 ----删掉所以重复的代码只要让测试代码还能通过 红绿重构法是TDD最高作战计划,他看起来很笼统,其实他具体到了每一行代码.\n如果我们按照TDD这种开发模式,我们会有什么好处呢\n如果我们新功能出了问题,质检部门能很快的将新代码回归到上一个稳定代码,尽快避免损失 如果代码的测试能将\u0026quot;惊喜\u0026quot;挖的差不多的话,项目经理能更准确的知道客户在使用过程中碰到的各种奇葩问题 如果我们的测试能够让各种技术交流变得更加清晰,我们能更快的进行技术排错 如果我们项目的bug更少的话,我们的项目能变得更加灵活,我们可以很轻松的添加新功能呈现给我们顾客 这些概念看起来很简单,但是我们的动机呢?为什么我要在写代码过程添加自动测试?为什么我要每次迈小小的一步在我能够做到更多的情况下?两个字:担忧\n担忧 # 测试驱动开发是一种管理你担忧的一种编程方式,担忧也不是说就是坏东西,自信过头也不好,但是恐惧给你一种在项目开头时,\u0026ldquo;我看不到这个项目的能够完成\u0026rdquo;,这种感觉,\n假如说痛给你一个\u0026quot;停下来\u0026quot;的信号,担忧给你一个\u0026quot;要认真\u0026quot;的提醒信号,但是与之而来的,担忧带来一些你消极的影响\n让你变得迟疑 让你开始抱怨 让你开始不愿交流 让你开始不想接受反馈 这些没有一个对你编程有帮助,尤其是当你在编一个比较复杂的软件的时候,所以你怎么来面对这些呢\n抛弃迟疑,学习快速有效的编程 不要拖沓,跟别人交流思路要清晰 不要拒绝反馈,去寻找更多有帮助的反馈 想象一下编程就是你提桶着水过河,当你水桶很小的时候,你轻微的震动没什么影响,但是当你的水桶很大,而且水很满的时候,你会很累,你无时无刻不在担心你的水是否会撒掉.\n这个时候你需要一个运水的管道,每当你用水桶打一点水,你可以把他放进管道,确保这点水安全到达对面,继续打水.\n这个测试驱动开发中的测试就是运水的管道,一旦你的测试通过,你就知道水已经送过去了,不需要担心水到不到对岸了,你就这样一步一步让所以的工作正常进行,但你测试失败的时候,专注于让他通过,这样下一个下一个,慢慢的我们接触到了编程难题,你的测试慢慢覆盖到整个项目.\nTDD给你一种控制的能力.当年在外面开车碰到下雪,你可以迅速停车去做其他琐事,当雪停了,你可以继续开车.\n所以很多人说他们使用TDD对于项目的变化更有控制力.\n接下来我会用一个例子来详细的介绍TDD开发的流程.\n由于原作者是用java来介绍的,本人用Python较多,所以就用自己写的一个项目sample来做介绍,详细链接 接下来翻译一下书后面关于TDD的一些答疑\nTDD答疑 # 我希望在这里提出一些或大或小问题帮助你思考如何将TDD引入到你的个人实践里面\n你的测试步伐到底该多大? # 这里有两个引申过来的问题\n每个测试覆盖范围该多大 当你重构时到底有要迈多大步伐 你的测试可以覆盖到你写的每一行代码和你每下一构,你的测试也可以覆盖你上百行代码和你几个小时后的重构,但是哪个才是我们该写的测试呢.\n从某方面来看,你应该要能做到其中一个测试,虽然TDD宗旨就是每一步都非常清晰,每一步伐都要求非常小,但是我们对软件驱动开发的经验会使这个步伐或多或少产生影响.\n当你开始重构的时候,你应该准备好将一个重构分成很多小的步伐.重构很有可能会发生错误,你把坑都踩一遍然后填上,然后你接下来重构的可能性就会小很多.一旦你完成每次用多个小步伐的一个重构,你可以试试遗漏一些步骤.\n你需要多少反馈 # ","date":"2016-08-21","externalUrl":null,"permalink":"/posts/essays/programming/tdd/tdd-byexample-transform/","section":"博客","summary":"翻译自TDD-byexample 作者Kent Beck, Three Rivers Institute 有删减 表","title":"用例子学TDD","type":"posts"},{"content":" 其实写这个项目的初衷是想实践一下TDD开发,因为自己刚看完一本\u0026lt;\u0026lt;Test-Driven Development with Python\u0026gt;\u0026gt;,以前只是了解一点开发测试,看完这本书感觉这种敏捷开发方式非常适合我,自己完整写过一些小项目,但是大项目经常由于各种代码框架,代码规模搞得最后成了烂项目,而且关于TTD一些建议比如YAGNI(You ain't gonna need it)(你不需要这个)对于你对项目的规模有一定的控制.写这篇博客一方面将我开发hookman 的TDD开发经历告诉大家,一方面希望更多人了解TDD开发,换一种开发方式,或许能让你找回编程的乐趣.\n##引言\n当然网上有些人对TDD测试开发嗤之以鼻,认为开发过程中不应该由测试驱动,应该先把所有的核心都先完善,最后在来完成项目.\n在这里我想说一点我的看法,测试驱动是一种从核心到细节的开发方式.\n用画一个人来打比方,给你一张白纸,TDD要求你先画一个躯干,这个人是人首先要有个人样,当我们想画一个奔跑的人时,TDD要求你得给他再画两条裸腿,然后这两条腿怎么摆才能控制平衡,这两条腿穿什么鞋才比较生动,要接下来你慢慢测试考虑.\n而我们传统的开发方式,是先从局部到整体,这种开发方式适合于小项目,而且像一个流水线出来的产物,比如说web开发,我们知道下一个要做的是视图层,然后模型层.但是这种开发方式不适合开发比如说一个新的软件,假如我们用传统开发方式,除非leader的掌控力非常好,一个越来越臃肿的项目很难坚持到最后,因为我们在项目完成的时候才能让项目真正的运行起来,在完成的过程中我们很容易迷失最后将项目烂掉.\n而且我比较欣赏TDD开发的一个主要原因就是他同linux推崇的那种简单的软件开发文化很默契,我们在让我们测试通过时候,我们感觉在:欺骗自己\u0026quot;.\n举个例子:\n比如我有一个比较两个值谁大谁小的函数\ndef my_max(a, b): pass 我们写一个单元测试\nMytest(unittest.TestCase): test_my_max(self): m = my_max(1,2) self.aseertEqual(m, 2) 我们运行这个测试肯定失败,然后我们开始\u0026quot;欺骗自己\u0026quot;让测试通过,修改my_max\ndef my_max(a, b): return b 再运行测试,通过了,ok,\u0026ldquo;欺骗成功\u0026rdquo;,但我们用脑袋一想就知道,不行这个代码不对,但是咋办\u0026quot;测试通过了\u0026quot;,修改测试.\nMytest(unittest.TestCase): test_my_max(self): m = my_max(1,2) self.assertEqual(m, 2) n = my_max(2, 1) self.assertEqual(n, 2) 这下测试又通不过了,我们想要的功能就在一步一步测试拖动中慢慢实现,我们尽量用最简单的代码通过我们的测试.\n看到这里有些人会认为我很傻,明明可以用两行代码(不能用系统函数max)\ndef my_max(a, b): if a \u0026gt; b: return a return b 轻轻松松通过测试,但是这反倒是TDD非常不推崇的,你一次走了太多步,你让你的代码跑到Test前面去了,其实很多程序员都会写测试,但是他们的测试很多都是基于代码的,当一个大功能实现的时候才开始测试,这个时候你会发现很多你的隐藏bug就藏在你一大堆代码里面,所以有些人为了排bug一行一行删代码来debug.\n诚然很多时候我们感觉我们能毫无bug的完成一大段的代码,但是我们不能保证100%正确,而且或许今天能行明天就不行了,TDD推崇测试山羊精神,想象一只山羊行走在陡崖峭壁上,他只能走一步停一步,我们写代码也一样,我们前面是陡崖峭壁,我们不能保证我们下一步就不掉坑里,所以我们要学习山羊精神一步一步.\n接下来关于TDD开发一些详细经历,我会每天抽出一点时候写.\n","date":"2016-08-18","externalUrl":null,"permalink":"/opensource/hookman-development-blog/","section":"开源","summary":"其实写这个项目的初衷是想实践一下TDD开发,因为自己刚看完一","title":"hookman develpment blog","type":"opensource"},{"content":" mrzhangboss/hookman Python 0 0 hookman 是基于github上的webhooks开发的一个用Python写的小程序 ,基于TDD开发. 如果你想向项目贡献代码,请看hookman 开发blog\n简单用法 # 安装 pip install hookman 配置github的webhooks,设置监听服务器ip和端口3610(默认) 使用 3.1 进入github项目目录cd /my/github/projectdir 3.2 运行 hookman --run -d 关闭 3.1 hookman --stop 版本差异 # v0.1.0 # 基本实现监听端口 添加 run,stop,daemon,pidfile,logfile,projectdir选项 v0.1.1 # 修复了在前台shell模式下的bug\n","date":"2016-08-18","externalUrl":null,"permalink":"/opensource/hookman-development-notebook/","section":"开源","summary":"mrzhangboss/hookman Python 0 0 hookman 是基于github上的webhooks开发的一个用","title":"hookman development notebook","type":"opensource"},{"content":" 隔离测试是相对与于整合测试来说的，现代软件架构流行分层式、模块化，而隔离测试就是相当于在每层上进行测试，整合测试就是跨越多个层进行测试\nintroduction # 举个简单例子来说，在django系列中，我们把表单提交分成两个层，一个form层，一个式model层\nform层相当于接近用户交互层，而model层与数据库联系更大， form层负责获取用户数据并验证，而model层根据form层数据将数据存入数据库。\n隔离测试就是隔离form层向model层提交，而整合测试就是直接测试form层和model层。\n判断一个测试是整合测试还是隔离测试就是看测试的边界，整合测试相当于我们更加熟悉，我们测试时通过伪造form提交，然后通过数据库获取存入数据来得到验证，而隔离测试就比较复杂，因为我们很难在一个耦合度高代码找到怎么隔离两个层的方法。\n接下来我就介绍python里一个神器：mock\n自己用过其他语言框架中的mock，但是python里面mock里面最神奇的的是里面的patch，就像一个超级补丁一样。\nExample # 接下来我用一个例子来介绍一下如何写隔离测试\n首先我们在 lists.forms.py中一个表单model\nclass ListForm(forms.models.ModelForm): def save(self): pass 我们在lists.models.py有个model\nclass List(models.Model): def create_now(): pass 现在来分析一下这个隔离测试，我们要测试ListForm中save方法\n首先ListForm和List两个类是耦合的，一个整合测试，我们只要调用save方法，然后查询数据库就可以完成这个测试，然而隔离测试不同，我们只能测试在调用save方法时，他“干”了什么。他可能调用了List的create_now方法，将得到表单数据传了过去。\n意味着我们只能测试到save方法调用了List方法。\n那这个隔离单元测试该怎么写？\n接下来我们隆重介绍mock里面的最强补丁patch\n根据TDD原则，我们先新建一个单元测试IsolutionFormTest\nimport unittest from unittest.mock import patch, Mock ##load super patch from lists.models import List from lists.forms import ListForm class IsolutionFormTest(unitest.TestCase): @patch('lists.forms.List.create_new') def test_save_creates_new_list(self, mock_list_create_new): form = ListForm(data={'text': \u0026quot;example text\u0026quot;}) form.is_valid() # get clean data form.save() mock_list_create_new.assert_called_once_with( text= \u0026quot;example text\u0026quot; ) # the major test 我来介绍一下这个patch，就像名字一样补丁，通过我们使用字符串将要替换的函数写出来，当test运行时，会自动将函数替换成一个mock对象，通过参数（上面的mock_list_create_new）赋给函数。\n你可以这样想象，当form.save()调用时，在save函数里面，我们如果使用了lists.forms.List.create_new这个函数，这个函数就会被直接被补丁替换掉，你如果使用了lists.forms.List.create_new(text=\u0026quot;xxx\u0026quot;)就会变成mock_list_create_new(text=\u0026quot;xxx\u0026quot;),当你调用了mock_list_create_new(text=\u0026quot;xxx\u0026quot;)时，mock_list_create_new这个mock对象就会记录下来。\n这样我们就通过mock_list_create_new测试了函数是否执行了没有，因为隔离开form层和model层的就是通过两者之间的接口。我们只用测试接口是否执行了没有就可以了。\n这样我们就完成了隔离单元测试，运行一下肯定失败，我们接下来就把save方法完善一下通过测试。\nfrom django import form form list.models import List class ListForm(forms.models.ModelForm): def save(self): List.create_new(text=self.cleaned_data['text']) ok测试通过了，我们就可以歇一口气了。\nConclusion # 相对于整合测试，隔离测试运行速度更快，但是相对的隔离测试对接口要求非常严格，好的方面利用我们进行更好的代码设计，更好的分析代码的复杂程度，并且当接口变迁的时候，隔离测试能迅速发现变化而报警，然而隔离测试工作量比较大，而且没有整合测试那么好理解。在实际生产中对于复杂的接口我们尽量进行隔离测试，对于简单接口我们使用整合测试能根据减少程序的耦合性，而且能迅速发现集成问题。\nps：对patch感兴趣的童鞋可以自行google，patch好玩的地方还有很多，这里为了篇幅我只介绍了最核心的使用方法，大家可以自行探索patch更多好玩的东西。s\n","date":"2016-08-09","externalUrl":null,"permalink":"/posts/essays/programming/tdd/tdd-isolution-test/","section":"博客","summary":"隔离测试是相对与于整合测试来说的，现代软件架构流行分层式、模","title":"TDD-隔离测试","type":"posts"},{"content":" 看了 \u0026laquo; Python Web 开发 测试驱动方法\u0026raquo; 以后, 感觉自己找到自己的一些项目之所以不能够很好的 维护下去的原因.总结了一下自己的理解, 记录下来.\n什么是 TDD # Test-Driven Development\nTDD 是一种以测试为驱动开发的方法, 自己以前也听到过这个名词, 但是平常只是稍微写一点测试,并没有让测试领导开发节奏.\n简单来说TDD是先写测试再写代码, 这于我们平时开发的时候有很大不同,比如我们写一个web页面, 我们一般是先写好代码,然后在浏览器上调试(相当于测试),然而测试驱动开发就不同了,我们一开始只写测试, 然后写代码让他通过我们的测试.\nTDD的主题基本上很多人都知道,但是具体步骤和诀窍很多人不明就里.下面来详细介绍怎么让Test 引导你的开发.\n怎么进行TDD # 首先让我来看一看Test是什么, Test 分两种\n功能测试 单元测试 功能测试 # 很多人知道单元测试却很少听到功能测试, 功能测试在开发过程中经常被人忽略,但是功能测试能很好的把握开发方向\n功能测试就是从用户角度出发, 从用户的角度测试代码.\n用web项目开发来打比方, 用户只能通过浏览器来浏览你的web, 所有的交互只能通过浏览器来实现,功能测试就是模拟用户进行浏览器上的操作,具体来说我们可用使用selenium来操控firefox或chrome,通过我们使用selenium提供的接口操作浏览器访问页面获取html, 来进行功能测试.\n功能测试就是测试一个项目成品的功能, 在平常的项目开发中,这个测试往往是提到项目完成之后人工进行, 然而在TDD中这个却被提到了最前面,他成了一个风向标,所有的代码目的都是为了实现这个功能测试.\n单元测试 # 单元测试这个很多人都很了解, python里面有unittest 这个单元测试框架.就是专门为单元测试而开发的.\n单元测试历史悠久,很多人都写过单元测试,不过大部分人写单元测试都是在函数或者类完成之后写的.\n在TDD中,单元测试进行最频繁的测试, 在功能测试完, 写每一个函数都提倡先写单元测试, 然后进行开发.\n单元测试关注点与功能测试不同, 单元测试注重的是每一个函数执行的结果, 给定一个输入就一定要得到一个确定的输出,比功能测试他更关注底层代码,毕竟功能测试只关注用户最后得到结果,单元测试将你的代码函数形成一个单元,逐个运行,逐个测试.\n前面介绍了一大堆概念,却没有落到实处, 功能测试很简单,我们评价一个项目,能很快的写出测试方法,但是对于单元测试来说,我到底该怎么进行,这同我平时的开发有什么不同,下面就详细的介绍单元测试的几个重要的要点.\n留个坑慢慢填.\n","date":"2016-07-26","externalUrl":null,"permalink":"/posts/essays/programming/tdd/tdd%E6%B5%8B%E8%AF%95%E5%BC%80%E5%8F%91%E7%90%86%E8%A7%A3/","section":"博客","summary":"看了 \u0026laquo; Python Web 开发 测试驱动方法\u0026raquo; 以后, 感觉自己找到自","title":"TDD测试开发理解","type":"posts"},{"content":" 最近这几天在开发一个hmtl5的游戏, 但是对于js怎么使用面对对象来编程有点困惑,查了一些资料 整理如下.\njs的this用法 # 非对象属性函数(内部函数)\nvar point = { x : 0, y : 0, moveTo : function(x, y) { // 内部函数 var moveX = function(x) { this.x = x;//this 绑定到了哪里？ }; // 内部函数 var moveY = function(y) { this.y = y;//this 绑定到了哪里？ }; moveX(x); moveY(y); } }; point.moveTo(1, 1); point.x; //==\u0026gt;0 point.y; //==\u0026gt;0 x; //==\u0026gt;1 y; //==\u0026gt;1 对象属性函数\nvar point = { x : 0, y : 0, moveTo : function(x, y) { this.x = this.x + x; this.y = this.y + y; } }; point.x; // ==\u0026gt; 1 point.y; // == \u0026gt;1 point.moveTo(1, 1)//this 绑定到当前对象，即 point 对象 point.x; // ==\u0026gt; 1 point.y; // == \u0026gt;1 其实this的用法还有很多,我为什么只列出上面两种是因为所有的this用法都可以归为这两类.\n非对象属性就是这个函数不存在对象的内存空间里面的函数 对象属性就是说这个函数存在于对象内存空间的函数\n这样说很绕, 抽象的来说, 把每个函数都看做是对象, 我们把这个对象point想象成一个上锁的柜子A, 每个变量名就是一把钥匙, 我们这个柜子A里面可用放很多钥匙,每个钥匙又对应着其他柜子,我们也可以放东西,但是这些东西只能是一些数字字符串什么的,这就对应着对象的这种继承性,\n现在我们来看放在柜子A里面的钥匙,有一个钥匙可用开其他某一个的柜子B(相当于对象的属性的moveTo函数),当我们就在柜子B里面使用this时,这个this是什么呢.\n这个this就要找一个柜子, 那为什么要找一个柜子(对象呢)\n这就是js语言设计的松散, 当在非严格模式下, this 会被强制转换成一个对象, 对于例子一,因为内部函数 this 并没有给他赋值(你可以把他看做一个我们找不到他的钥匙的柜子), 所以this被强制转换成了全局的柜子(全局变量)\nps: 严格模式在函数或变量前加上 'use strict'; # 怎么解决这个问题了, 有两种方法, 不是没有赋值给this嘛, 我们可用call来给函数内部的this赋值,\nmoveX.call(this, x); // 我们把当前函数的this赋给内部函数 但是这种方法不够优美, 我们还可以直接把 this用个变量that给内部函数用,\nvar that = this; var moveY = function(y) { that.y = y;//this }; 这样这个this 就不会被强制转换成全局变量了, 当然你可以开启严格模式, 这样this的话就会变成undefined, 你也不会因为你的代码问题而污染全部变量.\n总结 # js 的 this 是面对对象编程的一种体现, 但是js的this由于有点不严格,所以有时候会出现一些令人意向不到的结果,\n引用\n深入浅出 JavaScript 中的 this\n严格模式\n","date":"2016-07-20","externalUrl":null,"permalink":"/posts/essays/programming/js%E7%9A%84this%E5%BC%95%E5%8F%91%E7%9A%84%E6%80%9D%E8%80%83/","section":"博客","summary":"最近这几天在开发一个hmtl5的游戏, 但是对于js怎么使用面","title":"js的this引发的思考","type":"posts"},{"content":" 这段时间重新学习了web前端的技术栈,其实一开始加入社团时就是从前端开始入门的,等到了会仿写几个页面后就跳到了后端的坑,从html、css、js（只学了一点）又马不停蹄的转到.net平台、后来在老司机的带领下我们放弃了不开放的.net平台投入了python的怀抱\npython的确很好，“内裤”很多，相对于.net 相对于封闭的生态圈， python对第三方的类库的依赖很大，pip 是程序的常客，看到一个好的类库就pip下来了。\n学了不少程序语言，从静态语言到动态语言，这其中的转变刚开始的确让人很苦闷。\n自己从c系的语言过来，刚接触C的时候对于“过程编程”有很深的体会，结果并不重要，重要的是过程，有时候为了写好一个完美的函数自己苦思冥想好几天，花在一个函数上的时间比项目的时间都多。\n然而到了学习python, 自己把C系的学习习惯带到python , 在做项目的过程中,当自己写完一个函数的时候总在想怎么优化代码,怎么抽象化对象让事情简单,慢慢的走入一个误区,让我忘记了python的口号 life is short, I use python .\npython是一门目的性很强的语言, 先让我实现功能,其他以后再说, 我以前使用的时候对过程强调的太多了, 一个函数我得测试测试很多遍,确定每个参数的作用还有影响, 花了太多时间, 适得其反,我的代码一点都不 pythonic # 我在学习 python的过程中走了很多误区,总结起来有几点:\n太注重过程,不注重结果 太注重功能丰富,不注重简单 在我看来, pythonic 就是用最简短清晰的代码最快的完成自己的目的 # python和javascript # 这个都是动态的脚本语言,javascript更倾向于脚本\n两个家伙在我看来都是鸭式语言, 当然js更倾向函数式,而python更倾向于对象式.\npython 是目的性强, javascrip 是表现力强 # 以前在后端的时候总觉得只要把自己那端的语言学好就行了, 任何事python都能用来解决, 无论是数据库还是服务器,就算是图像都能用python处理(使用PIL),虽然有着GIL,有这性能低下等等的问题 . 颇有一番\u0026quot;学好数理化走遍全天下都不怕\u0026quot;的念头,\n然而在实际项目中慢慢发现, 计算我能用几百行python代码画出一朵花来,用js几句话就在canvas上弄出来了,而且很轻松就能换成其他的东西.\n得益于node.js社区的火热, 现在javascript也能在后端大显身手了, 学习了一点node.js,感觉通过node.js对js的封装,让javascript变成了一门类python的语言, 原本js文件只能运行在浏览器里面,通过node.js将每个文件封装成模块,就像python将每个文件封装成package,通过模块与模块的合作,js也能像python那样简单的用几行代码就构成一个强壮的服务器(虽然说现在node.js有点不稳定,但是随着node社区的努力也能将node.js强壮起来)\n总结 # 通过一个星期的重温js, 自己简单的涉猎了现在很火的Angular,Ionic.和Node.js,并简单的搭建 Electron + Cordova + Ionic + Angular 原来博客 的 Desktop端 (linux + windows + OS)的软件版,还有移动版(由于本人没有OS操作系统,只做了Android版),还花了半天时间搭建了这个基于hexo的博客,算是完成了全平台的搭建\n接下来我会自己工作前的大学时光好好的培养自己解决问题的能力,希望能在工作前爱上并享受自己将要做的工作!\n","date":"2016-07-19","externalUrl":null,"permalink":"/posts/essays/life/%E5%85%B3%E4%BA%8E%E6%8A%80%E6%9C%AF%E6%A0%88%E7%9A%84%E6%80%BB%E7%BB%93/","section":"博客","summary":"这段时间重新学习了web前端的技术栈,其实一开始加入社团时就","title":"关于技术栈的总结","type":"posts"},{"content":" 先谈谈我对机器学习的理解 # 什么是机器学习? # 我们人类有从婴儿开始就开始学习,父母教我们穿衣吃饭、老师教我们读书写字，我们开始能辨别好人坏人，开始通过自己的经验来判断新事物。\n机器学习很简单，就像人一样，我们教机器通过我们教的来判断新的事物，或者在从新的事物里面学习处理新的事物。 # 这看起来很复杂的样子,但是从我们神经网络来看,我们可以把学习当做建立一个神经元连接,通过输入的信号得到一个输出的信号.我们只要简单的把输入的信号分类就可以了.通过无数个分类我们就可以建立复杂的神经系统,进而实现\u0026rsquo;学习\u0026rsquo;这个功能.\n如何分类? # 涉及到分类,假如输入的信号种类只要两种,我们就可以简单用if-else来实现分类功能,但是有时候输入信号种类个个都有细微的差别,只是遵循某种规律,这时候我们不能用简单的if-else来进行分类了,下面我就按照书的顺序来解释各种强大的分类方法.\nK-近邻算法 ( k-Nearest Neighbor ) # 作为本书的第一个机器学习算法,K-NN算是我感觉原理最简单的一个了.\n假设我们有两个点, 红点为(-1, -1)分为红类, 绿点为(1, 1)分为绿类 接下来我一个点(0, -1),这个点应该分为红还是绿呢,我们添加两条辅助线 # 蓝点离红点距离为1,蓝点离绿点距离为2.2,我们很轻松的可以知道这个点应该分为红类. 现在我们进一步推广,当有很多种类点的时候,当我们二维扩展到N维,给一个点a我们只要选取距离a最近的K个种类,我们就基本能判断他属于这K个种类的,这就是K-近邻的原理了.\nK-近邻算法是最简单最有效的算法了,但是他也有缺点,比如他必须保存所有训练样本的数据,当训练样本很大的时候就会占用很多内存空间,我们后面会学到的KVM只取支持向量的训练样本来计算可以减少很多占用内存 # 而且K-近邻算法对训练数据集都要计算距离值,实际使用可能会非常耗时,我们后面学到的logistic回归能很好解决这个问题. # ######### 总而言之,KNN作为小样本时非常简单粗暴,但是他无法给出任何数据的基本结构信息.接下来我们要学习用概率测量解决分类问题,这个算法能解决这个问题\n(决策树)[]\n","date":"2016-07-18","externalUrl":null,"permalink":"/posts/ai/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%AE%9E%E6%88%98%E5%BF%83%E5%BE%97/","section":"博客","summary":"先谈谈我对机器学习的理解 # 什么是机器学习? # 我们人类有从婴儿","title":"\u003c\u003c机器学习实战\u003e\u003e心得.","type":"posts"},{"content":" GitHub推出一个对学生和教师的福利包,对于学生来说这是一个不小 的福利,只要通过一个edu邮箱就可以领取,但奈何国内有些无良人买卖 邮箱,所以GitHub对于.cn的邮箱一律拒绝,但是可以通过上传学生证的方法 得到验证,题主刚开始用学校邮箱试了试,失败了,抱着试一试的心态,上传了 学生证,没想到第二天就给我回复,并给我这个豪华大礼包,接下来我就介绍介绍 如何用这个包来.\n有些人在网上说,上传学生证没有用,可能是那个plan(GitHub会叫你写一点你想用GitHub做什么)用的是中文写的,最好用英文写-_-,回复的会快一点. # Digital Ocean \u0026mdash;VPS 50刀 # 以前貌似是100刀,现在缩水一半了,不知道为什么.\nDigitalOcean是一家以优质的VPS服务器著名,毕竟用SSD做存贮的服务器商没几家. # 这个是包小时的我们可以最便宜的5刀每月,提供20GSSD,1TB流量,我们可以用它来搭建服务器或者搭建一个shadowsocks服务器,安装shadowsocks很简单,但是怎么得到这50刀就要花点时间了. # 你要是想得到这50刀必须先充值5刀,但是怎么给钱有是个问题,真是有钱也花出去啊.你可以选择绑卡,但是很复杂不一定能绑的上,最好的方式使用PayPal付这5刀,PayPal可以绑定银联卡付款,但是这个PayPal注册又是个问题,当时题主注册的时候一直提示服务器故障. # 当时去上网搜了搜,中国大陆是有这个情况,可以通过贝宝(PayPal在中国的分公司)来注册 # 提醒一句,绑银联卡的时候最好用IE来绑定,别问我为什么谷歌浏览器不行\u0026mdash;- # 选择VPS的时候推荐San Francisco,延迟最低. # 搭建shadowsocks可以参考这篇博客 # NameCheap # ME域名一个（一年，价值8.99刀）PositiveSSL一个 （一年，价值9刀） # 这个要想得到域名必须通过邮箱验证还好NameCheape承认.edu.cn邮箱\n提醒一下,通过DVC验证的时候选择邮箱验证就够了.虽然不是你的邮箱但是会把资料发到你提供的邮箱 # 可以参考这篇博客搭建你的https网站,本站也是采用这种方法搭建的.但是有一点不同的时,现在NameCheap直接发给我一个.crt文件和.ca-bundle文件(用于Apache),所以把.key文件和.crt文件放到服务器上配置一下就好了. # GitHub Micro account 7刀/month # 这个不错我们可以有五个私有项目,一直可以用到你毕业.\n这个不错哦,妈妈再也不怕我写的stupid代码被人看到了 O(∩_∩)O哈哈~.\n","date":"2016-07-18","externalUrl":null,"permalink":"/posts/backend/software/githubeducationpack/","section":"博客","summary":"GitHub推出一个对学生和教师的福利包,对于学生来说这是一","title":"GitHub Education Pack","type":"posts"},{"content":"numpy简单来说就是python的C版数组实现,因为python原生列表虽然好使,但是生成大量数据时开销很大,而numpy是基于C的,生成大量数组非常简单,而且操作他们速度非常快.\n由于numpy是基于C的,所以numpy是一种强类型的,当然numpy是可以判断数组里面是数据类型,但是我们可以显示声明他们,dtype是声明的参数,一般我们通过下面的方式简单声明一个narray # import numpy as np arr = np.array([1, 2, 3], dtype=np.int32) numpy还有一个强大的地方是多维数组,numpy对多维数组的支持很好.只要简单的使用嵌套序列就能被转化成多维数组. 比如 arr2 = np.array([[1, 2], [3, 4]])\nnumpy另一个强大的地方是矢量化,这对于科学计算来说非常有用 # 比如前面的arr, 我们可以简单使用\narr3 = arr * arr 得到另外一个序列 [1, 4, 9],数组间的运算应用到了元素级.\nnumpy之所以成为数据分析的基本数据结构,还在用取数据的灵活性 # 对于一维数组来说,python自身的列表就支持切片处理,numpy不仅支持切片处理,还支持列表取出,比如:\n\u0026gt;\u0026gt;\u0026gt; num = np.arange(10) \u0026gt;\u0026gt;\u0026gt; num[[3, 1, 0]] array([3, 1, 0]) 在一维数组里面这个并没有什么优势,因为我们可以通过数据简单一个构造器[num[x] for x in [3, 1, 0]]构造出来. # 当但在多维数组我们使用构造器非常繁琐了, numpy使用了很多技术使我们很方便的取出多维数组\n我们先创建一个多维数组 # arr = np.arange(32).reshape((8, 4)) 生成的arr是\narray([[ 0, 1, 2, 3], [ 4, 5, 6, 7], [ 8, 9, 10, 11], [12, 13, 14, 15], [16, 17, 18, 19], [20, 21, 22, 23], [24, 25, 26, 27], [28, 29, 30, 31]]) 现在像一维数组一样的取出数据\nIn[13]: arr[[0, 3]] Out[13]: array([[ 0, 1, 2, 3], [12, 13, 14, 15]]) 我们取出了一个二维数组\n试试用这个 # In[14]: arr[arr \u0026gt; 8] Out[14]: array([ 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31]) 我们取出一个一维数组,这个arr \u0026gt; 8 是也是一个二维数组对应原来数组的每个位置都有一个布尔值代替,当numpy判断得到是一个array数组时会广播每个值来判断是否获取,这个可比你用for循环快了很多.\narray([[False, False, False, False], [False, False, False, False], [False, True, True, True], [ True, True, True, True], [ True, True, True, True], [ True, True, True, True], [ True, True, True, True], [ True, True, True, True]], dtype=bool) numpy很善于处理不同的选择,当你直接给一个数组时,如上面的arr[[0, 3, 4]],它默认第二维为全部选择也就是想当与arr[[0, 3, 4], :],当你给确定的值时,他就会在第二维上取响应的值,比如 # arr[[0, 3, 4], 1] 或 arr[[0, 3, 4], [1, 1, 1]] 从上面你可以看到,如果第二维你每个都想取第二个,你可以直接写一个整数就行,numpy会广播过去,假如想你分别再第一维的每个上分别对应取哪个你就可以用数组来分别选择.\n有时候我们想在二维数组上面取出一个矩形块,直接使用 arr[[0, 3, 4], [1, 2, 3]]只能取出二维数组矩形块的对角线,我们这时候就可以先取出第一维的矩形列,然后再在取出的列中取出矩形行 # arr[[0, 3, 4]][:, [1, 2, 3]] 当然我们还可以用二维数组来取出数据 arr[[[0], [3], [4]], [[1, 2, 3]]]\n[[0], [3], [4]]代表第一维的1, 4, 5列, [[1, 2, 3]]代表第二维的2, 3, 4行.\nnumpy给我们提供一个函数将一维数组转换成二维数组我们可以简单使用 # np.ix_([0, 3, 4], [1, 2, 3]) 生成二维数组,这样我们使用\narr[np.ix_([0, 3, 4], [1, 2, 3])] 就可以取出矩形块了.\n","date":"2016-07-18","externalUrl":null,"permalink":"/posts/backend/framework/python/numpy%E7%9A%84%E7%AE%80%E6%9E%90/","section":"博客","summary":"numpy简单来说就是python的C版数组实现,因为pyt","title":"Numpy的简析","type":"posts"},{"content":"Pillow 是 PIL的对Python3支持的另外一个分支，当然他对Python2也兼容，由于PIL安装起来比较烦，而使用pip可以很轻松的安装Pillow，所以我选择Pillow使用，但是其核心还是PIL库的。\nPython的图形处理库如PIL一直很强大，但是要想使用好它必须对图片有一定的知识储备。 使用起来很简单\nfrom PIL import Image 引用Image包\nim = Image.open('1.png') 打开图片，得到一个im对象，我们接下来就可以对这个对象进行操作（前提有这个1.png图片）\n我们先看一下他的一些属性\n\u0026gt;\u0026gt;\u0026gt; print im.format, im.size, im.mode PNG (83, 81) RGB 第一个我们输出图片的格式，图片有很多种格式，常用的有jpg、png还有gif动图啊，PIL支持很多种格式，我们可以使用PIL轻松的将格式转换，im.save('1.jpg'),当然你可以选择格式假如你没选好后缀名的话，im.size就是图片大小，他返回的是一个元组第一个长度第二个是宽度，单位是像素。 现在就谈谈 这三个属性对应的关系吧 首先我们使用一张像素图来说吧\n我们存贮图片的时候是将整个图像分成很多个相同的小方块，每个小方块我们称为像素，当然一张图片分的越小，像素越多，那么图片就越接近真实图片，上面的im.size属性就告诉我们，这张图片分成了，长为83px，宽为81px的图片，那么一共有83*81=6723个像素点，每个像素点里面存什么呢，这就是im.mode属性告诉我们的，贴一下属性有什么吧\n1 (1-bit pixels, black and white, stored with one pixel per byte) L (8-bit pixels, black and white) P (8-bit pixels, mapped to any other mode using a color palette) RGB (3x8-bit pixels, true color) RGBA (4x8-bit pixels, true color with transparency mask) CMYK (4x8-bit pixels, color separation) YCbCr (3x8-bit pixels, color video format) I (32-bit signed integer pixels) F (32-bit floating point pixels) 像素存贮就是涉及到颜色的存贮，在早期的黑白游戏机，只有黑和白两种，那么每个像素点就只有1位颜色来存贮，1位只能存贮两种颜色，八位色就能存256种颜色，像八位我们能用256个油漆桶/256色调色板来形容，像上面我们使用的RGB是由三种三原色红绿蓝混合而成，我们知道大自然所有的颜色都可以用红绿蓝三种颜色调配出来，所以RGB又被称为真彩（true color），每种颜色我们都分成256种，所以我们一共有256256256=16777216种颜色可以调配，像素的其他模式我们不介绍太多，有兴趣的可以自己钻研。 那么我们知道每个像素占多少字节，又知道共有多少个像素，那我们是不是就可以直接计算出来图片大小，来验证一下\n以第一张图片为例，共有8381=6723个像素点，用RGB模式，每个像素三个字节，共有67233=20667b=20kb，但是我这张图片只有11.6kb，误差太大了吧，这时候我们就要介绍一下上面那个im.format属性了，这张图片采用png格式，我们先尝试一下把他转成JPG格式吧\nim.save('1.jpg') 我们再查看一下这个1.jpg的大小，只有2.24kb了，我们用PIL打开这张图片\n\u0026gt;\u0026gt;\u0026gt; im2 = Image.open('1.jpg') \u0026gt;\u0026gt;\u0026gt; print im2.format, im2.size, im2.mode JPEG (83, 81) RGB 图片大小没有改变，但是format变成了JPEG，而且文件大小变成原来的1/5, JPEG和GIF和PNG是三种图片压缩技术，他们使用压缩算法把图片压缩成很小，当我们打开图片时，解密算法把他还原出来，所以我们算出来的大小与压缩后的大小是不一样的。 有了这些概念我们就能更好的使用PIL提供给我们的magic方法，下次在谈我对PIL的高级应用吧。\n","date":"2016-07-18","externalUrl":null,"permalink":"/posts/backend/framework/python/pil/","section":"博客","summary":"Pillow 是 PIL的对Python3支持的另外一个分支，当然他对Py","title":"PIL （Pillow）","type":"posts"},{"content":" # 最近学了两个python库，一个负责管理线程，一个负责管理进程，原来一直写的都 是些单线程的程序，虽然web也关于并发和多涉及到线程，但都是框架管理的，学习\u0026gt;过后发现了解线程和进程对python的web开发也有一定帮助。下面先谈谈这对python对线程和进程的支持再谈谈对这两个库的应用。\npython对线程的支持并不是非常好，所以你可以在很多文章上批评python的多线程的弊端，但是为什么python对多线程支持不好呢，为什么其他语言比如\n静态语言没有这个弊端呢。 # 首先我们要知道python是一种解释性语言，每段代码都需要解释器编译运行，解释器有很多种最主要的是CPython，其他还有IronPython和Jython，官方的是CPython解释器，我们一般说对多线程支持不好的就是说的CPython解释器（用的人最多就省略成python解释器),python解释器为什么对多线程支持不好呢，是因为GIL的存在，当然这个存在就是因为这门语言的的特性产生的。\nGIL是什么呢，下面是官方的解释\nIn CPython, the global interpreter lock, or GIL, is a mutex that prevents multiple native threads from executing Python bytecodes at once. This lock is necessary mainly because CPython’s memory management is not thread-safe. (However, since the GIL exists, other features have grown to depend on the guarantees that it enforces.)\n就是GIL是python的互斥锁，简单的理解就是代码会锁住python解释器。理解代码的锁定是什么必须要先了解什么是多线程\n多线程表示一个主线程，多个子线程，主线程是程序执行时系统自动给你申请的一个线程，而子线程我们可以理解为一个代码块，我们可以充分利用硬件的支持比如说多核，让一个CPU执行主线程，其他CPU执行子线程，通过操作系统的虚拟内存技术让所有线程共享相同代码空间达到提高代码效率的作用，我们可以通俗的把一个进程比作一辆火车，车厢头为主线程，每节车厢为子线程，只要你车厢(子线程)越多，你运的货物也越多，但是也要考虑硬件的方面，\n了解完多线程是什么我们就可以解释GIL对多核CPU工作性能的影响了，在单核CPU里面，主线程在释放GIL的时候，把CPU让给子线程，子线程代码块得到GIL，然后执行，这样就能充分利用CPU，这个GIL对单核性能的发挥没有影响，能得到100%的利用，但是在多核的的时候就有问题了，假如主线程的代码一直需要解释器来执行， 比如说下面\nGIL.acquire() try: while True: do_something() finally: GIL.release() 主线程代码对GIL的锁定和解开只间隔很小的一个系统时间，子线程在其他CPU核心得到GIL解开后CPU的调度命令后才能被唤醒，但是当唤醒后，主线程的代码又锁了GIL，然后只能等待主线程下次调度命令，但是到了切换时间又切换回去到待调整状态，一直处于唤醒，等待的恶性循环，多核的功能完全没有发挥出来而且还比单核更加差，所以python因为GIL的存在对密集型的线程支持不佳，但是假如主线程是在执行想web这样等待用户输入，而不是每分每秒都在使用解释器执行代码，多线程的优势就能发挥出来。\n解决方案 # GIL作为解释器的一个Bug一样的存在，我们也有一定的解决方法，开线程，和用Ctype绕过解释器是我们一般的解决方法，你想了解更多可以看这个 接下来主要解绍用multiprocessing来绕过多线程的瓶颈\n线程锁和进程锁 # 为了实现线程安全，我们也要借助锁的存在，我们先用下面的代码来验证一下多线程对于线程安全的问题。我们声明一个线程锁 threading.Lock(), # class Counter(object): def __init__(self, start=0): self.lock = threading.Lock() self.value = start def increment(self): logging.debug('Waiting for lock') self.lock.acquire() try: if self.value \u0026lt; 8: 模拟负载 # logging.debug('Acquired lock') self.value = self.value + 1 finally: self.lock.release() def worker(c): for i in range(2): pause = random.random() logging.debug('Sleeping %0.02f', pause) time.sleep(pause) c.increment() logging.debug('Done') counter = Counter() for i in range(20): t = threading.Thread(target=worker,args=(counter,)) t.start() main_thread = threading.currentThread() for t in threading.enumerate(): if t is not main_thread: 保护线程 # 得到value值 # 我们运行之后得到counter.value值为8，这很好理解因为我们限制了它的大小小于8时才自增1，但是如果我们把锁去掉呢，我们把self.lock.acquire()``self.lock.release()都注释掉，得到的结果却是一个21，而且每次运行的结果都可能不一样，由于线程在实现自增的时候有一定的时间(time.sleep(2)),所以当多个进程执行的时候当他们从堆栈上取到counter.value值都为7时，这时候他们都满足 counter.value小于8，所以都执行了自增，在系统负载2秒之间（time.sleep(2)）有多少个线程执行就会逃过我们给他的限制，这样就造成了线程的不安全，但是我们给他加上锁之后，无论开多少个线程，最终结果都是8。在python里面我们线程锁和进程锁我们可以看做是同一种东西。\nps：当同一线程相互争夺锁时，失败的会进出线程队列等待锁解开。 # 线程进程工作方式 # 单行 # 单行主要通过锁来实现，线程通过锁threading.Lock()对象创造锁，进程通过multiprocessing.Lock()对象创建进程锁，单行操作一般都是对共享数据修改的一种保护。\n并行 # 并行操作是一般是对数据的一种共享，一般不对公共数据涉及修改，我们可以创造很多线程和进程一起并行操作，也可以限制线程和进程的并行数量，两种方式选择主要是判断代码类型是I/O密集还是线程密集型的。如何限制并行数量我们可以通过threading.Semaphore（sizenum）(进程为multiprocessing.Semaphore(sizenum))我们可以控制对共享的线程数量。进程提供了一个进程池的类型(multiprocessing.Pool)，我们可以创建一个维护了一定程的进程池，但是他同时并行的数量并没有控制，只是帮我们创建了这个进程池，每个进程并不是只执行一个任务，可能执行多个方法通过一个进程.\n单行混合并行 # 单行和并行混合我们可以通过在代码中设置锁来实现，当然python给我们提供了两种对象来实现单行和并行的控制，线程的是threading.Event()和threading.Condition(),进程的是multiprocessing.Event()和multiprocessing.Condition() 两种对象都是提供了一种命令指令，但是Event对象可以用来判断命令是否下达而做出相应的反应，而Condition对象更倾向于当命令下达后才执行并行的操作。\n线程和进程通信方式 # 当我们想让线程和进程共同执行一些固定的任务，我们就需要线程和进程之间能够通信，线程和进程通信我们使用队列(Queue),进程和线程的Queue有点差异，就是进程Queue传递的对象必须pickle化，而且为了能够使用join()（保护进程)task_done(通知任务完成),我们一般使用JoinableQueue 代替Queue在进程中。\nQueue对象之间通过put和get通信，我们把任务put上去，Queue自动分配给当前的线程或进程， 这样就能实现对任务的流水作业话。\n引用\n12/26/2015 10:50:21 PM GIL维基资料\nGIL博文\n","date":"2016-07-18","externalUrl":null,"permalink":"/posts/backend/framework/python/python%E7%BA%BF%E7%A8%8B%E5%92%8C%E8%BF%9B%E7%A8%8B/","section":"博客","summary":"# 最近学了两个python库，一个负责管理线程，一个负责管理","title":"Python 线程(threading) 进程(multiprocessing)","type":"posts"},{"content":" 基于python2\nscrapy是一款非常轻量级的爬虫框架，但是由于它隐藏了太多关于网络请求的细节，所以我们有时候会遭遇到一下很尴尬的bug，当然这主要是因为碰到一些不规范的网站。\npython的编码转码网上有很多文章，如果你不了解这个你可以参考下面了解。\nNed Batchelder 关于python unicode和str的理解，通俗易懂\n关于scrapy 入门\n关于 encode的认识\n通过上面我们可以很好的理解python的转码译码，在这里我想谈一下我自己对其的认识吧，我一开始接触的c语言序列的基本上都是强类型，比如C里面假如我想写一个函数每个传人的参数都得是有类型的，但是python弱化了类型这一点，python也是面对对象的，但是他的对象就是鸡同鸭讲，照猫画虎就能运行，弱类型适合动态语言，我们不确定下一行代码输入的是什么，自从学python起，一直感觉python对类型一直不严格，这样就给了我一种错觉，只要长得差不多就能一样的比划，比如在两个string，'中国',u'中国',看起来差不多但是如果你把u'中国'存入文件中就会出错（假如你没定义编码规则)\nUnicodeEncodeError: 'ascii' codec can't encode characters in position 344-351: ordinal not in range(128) unicode字符编码错误，要想理解这个要对unicode字符集和unicode编码有一定的理解，推荐你读一下这篇博客字符编码的知识,python内部使用unicode字符集存贮所以的编码的字符，为什么要用unicode字符集举个栗子吧：\nA是米国的程序员，他使用asicc编码的文件上传了一封邮件， B是中国的程序员他使用gbk编码的文件上传了一封邮件， 现在C要用程序同时处理A和B的邮件，有两种解决方法他把A的文件译码再编码成B的gbk，或者将B的文件译码成asicc但是中文无法处理，那么只能使用第一种方法将A的文件编码成gbk，但是改天D又来啦，他是俄国人，天啊噜gbk可能没有把俄语编进去，那肿么办，我们迫切需要一种编码可以把所以的字符放进去，所以unicode出现了，Unicode中将字符集按照一定的类别划分到0~16这17个层面（Planes）中，每个层面中拥有216=65536个字符码，因此Unicode总共拥有的字符码，也即是Unicode的字符空间总共有17*65536=1114112，一共有1114112这么多的字符可以用，这下我们不用担心了吧，太好了这下不用愁了，\npython 内部使用unicode字符集作为一个译码中转站，因为他编码了所以的字符集，只要你能在自己编码方案上找到自己的字，我就能在unicode字符集找到你的位置，所以使用unicode可以很好的解决多种编码方案产生的问题（比如gbk，utf-8）\n当然其他编码方案如果想使用unicode解码成其他的必须同unicode有一一对应关系，不过现在主流的编码方案如gbk，gb2312，utf-8都是unicode系的。\n了解了这些基础知识就可以知道了为什么存贮u'中国'存不进文件里面去了，因为unicode并不提供给当今字符解析器的方法，就是\\u234e一个16进制数字，屏幕上不知道他对应什么图形，所以python系统要求存进文件的必须是字节流，也就是可以unicode是一种更高级的字符流，这个字符流能存贮当今世界所以定义的字符，但是他只是一个规定字符集合，我们只需要把发现的字符放进去占据一个位置，但是我们不需要考虑屏幕是否认识这个字符，这个字符的存贮由编码方案负责，如utf-8这些，假如没有字符编码方案可以存贮这些，我们虽然在unicode上有这个字符但是我们无法print出来，所以我们必须将unicode转换成普通字符流，有人就会问了，假如我真的没有找到一个合适编码方案可以存贮所有语言，我们可以将他编码成unicode—escape类型，这里我们不多讲。\n这就可以解释我们大部分碰到的错误unicodedecodeerror和unicodeencodeerror错误，都是因为字符编码方案不了解造成的，网上很多说碰到这种错误就encode，decode搞一下就行但是不弄清楚这背后的知识就会犯迷糊。\n接下来我谈谈我遇到的错误吧，在爬取http://yjsy.ncu.edu.cn/yjs_showmsg.asp?id=2770这个页面时（这是一个不规范的页面没有设置charset），因为每个spider调用的\nresponse.xpath('//xpath').extract()\t选择器返回的是一个unicode编码的字符集，但是他是接受的是一个字符流，spider可能调用了 response.body.decode(response.encoding)进行转码，但是这个response.encoding有时候会判断错误，比如将我一个gbk编码的文件判断成cp1253,这个时候假如我把他解码成encode成其他编码方式的话，我们就会得到乱码，那怎么纠正呢，我们可以这样干 先将得到的列表中每个content取出来，然后使用content.encode(resonse.encoding)转码成原始字符流，现在你可以将它用正确的编码转换成unicode了\n下面是我github上的关于这个scrapy的项目，在coding_pitch.py文件里面就是对于这个乱码的处理\n南昌大学教务处公告爬取\n","date":"2016-07-18","externalUrl":null,"permalink":"/posts/backend/framework/python/python%E7%9A%84%E7%BC%96%E7%A0%81%E9%97%AE%E9%A2%98%E7%A0%94%E7%A9%B6/","section":"博客","summary":"基于python2 scrapy是一款非常轻量级的爬虫框架，但","title":"python的编码问题研究------使用scrapy体验","type":"posts"},{"content":" 由于有其他编程语言基础，所以对于python的学习并不吃力，但是整体感觉python的确与前面学习c、c\u0026mdash;\u0026mdash;\u0026mdash; # 1. 实时编译VS静态编译 # 不需要输入任何前缀，直接将代码放在python解释器上面就能运行，虽然window下不支持直接点开文件就能使用，但是只要安装了python解释器就能很轻松的运行。\n分量轻是他的特点吧！相比打开vs等半天然后，编译连接最后执行。python是一门很轻巧的语言，没有满屏的分号，大括号，基本类型比如int、string、float不区分直接拿来用就可以了，任何一个变量都是一个对象，对象可以千变万化，感觉python是一门很野的熊孩子什么都不在乎，比如说你什么了相同的两个变量\n这门强类型语言则不允许，在同级作用域内他只允许声明一次，python或许已经没有声明了，每个名字只是一个对象而已并没有他的归属。 # 2.动态语言VS静态语言 # 给我感受是一颗静止的树的话，那么python就是一匹‘野马’，C # ","date":"2016-07-18","externalUrl":null,"permalink":"/posts/backend/framework/python/python%E5%AD%A6%E4%B9%A0/","section":"博客","summary":"由于有其他编程语言基础，所以对于python的学习并不吃力，","title":"python学习","type":"posts"},{"content":" 申请了一个阿里的15体验的云服务器，同自己玩的虚拟机还是有点不同的。 # 1.用户名和密码 # 找了半天没有找到那个是用户名，试了实例的id，没有用，最后终于在登录帮助名里面找到了，用户名竟然是root！！！！,我用的是Ubuntu系统，说好的Ubuntu不提供root权限的呢，阿里还真会改造Linux系统 ，但是我觉得用root登录还是不安全，我觉得新建一个用户吧\n在root权限下\nuseradd -s /bin/bash -r -m yourname 解释一下，本来直接useradd yourname 就可以新建一个yourname账号，但是如果用你新建的用户登录的话，你无法使用Tab键和上下左右键 ，你键入 echo $SHELL ，会发现是/bin/sh，因为Ubuntu默认创建账号使用/bin/sh，假如你没加这个可以删掉 用这个userdel yourname重新来一遍，搞完这个只是创建了一个普通用户，你使用不了sudo获取最高权限，这个怎么办，很简单在/etc/sudoers里面找到 root ALL=(ALL:ALL) ALL\n模仿它加上自己的账号名就可以\n","date":"2016-07-18","externalUrl":null,"permalink":"/posts/backend/software/%E9%98%BF%E9%87%8C%E4%BA%91%E6%9C%8D%E5%8A%A1%E5%99%A8%E5%88%9D%E4%BD%93%E9%AA%8C/","section":"博客","summary":"申请了一个阿里的15体验的云服务器，同自己玩的虚拟机还是有点","title":"阿里云服务器初体验","type":"posts"},{"content":" 最近在学习大数据,书看了不少,但是总是觉得很迷茫,不知道怎么学下去,今天 突然想写点什么来 总结一下这些天.\n起因 # 逛博客的时候看到一篇分享如何修炼成大数据高手感觉很有趣\n买了两本书 # 这两本书都是很多年前写的,但是最近这几年随着\u0026quot;大数据\u0026quot;被大家越炒越热,随即被大家所看重. # 这两个作者很牛,在30年前大数据还只是萌芽的时候就提出未来大数据的重要性和价值. # 大数据是什么了,现在为什么这么火,以前没有大数据吗? # 大数据的产生就像是科技带来的附带品一样,原来我们把信息存贮在图书馆里面,因为人类的活动产生的信息越来越多,图书馆也越来越大,但是由于管理和存贮的成本越来越多,我们被迫要缩减浓缩我们的信息,我们想出了很多很方法来对付这种情况,比如抽样,只要抽取一部分的代表信息存在图书馆里面然后记录主要特征和平均分量,我们就可以把信息降低几个数量级. # 但是现在不同. # 芯片做的越来越小,容量做的越来越大,我们有能力存贮所以的信息,一些先驱发现我们用另一种思维去对待信息. # 用全体数据而不是随机样本 # 用混杂性而不是精确性 # 用相关关系而不是因果性 # ","date":"2016-07-18","externalUrl":null,"permalink":"/posts/backend/framework/bigdata/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%AD%A6%E4%B9%A0%E5%B0%8F%E6%80%BB%E7%BB%93/","section":"博客","summary":"最近在学习大数据,书看了不少,但是总是觉得很迷茫,不知道怎么","title":"大数据学习小总结","type":"posts"},{"content":" 泛型对于解决面对对象编程的算法设计可以提高其运算速度，但是对于引用类型来说还是没什么差别，因为引用类型只是指针的地址的调用，简单来说泛型还是挺好理解的，但是对于泛型、非泛型、继承和接口的融合就有些迷惑了。 # 比如说这种接口 # public interface IEnumeratot\u0026lt;T\u0026gt;:IDisposable,IEnumerator,ICompare\u0026lt;T\u0026gt; 这个泛型接口继承了两个非泛型接口，和一个泛型接口。 # 我一开始理解泛型就是一个个模型，只要我们把类型一个参数赋给他，他就能生成一个标准的类型，他缺少的只是一个参数而已，我们引用的时候感觉就像我们引用一个“全体方法”，把参数赋给类型后就可以一直调用类中的方法了，但是对于接口的继承如何理解？ # 对于泛型类的继承，继承的类必须实现泛型的参数或者保留泛型的参数，比如下面\npublic class A\u0026lt;T\u0026gt; { public T tt; } public class C\u0026lt;T\u0026gt; : A\u0026lt;T\u0026gt; { public T tt; } 或者是这样\npublic class A\u0026lt;T\u0026gt; { public T tt; } public class C : A\u0026lt;string\u0026gt; { C cc; } 对于泛型继承非泛型类，比如下面\npublic class A { A aa; } public class B\u0026lt;T\u0026gt;:A { B\u0026lt;T\u0026gt; bb; } 基类是非泛型，而继承的是泛型类，我感觉这种构造就是让泛型类多了一种包容性,比如下面的链表实现的代码，让基类是非泛型，而继承是泛型，就能让链表可以连起很多种类型的数据，而本身的类型安全没有丢失。\npublic class Node { pretected Node next; public Node(Node next){ this.next=next; } } public class TypeNode\u0026lt;T\u0026gt;:Node { public T data ; public TypeNode(T data):this(data ,null){ } public TypeNode(T data,Node next):base(next){ this.data=data; } } 泛型的约束\n","date":"2016-07-18","externalUrl":null,"permalink":"/posts/essays/programming/%E6%B3%9B%E5%9E%8B%E7%BB%A7%E6%89%BF%E7%9A%84%E7%90%86%E8%A7%A3/","section":"博客","summary":"泛型对于解决面对对象编程的算法设计可以提高其运算速度，但是对","title":"泛型继承的理解","type":"posts"},{"content":" 由于网站要实现图片和文字的混排的上传 所以在网上找了富文本编辑器的插件，发现CKEditor这款还不错的插件\n我用的是4.5.1这个版本，的确很好用，只是引用了一个js文件就可以实现。 # 将网上下的CKEditor包解压放在根目录下\n在页面上引用CKEditor的核心包ckeditor.js\n\u0026lt;script src=\u0026quot;ckeditor/ckeditor.js\u0026quot; type=\u0026quot;text/javascript\u0026quot;\u0026gt;\u0026lt;/script\u0026gt;; 在页面添加一个输入框textarea\n\u0026lt;textarea name=\u0026quot;individual\u0026quot; id=\u0026quot;individual\u0026quot; runat=\u0026quot;server\u0026quot;\u0026gt;\u0026lt;/textarea\u0026gt; \u0026lt;script type=\u0026quot;text/javascript\u0026quot;\u0026gt; CKEDITOR.replace('individual'); \u0026lt;/script\u0026gt; 只要通过上面上面几个步骤就能实现富文本编辑器，但是点开图片上传功能，发现只有上传url的功能，并不能本地上传图片，百度了一下发现由于安全性问题CKEditor没有上传功能，只有安上CKFinder才能实现上传功能，于是我在官网下了ckfinder__aspnet_2.5.0.1,同CKEditor一样引用JS文件（只要引用ckfinder.js) # \u0026lt;script src=\u0026quot;ckfinder/ckfinder.js\u0026quot; type=\u0026quot;text/javascript\u0026quot;\u0026gt;\u0026lt;/script\u0026gt; 由于我是MVC的网站基于.net4.5，在官方给的包里面有一个asp.net网站实例放在**_source文件夹里面，里面有一个基于.net2的示例网站，一运行就报找不到 System.Web.UI.Design这个命名空间的错，所以我把它从项目中排除，把项目中bin中debug文件夹下的CKFinder.dll**复制出来，引用到我自己的项目中。 接下来要配置CKEditor来让CKFinder引用进来，在CKEditor文件夹下config.js在CKEDITOR.editorConfig = function (config) {};方法中添加如下代码： # config.filebrowserImageBrowseUrl = 'ckfinder/ckfinder.html?Type=Images'; config.filebrowserFlashBrowseUrl = 'ckfinder/ckfinder.html?Type=Flash'; config.filebrowserUploadUrl = 'ckfinder/core/connector/aspx/connector.aspx?command=QuickUpload\u0026amp;type=Files'; config.filebrowserImageUploadUrl = 'ckfinder/core/connector/aspx/connector.aspx?command=QuickUpload\u0026amp;type=Images'; config.filebrowserFlashUploadUrl = 'ckfinder/core/connector/aspx/connector.aspx?command=QuickUpload\u0026amp;type=Flash'; config.filebrowserWindowWidth = '800'; //“浏览服务器”弹出框的size设置 config.filebrowserWindowHeight = '500'; 注意在配置Url的时候要修改成相对于网站本地网站磁盘文件路径，比如说，你的网址是http://example.com.cn ,你把ckfinder文件夹放在Admin下的Editor文件夹，那么所有url要改成下面类似的格式 # config.filebrowserImageBrowseUrl ='/Admin/Editor/ckfinder/ckfinder.html?Type=Images'; 否则会报404错误， # 最后一步是修改一个函数让所有人能看到服务器上传文件夹里面的文件，在ckfinder文件夹下面的cofig.ascx文件，找到 CheckAuthentication函数将返回值改成true # 当然如果你想修改上传文件的地址，你可以在上面方法里面找到SetConfig()方法，找到BaseUrl，修改为你想上传的地址，\nPS： # 我是在在VS里面进行调试的，由于VS的IIS在调试的时候不允许对磁盘文件的路由地址访问，就是CKFinder通过ckfinder.html这个html来实现上传图片的功能，但是这个在调试的时候VS无法访问这个文件，所以一直报404错误，可以修改IIS来允许IIS访问磁盘文件，步骤如下： # 右键点击IIS Express，选择显示所有应用程序\n找到运行网站的配置，进入applicationhost.config文件夹\nctrl+f 寻找UrlRoutingModule\n将preCodition设置为空字符\n\u0026lt;add name=\u0026quot;UrlRoutingModule-4.0\u0026quot; type=\u0026quot;System.Web.Routing.UrlRoutingModule\u0026quot; preCondition=\u0026quot;\u0026quot; /\u0026gt; 如果你是用MVC进行表单传值的话，你必须在post方法上面添加 [ValidateInput(false)] 属性，如果不这样的话就会报下面的错 # “/”应用程序中的服务器错误。 从客户端(content=\u0026quot;\u0026lt;p\u0026gt;sdfsdafwewo shdfh...\u0026quot;)中检测到有潜在危险的 Request.Form 值。 ","date":"2016-07-18","externalUrl":null,"permalink":"/posts/backend/software/%E5%AF%8C%E6%96%87%E6%9C%AC%E7%BC%96%E8%BE%91%E5%99%A8ckeditor%E9%85%8D%E7%BD%AEckfinder/","section":"博客","summary":"由于网站要实现图片和文字的混排的上传 所以在网上找了富文本编辑","title":"富文本编辑器CKEditor配置CKFinder","type":"posts"},{"content":" 最近从python2转到python3,发现还是有一些不同,一些库改名字很好解决,但是这个import机制不了解原理是不好理解的.\npython2是默认相对路径导入,python3是默认绝对路径导入 # 首先这个包的导入机制,就是你在一个module里面引用另一个module,python运行文件有两种方式,一种是直接以主文件运行(默认以这种方式运行,同下面一种有点区别),一种是以module形式运行,就是用python -m filename方式调用.\n以module的方式运行 # ####### 什么是相对导入和绝对导入呢 相对导入是用一个.来声明的,相当于Unix上的选择当前文件夹.\n假设你的文件目录为下面的\nmain |\tmain.py | __init__py | momod+ | | __init__.py | | pack.py |\t| flask | | | __init__.py |\t| | myflask.py -------------------- python里面的module分三种,一种是build-in module(内建库),一种是第三方库,还有一种是你自己写的库(如上面的flask). # 在python2里面,当你import 一个module时,搜索的顺序是 内建库,自己的库,第三方库. ####### 而在python3里面顺序为 内建库,第三方库,你自己的库.\n我自己感觉python3的import的机制更为清晰,因为当你import一个库时,假如你写的库和第三方库重合时,你优先导入第三方库,如果你不适用声明相对导入的话,你无法正确的导入自己的库,而使用.来声明库来自自己的代码让代码的结构更加清晰了.所以如果你想让你的代码兼容py2和py3,你自己的库都要采用相对导入方法来导入. 比如在momod 里面的pack.py假如想引用flask的myflask.py要这样写\nfrom .flask import myflask ########### 假如你在py2中写了 from flask import myflask(并且你安装了flask库),这个是可以成功运行但是在py3中就会报错,因为他会优先导入flask库假如你没有显示声明相对导入的话.\n上面成立的前提是将pack.py以module方式运行,或者运行main.py在其中引入pack.py.接下来讲讲以主文件运行的不同.\n当你直接使用 # python pack.py 你假如在pack.py里面使用了这个\nfrom .flask import myflask 引用了自己的myflaskmodule,在py2和py3中都会下面报这个错\nSystemError: Parent module '' not loaded, cannot perform relative import 因为当你以主文件方式运行 pack.py ,python会吧pack.py重命名为__main__ ,所以用.相对路径也不会是当前文件.所以全部都只能用绝对引用.所以在主文件运行在python3里面有个问题,假如你自己的库与第三方库有重名.\npython3默认绝对路径,自己的库不会优先于第三方库被扫描.有两个解决方法,把自己的库重命名,第二个方法就是把包含主文件的文件夹加上__init__.py,你可以在sys.path的路径里加上..或者具体上一个上一个文件夹的路径.怪绕口的,其实你只要python能找到你的上一个文件夹,就行. # 说到这里顺便插一句对doctest和集成测试的理解.\n由于我平时喜欢一边写代码一遍测试功能,图方便就使用doctest直接插在方法里面,在代码后面加上 # if __name__ == '__main__': import doctest doctest.tesmod() 平时写小module的时候没有问题,在将python2转python3时候,出现问题,因为我要测试这个module时,会报上面的那个错,因为我要测试他的话必须将它作为主文件.查了资料知道,其实doctest虽然在当前页面代码测试,但是对于module的话,最好采用集成测试,一是module很多,假如一个一个运行很麻烦,二是有时候module必须多个一起测试,所以测试module时要用集成测试来取代doctest.\n","date":"2016-07-18","externalUrl":null,"permalink":"/posts/backend/framework/python/%E5%85%B3%E4%BA%8Epython3%E5%92%8Cpython2import%E5%8C%BA%E5%88%AB/","section":"博客","summary":"最近从python2转到python3,发现还是有一些不同,","title":"关于python3和python2 import区别","type":"posts"},{"content":"","date":"2016-07-18","externalUrl":null,"permalink":"/tags/%E5%BF%83%E5%BE%97/","section":"Tags","summary":"","title":"心得","type":"tags"},{"content":" 管线命令 (pipe)： # 撷取命令： cut, grep 排序命令： sort, wc, uniq 双向重导向： tee 字符转换命令： tr, col, join, paste, expand 分割命令： split 参数代换： xargs 分割文档 # 长长的一大片文档有时我们并不愿意看到全部内容,我们只想关注 部分内容的时候了可以考虑使用分割文档命令\ncut是一个很好的分割文档工具\nvi常用命令 # [Ctrl] + [u] 屏幕『向上』移动半页 光标移动到非空格符的下一列 光标移动到非空格符的上一列 n那个 n 表示『数字』，例如 20 。按下数字后会向右移动这一行的n 个字符。例如 20\u0026lt;spac移动 20 个字符距离。 0 这是数字『0 』：移动到这一行的最前面字符 $ 移动到这一行的最后面字符处(常用) H 光标移动到这个屏幕的最上方那一行 M 光标移动到这个屏幕的中央那一行 L 光标移动到这个屏幕的最下方那一行 G 移动到这个档案的最后一行(常用) nG n 为数字。移动到这个档案的第 n 行。例如 2档案的第 20 行(可配合 :set nu) gg 移动到这个档案的第一行，相当于 1G 啊！ ( n n 为数字。光标向下移动 n 行(常用) ","date":"2016-07-18","externalUrl":null,"permalink":"/posts/backend/software/%E6%9C%89%E7%94%A8%E7%9A%84%E5%87%A0%E4%B8%AAshell%E5%91%BD%E4%BB%A4/","section":"博客","summary":"管线命令 (pipe)： # 撷取命令： cut, grep 排序命令： sort, wc, uniq 双向重","title":"有用的几个Shell命令","type":"posts"},{"content":" 学了鸟哥的书前面基础后，突然想在Linux下用gcc玩C语言，然后了解到了Vim这个神一样的编译器，接下来经过超长时间虐心的安装无数插件无数依赖包，突然有种打自己一顿的感觉，还好终于把Vim装的和VS差不多了，接下来我介绍我安装Vim的经验吧。 # 我虚拟机下的Linux原来是红旗6的，但是我改了一下yum的包源成CentOS的并且全部update一下后就神奇的变成了CentOS6，虽然他们两个是同一家公司，但是总给我一种由盗版成了正版的感觉。。 # 闲话不多说，刚开始装第一个插件是Ctags # 刚开始装的时候我是在X-Windows里面的这里下载再转回shell敲\n$ tar -xzvf ctags-5.6.tar.gz $ cd ctags-5.6 $ make make install # 后来我发现不用这么复杂直接在X-Window下面复制到Terminal里面就可以了，毕竟后面安装的代码都是十几行，根本扛不住。。。。 # 装完三个感觉整个我太苦逼，简直是辛苦啊，不是说好的敲几下键盘就可以吗。 后来了解到linux有几个软件可以帮忙安装而且解决依赖性，比如ubuntu的apt-get，redhat的rmp，yum，但是问题来了，我的黄狗（yum：yellow dog）怎么没用啊，全都装不上，一查错误提示，原来我没用被授权，原来RedHat更新软件是要收钱的，但是CentOS不用，然后我就用了CentOS的源，当时我就想只想装个git，然后就没用然后了。终于全部更新完了，yum也可以用了，用yum果然爽多了，直接\nyum install git\tgit就装好了。如果想知道怎么改源，点击这里\n好了装好了源接下来装软件就敲一敲代码就行，不用去网上找包下载再安装。\n装好一些热门的插件后，我碰到了第一个最难装的插件YouCompleteMe(YCM)，装完之后感觉就是神器一般的存在，但是装完之前一直在感慨，我去怎么没个卵用啊，而且装llvm（安装YCM必须先安装的软件)时，我用编译安装方法因为一些库的缺失一直报错，后面采用二进制安装方法安装完llvm才成功按上YCM。 推荐新手采用二进制安装方法安装llvm，想知道怎么安装YCM这些点击这里\n安上一些实用的插件后，最后安装调试神器gdb，有三种方式使vim可以调试程序（与gdb一起工作）， # vimgdb，pyclewn，clewn，三个我都试了，第一个要编译一下vim，我失败了，第二个要用python2.4+支持，我python2.6然并卵，每次都说不支持python2，第三个是用c写的，终于成功，安装非常简单， 点击这里了解详情. 装完了，我终于可以使用clewn打开gdb和gvim，然并卵，我写的程序并不能在gdb里面运行，一直说无法识别。。。。。。搜了一下原来是编译的时候的问题，用gcc编译的程序要带上-g参数才能在gdb里面用（我还以为我clewn没装好） 例如\ngcc -g test.c -o test 加上-g参数就行了。终于我的vim可以用了。。。。。。\n","date":"2016-07-18","externalUrl":null,"permalink":"/posts/backend/software/%E5%9C%A8linux%E4%B8%8B%E7%8E%A9%E8%BD%ACvim/","section":"博客","summary":"学了鸟哥的书前面基础后，突然想在Linux下用gcc玩C语言","title":"在Linux下玩转Vim","type":"posts"},{"content":" string # 原来的很多函数都逐渐迁移到str和unicode对象上去了, 不过有两个函数没有迁移出去\n第一个是capwords\n个人觉得没什么卵用 就是将英文单词首字母大写\n比如\nstring.capwords('this are some words') 输出为 This Are Some Words\n第二个我觉得挺有趣,他可以帮你把对应的文字换成你设定的\n我们可以用它来设计出莫斯密码\u0026rsquo;='.\n\u0026quot;\u0026quot;\u0026quot;create a table\u0026quot;\u0026quot;\u0026quot; table = string.maketrans('abc','123') print 'abc123'.translate(table) 输出为123123成功把abc转成了123\nstring还有一个模板类型Template 这个类型同我们转义差不多(%),不过能够实现的 更加自主化 我们可以继承这个类来修改模板类的具体实现 而且这个模板类有一个\ntextwrap # 由于sublime输出一个很长的字符串很卡,这个格式化字符串的类能够帮我们 解决很多问题\n我一般去除整体缩进然后去填充字符串\ndedented_text = textwrap.dedent('...a long word...') print textwrap.fill(dedented_text, width=50) ","date":"2016-07-18","externalUrl":null,"permalink":"/posts/backend/framework/python/%E5%AD%97%E7%AC%A6%E4%B8%B2%E5%A4%84%E7%90%86/","section":"博客","summary":"string # 原来的很多函数都逐渐迁移到str和unicode对象上去","title":"字符串处理","type":"posts"},{"content":" 一眨眼，四年过去了，再过几年就到了三十而立的年龄了，年轻的时候走马观花， 好像一直为了学习而学习，为了规划而规划，在经过被社会的毒打之后， 自己眼睛里面好像也没有当年的光了\n起因 # 突然发觉自己该给自己重新规划一下未来了，之前定的规划好像变成了墓碑，矗立在哪，纹丝不动\n在网上找到这本书，静下心来细细品读，颇有一番感悟\n感触 # 热爱编程，而不是为了工作 假如编程只是一份养家糊口的工作，那么你会越来越抗拒他，当你下班，你只会去做让你开心的事。 而程序员进步最大的时候就是在他下班后。\n不是说你下班之后继续忙工作的事情，而是脱离你的工作，专注于编程的进步，你当然也可以钻研你工作 上有疑惑的东西，去搞懂他，去优化它。\n你可以把编程当做一个游戏， 花时间在它上面，打通关，升级自己。\n对于我来说，我还算挺喜欢编程了，假如是个新技术，我会立马把它钻研到可以用的地步， 但是每当我想深入一个领域， 我最大的问题拖延症， 假如没有什么deadline，我一般就会能拖就拖，结果拖着拖着就忘了，所以其实对于我来说， 我需要把编程当做一个游戏，自己设定每个阶段的deadline，像升级打怪一样把某个知识点钻通\n向人请教如何成长 不知道是不是编程开发总觉得问别人不去很羞耻，应该面向google或者百度编程，但是很多时候很多问题不是百度就能解决\n我们需要抱着一个班级最差生的态度去向别人请教，不是问了懂而问，而是为了变的和他优秀去发问，他是如何做到\n大部分人比如我都很乐意把自己所学教给别人，也会把自己觉得让自己成长有帮助的方法告诉别人，你要勇于问\n变强的最快的方式不是像小说里面闭关修炼，而是找到一个良师，21世纪了，我们要站在巨人的肩膀上才能变得更强\n自己反思、适时放弃 书中讲到一个抓猴陷阱，就是在地上挖一个曲折的洞，里面放满米，傻猴子会拼命往洞里掏米，当猴子手里抓满米 之后手就拿不出来了，即使你站在他旁边他也不会选择放手手里的米，这样就抓住了一只猴子\n虽然我们比猴子聪明，但是当我们手里抓住的是我们看中的东西的时候，也不会舍得放手，所以给我的一个感受就是得学会分轻重\n好比打游戏，千万不要上头，人家交闪了还越塔追，最后被人家丝血反杀，我们得放弃击杀别人的机会，即使只需要a一下就能收到人头\n总结 # 这次分享自己也学着书中的方法，提高自己的写作能力，当然书中说了一些其他不错的观点，但是我选择了三个对我感触最大的， 这些缺点在我身上也有，所以我感触也很深，也第一次应用上了哈哈，我给自己deadline是今晚写完，bingo！\n","externalUrl":null,"permalink":"/posts/essays/life/%E6%AF%95%E4%B8%9A%E5%9B%9B%E5%B9%B4%E6%9D%82%E6%83%B3/","section":"博客","summary":"一眨眼，四年过去了，再过几年就到了三十而立的年龄了，年轻的时","title":"《我编程，我快乐：程序员职业规划之道读后感","type":"posts"},{"content":" 不知不觉2020已经过了快一半了，2020的确是个魔幻的一年，两年都没生过病的我却在疫情开始的时候发了个烧，幸好一天就退了，但是也是魔幻\n这次疫情的确影响颇深，我也花了一段时间才慢慢从担心确诊人数到开始安之若素，现在回头看看专心做事能消除很多恐惧和不安\n现在回头看看，自己毕业也快两年了，加上我在大学的四年，我在编程这行已经呆了六年了。回头看看我这六年经历\n在大学四年是入门区，开始接触编程，像一块海绵拼命的汲取知识，慢慢的扩展了自己的广度，但是当我到第三年的时候我就发现自己陷入了一个迷茫区，自己越来越“膨胀”，学了很多东西，感觉编程也不是很难，只要自己想花时间就能学会，这个时候自己就像一个稚童编程就像玩具，每次拿到一个新的玩具（框架、软件），玩一会会基本操作之后就丢了\n接着到了工作时候变成了一个熟练工，在业务的驱动下做了很多很多项目，但是都是新瓶装旧酒，看起来自己编程技术过硬什么问题都能解决，但是其实这个时候只是会一些基本操作而已\n其实在熟练工这个阶段是最难熬的，自己能清楚自己其实只是会编程，但是不是精通，这个时候段我也看了很多编程书，在实际工作中也开始运用大师们的智慧，但是自己对编程越来越迷惑了\n在看了很多书之后，知道了各种编程模式，各种编码规范，自己实际编程的时候越来来畏手畏脚了，总感觉自己写的代码就是shit，越来越怕写垃圾代码，每次编程都在考虑自己的编码是否符合规范是否优美，虽然想了很多，但是最后回头看还是写的一地鸡毛，最后在逐渐自责中慢慢的淡忘编程的条条框框，开始野蛮生长模式\n这种情况直到自己开始在自己项目开发近一年才开始改变，项目从设计原型到所以的设计稿出来全部都是自己的主意，项目从v0.1开始开发，慢慢升级到v1、v2、v3，每个版本升级都是几乎全部重写，这个项目特别有趣，基本上都是自己一直在催促自己，把所有功能都加紧上线，测试能不写就不写，代码能跑就行，终于重要所有的功能都开发完成了，在这过程中由于各种不规范编程，花了无数时间debug，由于是自己的项目，感觉写的这么烂都对不起自己，所有所有的代码又开始重构，在重构的过程发现干脆重写一个算了，基本上每一行代码都被千锤百炼，直到目前还是有很多问题\n但是这个过程中，自己似乎领悟到了一点别的东西，好像编程大师说的那些东西都可以用，当把那些东西稍微的应用到自己的项目中的时候，突然感悟到了点什么\n其实在做项目的时候我们都是为了完成功能而写的代码，从结构上他们完全围绕这业务，以素描来打比方，我们想画一幅人，我们为了快速完成一个功能，我们可能会先把一个脚画出来，一般业务不明朗的情况下我们都会把这个脚画在图片最中心，这样最快，但是当我们开始画头的时候，完了图片不够大，假如不想搽掉重来的话，你只能把一个人设计成一个矮子了\n这就是我们大部分也包括我在平常工作中遇到的情况，这样的情况会造成一个很恶劣的影响，随着项目越来越大，你就越难纠正，所以我领悟到第一个就是”勇气“，敢于把你代码给重构掉\n当然还有重要的一点就是快速的勇气，其实重构多了你自己会开始畏手畏脚，每次你添加新功能的时候你害怕写垃圾代码或者害怕自己考虑的不够全面为了未来不可能的事写了很多代码，其实我们必须要有写脏代码的勇气，而且我们对这些代码写的越快越好，这样你把他全删了你才不会伤心。\n只有开始习惯重构你才发现，把代码写好是一个多么艰难的事情也是多么有趣的事情，终于明白大师为什么是大师，大师能用的工具框架我能懂都能用，但是大师通过这些工具搭建起来的项目我们却只能望其项背。其实越重构你就会越钦佩大师们总结的经验，他们是如此准确如此实用，虽然我很早之前就看过了解过，但只有踩了无数的bug之后才能发现并理解这些。\n虽然目前我只是一个熟练工，但是我似乎发现了前进的突破的方向。以前一直觉得编程很简单，现在才知道编程没那么简单，但是有那么有趣了。\n","externalUrl":null,"permalink":"/posts/essays/life/5%E6%9C%88%E6%9D%82%E6%84%9F/","section":"博客","summary":"不知不觉2020已经过了快一半了，2020的确是个魔幻的一年","title":"5月杂感","type":"posts"},{"content":" 因为这段时间待在家没什么事干,虽然完成一个小的项目,也还有很多功能需要完善,但是说实话还是有点迷茫,明年就大三了,接下来有两条路给我走,要么是考研,要么是出去工作. 自己算是个半个程序员吧,web开发那一套前后端撸起袖子都能搞出点名堂来,但是为什么说我是半个程序员呢,因为现在我还不知道我是爱他还是不爱他呢.\n兴趣这东西是培养来的,谁也不是天生就死心塌地的爱上某个东西.\n现在学习新东西的成本太低了,这两个月来我也尝试过很多以前没有接触的东西,Android,Deskapp,用了几个框架做了几个apk和软件,虽然用前端栈实现的东西但是所以的语言都差不多.\n当然项目基本功能都很单一,当然复杂的也就是简单的堆积而成.\n完成之后反倒更迷茫了,为什么了,互联网技术太好学了,无论你的文化程度有多低,按照demo依葫芦画瓢搞几下也能像模像样,我们作为一个文化程度比较高的一群人,或许花半天就能比别人一天能掌握的东西.\n但是人家索要的报酬却比你的四分之一还要低,你拿什么跟人家竞争\n我有一些志同道合的同学虽然像我一样学了几年web这一块,但是他们决定考研,web谁都能做,不用读大学就能做,他们想多读点出来改造世界,也可以说让世界更美好吧.\n其实我对考研的态度是消极的,我现在嘛并不准备投身科研这方面,在美国其实大学都是少部分人才能上,比起我们现在博士硕士满天飞,在美国其实只有真正喜欢科研的人才会去读大学.\n其实我写的时候很纠结,该怎么写才不跑题\u0026hellip;\u0026hellip;\n我其实想总结的是注重基础,但是我脑海里面出现无数个论点,好像回到了高中时代写议论文了, 想把主人公塑造成一个焕然大悟顿悟人生的形象\u0026hellip;..\n以上是个小插曲跳过跳过\n现在开始正式总结了,越短越好.\n总结 # 七八两个月自己还是很急躁的两个月,一方面在于选择技术栈,一方面在于选择出路.\n虽然没有想通,但是在想的过程中发现自己好焦躁\u0026hellip;\u0026hellip;.\n总是想一秒钟决定自己未来N年的时,有点像人生规划一样\n虽然说搞个人生规划很好,但是发现真的是白费力气,因为无论那个伟人,谁也不知道自己将来会干什么,奥巴马在大学客堂睡觉的时候也不知道自己将领导美国.\n焦躁点找到了,接下来就静下来想了怎么解决了,既然无法预测未来那就好好准备未来吧.\n怎么准备呢?做你不屑做的事,并坚持下去.\n你们不要想歪了,不屑做的事是指那些你看起来没什么卵用的建议.\n比如说每天背几个单词,每天看一点英文原著,每天刷一刷编程题\n在这我要提一句,虽然很多人从高中过来有点嫌弃刷题了,但是我们是大学生不能用高中生的观点去看待问题,高中的刷题是为了分数,大学的刷题是为了醒脑\n你看不起一天在leetcode刷几道题,那是因为你刷不动,不愿刷,以前刷题刷到你一眼就能看出答案,现在我们刷题要每刷一遍都要有新的感受,让大脑活跃起来.\n而且当年把运动刷题背单词看书写博客当成副本规定题量字数的时候,每次完成你的任务都会很爽,一方面脑子变得活跃了,另一方面你有种马上就升级的感觉.\n每天都能发现自己的进步这是你前进的动力,虽然我们无法预测未来,好好准备,当未来真的来的时候,不会慌.\n私下插个话,现在app泛滥,你刷单词完全可以搞个扇贝,刷题搞个牛客或leetcode,健身搞个keep,虽然我没收他们的广告费,但是这些东西给我的反馈比我自己搞个小本记着好多了.不多说了与君共勉,希望大家都能沉下焦躁的心,一起慢慢\u0026quot;升级\u0026quot;.\n","externalUrl":null,"permalink":"/posts/essays/life/8-conclusion/","section":"博客","summary":"因为这段时间待在家没什么事干,虽然完成一个小的项目,也还有很","title":"8月 总结","type":"posts"},{"content":" 前言 # 因为手头自己有三个服务器，所以想折腾一下负载均衡。\n两个CentOS，一个Ubuntu,都是比较新的。\n一开始准备用haproxy来做负载均衡服务器，因为haproxy相比与nginx对cookie和session支持比较好，但是由于两个原因还是放弃了。\n服务器被阿里云封掉 简单的在haproxy中设置后端服务器后，过一段时间就显示强制备案页面，由于我的域名没有备案。\n后来我翻看了nginx日志发现，haproxy默认在request header里面带了X-Host，被阿里云发现了，这里提供一个解决方法\n# 删除掉你header里面的 Host # 在backend里面添加一句 http-request del-header Host 然而nginx里面默认是没有添加Host这个的，要你在localtion中添加两句，如下面\nserver { listen 80; server_name example.com; location / { proxy_pass http://main; proxy_set_header Host www.example.com; # add Host proxy_set_header X-Forwarded-For $remote_addr; # add X-Forwarded } } haproxy支持多开 我试了很多种选项，确定pidfile、改变uid、gid等等，haproxy似乎可以允许很多个相同进程绑定同一个端口，虽然可以通过pid来写一套类似service管理的脚本，但终归很麻烦\n我看网上有人写了这个脚本，但是nginx自带了，还是用nginx比较好，而且ansible与service的交互还不错。\nnginx负载均衡 # nginx负载均衡是通过反向代理来实现的，也就是把一台服务器的压力分摊到多台上面\n要想实现这个必须要有后端服务器，假设我们有一台后端服务器1.1.1.1，在代理主机的nginx配置系统location里面只要添加一条proxy_pass就行了\nserver { listen 80; server_name example.com; location / { proxy_pass http://1.1.1.1; } } 上面只是简单的实现了一个反向代理的功能，当你有一个后端服务群的时候，你就要使用负载均衡模块了，负载均衡模块在nginx配置特别简单，添加一个upstream模块，把服务器ip或者域名放到里面\nupstream webservers{ server 1.1.1.1 weight=10; server my.domain.com weight=10; } 然后修改proxy_pass后的为http://webservers就行了\nps： nginx对于后端反向代理服务器有个max_fails和fail_timeout属性，你要是设定了一个max_fails次数，你代理服务器拿取失败了几次就会在fail_timeout值之后尝试，和haproxy的retry属性差不多，但是似乎haproxy的retry不好使，我故意使用两个错误ip和正确ip，结果nginx能一直正确返回正确ip响应，而haproxy有时候能，有时候不行。\nnginx错误日志 # 在调试nginx碰到一些错误，记录一下如何系统的解决方法\n调用service nginx start失败 首先看给的错误信息，假如让你看systemctl status nginx.service或journalctl -xn，输入去看\n格式错误（format error） 一般你写的nginx的配置文件有问题，这时候可以用nginx -t检查格式，修改正确后会显示success\n无法绑定地址（bind error） 一般是因为有别的应用程序占用端口造成的，这时候用netstat -tulpn检查端口，然后选择kill掉占用端口的程序或者换一个端口\nansible playbook 编写 # 具体代码可以参考nginx均衡负载ansible-playbook 首先你得写一个hosts\n[ali] my ansible_ssh_host=1.1.1.1 ansible_ssh_user=root [tencent] main ansible_ssh_host=1.1.1.2 ansible_ssh_user=root [digital] google ansible_ssh_host=1.1.1.3 ansible_ssh_user=root 前面[ ]包着的是组名，最前面的my和main和google是别名，后面就是ip和用户名了。\n写完hosts后要写两个nginx配置文件一个代理服务器的配置文件和一个后端服务器配置文件，playbook很简单就是复制nginx配置文件和重启nginx。\n--- - hosts: tencent remote_user: root tasks: - name: copy nginx config file template: src=~/test/lunge_proxy.conf dest=/etc/nginx/conf.d/lungelog.conf notify: restart nginx handlers: - name: restart nginx service: name=nginx state=restarted enabled=yes 解释一下notify，在复制完成之后就启用一个handler完成nginx的重启，当然这里也可以使用reload，假如在生产环境的话。\n客户端和代理的playbook差不多就不多介绍了。\n引用 # nginx的配置、虚拟主机、负载均衡和反向代理\n","externalUrl":null,"permalink":"/posts/backend/software/ansible%E7%AE%A1%E7%90%86nginx%E8%B4%9F%E8%BD%BD%E5%9D%87%E8%A1%A1/","section":"博客","summary":"前言 # 因为手头自己有三个服务器，所以想折腾一下负载均衡。 两个","title":"ansible管理nginx负载均衡","type":"posts"},{"content":" 一眨眼暑假就要过完了，这段博客又停下来了，倒不是因为我停下来学东西，而是我又忘了去归纳总结自己学的东西，总是一个鲁莽的心态去学，总感觉停下来会阻挡自己学习的脚步，但是其实写博客也是学习，把自己的学到的知识转换成自己的东西，写出来的过程就是帮自己的过程，其实这个方法也是一个非常牛逼的学习的方法叫做费尔曼学习法\n但是我这个人有点啰嗦，写的东西总是东一句西一句，没有一针见血，这个在接下来的博客里面也要改掉，尽量不要写废话\n接下来说一下我最近的学习方向，其实主要就是金融方向，主要设计到金融、股票、量化等等。牵涉到很多金融知识\n这里我再谈一下为什么我会去想精通业务，其实我一开始接触编程，我一直把自己往框架、底层方向搞（俗称造轮子），忽视了业务，一直觉得业务就是增删查改。但是其实本末倒置。\n所以去年到今年，我一直在犯错误，我一直想造一个轮子出来，但是没有灵魂，再好看的轮子也没有用，所以现在我从头开始，把金融知识掌握透，这个过程会很难，但是应该会很有趣\n","externalUrl":null,"permalink":"/posts/essays/life/%E5%85%AB%E6%9C%88%E6%9D%82%E6%84%9F/","section":"博客","summary":"一眨眼暑假就要过完了，这段博客又停下来了，倒不是因为我停下来","title":"八月杂感","type":"posts"},{"content":"","externalUrl":null,"permalink":"/posts/frontend/layout/","section":"博客","summary":"","title":"布局","type":"posts"},{"content":"","externalUrl":null,"permalink":"/posts/design/illustration/","section":"博客","summary":"","title":"插画","type":"posts"},{"content":"","externalUrl":null,"permalink":"/posts/frontend/animation/","section":"博客","summary":"","title":"动画","type":"posts"},{"content":"","externalUrl":null,"permalink":"/posts/design/composition/","section":"博客","summary":"","title":"构图","type":"posts"},{"content":" 驱动力是什么 # 驱动力就鞭子，小的时候我们被父母教育考的好就是棒棒糖,考的差就是鞭子，等我们走进社会,工资就是我们是驱动力。\n我们看到过那些年薪百万的程序员，也看到过一些碌碌无为的码农,每个人都想成为那群大牛，工作得心应手、万人敬仰，工资难以\u0026quot;望其项背\u0026quot;.\n但是我们同大牛和码农(差点打成马蓉\u0026hellip;..)有什么区别呢，有些人说是人家那些大牛早早就积累了十万个小时，我们同大牛只是差了十万个小时.\n这从某一方面上来看是对的，从某一方面来说又是不对的，君不看那些在公司辛辛苦苦工作几十年的码农早就积累了几十万小时，但是他们依旧是码农，除了业务逻辑比新手强。\n那是什么原因让几十万个小时造不了一个大神呢？\n很简单，就是驱动力。\n码农是以工资作为驱动力的，而大神是以兴趣为驱动力的，很多码农一开始都同大神一样被编程的乐趣而吸引，然而大神坚持下来了，而码农呢，慢慢的像小孩子玩厌了新玩具，在慢慢的走入社会被工资左右，在慢慢的就开始盼望早点下班\u0026hellip;\u0026hellip;\n怎么改变 # 环境对我们的影响是潜移默化的，我们处在这个环境里面可能不知不觉就慢慢改变我们自己了\n那我们如何改变自己\n工作环境 # 假如你是一个工作党，找一个好的开发团队对你的影响是巨大的，假如你的小组死气沉沉，最好换一个即使工资很低\n假如你是一个学生党，比如我，尽量参加学生社团，那种偏技术的部门，在同一个部门里面一起奋斗的感觉非常好。\n学习环境 # 在搜索引擎上面多走几步 很多时候我们遇到问题，google一下解决了就完了，我们要多问几个自己几个问题，这个问题为什么产生，如何避免，下一次还会遇到吗，还有及时收集自己的问题多总结，你要知道圣斗士之所以那么牛是因为人家从不在跌倒的地方跌倒第二次，你要知道bug不是我们的试卷，bug是我们的成神的补丁\n培养开源精神 github没事都上去溜达溜达，看到好的项目可以跟进，看看人家的代码同你的有什么不同，开源不代表抄袭，任何创新都是从模仿开始，不要老想着搞个大新闻大项目，其实很多项目都是从小项目开始的\n多输出 其实很多人不知道，写东西也是学习的一种方法，因为很多时候我们学的东西有时候学的模拟两可，写出来有助你理清脉络，而且帮助后来者少走弯路，何乐而不为呢。\n有些人说我没什么想写的，你找一些外国博客翻译也是可以的，通过翻译学习，一方面锻炼自己，一方面让更多人了解国外文化。\n总结 # 成神是很难的，但是只要你在路上，不要回头就不难了。\n","externalUrl":null,"permalink":"/posts/essays/life/about-diving-force/","section":"博客","summary":"驱动力是什么 # 驱动力就鞭子，小的时候我们被父母教育考的好就是","title":"关于驱动力","type":"posts"},{"content":" 最近的文章 Hugo 使用教程 2024-07-15\u0026middot;3 分钟 后端 软件 背景 # 博客这次由Hexo 转为 Hugo，我来简单介绍为什么要用 关于 2024-07-14\u0026middot;1 分钟 I am a software engineer. # 秒杀系统的思考 2022-07-02\u0026middot;2 分钟 后端 框架 Java 秒杀系统 RPC大纲 2022-03-19\u0026middot;1 分钟 后端 框架 Java RPC 最近想系统学一下RPC框架，下面是我学习路径 阅读一个简易的R 王者荣耀感悟 2022-03-05\u0026middot;3 分钟 随笔 人生 最近玩王者玩着玩着突然有些感悟，感觉有时候通过玩一个游戏也可 IO总结 2022-02-19\u0026middot;2 分钟 后端 框架 Java 最近被NIO这个概念弄得有点晕，一会是Linux的NIO一会 Bean复制真的那么慢吗 2022-01-17\u0026middot;3 分钟 后端 框架 Java 引言 # 最近在业务代码中经常用到的BeanUtils.copy 2021年终总结 2022-01-17\u0026middot;1 分钟 随笔 人生 引言 # 2021年，对于世界来说都是一个魔幻的元年，新冠在全世 泛型的前世今生 2021-12-10\u0026middot;1 分钟 后端 框架 Java 一、什么是泛型 # 二、泛型的作用是什么 # # SPI破坏了双亲委派吗 2021-12-09\u0026middot;4 分钟 后端 框架 Java 最近在学JVM的时候，把ClassLoader部分给过了一遍 AOP再思考 2021-12-08\u0026middot;4 分钟 后端 框架 Java 我记得之前写过Spring AOP相关的文章，但是最近在观看C JVM之ClassLoader的思考 2021-10-30 后端 框架 Java JVM 压测卡顿20秒引发的思考 2021-10-23\u0026middot;4 分钟 后端 框架 Java 压测让人爆炸的事，就算把你写的代码都注释掉了这个问题依旧存在 压测心得 2021-10-10\u0026middot;2 分钟 后端 框架 Java 最近在做一个接口的压测，一开始以为自己优化后的代码应该没得问 LinkedHashMap实现LRU 2021-09-19\u0026middot;1 分钟 后端 框架 Java 引用 # https://zhuanlan.zhihu.com/p/62322204 https://cloud.tencent.com/developer/article/1519813 mybatis二级Redis缓存 2021-09-15\u0026middot;3 分钟 后端 框架 Java Mybatis 这篇文章不是介绍mybatis二级缓存，而是基于我们目前业务 FlatMap用法 2021-06-20 后端 框架 Java Stream Stream源码（2）：从问题出发看源码 2021-05-23\u0026middot;4 分钟 后端 框架 Java Stream 之前看一些开源项目源码的时候，发现一个问题，假如你贪全，一口 Stream源码（1）：如何实现去重 2021-05-15\u0026middot;3 分钟 后端 框架 Java Stream 本篇博客是在看代码的时候看到使用Java8使用Stream去 MySQL Group By 还可以用来去重 2021-04-05\u0026middot;5 分钟 后端 框架 Java MySQL 最近在接手老项目的时候，看到一个SQL： select * from xx group by id， 当 从因材施教的角度来看写博客 2021-04-04\u0026middot;3 分钟 一开始本来想写一篇反思自己最近文章比较浅显的博客，但是想着想 新工作两周总结 2021-04-02\u0026middot;2 分钟 一眨眼2周就过去了，在新的公司感触颇多 改变 # 工作流程 新的公司 Dubbo浅探 2021-03-27\u0026middot;3 分钟 后端 框架 Java Dubbo 繁忙的一周终于过去了，加入小影第一周主要是熟悉后端架构，同事 Spring Cloud Alibaba浅探 2021-03-16\u0026middot;2 分钟 后端 框架 Java SpringBoot 花了半天时间把Spring Cloud Alibaba 的Nacos 、 Sentinel 和 Seata简 UML最佳实践 2021-03-15\u0026middot;2 分钟 后端 软件 最近面试的时候一个面试官指出了我对系统架构没有固定的认知，推 SpringCloud浅析 2021-03-14\u0026middot;5 分钟 后端 框架 Java SpringBoot 最近在学SpringCloud，之前一直对用视频学嗤之以鼻， 面试杂思 2021-03-07\u0026middot;5 分钟 随笔 人生 最近在面试，面试之前总觉得有一丝焦虑 我不知道这丝焦虑是不是学 技术如何深入 2021-01-07\u0026middot;2 分钟 随笔 人生 为什么会有这篇博客呢，因为随着你阅历的增长，以及各种你即将面 浅析Spring 2021-01-07\u0026middot;3 分钟 后端 框架 Java SpringBoot \u0026gt; Spring核心就是IoC（依赖注入）AoP（面向切面）本 浅析微服务 2021-01-07\u0026middot;5 分钟 后端 框架 Java 这篇博客主要是从web技术发展来探索微服务的起源 要想了解微服 Tomcat高并发浅析 2020-12-31\u0026middot;3 分钟 后端 网络 Tomcat作为Java老牌web服务器，所以研究Tomca Select和epoll浅析 2020-12-30\u0026middot;2 分钟 后端 网络 这篇文章是【Tomcat高并发浅析】的姊妹篇，专门通过分析S 大数据架构小结 2020-12-21\u0026middot;4 分钟 后端 框架 大数据 首先说说什么是大数据，最简单用数据量为单位，大于1亿就算大的 深夜有感 2020-01-07\u0026middot;3 分钟 随笔 人生 去年的年终总结还没有写，2020就来了，已经很久没有写博客， Flink批和流处理的思考 2019-10-16\u0026middot;3 分钟 后端 软件 随着2019年8月份，Flink1.9.0正式发布，Flin Apache Atlas 2.0.0部署实践 2019-10-12\u0026middot;6 分钟 后端 软件 Atlas 是一个可扩展和可扩展的核心基础治理服务集 - 使企业能够有效地 FlinkSQL Client实战 2019-10-12\u0026middot;2 分钟 后端 软件 Flink SQL Client 是一个帮助用户直接运行SQL，而不要进行编译运行的工具 如何真正精通 2019-10-05\u0026middot;8 分钟 随笔 人生 引言 # 写这篇博客是因为自己从去年就开始学习大数据，但是当自己 推荐系统微博数据实战 2019-05-05\u0026middot;10 分钟 AI 推荐系统 虽然自己刚解决掉一个TB级数据导入“大项目”，但是感觉自己对 TB级数据存贮经验总结 2019-04-13\u0026middot;8 分钟 后端 框架 大数据 项目背景 这个项目是深圳一家证卷公司一个TB级日志离线导入项目 重构-总结 2019-03-17 随笔 编程 阅读总结 代码整洁之道-总结 2019-03-13\u0026middot;1 分钟 随笔 编程 阅读总结 很早就听说过思维导图，但是怎么去画，怎么让它帮助你去学习，我 如何在Ubuntu上发布Scala Jar包到Maven中央仓库 2019-01-13\u0026middot;7 分钟 后端 框架 Java 由于网上的教程大多数是Windows下的，而且都是介绍怎么打 Big Data Questions 2018-11-07\u0026middot;1 分钟 后端 框架 大数据 Recently I found that I\u0026rsquo;m kind of over talking when I speak with others, so I want to train my conclusion of solving problem. Here are some problems I meet Learning Scala From Java 2018-10-20\u0026middot;3 分钟 后端 框架 Java Recently I had finshed reading one book : \u0026laquo; Scala for The Impatient\u0026raquo;, I found a lot intrest thing during reading this book.Cause I had known Python and Java before, Build Hadoop Cluster in One Computer 2018-10-14\u0026middot;4 分钟 后端 软件 If you are hadoop novice, I strongly suggest you beginning your study from single node building,you can learn from this website, after you having finshed build one single node, then 触摸Python的GIL 2018-09-15\u0026middot;18 分钟 后端 框架 Python Python的GIL一直是被大家攻击其语言的一个弊端，每次在 如何让你的Python更快 2018-09-12\u0026middot;11 分钟 后端 框架 Python 引言 # 其实一开始没有想到写关于Python的加速，一开始只想 千万级数据处理小结 2018-09-06\u0026middot;8 分钟 后端 框架 大数据 引言 # 这两个星期的工作主要是对千万文本数据的处理，由于我以前 毕业两月反思 2018-08-25\u0026middot;3 分钟 随笔 人生 毕业近两个月了，然而这两个月对自己的状态不太满意故写下这篇博 从关系角度来看XPath 2018-08-09\u0026middot;5 分钟 后端 框架 Python 这段时间没有写博客，一个原因是由于刚毕业没了学校的学习动力反 Java的char类型到底几个字节 2018-07-01\u0026middot;6 分钟 后端 框架 Java 引言 # 之所以有这个疑问，是上次阅读Java基础书时碰到讲解c 从例子里解Spring IOC 2018-05-28\u0026middot;7 分钟 后端 框架 Java SpringBoot Spring的IOC理解 # 什么是IOC # 在这里我们不谈Spr 从MNIST了解卷积神经网络 2018-04-24\u0026middot;12 分钟 AI 卷积神经网络 引言 # 本文是学习Tensorflow官方文档的过程中的一点感 Ubuntu16.04安装Tensorflow的CPU优化 2018-04-19\u0026middot;3 分钟 后端 软件 由于我的笔记本是农卡，没法安装CUDA加速，而且我的显卡只有 Python正则表达式 2018-03-17\u0026middot;5 分钟 后端 框架 Python 引言 # 这篇博客其实写于2016年，最近在重新学了一下正则表达 用户空间隔离 2018-02-20\u0026middot;10 分钟 随笔 编程 MIT6.828 通过前面的学习我们知道，在前两个实验中最主要的程序就是ker KERNBASE对操作系统的影响 2018-02-19\u0026middot;5 分钟 随笔 编程 MIT6.828 概括 # 这个问题主要在这本xv6-ref的第一章的练习题2中提 内存分页设计 2018-02-05\u0026middot;3 分钟 随笔 编程 MIT6.828 引言 # 前面已经通过lab1的这篇博文了解了内存分页的实现细节 内存分页 2018-01-31\u0026middot;2 分钟 随笔 编程 MIT6.828 引言 # 本来自己查了很多资料，想自己写出来，结果下笔的时候发现 引导和操作系统的交互 2018-01-31\u0026middot;3 分钟 随笔 编程 MIT6.828 引言 # 本来自己查了很多资料，想自己写出来，结果下笔的时候发现 从CS寄存器看段的前世今生 2018-01-30\u0026middot;2 分钟 随笔 编程 MIT6.828 引言 # Intel作为作为微处理器的航头老大，一直引导CPU的 什么是操作系统 2018-01-29\u0026middot;8 分钟 随笔 编程 MIT6.828 引言 # 本文是基于mit6.828 的lab1对操作系统的思考， mit6.828课程总结 2018-01-09\u0026middot;2 分钟 随笔 编程 MIT6.828 引言 # 一开始想直接做一个操作系统，但是万事开头难，学习操作系 新年展望 2018-01-06\u0026middot;3 分钟 随笔 人生 引言 # 最近逛知乎的时候看到一篇知乎回答很有趣，也给了很深的感 红黑树实现原理 2018-01-05\u0026middot;1 分钟 随笔 编程 红黑树 本篇博客主要基于这篇博客的扩展，建议阅读前先阅读这篇博文，这 AVL树实现原理 2018-01-04\u0026middot;2 分钟 随笔 编程 本篇博客主要基于这篇博客的扩展，建议阅读前先阅读这篇博文，这 二叉搜索树实现原理 2018-01-04\u0026middot;4 分钟 随笔 编程 本篇博客主要基于这篇博客的扩展，建议阅读前先阅读这篇博文，这 浅谈\"树\"这种数据结构 2017-12-27\u0026middot;10 分钟 随笔 编程 树 一直以来我对树这种数据结构就比较头疼，随便找一个红黑树的博客 几个有趣的动态规划 2017-12-26\u0026middot;6 分钟 随笔 编程 这篇博文是从问题理解动态规划的练习篇，通过几个动态规划的问题 从问题理解动态规划 2017-12-25\u0026middot;5 分钟 随笔 编程 网上关于动态规划的资料，大部分直接给结论，所以一开始我一头雾 从子网到“互联网” 2017-12-14\u0026middot;8 分钟 后端 网络 引言 # 最近重温《TCP/IP协议簇》，读到子网这个部分，概念 巧用Git钩子 2017-11-25\u0026middot;5 分钟 后端 软件 Git 以前听学长提过Git钩子，但是自己一直没有仔细了解过，记得我 FastProxyScan项目介绍 2017-11-23\u0026middot;3 分钟 OpenSource mrzhangboss/FastProxyScan fast proxy scan project Python 8 2 为了给我的站点增加人气，我把这个项目的介绍放 代理的前世今生 2017-11-20\u0026middot;5 分钟 后端 网络 引言 # 自己对代理认识不深，也只是会使用而已，由于最近想做一个 漫谈排序算法 2017-11-12\u0026middot;4 分钟 随笔 编程 0x00 引子 # 排序是很多算法的基础，简简单单的排序前人就归纳出很多 编程小结 2017-10-28\u0026middot;5 分钟 随笔 人生 引言 # 编程开发有时候也像雕刻一件艺术品 以前一直有一种错觉，觉 HTTP复用 2017-10-10\u0026middot;10 分钟 后端 网络 曾经有人问过我一个问题什么是TCP复用，我当时没有回答上来， Python异步的理解 2017-10-09\u0026middot;6 分钟 后端 框架 Python 起因 # 异步的出现主要是单线程的io等待，由于任务大部分是io PostgreSQL的自增键 2017-10-08\u0026middot;3 分钟 后端 软件 平常在Django项目中大量使用自增这个键，平常都是使用OR 大学的错觉 2017-10-07\u0026middot;4 分钟 随笔 人生 其实这篇文章很早就像写了，但是自己也一直没有明白自己想写什么 唯品会预测和携程房屋预测总结 2017-08-09\u0026middot;8 分钟 AI 比赛 最近打了两个比赛，一直忙着工作和打比赛，没有时间总结，今天抽 如何搭建自动生成上万特征的管道 2017-06-13\u0026middot;9 分钟 AI 比赛 基于:大航杯“智造扬中”电力AI大赛参赛经验 赛题背景 主办方为 如何成为一名数据挖掘机 2017-05-16\u0026middot;1 分钟 AI 感悟 自我介绍 前端 - \u0026gt; 后端 -\u0026gt; 数据挖掘机 ML DM AI 的区别 我的自学之旅 给新 爬虫分布式总结 2017-04-04\u0026middot;3 分钟 后端 框架 Python 由于最近在一家数据服务公司实习，项目需要了解分布式，所以在这 协程解决递归错误原理 2017-03-15\u0026middot;5 分钟 后端 框架 Python 引言 # 递归是高度抽象化问题的一个好东西，我们能从很多算法书里 天池大赛-口碑预测参赛感想 2017-02-17\u0026middot;2 分钟 AI 比赛 第一次听说这个比赛还是去年在知乎上，当时也不知道这个比赛具体 怎么成为数据科学家（翻译） 2016-12-23\u0026middot;2 分钟 随笔 编程 阅读总结 这是我从Quora上看到的一篇非常简短但详细的数据科学家的‘ 集体智慧编程之推荐系统(Programming Colletive Intelligence) 2016-09-17\u0026middot;2 分钟 AI 推荐系统 看过好几把关于机器学习的书,但是很多书只是停留于算法原理阶段 git 工作流程 2016-09-07\u0026middot;3 分钟 后端 软件 git是当今流行的版本控制工具,一般我们可能只要会push, 刷题笔记 2016-08-21\u0026middot;1 分钟 随笔 编程 在牛客网刷了16道题了,在这做个总结 概况 # 编程题无非两种一种 用例子学TDD 2016-08-21\u0026middot;4 分钟 随笔 编程 TDD 翻译自TDD-byexample 作者Kent Beck, Three Rivers Institute 有删减 表 hookman develpment blog 2016-08-18\u0026middot;3 分钟 OpenSource 其实写这个项目的初衷是想实践一下TDD开发,因为自己刚看完一 hookman development notebook 2016-08-18\u0026middot;1 分钟 OpenSource mrzhangboss/hookman Python 0 0 hookman 是基于github上的webhooks开发的一个用 TDD-隔离测试 2016-08-09\u0026middot;4 分钟 随笔 编程 TDD 隔离测试是相对与于整合测试来说的，现代软件架构流行分层式、模 TDD测试开发理解 2016-07-26\u0026middot;3 分钟 随笔 编程 TDD 看了 \u0026laquo; Python Web 开发 测试驱动方法\u0026raquo; 以后, 感觉自己找到自 js的this引发的思考 2016-07-20\u0026middot;2 分钟 随笔 编程 最近这几天在开发一个hmtl5的游戏, 但是对于js怎么使用面 关于技术栈的总结 2016-07-19\u0026middot;3 分钟 随笔 人生 这段时间重新学习了web前端的技术栈,其实一开始加入社团时就 \u003c\u003c机器学习实战\u003e\u003e心得. 2016-07-18\u0026middot;2 分钟 AI 心得 先谈谈我对机器学习的理解 # 什么是机器学习? # 我们人类有从婴儿 GitHub Education Pack 2016-07-18\u0026middot;3 分钟 后端 软件 GitHub推出一个对学生和教师的福利包,对于学生来说这是一 ","externalUrl":null,"permalink":"/latest/","section":"欢迎来到 Blowfish! 🎉","summary":"最近的文章 Hugo 使用教程 2024-07-15\u0026middot;3 分钟 后端 软件 背景 # 博客这次由Hexo 转","title":"最新文章","type":"page"}]